Gap Analysis of Natural Language Processing
Systems with respect to Linguistic Modality
Vishal Shukla
Abstract
Modality is one of the important components of grammar in linguistics It
lets speaker to express attitude towards or give assessment or potentiality
of state of aairs It implies dierent senses and thus has dierent percep-
tions as per the context This paper presents an account showing the gap
in the functionality of the current state of art Natural Language Processing
NLP systems The contextual nature of linguistic modality is studied In
this paper the works and logical approaches employed by Natural Language
Processing systems dealing with modality are reviewed It sees human cog-
nition and intelligence as multi-layered approach that can be implemented
by intelligent systems for learning Lastly current ow of research going on
within this eld is talked providing futurology
Keywords modality NLP gap shortcomings multi-layered learning
1 Introduction
Research in NLP is gaining interest as its applications are becoming more
signicant Natural language is highly ambiguous and understanding it ef-
fectively can be considered as a primary concern Modality in language is
associated with contextual understanding and implied perceptions Dening
modality from a computational linguistics perspective is somewhat dicult
because several concepts are used to refer to phenomena that are related to
modality depending on the task at hand and the specic singularities that
V Shukla
Systems Engineer
Infosys Bangalore India
Contact no 91-97394 99580
E-mail vishal shukla04infosyscom
the speaker addresses There are dierent senses that can be articulated by
modality Understanding precise sense conveyed is important
The major tasks involved in dealing with modality in text are detecting
the occurrence of modality categorizing the type of modality and perceiving
the sense conveyed through it Parsers and language processing tools iden-
ties the occurrence of modals by Part of Speech POS tagging Talking
about type of modality in the literature standard classication of types is
not available as various types are dened But we consider the classication
broadly as in two types epistemic and deontic According to Palmer 2014
epistemic is used by the speakers to express their judgment about the factual
status of the proposition Whereas deontic modality is concerned with the
speakers directive attitude towards an action to be carried out It relates
to obligation or permission and to conditional factors that are external to
the relevant individual The senses of necessity and possibility are incorpo-
rated by epistemic type while those of permission and obligation by deontic
Modality type categorization and the sense recognition has been carried out
by annotation approaches
This paper will closely observe the methods applied for the abovemen-
tioned tasks and also review the works in the subject eld
It will then
highlight shortcomings of the state of art natural language systems in con-
text to linguistic modality that forming the gap in their functionality Latter
sections will be covering the directions for the scope of improvement in the
same context
2 Context Review
As tagging and annotating are the two key aspects lets summarize them
regarding to the language processing The process of tagging involves assign-
ing tags to each word in the corpus corresponding to the part of speech that
it embodies The Part of Speech tagging is an essential subtask in language
processing and is very useful for subsequent phases like syntactic parsing
The tags into which a token can possibly classied depends on the tagset
adopted for this task That is it depends upon the Treebank taken into use
which denes the directory of the tags
Annotation on the other hand uses machine learning approach It uses
pre-annotated corpus to learn and annotate plain text The annotations
made are markups usually representing extended features The extended fea-
tures may include polarity certainty subjectivity sense and type of modality
event mentions etc POS tagging is also carried out with machine learn-
ing methods in a similar way as annotation Manning and Schutze 1999
Ratnaparkhi et al 1996 Toutanova and Manning 2000
21 Detecting occurrence
A number of dierent expressions in language can have modal meanings
Von Fintel 2006 discusses on a subset of variety of modal expressions Tak-
ing into account the tagging output of Stanford NLP parser of few sample
sentences with dierent expressions some points will be highlighted
In case of modal auxiliaries used the parser tags the same with MD ie
1 Sandy must be home
SandyNNP mustMD
homeNN
Thus any explicit occurrence of modal auxiliaries such as must shouldshall
might may couldcan will be detected well and clear
Whereas semi-modals like has to need to ought to follow dierent treat-
ment Sometimes ought to is considered as modal and sometimes as semi-
modal because of dierence in its syntax Anyhow it is tagged as a modal
Rest of the semi-modals are not tagged as modal but auxiliary verbs and
structure of sentence can be identied by parsers
2 Sandy has to be home
SandyNNP
hasVBZ
homeNN
3 Sandy ought to be home
SandyNNP
oughtMD
homeNN
Apart from modal auxiliaries and semi-modals modal meaning can also
be conveyed using adverbs perhaps probably etc nouns possibility ne-
cessity etc adjectives bound certain etc and also conditional constructs
if then
4 It is far from necessary that Sandy is home
SandyNNP
homeNN
fromIN
necessaryJJ
thatIN
5 There is a slight possibility that Sandy is home
ThereEX
SandyNNP
homeNN
slightJJ
possibilityNN
thatIN
6 Perhaps Sandy is home
PerhapsRB
SandyNNP
homeNN
7 If the light is on Sandy is home
IfIN theDT
homeNN
lightNN
SandyNNP
Although the advances in calibration of parsers has improved the ability
to tag words accurately but above certain point the mechanism seem to
become insucient to gather underlying information that is not supercial
or apparent
22 Type Categorization and Sense Perception
Categorization of type of modality in text and the identication of sense
conveyed can be done by developing annotation schemes There are works
that accommodate annotating of those features but not necessarily they are
organized with study of languages perspective which makes it dicult to
summarize and separate out the relevant points of interest Upon review-
ing the literature it can be seen that various annotating schemes has been
constructed over time for marking annotations of dierent components
Baker et al 2012 described a modalitynegation MN annotation scheme
which isolates three components of modality and negation a trigger that is
source of modality or negation a target action associated with modality or
negation and a holder the experiencer of modality Moreover they have
constructed MN lexicon and two automated MN taggers using the annotation
scheme
Ruppenhofer and Rehbein 2012 presents annotation scheme that anno-
tates type of English modals in MPQA corpus The modal verbs targeted
were cancould maymight must ought shallshould The annotation in-
volved categorization of the modals in to six types epistemic deontic dy-
namic optative concessive conditional
Pakray et al 2012 experimented on QA4MRE data sets to identify modal-
ity and negation in text and assign labels mod neg neg-mod none for oc-
currences of modal negation modality and negation and absence of both
modality and negation respectively And the detected modals were catego-
rized into epistemic and deontic
Hendrickx et al 2012 presents a scheme for annotation of modality in
Portuguese using MMAX2 tool The components annotated were trigger
the element conveying the modal value target the expression in the scope
of the trigger source of the event mention speaker or writer and source
of the modality agent or experiencer Also for trigger two attributes were
specied modal value and polarity They stated thirteen dierent types into
which the modal value could be categorized
Rubinstein et al 2013 proposed ne-grained annotation approach utiliz-
ing MPQA corpus and MMAX2 tool It was said to be ne-grained as it ex-
tends some of the previous works with a number of novel features to improve
detection and interpretation of modals in text The features annotated were
modality type polarity propositional arguments source background modi-
ed element degree indicator outscoping quantier and lemma The types
into which modality was categorized are ne grained types epistemic cir-
cumstantial ability deontic bouletic teleological and bouleticteleological
and the coarse grained types epistemic or circumstantial ability or circum-
stantial and priority
On surveying other studies carried out using annotations apparently it
seems that more and more attributes were annotated to make text under-
standing precise Certain works moved in the direction of subjectivity anal-
ysis some in certainty analysis whereas others focused on time events and
temporal analysis all of them being implicitly useful in the study of type
andor sense understanding of modality
Dierent types of subjectivity is implied in discourse by dierent types
epistemic and deontic modals The relationship between subjectivity and
modality is elaborately discussed in Sanders and Spooren 1997
Rubin et al
2004 2006 presented a certainty categorization model
based on four hypothesized dimensions and tested the model on a sample of
news articles Rubin 2010 identies that certainty can be seen as a variety
of epistemic modality expressed in form of markers like probably perhaps
undoubtedly etc
Nissim et al 2013 proposed modality annotation model that they say
to be two layered Factuality and speakers attitude being two components
marked they also plan to make the model more coherent by annotating
strength of modality
Matsuyoshi et al 2010 clearly draws attention to the point that recent
developments in language processing has improved precision but is insuf-
cient for applications such as information extraction question answering
and recognizing textual entailment Such applications require more informa-
tion such as modality polarity and other associated information collectively
referred as extended modality Matsuyoshi proposes an annotation scheme
that represents extended modality and consists of seven components source
time conditional primary modality type actuality evaluation and focus
Utilizing the work they also constructed an annotated corpus in Japanese
Emphasizing upon event modality Saur et al 2006a says that modality
is an important component of discourse together with other levels of infor-
mation such as argument structure and temporal information That made
an apparent need for a more sophisticated approach that is sensitive to such
additional information They worked on annotation scheme that annotates
event modality and also identifying its scope using TimeML Saur 2006b
has also worked on SlinkET attempting the construction of modal parser for
events Pustejovsky et al 2003 built the TimeBank corpus which is an-
notated with information like modals events times relation between events
and temporal expressions Saur and Pustejovsky 2009 built the FactBank
on the basis of TimeBank where events are assigned with dierent degrees of
factuality according to their source-introducing predicates SIPS and source
Dierent degrees of factuality are determined by dierent degrees of certainty
and polarity axes Degrees of certainty include certain probable possible and
unknown Whereas polarity axis contained positive negative and underspec-
ied
3 Gap and Futurology
Linguistic modality is one of the components of discourse that is asso-
ciated with context sense and meaning mental and real spaces and force
dynamics
Also there is no proper well dened classication of dierent types and
senses of modality in linguistic literature This lack of taxonomy has con-
trived enormous confusions regarding types and senses Considering the fact
that the section is complex and modality in discourse has many aspects as-
sociated with it and its wider scope even language scientists can contribute
towards it
First point noted is that dierent kinds of expressions can be used to
convey modal meanings And we saw that the tagging approach is limited
to tag modals explicit use in text Moreover taggers dont put any further
light on the type and sense of the modality Though attempts have made on
type and sense classication using annotation methods but due to exibility
of meanings it is dicult to standardize Flexibility of meaning means the
modal verb has dierent meaning according to context Modality in language
has contextual meanings and implied perceptions And mechanisms fail to
identify perspective aspect and contextuality
Another drawback is dependency in both methods tagging as well as
annotating The process of tagging is dependent on the tagset and the an-
notation scheme is limited by its own training corpus Although this lexicon
dependency of the available approaches are useful for preliminary passes of
processing but not an eective way for understanding natural language which
includes cognition and perception
At this point inspiring from human information processing mechanisms
would be seen appropriate In this context understanding of human cognitive
process can denitely enlighten the path of development to make our systems
articially intelligent Computational models of cognition creative insight
skill acquisition and the design of instructional software as well as other top-
ics in higher cognition needs to be reconsidered It is noteworthy to see the
humans perceptive systems visual or speechaudio etc both are essentially
layered and hierarchical in structure Thus it is natural to believe that the
state-of-the-art can be advanced in processing these types of natural lan-
guage if structurally ecient and eective learning model can be developed
The layered structure of human learning shown in Kaplan  Sadocks Com-
prehensive Textbook of Psychiatry Sadock 2000 g 24-1 is instrumental
in visualization of complexity and multi-stage structure involved Moreover
the interconnection and synergy between the layers is equally vital
Real systems are dynamic in nature and reform continuous shift that
produce perceptual dierence If the shift is in upward direction in hierar-
chical multi-layered model that results in high dimensionality in nature of
expression Thus the expression becomes nonspecic and losses subjectiv-
ity Nonspecic discourse are too complex to interpret and to reach to any
conclusion is very dicult From viewing the software dealing with language
processing looks in direction and trends accordance to the subject Articial
Intelligence AI and Robotics that attempts to mimic the dynamic and be-
havioral output from human In context of natural language processing with
special reference to modality processing noticeable development observed in
static format of expressions and expression of contextuality also attempted
within one document as in MMAX2 from German NLP group
Neural network hidden layer processing and continuous modication were
successfully executed in mechanical output in eld of machine learning The
directions were explored with annotated titles like deep learning and lay-
ered approach handling are most popular among research works in language
processing
As per the objectives of a system some key features that should be consid-
ered such as relevant well-structured knowledge base with improved feature
space should be formed upon each processing And this knowledge base must
be dynamically updated active learning that means continuous updatation
of the knowledge base by each layer of the model so that it can eectively
be useful in applications of the system
Machine learning has been a dominant tool in NLP for many years How-
ever the use of traditional machine learning in NLP has been mostly lim-
ited to numerical optimization of weights for human designed representations
and features from the text data The goal of representation learning is to
automatically develop features or representations from the raw text material
appropriate for a wide range of NLP tasks
Deep learning is gaining popularity very recently as it provides levels of
abstraction The multi-layered architecture formed due to the levels ensure
natural progression from low level to high level structure as seen in natural
complexity Deep learning works on the principle of formation of learning
representations The essence of deep learning is to automate the process
of discovering eective features or representations for any machine learning
task including automatically transferring knowledge from one task to an-
other concurrently In regard to NLP deep learning develops and makes use
an important concept called embedding which refers to the representation
of symbolic information in natural language text at word-level phrase-level
and even sentence-level in terms of continuous-valued vectors
Another concept of multi-task learning has also shown improvements in
learning approaches Multi-task learning is a machine learning approach that
learns to solve several related problems at the same time using a shared
representation It can be regarded as one of the two major classes of transfer
learning or learning with knowledge transfer which focuses on generalizations
across distributions domains or tasks The other major class of transfer
learning is adaptive learning where knowledge transfer is carried out in a
sequential manner typically from a source task to a target task
31 Recent works with deep architectures in NLP
Variety of deep architectures like neural networks deep belief networks
and others has shown signicant performance in various applications of lan-
guage processing including other elds
Collobert et al 2011 provide a comprehensive review on ways of apply-
ing unied neural network architectures and related deep learning algorithms
to solve NLP problems from scratch meaning that no traditional NLP meth-
ods are used to extract features The recent work by Mikolov et al 2013a
derives word embeddings by simplifying the Neural Network Language Model
NNLM It is found that the NNLM can be successfully trained in two
steps Yet another deep learning approach to machine translation appeared
in Mikolov et al 2013b
One most interesting NLP task recently tackled by deep learning meth-
ods is that of knowledge base ontology completion which is instrumental in
question-answering and many other NLP applications An early work in this
space came from Bordes et al 2011 where a process is introduced to auto-
matically learn structured distributed embeddings of knowledge bases The
proposed representations in the continuous-valued vector space are compact
and can be eciently learned from large-scale data of entities and relations
A specialized neural network architecture is used In the follow-up work that
focuses on multi-relational data Bordes et al 2014 the semantic matching
energy model is proposed to learn vector representations for both entities
and relations
Other recent works Socher et al 2013 and Bowman 2013 adopts an
approach based on the use of neural tensor networks to attack the problem
of reasoning over a large joint knowledge graph for relation classication
The knowledge graph is represented as triples of a relation between two en-
tities and the authors aim to develop a neural network model suitable for
inference over such relationships The model they presented is a neural ten-
sor network with one layer only but it would be encouraged to work further
on multi-layered network models The network is used to represent entities
in xed-dimensional vectors which are created separately by averaging pre-
trained word embedding vectors It then learns the tensor with the newly
added relationship element that describes the interactions among all the la-
tent components in each of the relationships Experimentally Socher et al
shows that this tensor model can eectively classify unseen relationships in
WordNet and FreeBase Thus models built on tensors can contribute upto
certain extent for reasoning over relationships between entities enhancing
knowledge bases for improved performance Works utilizing Recursive Neu-
ral Networks RNN for syntactic parsing and word representations has been
performed in Luong et al 2013 Socher et al 2010 Deep neural networks
have been popular and are well performing as they are intrinsically multi-
layered in structure
Deep learning is a hot area of research and there is still much potential
for signicant advances It can be said that the paradigm of deep learning
architectures can improve the results of our models upto quite a certain ex-
tent but there is still a limit to it considering the whole problem statement
This is because the deep neural networks are yet a kind of black box model
in terms of functionality By revising models and designs enhancement in
performance of deep learning algorithms can surely be made as per specic
application domain but there would be a bound to the possible improvements
and the available approaches wouldnt provide enough means to the desired
level of Articial Intelligence Approaches that are multi-layered and prefer-
ably white box models would be essentially important for organization and
control of intermediate layers functionalities
As mentioned above not only for application in NLP but for any ap-
plication that deals with dynamics of real world conditions development
of complex system using combination of several simple modular and multi-
layered hierarchal architectures with the key features will be helpful And
hopefully such models can attain better linguistic understanding accuracy
that would be contributory to the application expertise
References
Baker K Bloodgood M Dorr BJ Callison-Burch C Filardo NW Piatko C
Levin L Miller S 2012 Modality and negation in simt use of modality and
negation in semantically-informed syntactic mt Computational Linguistics
382411438
Bordes A Weston J Collobert R Bengio Y et al 2011 Learning structured
embeddings of knowledge bases In AAAI
Bordes A Glorot X Weston J Bengio Y 2014 A semantic matching en-
ergy function for learning with multi-relational data Machine Learning
942233259
Bowman SR 2013 Can recursive neural tensor networks learn logical rea-
soning arXiv preprint arXiv13126192
Collobert R Weston J Bottou L Karlen M Kavukcuoglu K Kuksa P 2011
Natural language processing almost from scratch The Journal of Ma-
chine Learning Research 1224932537
Hendrickx I Mendes A Mencarelli S 2012 Modality in text a proposal for
corpus annotation In LREC pp 18051812
Luong MT Socher R Manning C 2013 Better word representations with
recursive neural networks for morphology CoNLL-2013 104
Manning CD Schutze H 1999 Foundations of statistical natural language
processing MIT press
Matsuyoshi S Eguchi M Sao C Murakami K Inui K Matsumoto Y 2010
Annotating event mentions in text with modality focus and source infor-
mation In LREC
Mikolov T Chen K Corrado G Dean J 2013a Ecient estimation of word
representations in vector space arXiv preprint arXiv13013781
Mikolov T Le QV Sutskever I 2013b Exploiting similarities among lan-
guages for machine translation arXiv preprint arXiv13094168
Nissim M Pietrandrea P Sanso A Mauri C 2013 Cross-linguistic anno-
tation of modality a data-driven hierarchical model In Workshop on
Interoperable Semantic Annotation p 7
Pakray P Bhaskar P Banerjee S Bandyopadhyay S Gelbukh AF 2012 An
automatic system for modality and negation detection In CLEF Online
Working NotesLabsWorkshop Citeseer
Palmer FR 2014 Modality and the English modals Routledge
Pustejovsky J Hanks P Sauri R See A Gaizauskas R Setzer A Radev
D Sundheim B Day D Ferro L et al 2003 The timebank corpus In
Corpus linguistics vol 2003 p 40
Ratnaparkhi A et al 1996 A maximum entropy model for part-of-speech
tagging In Proceedings of the conference on empirical methods in natural
language processing Philadelphia PA vol 1 pp 133142
Rubin VL 2010 Epistemic modality From uncertainty to certainty in the
context of information seeking as interactions with texts Information Pro-
cessing  Management 465533540
Rubin VL Kando N Liddy ED 2004 Certainty categorization model In
AAAI spring symposium Exploring attitude and aect in text Theories
and applications Stanford CA
Rubin VL Liddy ED Kando N 2006 Certainty identication in texts Cat-
egorization model and manual tagging results In Computing attitude and
aect in text Theory and applications Springer pp 6176
Rubinstein A Harner H Krawczyk E Simonson D Katz G Portner P 2013
Toward ne-grained annotation of modality in text In Proceedings of the
Tenth International Conference for Computational Semantics IWCS 2013
Ruppenhofer J Rehbein I 2012 Yes we can annotating the senses of
english modal verbs In Proceedings of the 8th International Conference
on Language Resources and Evaluation LREC pp 2426
Sadock BJ 2000 Kaplan  Sadocks Comprehensive Textbook of Psychiatry
2 Volume Set Lippincott Williams  Wilkins
Sanders J Spooren W 1997 Perspective sulijectivity and modality from
a cognitive inguis ic point of view Discourse and perspective in cognitive
linguistics 15185
Saur R Pustejovsky J 2009 Factbank A corpus annotated with event
factuality Language resources and evaluation 433227268
Saur R Verhagen M Pustejovsky J 2006a Annotating and recognizing
event modality in text In Proceedings of 19th International FLAIRS
Conference
Saur R Verhagen M Pustejovsky J 2006b Slinket a partial modal parser
for events In In Language Resources and Evaluation Conference LREC
2006 Citeseer
Socher R Manning CD Ng AY 2010 Learning continuous phrase represen-
tations and syntactic parsing with recursive neural networks In Proceed-
ings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning
Workshop pp 19
Socher R Chen D Manning CD Ng A 2013 Reasoning with neural tensor
networks for knowledge base completion In Advances in Neural Informa-
tion Processing Systems pp 926934
Toutanova K Manning CD 2000 Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger In Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in natural language processing
and very large corpora held in conjunction with the 38th Annual Meeting
of the Association for Computational Linguistics-Volume 13 Association
for Computational Linguistics pp 6370
Von Fintel K 2006 Modality and language
Gap Analysis of Natural Language Processing
Systems with respect to Linguistic Modality
Vishal Shukla
Abstract
Modality is one of the important components of grammar in linguistics It
lets speaker to express attitude towards or give assessment or potentiality
of state of aairs It implies dierent senses and thus has dierent percep-
tions as per the context This paper presents an account showing the gap
in the functionality of the current state of art Natural Language Processing
NLP systems The contextual nature of linguistic modality is studied In
this paper the works and logical approaches employed by Natural Language
Processing systems dealing with modality are reviewed It sees human cog-
nition and intelligence as multi-layered approach that can be implemented
by intelligent systems for learning Lastly current ow of research going on
within this eld is talked providing futurology
Keywords modality NLP gap shortcomings multi-layered learning
1 Introduction
Research in NLP is gaining interest as its applications are becoming more
signicant Natural language is highly ambiguous and understanding it ef-
fectively can be considered as a primary concern Modality in language is
associated with contextual understanding and implied perceptions Dening
modality from a computational linguistics perspective is somewhat dicult
because several concepts are used to refer to phenomena that are related to
modality depending on the task at hand and the specic singularities that
V Shukla
Systems Engineer
Infosys Bangalore India
Contact no 91-97394 99580
E-mail vishal shukla04infosyscom
the speaker addresses There are dierent senses that can be articulated by
modality Understanding precise sense conveyed is important
The major tasks involved in dealing with modality in text are detecting
the occurrence of modality categorizing the type of modality and perceiving
the sense conveyed through it Parsers and language processing tools iden-
ties the occurrence of modals by Part of Speech POS tagging Talking
about type of modality in the literature standard classication of types is
not available as various types are dened But we consider the classication
broadly as in two types epistemic and deontic According to Palmer 2014
epistemic is used by the speakers to express their judgment about the factual
status of the proposition Whereas deontic modality is concerned with the
speakers directive attitude towards an action to be carried out It relates
to obligation or permission and to conditional factors that are external to
the relevant individual The senses of necessity and possibility are incorpo-
rated by epistemic type while those of permission and obligation by deontic
Modality type categorization and the sense recognition has been carried out
by annotation approaches
This paper will closely observe the methods applied for the abovemen-
tioned tasks and also review the works in the subject eld
It will then
highlight shortcomings of the state of art natural language systems in con-
text to linguistic modality that forming the gap in their functionality Latter
sections will be covering the directions for the scope of improvement in the
same context
2 Context Review
As tagging and annotating are the two key aspects lets summarize them
regarding to the language processing The process of tagging involves assign-
ing tags to each word in the corpus corresponding to the part of speech that
it embodies The Part of Speech tagging is an essential subtask in language
processing and is very useful for subsequent phases like syntactic parsing
The tags into which a token can possibly classied depends on the tagset
adopted for this task That is it depends upon the Treebank taken into use
which denes the directory of the tags
Annotation on the other hand uses machine learning approach It uses
pre-annotated corpus to learn and annotate plain text The annotations
made are markups usually representing extended features The extended fea-
tures may include polarity certainty subjectivity sense and type of modality
event mentions etc POS tagging is also carried out with machine learn-
ing methods in a similar way as annotation Manning and Schutze 1999
Ratnaparkhi et al 1996 Toutanova and Manning 2000
21 Detecting occurrence
A number of dierent expressions in language can have modal meanings
Von Fintel 2006 discusses on a subset of variety of modal expressions Tak-
ing into account the tagging output of Stanford NLP parser of few sample
sentences with dierent expressions some points will be highlighted
In case of modal auxiliaries used the parser tags the same with MD ie
1 Sandy must be home
SandyNNP mustMD
homeNN
Thus any explicit occurrence of modal auxiliaries such as must shouldshall
might may couldcan will be detected well and clear
Whereas semi-modals like has to need to ought to follow dierent treat-
ment Sometimes ought to is considered as modal and sometimes as semi-
modal because of dierence in its syntax Anyhow it is tagged as a modal
Rest of the semi-modals are not tagged as modal but auxiliary verbs and
structure of sentence can be identied by parsers
2 Sandy has to be home
SandyNNP
hasVBZ
homeNN
3 Sandy ought to be home
SandyNNP
oughtMD
homeNN
Apart from modal auxiliaries and semi-modals modal meaning can also
be conveyed using adverbs perhaps probably etc nouns possibility ne-
cessity etc adjectives bound certain etc and also conditional constructs
if then
4 It is far from necessary that Sandy is home
SandyNNP
homeNN
fromIN
necessaryJJ
thatIN
5 There is a slight possibility that Sandy is home
ThereEX
SandyNNP
homeNN
slightJJ
possibilityNN
thatIN
6 Perhaps Sandy is home
PerhapsRB
SandyNNP
homeNN
7 If the light is on Sandy is home
IfIN theDT
homeNN
lightNN
SandyNNP
Although the advances in calibration of parsers has improved the ability
to tag words accurately but above certain point the mechanism seem to
become insucient to gather underlying information that is not supercial
or apparent
22 Type Categorization and Sense Perception
Categorization of type of modality in text and the identication of sense
conveyed can be done by developing annotation schemes There are works
that accommodate annotating of those features but not necessarily they are
organized with study of languages perspective which makes it dicult to
summarize and separate out the relevant points of interest Upon review-
ing the literature it can be seen that various annotating schemes has been
constructed over time for marking annotations of dierent components
Baker et al 2012 described a modalitynegation MN annotation scheme
which isolates three components of modality and negation a trigger that is
source of modality or negation a target action associated with modality or
negation and a holder the experiencer of modality Moreover they have
constructed MN lexicon and two automated MN taggers using the annotation
scheme
Ruppenhofer and Rehbein 2012 presents annotation scheme that anno-
tates type of English modals in MPQA corpus The modal verbs targeted
were cancould maymight must ought shallshould The annotation in-
volved categorization of the modals in to six types epistemic deontic dy-
namic optative concessive conditional
Pakray et al 2012 experimented on QA4MRE data sets to identify modal-
ity and negation in text and assign labels mod neg neg-mod none for oc-
currences of modal negation modality and negation and absence of both
modality and negation respectively And the detected modals were catego-
rized into epistemic and deontic
Hendrickx et al 2012 presents a scheme for annotation of modality in
Portuguese using MMAX2 tool The components annotated were trigger
the element conveying the modal value target the expression in the scope
of the trigger source of the event mention speaker or writer and source
of the modality agent or experiencer Also for trigger two attributes were
specied modal value and polarity They stated thirteen dierent types into
which the modal value could be categorized
Rubinstein et al 2013 proposed ne-grained annotation approach utiliz-
ing MPQA corpus and MMAX2 tool It was said to be ne-grained as it ex-
tends some of the previous works with a number of novel features to improve
detection and interpretation of modals in text The features annotated were
modality type polarity propositional arguments source background modi-
ed element degree indicator outscoping quantier and lemma The types
into which modality was categorized are ne grained types epistemic cir-
cumstantial ability deontic bouletic teleological and bouleticteleological
and the coarse grained types epistemic or circumstantial ability or circum-
stantial and priority
On surveying other studies carried out using annotations apparently it
seems that more and more attributes were annotated to make text under-
standing precise Certain works moved in the direction of subjectivity anal-
ysis some in certainty analysis whereas others focused on time events and
temporal analysis all of them being implicitly useful in the study of type
andor sense understanding of modality
Dierent types of subjectivity is implied in discourse by dierent types
epistemic and deontic modals The relationship between subjectivity and
modality is elaborately discussed in Sanders and Spooren 1997
Rubin et al
2004 2006 presented a certainty categorization model
based on four hypothesized dimensions and tested the model on a sample of
news articles Rubin 2010 identies that certainty can be seen as a variety
of epistemic modality expressed in form of markers like probably perhaps
undoubtedly etc
Nissim et al 2013 proposed modality annotation model that they say
to be two layered Factuality and speakers attitude being two components
marked they also plan to make the model more coherent by annotating
strength of modality
Matsuyoshi et al 2010 clearly draws attention to the point that recent
developments in language processing has improved precision but is insuf-
cient for applications such as information extraction question answering
and recognizing textual entailment Such applications require more informa-
tion such as modality polarity and other associated information collectively
referred as extended modality Matsuyoshi proposes an annotation scheme
that represents extended modality and consists of seven components source
time conditional primary modality type actuality evaluation and focus
Utilizing the work they also constructed an annotated corpus in Japanese
Emphasizing upon event modality Saur et al 2006a says that modality
is an important component of discourse together with other levels of infor-
mation such as argument structure and temporal information That made
an apparent need for a more sophisticated approach that is sensitive to such
additional information They worked on annotation scheme that annotates
event modality and also identifying its scope using TimeML Saur 2006b
has also worked on SlinkET attempting the construction of modal parser for
events Pustejovsky et al 2003 built the TimeBank corpus which is an-
notated with information like modals events times relation between events
and temporal expressions Saur and Pustejovsky 2009 built the FactBank
on the basis of TimeBank where events are assigned with dierent degrees of
factuality according to their source-introducing predicates SIPS and source
Dierent degrees of factuality are determined by dierent degrees of certainty
and polarity axes Degrees of certainty include certain probable possible and
unknown Whereas polarity axis contained positive negative and underspec-
ied
3 Gap and Futurology
Linguistic modality is one of the components of discourse that is asso-
ciated with context sense and meaning mental and real spaces and force
dynamics
Also there is no proper well dened classication of dierent types and
senses of modality in linguistic literature This lack of taxonomy has con-
trived enormous confusions regarding types and senses Considering the fact
that the section is complex and modality in discourse has many aspects as-
sociated with it and its wider scope even language scientists can contribute
towards it
First point noted is that dierent kinds of expressions can be used to
convey modal meanings And we saw that the tagging approach is limited
to tag modals explicit use in text Moreover taggers dont put any further
light on the type and sense of the modality Though attempts have made on
type and sense classication using annotation methods but due to exibility
of meanings it is dicult to standardize Flexibility of meaning means the
modal verb has dierent meaning according to context Modality in language
has contextual meanings and implied perceptions And mechanisms fail to
identify perspective aspect and contextuality
Another drawback is dependency in both methods tagging as well as
annotating The process of tagging is dependent on the tagset and the an-
notation scheme is limited by its own training corpus Although this lexicon
dependency of the available approaches are useful for preliminary passes of
processing but not an eective way for understanding natural language which
includes cognition and perception
At this point inspiring from human information processing mechanisms
would be seen appropriate In this context understanding of human cognitive
process can denitely enlighten the path of development to make our systems
articially intelligent Computational models of cognition creative insight
skill acquisition and the design of instructional software as well as other top-
ics in higher cognition needs to be reconsidered It is noteworthy to see the
humans perceptive systems visual or speechaudio etc both are essentially
layered and hierarchical in structure Thus it is natural to believe that the
state-of-the-art can be advanced in processing these types of natural lan-
guage if structurally ecient and eective learning model can be developed
The layered structure of human learning shown in Kaplan  Sadocks Com-
prehensive Textbook of Psychiatry Sadock 2000 g 24-1 is instrumental
in visualization of complexity and multi-stage structure involved Moreover
the interconnection and synergy between the layers is equally vital
Real systems are dynamic in nature and reform continuous shift that
produce perceptual dierence If the shift is in upward direction in hierar-
chical multi-layered model that results in high dimensionality in nature of
expression Thus the expression becomes nonspecic and losses subjectiv-
ity Nonspecic discourse are too complex to interpret and to reach to any
conclusion is very dicult From viewing the software dealing with language
processing looks in direction and trends accordance to the subject Articial
Intelligence AI and Robotics that attempts to mimic the dynamic and be-
havioral output from human In context of natural language processing with
special reference to modality processing noticeable development observed in
static format of expressions and expression of contextuality also attempted
within one document as in MMAX2 from German NLP group
Neural network hidden layer processing and continuous modication were
successfully executed in mechanical output in eld of machine learning The
directions were explored with annotated titles like deep learning and lay-
ered approach handling are most popular among research works in language
processing
As per the objectives of a system some key features that should be consid-
ered such as relevant well-structured knowledge base with improved feature
space should be formed upon each processing And this knowledge base must
be dynamically updated active learning that means continuous updatation
of the knowledge base by each layer of the model so that it can eectively
be useful in applications of the system
Machine learning has been a dominant tool in NLP for many years How-
ever the use of traditional machine learning in NLP has been mostly lim-
ited to numerical optimization of weights for human designed representations
and features from the text data The goal of representation learning is to
automatically develop features or representations from the raw text material
appropriate for a wide range of NLP tasks
Deep learning is gaining popularity very recently as it provides levels of
abstraction The multi-layered architecture formed due to the levels ensure
natural progression from low level to high level structure as seen in natural
complexity Deep learning works on the principle of formation of learning
representations The essence of deep learning is to automate the process
of discovering eective features or representations for any machine learning
task including automatically transferring knowledge from one task to an-
other concurrently In regard to NLP deep learning develops and makes use
an important concept called embedding which refers to the representation
of symbolic information in natural language text at word-level phrase-level
and even sentence-level in terms of continuous-valued vectors
Another concept of multi-task learning has also shown improvements in
learning approaches Multi-task learning is a machine learning approach that
learns to solve several related problems at the same time using a shared
representation It can be regarded as one of the two major classes of transfer
learning or learning with knowledge transfer which focuses on generalizations
across distributions domains or tasks The other major class of transfer
learning is adaptive learning where knowledge transfer is carried out in a
sequential manner typically from a source task to a target task
31 Recent works with deep architectures in NLP
Variety of deep architectures like neural networks deep belief networks
and others has shown signicant performance in various applications of lan-
guage processing including other elds
Collobert et al 2011 provide a comprehensive review on ways of apply-
ing unied neural network architectures and related deep learning algorithms
to solve NLP problems from scratch meaning that no traditional NLP meth-
ods are used to extract features The recent work by Mikolov et al 2013a
derives word embeddings by simplifying the Neural Network Language Model
NNLM It is found that the NNLM can be successfully trained in two
steps Yet another deep learning approach to machine translation appeared
in Mikolov et al 2013b
One most interesting NLP task recently tackled by deep learning meth-
ods is that of knowledge base ontology completion which is instrumental in
question-answering and many other NLP applications An early work in this
space came from Bordes et al 2011 where a process is introduced to auto-
matically learn structured distributed embeddings of knowledge bases The
proposed representations in the continuous-valued vector space are compact
and can be eciently learned from large-scale data of entities and relations
A specialized neural network architecture is used In the follow-up work that
focuses on multi-relational data Bordes et al 2014 the semantic matching
energy model is proposed to learn vector representations for both entities
and relations
Other recent works Socher et al 2013 and Bowman 2013 adopts an
approach based on the use of neural tensor networks to attack the problem
of reasoning over a large joint knowledge graph for relation classication
The knowledge graph is represented as triples of a relation between two en-
tities and the authors aim to develop a neural network model suitable for
inference over such relationships The model they presented is a neural ten-
sor network with one layer only but it would be encouraged to work further
on multi-layered network models The network is used to represent entities
in xed-dimensional vectors which are created separately by averaging pre-
trained word embedding vectors It then learns the tensor with the newly
added relationship element that describes the interactions among all the la-
tent components in each of the relationships Experimentally Socher et al
shows that this tensor model can eectively classify unseen relationships in
WordNet and FreeBase Thus models built on tensors can contribute upto
certain extent for reasoning over relationships between entities enhancing
knowledge bases for improved performance Works utilizing Recursive Neu-
ral Networks RNN for syntactic parsing and word representations has been
performed in Luong et al 2013 Socher et al 2010 Deep neural networks
have been popular and are well performing as they are intrinsically multi-
layered in structure
Deep learning is a hot area of research and there is still much potential
for signicant advances It can be said that the paradigm of deep learning
architectures can improve the results of our models upto quite a certain ex-
tent but there is still a limit to it considering the whole problem statement
This is because the deep neural networks are yet a kind of black box model
in terms of functionality By revising models and designs enhancement in
performance of deep learning algorithms can surely be made as per specic
application domain but there would be a bound to the possible improvements
and the available approaches wouldnt provide enough means to the desired
level of Articial Intelligence Approaches that are multi-layered and prefer-
ably white box models would be essentially important for organization and
control of intermediate layers functionalities
As mentioned above not only for application in NLP but for any ap-
plication that deals with dynamics of real world conditions development
of complex system using combination of several simple modular and multi-
layered hierarchal architectures with the key features will be helpful And
hopefully such models can attain better linguistic understanding accuracy
that would be contributory to the application expertise
References
Baker K Bloodgood M Dorr BJ Callison-Burch C Filardo NW Piatko C
Levin L Miller S 2012 Modality and negation in simt use of modality and
negation in semantically-informed syntactic mt Computational Linguistics
382411438
Bordes A Weston J Collobert R Bengio Y et al 2011 Learning structured
embeddings of knowledge bases In AAAI
Bordes A Glorot X Weston J Bengio Y 2014 A semantic matching en-
ergy function for learning with multi-relational data Machine Learning
942233259
Bowman SR 2013 Can recursive neural tensor networks learn logical rea-
soning arXiv preprint arXiv13126192
Collobert R Weston J Bottou L Karlen M Kavukcuoglu K Kuksa P 2011
Natural language processing almost from scratch The Journal of Ma-
chine Learning Research 1224932537
Hendrickx I Mendes A Mencarelli S 2012 Modality in text a proposal for
corpus annotation In LREC pp 18051812
Luong MT Socher R Manning C 2013 Better word representations with
recursive neural networks for morphology CoNLL-2013 104
Manning CD Schutze H 1999 Foundations of statistical natural language
processing MIT press
Matsuyoshi S Eguchi M Sao C Murakami K Inui K Matsumoto Y 2010
Annotating event mentions in text with modality focus and source infor-
mation In LREC
Mikolov T Chen K Corrado G Dean J 2013a Ecient estimation of word
representations in vector space arXiv preprint arXiv13013781
Mikolov T Le QV Sutskever I 2013b Exploiting similarities among lan-
guages for machine translation arXiv preprint arXiv13094168
Nissim M Pietrandrea P Sanso A Mauri C 2013 Cross-linguistic anno-
tation of modality a data-driven hierarchical model In Workshop on
Interoperable Semantic Annotation p 7
Pakray P Bhaskar P Banerjee S Bandyopadhyay S Gelbukh AF 2012 An
automatic system for modality and negation detection In CLEF Online
Working NotesLabsWorkshop Citeseer
Palmer FR 2014 Modality and the English modals Routledge
Pustejovsky J Hanks P Sauri R See A Gaizauskas R Setzer A Radev
D Sundheim B Day D Ferro L et al 2003 The timebank corpus In
Corpus linguistics vol 2003 p 40
Ratnaparkhi A et al 1996 A maximum entropy model for part-of-speech
tagging In Proceedings of the conference on empirical methods in natural
language processing Philadelphia PA vol 1 pp 133142
Rubin VL 2010 Epistemic modality From uncertainty to certainty in the
context of information seeking as interactions with texts Information Pro-
cessing  Management 465533540
Rubin VL Kando N Liddy ED 2004 Certainty categorization model In
AAAI spring symposium Exploring attitude and aect in text Theories
and applications Stanford CA
Rubin VL Liddy ED Kando N 2006 Certainty identication in texts Cat-
egorization model and manual tagging results In Computing attitude and
aect in text Theory and applications Springer pp 6176
Rubinstein A Harner H Krawczyk E Simonson D Katz G Portner P 2013
Toward ne-grained annotation of modality in text In Proceedings of the
Tenth International Conference for Computational Semantics IWCS 2013
Ruppenhofer J Rehbein I 2012 Yes we can annotating the senses of
english modal verbs In Proceedings of the 8th International Conference
on Language Resources and Evaluation LREC pp 2426
Sadock BJ 2000 Kaplan  Sadocks Comprehensive Textbook of Psychiatry
2 Volume Set Lippincott Williams  Wilkins
Sanders J Spooren W 1997 Perspective sulijectivity and modality from
a cognitive inguis ic point of view Discourse and perspective in cognitive
linguistics 15185
Saur R Pustejovsky J 2009 Factbank A corpus annotated with event
factuality Language resources and evaluation 433227268
Saur R Verhagen M Pustejovsky J 2006a Annotating and recognizing
event modality in text In Proceedings of 19th International FLAIRS
Conference
Saur R Verhagen M Pustejovsky J 2006b Slinket a partial modal parser
for events In In Language Resources and Evaluation Conference LREC
2006 Citeseer
Socher R Manning CD Ng AY 2010 Learning continuous phrase represen-
tations and syntactic parsing with recursive neural networks In Proceed-
ings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning
Workshop pp 19
Socher R Chen D Manning CD Ng A 2013 Reasoning with neural tensor
networks for knowledge base completion In Advances in Neural Informa-
tion Processing Systems pp 926934
Toutanova K Manning CD 2000 Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger In Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in natural language processing
and very large corpora held in conjunction with the 38th Annual Meeting
of the Association for Computational Linguistics-Volume 13 Association
for Computational Linguistics pp 6370
Von Fintel K 2006 Modality and language
