Natural Language Understanding with
Distributed Representation
Kyunghyun Cho
Courant Institute of Mathematical Sciences and
Center for Data Science
New York University
November 26 2015
Abstract
This is a lecture note for the course DS-GA 3001 cid104Natural Language Understanding
with Distributed Representationcid105 at the Center for Data Science1 New York University
in Fall 2015 As the name of the course suggests this lecture note introduces readers
to a neural network based approach to natural language understandingprocessing In
order to make it as self-contained as possible I spend much time on describing basics of
machine learning and neural networks only after which how they are used for natural
languages is introduced On the language front I almost solely focus on language
modelling and machine translation two of which I personally nd most fascinating
and most fundamental to natural language understanding
After about a month of lectures and about 40 pages of writing this lecture note I
found this fascinating note 47 by Yoav Goldberg on neural network models for natural
language processing This note deals with wider topics on natural language processing
with distributed representations in more details and I highly recommend you to read it
hopefully along with this lecture note I seriously wish Yoav had written it earlier so
that I couldve simply used his excellent note for my course
This lecture note had been written quite hastily as the course progressed meaning
that I could spare only about 100 hours in total for this note This is my lame excuse
for likely many mistakes in this lecture note and I kindly ask for your understanding
in advance Again how grateful I wouldve been had I found Yoavs note earlier
I am planning to update this lecture note gradually over time hoping that I will
be able to convince the Center for Data Science to let me teach the same course next
year The latest version will always be available both in pdf and in latex source code
from httpsgithubcomnyu-dlNLPDLLectureNote The arXiv
version will be updated whenever a major revision is made
I thank all the students and non-students who took2 this course and David Rosen-
berg for feedback
1 httpcdsnyuedu
2 In fact they are still taking the course as of 24 Nov 2015 They have two guest lectures and a nal exam
left until the end of the course
Contents
Introduction
11 Route we will not take
     
            
111 What is Language 
                     
112 Language Understanding                    
               
121 Language as a Function                    
122 Language Understanding as a Function Approximation    
     
12 Road we will take 
2 Function Approximation as Supervised Learning
21 Function Approximation Parametric Approach            
211 Expected Cost Function                    
212 Empirical Cost Function                    
22 Learning as Optimization                        
221 Gradient-based Local Iterative Optimization          
                 
23 When do we stop learning                       
231 Early Stopping                         
                     
232 Model Selection  
24 Evaluation 
                        
25 Linear Regression for Non-Linear Functions              
Feature Extraction                       
Stochastic Gradient Descent
3 Neural Networks and Backpropagation Algorithm
31 Conditional Distribution Approximation                
311 Why do we want to do this                  
312 Other Distributions                       
32 Feature Extraction is also a Function                  
33 Multilayer Perceptron                          
331 Example Binary classication with a single hidden unit
332 Example Binary classication with more than one hidden units 29
34 Automating Backpropagation                      
341 What if a Function is not Differentiable
    
4 Recurrent Neural Networks and Gated Recurrent Units
   
      
Fixed-Size Output y
42 Gated Recurrent Units
41 Recurrent Neural Networks                       
       
412 Multiple Child Nodes and Derivatives             
413 Example Sentiment Analysis
                
414 Variable-Length Output y x  y
   
                       
421 Making Simple Recurrent Neural Networks Realistic    
422 Gated Recurrent Units                     
423 Long Short-Term Memory                   
                         
                     
431 Rectiers Explode 
                     
Is tanh a Blessing 
433 Are We Doomed  
                     
434 Gated Recurrent Units Address Vanishing Gradient      
43 Why not Rectiers 
    
    
5 Neural Language Models
51 Language Modeling First Step                     
511 What if those linguistic structures do exist           
512 Quick Note on Linguistic Units
               
52 Statistical Language Modeling                     
521 Data SparsityScarcity                     
n-Gram Language Model  
                     
Smoothing and Back-Off                    
532 Lack of Generalization                     
                       
541 How does Neural Language Model Generalize to Unseen n-
Grams  Distributional Hypothesis              
54 Neural Language Model
542 Continuous Bag-of-Words Language Model
Maximum PseudoLikelihood Approach           
Semi-Supervised Learning with Pretrained Word Embeddings
55 Recurrent Language Model                       
56 How do n-gram language model neural language model and RNN-LM
                      
compare 
6 Neural Machine Translation
61 Statistical Approach to Machine Translation              
Parallel Corpora Training Data for Machine Translation   
612 Automatic Evaluation Metric                  
62 Neural Machine Translation
Simple Encoder-Decoder Model
                   
Sampling vs Decoding                     
63 Attention-based Neural Machine Translation              
          
64 Warren Weavers Memorandum                    
631 What does the Attention Mechanism do
7 Final Words
71 Multimedia Description Generation as Translation           
72 Language Understanding with World Knowledge           
73 Larger-Context Language Understanding
Beyond Sentences and Beyond Words                 
74 Warning and Summary                         
Chapter 1
Introduction
This lecture is going to be the only one where I discuss some philosophical meaning
nonpractical arguments because according to Chris Manning and Hinrich Schuetze
even practically-minded people have to confront the issue of what prior knowledge to
try to build into their model 77
11 Route we will not take
111 What is Language
The very rst question we must ask ourselves before starting this course is the ques-
tion of what natural language is Of course the rest of this course does not in any
way require us to know what natural language is but it is a philosophical question I
recommend everyone including myself to ponder upon once a while
When I start talking about languages with anyone there is a single person who
never misses to be mentioned that is Noam Chomsky His view has greatly inuenced
the modern linguistics and although many linguists I have talked to claim that their
work and eld have long moved on from Chomskys I can feel his shadow all over
My rst encounter with Chomsky was at the classroom of Automata from my
early undergrad years I was not the most attentive student back then and all I can
remember is Chomskys hierarchy and how it has shaped our view on languages in this
context programmingcomputer languages A large part of the course was dedicated
to explaining which class of languages emerges given a set of constraints on a set of
generating rules or production rules
For instance if we are given a set of generating rules that do not depend on the con-
textmeaning of non-terminal symbols context-free grammar CFG we get a context-
free language If we put a bit of constraints to CFG that each generating rule is such
that a non-terminal symbol is replaced by either a terminal symbol a terminal symbol
by a non-terminal symbol or an empty symbol then we get a regular grammar Sim-
ilarly to CFG we get a regular language from the regular grammar and the regular
language is a subset of the context-free language
What Chomsky believes is that this kind of approach applies also to human lan-
guages or natural languages There exists a set of generating rules that generates a
natural language But then the obvious question to follow is where those generating
rules are Where are they stored How are they stored Do we have separate generating
rules for different languages
112 Language Understanding
Understanding Human Language Those questions are interesting but out of scope
for this course Those questions are the ones linguists try to answer Generative linguis-
tics aims at guring out what those rules are how they are combined to form a valid
sentence how they are adapted to different languages and so on We will leave these to
linguists and continue on to our journey of building a machine that understands human
languages
Natural Language Understanding So lets put these questions aside and trust Chom-
sky that we humans are specially designed to store those generating rules somewhere
in the brain 30 21 Or better yet lets trust Chomsky that theres a universal gram-
mar built in our brain In other words lets say we were born with this set of generating
rules for natural languages and while growing we have adapted this universal gram-
mar toward our native tongue language variation
When we decide to speak of something whatever that is and however implausi-
ble that is our brain quickly picks up a sequence of some of those generating rules
and starts generating a sentence accordingly Of course those rules do not generate a
sentence directly but generates a sequence of control signals to move our muscles to
make sound When heard by other people who understand your language the sound
becomes a sentence
In our case we are more interested in a machine hearing that sound or a sentence
from here on When a machine heard this sentence what wouldshould a language un-
derstanding machine do to understand a language or more simply a sentence Again
we are assuming that this sentence was generated from applying a sequence of the
existing generating rules
Under our assumption a natural rst step that comes to my mind is to gure out
that sequence of the generating rules which led to the sentence Once the sequence is
found or in a fancier term inferred the next step will be to gure out what kind of
mental state of the speaker led to those generating rules
Lets take an example sentence Our company is training workers from Sec 13
of 77 which is a horrible choice because this was used as an example of ambiguity
in parsing Regardless a speaker obviously has an awesome image of her company
which trains its workers and wants to tell a machine about this This mental state is
used to select the following generating rules assuming a phrase structure grammar1
1 Stanford Parser httpnlpstanfordedu8080parser
NP PRP Our NN company
VP VBZ is
VP VBG training
NP NNS workers
Figure 11 A parse of Our company is training workers
The machine hears the sentence Our company is training workers and infers
the parse in Fig 11 Then we can make a simple set of rules again
to let the
machine answer questions about this sentence kinds of questions that imply that the
machine has understood the sentence language For instance given a question Who
is training workers the machine can answer by noticing that the question is asking
for the subject of the verb phrase is training acted on the object workers and that
the subject is Our company
Side Note Bayesian Language Understanding This generative view of languages
ts quite well with Bayesian modelling see eg 84 There exists a hidden mecha-
nism or a set of generating rules and a rule governing their composition which can be
modelled as a latent variable Z Given these rules a language or a sentence X is gen-
erated according to the conditional distribution PXZ Then understanding language
by humans is equivalent to computing the posterior distribution over all possible sets
of generating rules and their compositional rules ie PZX This answers the ques-
tion of what is the most likely mechanism underlying the observed language
Furthermore from the perspective of machines Bayesian approach is attractive In
this case we assume to know the set of rules in advance and let the latent variable Z
denote the specic conguration use of those rules Given this sequence of applying
the rules a sentence X is generated via the conditional distribution PXZ Machine
understanding of language is equivalent to inferring the posterior distribution over Z
given X
For more details about Bayesian approaches in the context of machine learning
please refer to 13 or take the course DS-GA 1005 Inference and Representation by
Prof David Sontag
pa13TheAmbiguityofLanguageWhyNLPIsDicult17PhilosophicallythisbringsusclosetothepositionadoptedinthelaterwritingsofWittgensteinthatisWittgenstein1968wherethemean-ingofawordisdenedbythecircumstancesofitsuseausetheoryofusetheoryofmeaningmeaningseethequotationsatthebeginningofthechapterUnderthisconceptionmuchofStatisticalNLPresearchdirectlytacklesquestionsofmeaning13TheAmbiguityofLanguageWhyNLPIsDicultAnNLPsystemneedstodeterminesomethingofthestructureoftextnormallyatleastenoughthatitcananswerWhodidwhattowhomConventionalparsingsystemstrytoanswerthisquestiononlyintermsofpossiblestructuresthatcouldbedeemedgrammaticalforsomechoiceofwordsofacertaincategoryForexamplegivenareasonablegrammarastandardNLPsystemwillsaythatsentence110has3syntacticanal-ysesoftencalledparses110OurcompanyistrainingworkersThethreedieringparsesmightberepresentedasin111111aSNPOurcompanyVPAuxisVPVtrainingNPworkersbSNPOurcompanyVPVisNPVPVtrainingNPworkersUnderstanding vs Using Whats clear from this example is that in this generative
view of languages there is a clear separation between understanding and using In-
ferring the generating rules from a given sentence is understanding and answering a
question based on this understanding using is a separate activity Understanding part
is done when the underlying true structure has been determined regardless of how
this understanding be used
To put it in a slightly different wording language understanding does not require its
use or downstream tasks In this road that we will not take in this course understanding
exists as it is regardless of what the understood insightknowledge will be used for
And this is the reason why we do not walk down this road
12 Road we will take
121 Language as a Function
In this course we will view a naturalhuman language as a system intended to com-
municate ideas from a speaker to a hearer 110 What this means is that we do not
view a language as a separate entity that exists on its own Rather we view a whole
system or behaviour of communication as a language Furthermore this view dictates
that we must take into account the world surrounding a speaker and a hearer in order
to understand language
Under this view of language language or rather its usage become somewhat similar
to action or behaviour Speaking of something is equivalent to acting on a listener as
both of them inuence the listener in one way or another The purpose of language
is then to inuence another by efciently communicate ones will or intention2 This
hints at how language came to be or may have come to be evolution language
has evolved to facilitate the exchange of ideas among people learning humans learn
language by being either encouraged or punished for the use of language This latter
view on how language came to be is similar in spirit to the behaviourism of B F
Skinner necessary mediation of reinforcement by another organism 97
This is a radical departure from the generative view of human language where
language existed on its own and its understanding does not necessarily require the
existence of the outside world nor the existence of a listener
It is no wonder why
Chomsky was so harsh in criticizing Skinners work in 30 This departure as I see
it is the departure toward a functional view of language Language is a function of
communication
122 Language Understanding as a Function Approximation
Lets make a large jump here such that we consider this function as a mathematical
function This function called language takes as input the state of the surrounding
world the speakers speech either written spoken or signed and the listeners mental
2 Chomsky does not agree it is wrong to think of human use of language as characteristically informa-
tive in fact or in intention 31
state3 Inside the function the listeners mental state is updated to incorporate the new
idea from the speakers speech The function then returns a response by the listener
which may include no response as well and a set of non-verbal action sequences
what would be the action sequence if the speaker insulted the listener
In this case language understanding both from humans and machines perspec-
tive boils down to guring out the internal working of this function In other words we
understand language by learning the internal mechanism of the function Furthermore
this view suggests that the underlying structures of language are heavily dependent on
the surrounding environment context as well as on the target task The former con-
text dependence is quite clear as the function takes as input the context but the latter
may be confusing now Hopefully this will become clearer later in the course
How can we approximate this function How can we gure out the internal working
mechanism of this function What tools do we have
Language Understanding by Machine Learning This functional view of languages
suddenly makes machine learning a very appealing tool for understanding human lan-
guages After all function approximation is the core of machine learning Classica-
tion is a classical example of function approximation clustering is a function approxi-
mation where the target is not given generative modeling learns a function that returns
a probability of an input and so on
When we approximate a function in machine learning the prime ingredient is data
We are given data which was either generated from this function unsupervised learn-
ing or well t this function supervised learning based on which we adjust our ap-
proximation to the function often iteratively to best t the data But I must note here
that it does not matter how well the approximated function ts the data it was tted to
but matters how well this approximation ts unseen data4
In language understanding this means that we collect a large data set of input and
output pairs or conversations together with the recording of the surrounding environ-
ment and t some arbitrary function to well predict the output given an input We
probably want to evaluate this approximation in a novel conversation If this function
makes a conversation just like a person voila we made a machine that passed the
Turing test Simple right
Problem Unfortunately as soon as we try to do this we run into a big problem This
problem is not from machine learning nor languages but the denition of this function
of language
Properly approximating this function requires us to either simulate or record the
whole world in fact the whole universe For this function takes as input and main-
tains as internal state the surrounding world context and the mental state of the in-
dividual speaker This is unavoidable if we wanted to very well approximate this
function as a whole
It is unclear however whether we want to approximate the full function For a
human to survive yes it is likely that the full function is needed But if our goal is
3 We assume here that a such thing exists however it is represented in our brain
4 This is a matter of generalization and we will talk about this more throughout the course
restricted to a certain task such as translation language modelling and so on we may
not want to approximate this function fully We probably want to approximate only a
subset of this whole function For instance if our goal is to understand the process
of translation from one language to another we can perhaps ignore all but the speech
input to the function and all but the speech output from the function because often a
trained person can translate a sentence in one language to another without knowing
the whole context
This latter approach to language understandingapproximating a partial function
of languages will be at the core of this course We will talk about various language
tasks that are a part of this whole function of language These tasks will include but
are not limited to language modelling machine translation imagevideo description
generation and question answering For these tasks and potentially more we will study
how to use machine learning or more specically deep learning to solve these tasks
by approximating sub-functions of language
Chapter 2
Function Approximation as
Supervised Learning
Throughout this course we will extensively use articial neural networks1 to approx-
imate a part of the function of natural language This makes it necessary for us to
study the basics of neural networks rst and this lecture and a couple of subsequent
ones are designed to serve this purpose
21 Function Approximation Parametric Approach
211 Expected Cost Function
Let us start by dening a data distribution pdata pdata is dened over a pair of input
and output vectors x  Id and y  Ok respectively I and O are respectively sets of
all possible input and output values such as R 01 and 01    L This data
distribution is not known to us
The goal is to nd a relationship between x and y More specically we are in-
terested in nding a function f  Rd  Ok that generates the output y given its corre-
sponding input x The very rst thing we should do is to put some constraints on the
function f to make our search for the correct f a bit less impossible In this lecture
and throughout the course I will consider only a parametric function f  in which case
the function is fully specied with a set of parameters 
Next we must dene a way to measure how well the function f approximates
the underlying mechanism of generation x  y Lets denote by y the output of the
function with a particular set  of parameters and a given input x
1 From here on I will simply drop articial and call them neural networks Whenever I say neural
network it refers to articial neural networks
y  f x
How well f approximates the true generating function is equivalent to how far y is from
the correct output y Lets use Dyy for now call this distance2 between y and y
It is clear that we want to nd  that minimizes Dyy for every pair in the space
RRd Ok But wait every pair equally likely Probably not for we do not care how
well f approximates the true function when a pair of input x and output y is unlikely
meaning we do not care how bad the approximation is if pdataxy is small However
this is a bit difcult to take into account as we must decided on the threshold below
which we consider any pair irrelevant
Hence we weight the distance between the approximated y and the correct y of
each pair xy in the space by its probability pxy Mathematically saying we want
to nd
where the integral cid82 should be replaced with the summation  if any of x and y is
pdataxyDyydxdy
cid90
cid90
discrete
We call this quantity being minimized with respect to the parameters  a cost func-
tion C  This is equivalent to computing the expected distance between the predicted
output y and the correct one y
C  
pdataxyDyydxdy
cid90
cid90
Exypdata Dyy
This is often called an expected loss or risk and minimizing this cost function is re-
ferred to as expected risk minimization 105
Unfortunately C  cannot be exactly computed for a number of reasons The
most important reason among them is simply that we dont know what the data distri-
bution pdata is Even if we have access to pdata we can exactly compute C  only with
heavy assumptions on both the data distribution and the distance function3
212 Empirical Cost Function
This does not mean that we are doomed from the beginning Instead of the full-blown
description of the data distribution pdata we will assume that someone miraculously
gave us a nite set of pairs drawn from the data distribution We will call this a training
cid8x1y1     xNyNcid9 
As we have access to the samples from the data distribution we can use Monte
Carlo method to approximate the expected cost function C  such that
C   C  
D ynyn
2 Note that we do not require this distance to satisfy the triangular inequality meaning that it does not
have to be a distance However I will just call it distance for now
We call this approximate C  of the expected cost function an empirical cost function
or empirical risk or empirical loss
Because empirical cost function is readily computable we will mainly work with
the empirical cost function not with the expected cost function However keep in mind
that at the end of the day the goal is to nd a set of parameters that minimizes the
expected cost
22 Learning as Optimization
We often call this process of nding a good set of parameters that minimizes the ex-
pected cost learning This term is used from the perspective of a machine which imple-
ments the function f  as it learns to approximate the true generating function f from
training data
From what I have described so far it may have become clear even without me men-
tioning that learning is optimization We have a clearly dened function the empirical
cost function C which needs to be minimized with respect to its input 
221 Gradient-based Local Iterative Optimization
There are many optimization algorithms one can use to nd a set of parameters that
minimizes C Sometimes you can even nd the optimal set of parameters in a closed
form equation4 In most cases because there is no known closed-form solution it is
typical to use an iterative optimization algorithm see 42 for in-depth discussion on
optimization
By an iterative optimization I mean an algorithm which renes its estimate of the
optimal set of parameters little by little until the values of the parameters converge to
the optimal expected cost function Also it is worthwhile to note that most iterative
optimization algorithms are local in the sense that they do not require us to evaluate
the whole parameter space but only a small subset along the path from the starting
point to the convergence point5
Here I will describe the simplest one among those local iterative optimization algo-
rithms called gradient descent GD algorithm As the name suggests this algorithm
depends entirely on the gradient of the cost function6
4 One such example is a linear regression where
 f Wx  Wx
 Dyy  1
2cid107y ycid1072
In this case the optimal W is
W  YXcid62XXcid621
X cid2x1   xNcid3 Y cid2y1   yNcid3 
Try it yourself
5 There are global optimization algorithms but they are out of scope for this course See for instance
18 for one such algorithm called Bayesian optimization
6 From here on I will use the cost function to refer to the empirical cost function
blue
Figure 21
sin10x  x
x  06
gradient at x  06
f x 
red a gradient at
magenta a negative
The gradient of a function  C is a vector whose direction points to the direction of
the greatest rate of increase in the functions value and whose magnitude measures this
rate At each point  t in the parameter space the gradient of the cost function  C t 
is the opposite direction toward which we want to move the parameters See Fig 21
for graphical illustration
One important point of GD that needs to be mentioned here is on how large a
step one takes each time As clear from the magenta line the direction opposite to
the direction given by the gradient in Fig 21 if too large a step is taken toward the
negative gradient direction the optimization process will overshoot and miss the local
minimum around x  08 This step size or sometimes called learning rate  is one
most important hyperparameter of the GD algorithm
Now we have all the ingredients for the GD algorithm  C and  The GD algo-
rithm iterates the following step
     C 
The iteration continues until a certain stopping criterion is met which we will discuss
shortly
222 Stochastic Gradient Descent
This simple GD algorithm works surprisingly quite well and it is a fundamental basis
upon which many advanced optimization algorithms have been built I will present a
list of few of those advanced algorithms later on and discuss them briey But before
going into those advanced algorithms lets solve one tiny but signicant issue of the
GD algorithm
This tiny but signicant issue arises especially often in machine learning That is
it is computationally very expensive to compute C and consequently its gradient  C
thanks to the ever increasing size of the training set D
Why is the growing size of the training set making it more and more computation-
ally demanding to compute C and  C This is because both of them are essentially
the sum of as many per-sample costs as there are examples in the training set In other
Cxnyn 
C  
 C  
 Cxnyn 
And N goes up to millions or billions very easily these days
This enormous computational cost involved in each GD step has motivated the
stochastic gradient descent SGD algorithm 88 15
First recall from Eq 23 that the cost function we minimize is the empirical
cost function C which is the sample-based approximation to the expected cost function
C This approximation was done by assuming that the training examples were drawn
randomly from the data distribution pdata
C   C  
D ynyn
In fact as long as this assumption on the training set holds we can always approximate
the expected cost function with a fewer number of training examples
C   CM   
M 
D ymym
where M cid28 N and M is the indices of the examples in this much smaller subset of the
training set We call this small subset a minibatch
Similarly this leads to a minibatch-based estimate of the gradient as well
 CM   
M 
D ymym
It must now be clear to you where I am headed toward At each GD step instead
of using the full training set we will use a small subset M which is randomly selected
to compute the gradient estimate In other words we use CM instead of C and  CM
instead of  C in Eq 25
Because computing CM and  CM is independent of the size of the training set we
can use SGD to make as many steps as we want without worrying about the growing
size of training examples This is highly benecial as regardless of how many train-
ing examples you used to compute the gradient we can only take a tiny step toward
that descending direction Furthermore the increased level of noisy in the gradient
estimate due to the small sample size has been suspected to help reaching a better so-
lution in high-dimensional non-convex problems such as those in training deep neural
networks 717
7 Why would this be the case It is worth thinking about this issue further
We can set M to be any constant and in an extreme we can set it to 1 as well In
this case we call it online SGD8 Surprisingly already in 1951 it was shown that using
a single example each time is enough for the SGD to converge to a minimum under
certain conditions obviously 88
This SGD algorithm will be at the core of this course and will be discussed further
in the future lectures
23 When do we stop learning
From here on I assume that we approximate the ground truth function by iteratively
rening its set of parameters in most cases using stochastic gradient descent In other
words learning of a machine that approximates the true generating function f happens
gradually as the machine goes over the training examples little by little over time
Let us go over again what kind of constraintsissue we have rst
1 Lack of access to the expected cost function C 
2 Computationally expensive empirical cost function C 
3 Potential non-convexity of the empirical cost function C 
The most severe issue is that we do not have access to the expected cost function
which is the one we want to minimize in order to work well with any pair of input x
and output y Instead we have access to the empirical cost function which is a nite
sample approximation to the expected cost function
Why is this a problem Because we do not have a guarantee that the local mini-
mum of the empirical cost function corresponds to the local minimum of the expected
cost function An example of this mismatch between the expected and empirical cost
functions is shown in Fig 22
As in the case shown in Fig 22 it is not desirable to minimize the empirical cost
function perfectly The parameters that perfectly minimize the empirical cost function
in the case of Fig 22 the slope a of a linear function f x  ax will likely be a
sub-optimal cost for the expected cost function about which we really care
231 Early Stopping
What should we do There are many ways to avoid this weird contradiction where
we want to optimize the cost function well but not too well Among those one most
important trick is early stopping which is only applicable when iterative optimization
is used
First we will split the training set D into two partitions Dtrain and Dval9 We call
them a training set and a validation set respectively In practice it is a good idea to
keep D much larger than Dcid48 because of the reasons that will become clear shortly
8 Okay this is not true in a strict sense SGD is an online algorithm with M  1 originally and using
M  1 is a variant of SGD often called minibatch SGD However as using minibatches M  1 is almost
always the case in practice I will refer to minibatch SGD as SGD and to the original SGD as online SGD
9 Later on we will split it further into three partitions
Figure 22 blue Expected cost
function C 
red Empirical
cost function C 
The un-
derlying true generating function
was f x  sin10x  x The
cost function uses the squared Eu-
clidean distance
The empiri-
cal cost function was computed
based on 10 noisy examples of
which xs were sampled from the
uniform distribution between 0
and 1 For each sample input x
noise from zero-mean Gaussian
distribution with standard devia-
tion 001 was added to f x to
emulate the noisy measurement
channel
Further let us dene the training cost as
C   Ctrain  
Dtrain 
xyDtrain
Dtrain yy
and the validation cost as
Cval  
Dval 
xyDval
D yy
With these two cost functions we are all ready to use early stopping now
After every few updates using SGD or GD the validation cost function is evalu-
ated with the current set of parameters The parameters are updated ie the training
cost function is optimized until the validation cost does not decrease or starts to in-
crease instead of decreasing
Thats it It is almost free as long as the size of the validation set is reasonable
since each evaluation is at most as expensive as computing the gradient of the empirical
cost function Because of the simplicity and effectiveness this early stopping strategy
has become de facto standard in deep learning and in general machine learning
The question that needs to be asked here is what the validation cost function does
here Clearly it approximates the expected cost function C similarly to the empirical
cost function C as well as the training cost function Ctrain In the innite limit of the
size of either training or validation set they should coincide but in the case of a nite
set those two cost functions differ by the noise in sampling sampling pairs from the
data distribution and observation noise in y  f x
The fact that we explicitly optimize the training cost function implies that there is
a possibility in fact almost surely in practice that the set of parameters found by this
optimization process may capture not only the underlying generating function but also
noise in the observation and sampling procedure This is an issue because we want our
machine to approximate the true generating function not the noise process involved
The validation cost function measures both the true generating structure as well as
noise injected during sampling and observation However assuming that noise is not
correlated with the underlying generating function noise introduced in the validation
cost function differs from that in the training cost function In other words the set
of parameters that perfectly minimizes the training cost function thereby capturing
even noise in the training set will be penalized when measured by the validation cost
function
232 Model Selection
In fact the use of the validation cost does not stop at the early stopping Rather it has a
more general role in model selection First we must talk about model selection itself
This whole procedure of optimization or learning can be cast as a process of
searching for the best hypothesis over the entire space H of hypotheses Here each
hypothesis corresponds to each possible function with a unique set of parameters and
a unique functional form that takes the input x and output y In the case of regression
x  Rd and y  R the hypothesis space includes an n-th order polynomial function
f x 
k1 iknik0
ai1i2ik
kcid481
kcid48
where ai1i2iks are the coefcients and any other functional form that you can imag-
ine as long as it can process x and return a real-valued scalar In the case of neural
networks this space includes all the possible model architectures which are dened by
the number of layers the type of nonlinearities the number of hidden units in each
layer and so on
Let us use M  H to denote one hypothesis10 One important thing to remember is
that the parameter space is only a subset of the hypothesis space because the parameter
space is dened by a family of hypotheses the parameter space of a linear function
cannot include a set of parameters for a second-order polynomial function
Given a denition of expected cost function we can score each hypothesis M by
the corresponding cost CM Then the whole goal of function approximation boils down
to the search for a hypothesis M with the minimal expected cost function C But of
course we do not have access to the expected cost function and resort to the empirical
cost function based on a given training set
The optimization-based approach we discussed so far searches for the best hypoth-
esis based on the empirical cost iteratively However because of the issue of overtting
which means that the optimization algorithm overshot and missed the local minimum
of the expected cost function because it was aimed at the local minimum of the empir-
ical cost function I introduced the concept of early stopping based on the validation
10 M because each hypothesis corresponds to one learning machine
This is unfortunately not satisfactory as we have only searched for the best hypoth-
esis inside a small subset of the whole hypothesis space H  What if another subset
of the hypothesis space includes a function that better suits the underlying generating
function f  Are we doomed
It is clearly better to try more than one subsets of the hypothesis space For in-
stance for a regression task we can try linear functions H1 quadratic second-order
polynomial functions H2 and sinusoidal functions H3 Lets say for each of these
subsets we found the best hypothesis using iterative optimization and early stopping
MH1 MH2 and MH3 Then the question is how we should choose one of those hy-
potheses
Similar to what weve done with early stopping we can use the validation cost to
compare these hypotheses Among those three we choose one that has the smallest
validation cost CvalM
This is one way to do model selection and we will talk about another way to do
this later
24 Evaluation
But wait if this is an argument for using the validation cost to early stop the optimiza-
tion or learning one needs to notice something weird What is it
Because we used the validation cost to stop the optimization there is a chance
that the set of parameters we found is optimal for the validation set whose structure
consists of both the true generating function and samplingobservation noise but not
to the general data distribution This means that we cannot tell whether the function
estimate f approximating the true generating function f is a good t by simply early
stopping based on the validation cost Once the optimization is done we need yet
another metric to see how well the learned function estimate f approximates f 
Therefore we need to split the training set not into two partitions but into three
partitions We call them a training set Dtrain a validation set Dval and a test set Dtest
Consequently we will have three cost functions a training cost function Ctrain a vali-
dation cost function Cval and a test cost function Ctest similarly to Eqs 2627
This test cost function is the one we use to compare different hypotheses or models
fairly Any hypothesis that worked best in terms of the test cost is the one that you
choose
Lets not Cheat One most important lesson here is that you must never look at a test
set As soon as you take a peak at the test set it will inuence your choice in the model
structure as well as any other hyperparameters biasing toward a better test cost The
best option is to never ever look at the test set until it is absolutely needed eg need
to present your result
25 Linear Regression for Non-Linear Functions
Let us start with a simple linear function to approximate a true generating function such
y  f x  Wcid62x
where W  Rdl is the weight matrix
parameter ie   W
The empirical cost function is then
C  
The gradient of the empirical cost function is
 C    1
In this case this weight matrix is the only
cid13cid13cid13yn  Wcid62xncid13cid13cid132
cid16
yn  Wcid62xncid17cid62
With these two well dened we can use the iterative optimization algorithm such
as GD or SGD to nd the best W that minimizes the empirical cost function11 Or
better is to use a validation set to stop the optimization algorithm at the point of the
minimal validation cost function remember early stopping
Now but we are not too satised with a linear network are we
251 Feature Extraction
Why are we not satised
First we are not sure whether the true generating function f was a linear function
If it is not can we expect linear regression to approximate the true function well Of
course not We will talk about this shortly
Second because we were given x meaning we did not have much control over what
we want to measure as x it is unclear how well x represents the input For instance
consider doing a sales forecast of air conditioner at one store which opened ve years
ago The input x is the number of days since the opening date of the store 1 Jan 2009
and the output y is the number of units sold on each day
Clearly in this example the relationship between x and y is not linear Furthermore
perhaps the most important feature for predicting the sales of air conditioners is missing
from the input x which is a month or a season if you prefer It is likely that the
sales bottoms out during the winter perhaps sometime around December January and
February and it hits the peak during summer months around May June and July
In other words if we look at how far the month is away from July we can predict the
sales quite well even with linear regression
11 In fact looking at Eq 28 its quite clear that you can compute the optimal W analytically See
Eq 24
Let us call this quantity  x or equivalent feature such that
 x  mx  
where mx  12    12 is the month of x and   55 With this feature we can t
linear regression to better approximate the sales gure of air conditioners Furthermore
we can add yet another feature to improve the predictive performance For instance
one such feature can be which day of week x is
This whole process of extracting a good set of features that will make our choice
of parametric function family such as linear regression in this case is called feature
extraction This feature extraction is an important step in machine learning and has
often been at the core of many applications such as computer vision the representative
example is SIFT 74
Feature extraction often requires heavy knowledge of the domain in which this
function approximation is applied To use linear regression for computer vision it is
a good idea to use computer vision knowledge to extract a good set of features If we
want to use it for environmental problems we must rst notice which features must be
important and how they should be represented for linear regression to work
This is okay for a machine learning practitioner in a particular eld because the
person has in-depth knowledge about the eld There are however many cases where
theres simply not enough domain knowledge to exploit To make the matter worse it
is likely that the domain knowledge is not correct making the whole business of using
manually extracted features futile
Chapter 3
Neural Networks and
Backpropagation Algorithm
31 Conditional Distribution Approximation
I have mainly described so far as if the function we approximate or the function we
use to approximate returns only a constant value as in one point y in the output space
This is however not true and in fact the function can return anything including a
distribution 17 35 12
Lets rst decompose the data distribution pdata into the product of two terms
pdataxy  pdataxpdatayx
It becomes clear that one way to sample from pdata is to sample an input xn from
pdatax and subsequently sample the corresponding output yn from the conditional
distribution pdatayxn
This implies that the function approximation of the generating function  f  x  y
is effectively equivalent to approximating the conditional distribution pdatayx This
may suddenly sound much more complicated but it should not alarm you at all As
long as we choose to use a distribution parametrized by a small number of param-
eters to approximate the conditional distribution pdatayx this is quite manageable
without almost any modication to the expected and empirical cost functions we have
discussed
approximating the true underlying probability distribution pdatayx As the notation
suggests the function now returns the parameters of the distribution  x given the
input x
For example lets say y  01k is a binary vector and we chose to use inde-
pendent Bernoulli distribution to approximate the conditional distribution pdatayx
In this case the parameters that dene the conditional distribution are the means of k
Let us use  x to denote a set of parameters for the probability distribution pyx x
dimensions
pyx 
kcid481
pykcid48x 
kcid481
kcid48 1 kcid481ykcid48 
ykcid48
Then the function  x should output a k-dimensional vector of which each element is
between 0 and 1
Another example lets say y  Rk is a real-valued vector It is quite natural to use a
Gaussian distribution with a diagonal covariance matrix to approximate the conditional
distribution pyx
cid32
cid33
pyx 
kcid481
2kcid48
ykcid48  kcid482
kcid48
The parameters for this conditional distribution are  x 1 2     k12    k
where k  R and k  R0
In this case of probability approximation it is natural to use Kullback-Leibler KL
divergence to measure the distance1 The KL divergence from one distribution P to the
other Q is dened2 by
KLPcid107Q 
Pxlog
In our case of functiondistribution approximation we want to minimize the KL di-
vergence from the data distribution pdatayx to the approximate distribution pyx
averaged over the data distribution pdatax
cid90
cid90
cid90
C  
pdataxKLpdatacid107 pdx 
pdatax
cid90
pdatayxlog
pdatayx
pyx
But again we do not have access to pdata and cannot compute this expected cost func-
Similarly to how we dened the empirical cost function earlier we must approxi-
mate this expected KL divergence using the training set
C  
log pynxn
As an example if we choose to return the binary vector y as in Eq 31 the empirical
cost function will be
C    1
ykcid48 log kcid48  1 ykcid48log1 kcid48
kcid481
1 Again we use a loose denition of the distance where triangular inequality is not enforced
2 Why dont I say the KL divergence between two distributions here Because the KL divergence is not
a symmetric measure ie KLPcid107Q cid54 KLQcid107P
which is often called a cross entropy cost In the case of Eq 32
C    1
kcid481
ykcid48  kcid482
kcid48
 logkcid48
Do you see something interesting in Eq 34 If we assume that the function
outputs 1 for all kcid48s we see that this cost function reduces to that using the Euclidean
distance between the true output y and the mean  What does this mean
There will be many occasions later on to discuss more about this perspective when
we discuss language modelling However one thing we must keep in our mind is that
there is nothing different between approximating a function and a distribution
311 Why do we want to do this
Before we move on to the main topic of todays lecture lets try to understand why
we want to output the distribution Unlike returning a single point in the space the
distribution returned by the function f incorporates both the most likely outcome y as
well as the uncertainty associated with this value
In the case of the Gaussian output in Eq 32 the standard deviation kcid48 or the
variance  2
kcid48 indicates how uncertain the function is about the output centered at kcid48
Similarly the mean kcid48 of the Bernoulli output in Eq 31 is directly proportional to
the functions condence in predicting that the kcid48-th dimension of the output is 1
Figure 31 Is this a duck or a rab-
bit 68 At the end of the day
we want our function f to return
a conditional distribution saying
that pduckx  prabbitx in-
stead of returning the answer out
of these two possible answers
This is useful in many aspects but one important aspect is that it reects the natural
uncertainty of the underlying generating function One input x may be interpreted in
more than one ways leading to two possible outputs which happens more often than
not in the real world For instance the famous picture in Fig 31 can be viewed as a
picture of a duck or a picture of a rabbit in which case the function needs to output the
probability distribution by which the same probability mass is assigned to both a duck
and a rabbit Furthermore there is observational noise that cannot easily be identied
and ignored by the function in which case the function should return the uncertainty
due to the observational noise along with the most likely or the average prediction
312 Other Distributions
I have described two distributions densities that are widely used
 Bernoulli distribution binary classication
 Gaussian distribution real value regression
Here let me present one more distribution which we will use almost everyday through
this course
Categorical Distribution Multi-Class Classication Multi-class classication is a
task in which each example belongs to one of K classes For each input x the problem
reduces to nd a probability pkx of the k-th class under the constraint that
pkx  1
It is clear that in this case the function f returns K values 1 2     K each
of which is between 0 and 1 Furthermore the sum of ks must sum to 1 This can be
achieved easily by letting f to compute afne transformation of x or  x to return K
unbounded real values followed by a so called softmax function 17
expwcid62
kcid481 expwcid62
k  x  bk
kcid48 x  bk
where wk  Rdim x and bk  R are the parameters of afne transformation
In this case the empirical cost function based on the KL divergence is
Ikyn 
C    1
Ikynk
cid26 1
if k  yn
0 otherwise
32 Feature Extraction is also a Function
We talked about the manual feature extraction in the previous lecture see Sec 251
But this is quite unsatisfactory because this whole process of manual feature extraction
is heavily dependent on the domain knowledge meaning that we cannot have a generic
principle on which we design features This raises a question instead of manually
designing features ourselves is it possible for this to happen automatically
One thing we notice is that the feature extraction process  x is nothing but a
function A function of a function is a function right In other words we will extend
our denition of the function to include the feature extraction function
y  f  x
We will assume that the feature extraction function  is also parametrized and its
parameters are included in the set of parameters which includes those of f  As an
example  in Eq 29 is a parameter of the feature extraction 
A natural next question is which family of parametric functions we should use for
 We run into the same issue we talked about earlier in Sec 23 the size of hypothesis
space is simply too large
Instead of choosing one great feature extraction function we can go for a stack of
simple transformations which are all learned3 Each transformation can be as simple
as afne transformation followed by a simple point-wise nonlinearity
where W0 is the weight matrix b0 is the bias and g is a point-wise nonlinearity such
as tanh4
0x  gW0x  b0
One interesting thing is that if the dimensionality of the transformed feature vector
0x is much larger than that of x the function f 0x can approximate any func-
tion from x to y under some assumptions even when the parameters W0 and b0 are
randomly selected 34
The problem solved right We just put a huge matrix W0 apply some nonlinear
function g to it and t linear regression as I described earlier We dont even need to
touch W0 and b0 All we need to do is replace the input xn of all the pairs in the training
set to 0xn
In fact there is a group of researchers claiming to have gured this out by them-
selves less than a decade ago as of 2015 who call this model an extreme learning
machine 54 There have been some debates about this so-called extreme learning
machine Here I will not make any comment myself but would be a good exercise for
you to gure out why there has been debates about this
But regardlessly this is not what we want5 What we want is to fully tune the
whole thing
33 Multilayer Perceptron
The basic idea of multilayer perceptron is to stack a large number of those feature
extraction layers in Eq 38 between the input and the output This idea is as old as
the whole eld of neural network research dating back to early 1960s 89 However
it took many more years for people to gure out a way to tune the whole network both
f and s together See 91 and 70 if you are interested in the history
3 A great article about
this was posted recently in httpcolahgithubioposts
2014-03-NN-Manifolds-Topology
4 Some of the widely used nonlinearities are
 Sigmoid  x 
 Hyperbolic function tanhx  1exp2x
1exp2x
 Rectied linear unit rectx  max0x
1expx
5 And more importantly I will not accept any nal project proposal whose main model is based on the
331 Example Binary classication with a single hidden unit
Let us start with the simplest example The input x  R is a real-valued scalar and
the output y  01 is a binary value corresponding to the inputs label The feature
extractor  is dened as
 x   ux  c
where u and c are the parameters The function f returns the mean of the Bernoulli
conditional distribution pyx
  f x   w x  b
In both of these equations  is a sigmoid function
 x 
1  expx
We use the KL divergence to measure the distance between the true conditional
distribution pyx and the predicted conditional distribution pyx
KLpcid107 p  
y01
y01
pyxlog
pyx
pyxlog pyx pyxlog pyx
Note that the rst term in the summation pyxlog pyx can be safely ignored in our
case Why Because this does not concern p which is one we change in order to
minimize this KL divergence
Lets approximate this KL divergence with a single sample from pyx and leave
only the relevant part We will call this a per-sample cost
Cx  log pyx
 log y1 1y
 ylog   1 ylog1 
where  is from Eq 310
It is okay to work with this per-sample cost function
instead of the full cost function because the full cost function is almost always the
unweighted sum of these per-sample cost functions See Eq 23
We now need to compute the gradient of this cost function Cx with respect to all the
parameters w b u and c First lets start with w
 w 
 w 
which is a simple application of chain rule of derivatives Compare this to
 b 
 b 
In both equations   w x  b which is the input to f 
Both of these derivatives share Cx
   where
y  y    y
  y
cid48 
1 
cid48 
1 
 cid124cid123cid122cid125
cid48
cid48 
  y
1 
cid48    y
because the derivative of the sigmoid function  
  is
cid48  1 
Note that this corresponds to computing the difference between the correct label y and
the predicted label probability 
Given this output derivative Cx
   all we need to compute are
 w   x
 b  1
From these computations we see that
 w    y x
 b    y
Let us continue on to u and c We can again rewrite the derivatives wrt these into
 u 
 c 
 c 
where  is the input to  similarly to  was to the input to 
There are two things to notice here First we already have Cx
derivatives wrt w and b meaning there is no need to re-compute it Second  
shared between the derivatives wrt u and c
  from computing the
 is
Therefore we rst compute  
 
cid124cid123cid122cid125
cid48
 wcid48  w x1  x
Next we compute
 u  x
 c  1
Now all the ingredients are there
 u   yw x1  xx
 c   yw x1  x
The most important lession to learn from here is that most of the computations
needed to get the derivatives in this seemingly complicated multilayered computational
graph multilayer perceptron are shared At the end of the day the amount of compu-
tation needed to compute the gradient of the cost function wrt all the parameters in
the network is only as expensive as computing the cost function itself
332 Example Binary classication with more than one hidden
Let us try to generalize this simple or rather simplest model into a slightly more
general setting We will still look at the binary classication but with multiple hidden
units and a multidimensional input such that
 x  Ux  c
where U  Rld and c  Rl Consequently w will be a l-dimensional vector
The output derivative Cx
  stays same as before See Eq 315 However we
note that the derivative of  with respect to w should now differ because its a vector6
Lets look at what this means
The  can be expressed as
  wcid62 x  b 
wiix  b
In this case we can start computing the derivative with respect to each element of wi
separately
 ix
6 The Matrix Cookbook 85 is a good reference for this section
and will put them into a vector
cid20  
 w 
    
cid21cid62
cid62
 1x2x    lx
  x
Then the derivative of the cost function Cy with respect to w can be written as
 w    y x
Now lets look at Cy
in which case nothing really changed from the case of a single hidden unit in Eq 316
  Again because  x is now a vector there has to be some
  In fact the
 w due to the symmetry
changes Because Cy
procedure for computing this is identical to that for computing  
in Eq 318 That is
  is already computed we only need to look at  
  w
Next what about 
  Because the nonlinear activation function  is applied
element-wise we can simply compute this derivative for each element in  x such
cid16cid2cid48
  diag
1xcid48
2x    cid48
l xcid3cid62cid17
where diag returns a diagonal matrix of the input vector In short we will denote this
as cid48
Overall so far we have got
    ywcid62cid48x    ywcid12 diagcid48x
where cid12 is an element-wise multiplication
Now it is time to compute 
Ucid62x
U  x
according to the Matrix Cookbook 85 Then lets look at the whole derivative wrt
U    ywcid12 diagcid48xxcid62
Note that all the vectors in this lecture note are column vectors
For c its straightforward since
 c  1
34 Automating Backpropagation
This procedure presented as two examples is called a backpropagation algorithm If
you read textbooks on neural networks you see a fancier way to explain this back-
propagation algorithm by introducing a lot of fancy terms such as local error  and
so on But personally I nd it much easier to understand backpropagation as a clever
application of the chain rule of derivatives to a directed acyclic graph DAG in which
each node computes a certain function  using the output of the previous nodes I will
refer to this DAG as a computational graph from here on
Figure 32 a A graphical representation of the computational graph of the example
network from Sec 332 b A graphical illustration of a function node  forward
pass  backward pass
A typical computational graph looks like the one in Fig 32 a This computational
graph has two types of nodes 1 function node cid13 and 2 variable node 2 There
are four different types of function nodes 1 MatMulAB  AB 2 MatSumAB 
AB 3  element-wise sigmoid function and 4 Cy cost node The variables nodes
correspond to either parameters or data x and y Each function node has a number
associated with it to distinguish between the nodes of the same function
Now in this computational graph let us start computing the gradient using the
 y and Cy
 1 
 MatSum1 and multiply it
backpropagation algorithm We start from the last code Cy by computing Cy
Then the function node  1 will compute its own derivative
with Cy
 1 passed back from the function node Cy So far weve computed
 1
 MatSum1 
 1
 1
 MatSum1
The function node MatSum1 has two inputs b and the output of MatMul1 Thus
 MatMul1  Each of these is multiplied
 MatSum1 from Eq 319 At this point we already
this node computes two derivatives  MatSum1
with the backpropagated derivative
have the derivative of the cost function Cy wrt one of the parameters b
and  MatSum1
 b 
 MatSum1
 MatSum1
111222This process continues mechanically until the very beginning of the graph a set
of root variable nodes is reached All we need in this process of backpropagating the
derivatives is that each function node implements both forward computation as well
as backward computation In the backward computation the function node received
the derivative from the next function node evaluates its own derivative with respect to
the inputs at the point of the forward activation and passes theses derivatives to the
corresponding previous nodes See Fig 32 b for the graphical illustration
Importantly the inner mechanism of a function node does not change depending on
its context or equivalently where the node is placed in a computational graph In other
words if each type of function nodes is implemented in advance it becomes trivial to
build a complicated neural network including multilayer perceptrons and compute
the gradient of the cost function which is one such function node in the graph with
respect to all the parameters as well as all the inputs
This is a special case called the reverse mode of automatic differentiation7 It
is probably the most valuable tool in deep learning and fortunately many widely used
toolkits such as Theano 10 4 have implemented this reverse mode of automatic differ-
entiation with an extensive number of function nodes used in deep learning everyday
Before nishing this discussion on automating backpropagation Id like you to
think of pushing this even further For instance you can think of each function node
returning not its numerical derivative on its backward pass but a computational sub-
graph computing its derivative This means that it will return a computational graph
of gradient where the output is the derivatives of all the variable nodes or a subset
of them Then we can use the same facility to compute the second-order derivatives
341 What if a Function is not Differentiable
From the description so far one thing we notice is that backpropagation works only
when each and every function node in a computational graph is differentiable In
other words the nonlinear activation function must be chosen such that almost every-
where it is differentiable All three activation functions I have presented so far have
this property
Logistic Functions A sigmoid function is dened as
and its derivative is
 x 
1  expx
cid48x   x1  x
A hyperbolic tangent function is
tanhx 
exp2x 1
exp2x  1
7 If anyones interested in digging more into the whole eld of automatic differentiation try to Google it
and youll nd tons of materials One such reference is 5
and its derivative is
cid18
tanhcid48x 
cid192
expx  expx
Piece-wise Linear Functions
81 46 earlier
I described a rectied linear unit rectier or ReLU
rectx  max0x
It is clear that this function is not strictly differentiable because of the discontinuity
at x  0 However the chance of the input to this rectier lands exactly at 0 has
zero probability meaning that we can forget about this extremely unlikely event The
derivative of the rectier in this case is
rectcid48x 
cid26 1
if x  0
if x  0
Although the rectier has become the most widely used nonlinearity especially
in deep learnings applications to computer vision8 there is a small issue with the
rectier That is for a half of the input space the derivative is zero meaning that the
error the output derivative from Eq 315 will be not well propagated through the
rectier function node
In 48 the rectier was extended to a maxout unit so as to avoid this issue of the
existence of zero-derivative region in the input to the rectier The maxout unit of rank
k is dened as
maxoutx1    xk  maxx1    xk
and its derivative as
 maxout
x1    xk 
cid26 1
if maxx1    xk  xi
0 otherwise
This means that the derivative is backpropagated only through one of the k inputs
Stochastic Variables These activation functions work well with the backpropagation
algorithm because they are differentiable almost everywhere in the input space How-
ever what happens if a function is non-differentiable at all One such example is a
binary stochastic node which is computed by
1 Compute p   x where x is the input to the function node
2 Consider p as a mean of a Bernoulli distribution ie Bp
3 Generate one sample s  01 from the Bernoulli distribution
4 Output s
8 Almost all the winning entries in ImageNet Large Scale Visual Recognition Challenges ILSVRC use a
convolutional neural network with rectiers See httpimage-netorgchallengesLSVRC
Clearly there is no derivative of this function node
Does it mean that were doomed in this case Fortunately no Although I will not
discuss about this any further in this course Bengio et al 7 provide an extensive list
of approaches we can take in order to compute the derivative of the stochastic function
Chapter 4
Recurrent Neural Networks and
Gated Recurrent Units
After the last lecture I hope that it has become clear how to build a multilayer percep-
tron Of course there are so many details that I did not mention but are extremely im-
portant in practice For instance how many layers of simple transformations Eq 38
should a multilayer perceptron have for a certain task How wide equiv dim0x
should each transformation be What other transformation layers are there What kind
of learning rate  see Eq 25 should we use How should we schedule this learning
rate over training Answers to many of these questions are unfortunately heavily task-
data- and model-dependent and I cannot provide any general answer to them
41 Recurrent Neural Networks
Instead I will move on to describing how we can build a neural network1 to handle
a variable length input Until now the input x was assumed to be either a scalar or
a vector of the xed number of dimensions From here on however we remove this
assumption of a xed size input and consider the case of having a variable length input
What do I mean by a variable length input A variable length input x is a sequence
where each input x has a different number of elements For instance the rst training
examples input x1 may consist of l1 elements such that
x1  x1
2    x1
Meanwhile another examples input xn may be a sequence of ln cid54 l1 elements
xn  xn
2    xn
Lets go back to very basic about dealing with these kinds of sequences Further-
more let us assume that each element xi is binary meaning that it is either 0 or 1 What
1 Now let me begin using a term neural network instead of a general function
would be the most natural way to write a function that returns the number of 1s in
an input sequence x  x1x2    xl My answer is to rst build a recursive function
called ADD1 shown in Alg 1 This function ADD1 will be called for each element of
the input x as in Alg 2
Algorithm 1 A function ADD1
s  0
function ADD1vs
if v  0 then return s
else return s  1
end function
Algorithm 2 A function ADD1
s  0
for i  12    l do s  ADD1xis
end for
There are two important components in this implementation First there is a mem-
ory s which counts the number of 1s in the input sequence x Second a single function
ADD1 is applied to each symbol in the sequence one at a time together with the mem-
ory s Thanks to these two properties our implementation of the function ADD1 can be
used with the input sequence of any length
Now let us generalize this idea of having a memory and a recursive function that
works over a variable length sequence One likely most general case of this idea is
a digital computer we use everyday A computer program is a sequence x of instruc-
tions xi A central processing unit CPU reads each instruction of this program and
manipulates its registers according to what the instruction says Manipulating registers
is often equivalent to manipulating any inputoutput IO device attached to the CPU
Once one instruction is executed the CPU moves on to the next instruction which will
be executed with the content of the registers from the previous step In other words
these registers work as a memory in this case s from Alg 2 and the execution of an
instruction by the CPU corresponds to a recursive function ADD1 from Alg 1
Both ADD1 and CPU are hard coded in the sense that they do what they have been
designed and manufactured to do Clearly this is not what we want because nobody
knows how to design a CPU or a recursive function for natural language understanding
which is our ultimate goal Instead what we want is to have a parametric recursive
function that is able to read a sequence of linguistic symbols and use a memory in
order to understand natural languages
To build this parametric recursive function2 that works on a variable-length input
sequence x  x1x2    xl we now know that there needs to be a memory We will
use one vector h  Rdh as this memory vector As is clear from Alg 1 this recursive
function takes as input both one input symbol xt and the memory vector h and it
2 In neural network research we call this function a recurrent neural network
returns the updated memory vector It often helps to time index the memory vector
as well such that the input to this function is ht1 the memory after processing the
previous symbol xt1 and we use ht to denote the memory vector returned by the
function This function is then
ht  f xt ht1
Now the big question is what kind of parametric form this recursive function f
takes We will follow the simple transformation layer from Eq 38 in which case we
f xt ht1  gW xt   Uht1
where  xt  is a function that transforms the input symbol often discrete into a d-
dimensional real-valued vector W  Rdhd and Udhdh are parameters of this function
A nonlinear activation function g can be any function but for now we will assume that
it is an element-wise nonlinear function such as tanh
411 Fixed-Size Output y
Because our goal is to approximate an underlying true function we now need to think
of how we use this recursive function to return an output y As with the case of variable-
length sequence input x y can only be either a xed-size output such as a category to
which the input x belongs or a variable-length sequence output Here let us discuss the
case of having a xed-size output y
The most natural approach is to use the last memory vector hl to produce the output
or more often output distribution Consider a task of binary classication where y is
either positive 1 or negative 0 in which case a Bernoulli distribution ts perfectly
A Bernoulli distribution is fully characterized by a single parameter  Hence
   vcid62hl
where v  Rdh is a weight vector and  is a sigmoid function
This now looks very much like the multilayer perceptron from Sec 33 The whole
function given an input sequence x computes
cid125
   vcid62 gW xl  UgW xl1  UgW xl2 gW x1  Uh0 
cid123cid122
cid124
a recurrence
where h0 is an initial memory state which can be simply set to an all-zero vector
The main difference is that the input is not given only to the rst simple trans-
formation layer but is given to all those transformation layers one at a time Also
each transformation layer shares the parameters W and U3 The rst two steps of the
3 Note that for brevity I have omitted bias vectors This should not matter much as having a bias vector
is equivalent to augmenting the input with a constant element whose value is xed at 1 Why Because
cid21
cid20 x
 Wx  b
Note that as I have declared before all vectors are column vectors
recurrence part a of Eq 42 are shown as a computational graph in Fig 41
Figure 41 Sample computational graph of the recurrence in Eq 42
As this is not any special computational graph the whole discussion on how to au-
tomate backpropagation computing the gradient of the cost function wrt the parame-
ters in Sec 34 applies to recurrent neural networks directly except for one potentially
confusing point
412 Multiple Child Nodes and Derivatives
It may be confusing how to handle those parameters that are shared across multiple
time steps W and U in Fig 41 In fact in the earlier section Sec 34 we did not
discuss about what to do when the output of one node is fed into multiple function
nodes Mathematically saying what do we do in the case of
c  g f1x f2x     fnx
g can be any function but let us look at two widely used cases
 Addition g f1x     fnx  n
 x 
 Multiplication g f1x     fnx  n
i12n
i1 fix
 g 
cid32
jcid54i
i1 fix
 x 
cid33
 x 
 g 
i12n
 x 
From these two cases we can see that in general
 x 
 g 
i12n
 x 
This means that when multiple derivatives are backpropagated into a single node the
node should rst sum them and multiply its summed derivative with its own derivative
What does this mean for the shared parameters of the recurrent neural network In
an equation
 MatSuml
 MatSuml
 MatMull
 MatMull
cid124
cid124
cid124
cid123cid122
cid123cid122
cid123cid122
cid125
cid125
cid125
cid124
cid124
 
cid123cid122
cid123cid122
cid125
cid125
cid124
 MatSuml
 MatSuml1
 MatSuml1
 MatMull1
 MatMull1
 MatSuml
 MatSuml
 MatSuml1
 MatSuml1
 MatSuml2
 MatSuml2
 MatMull2
 MatMull2
 MatSuml
cid123cid122
cid125
where the superscript l of each function node denotes the layer at which the function
node resides
Similarly to what weve observed in Sec 34 many derivatives are shared across
the terms inside the summation in Eq 43 This allows us to compute the derivative
of the cost function wrt the parameter W efciently by simply running the recurrent
neural network backward
413 Example Sentiment Analysis
There is a task in natural language processing called sentiment analysis As the name
suggests the goal of this task is to predict the sentiment of a given text This is de-
nitely one function that a human can do fairly well when you read a critiques review
of a movie you can easily tell whether the critique likes hates or is neutral to the
movie Also even without a star rating of a product on Amazon you can quite easily
tell whether a user like it by reading herhis review of the product
In this task an input sequence x is a given text and the xed-size output is its label
which is almost always one of positive negative or neutral Let us assume for now
that the input is a sequence of words where each word xi is represented as a so-called
one-hot vector4 In this case we can use
in Eq 41
 xt   xt
4 A one-hot vector is a way to represent a discrete symbol as a binary vector The one-hot vector vi of a
symbol i  V  12    V is
cid124 cid123cid122 cid125
vi  0    0
1i1
 1cid124cid123cid122cid125
cid124 cid123cid122 cid125
 0    0
i1V
cid62
Once the input sequence or paragraph in this specic example is read we get
the last memory state hl of the recurrent neural network We will afne-transform hl
followed by the softmax function to obtain the conditional distribution of the output
y  123 1 positive 2 neutral and 3 negative
cid62
  1 2 3
 softmaxVhl
where 1 2 and 3 are the probabilities of positive neural and negative See
Eq 35 for more details on the softmax function
Because this network returns a categorial distribution it is natural to use the cate-
gorical cross entropy as the cost function See Eq 36 A working example of this
sentiment analyzer based on recurrent neural networks will be introduced and discussed
during the lab session5
414 Variable-Length Output y x  y
Lets generalize what we have discussed so far to recurrent neural networks here In-
stead of a xed-size output y we will assume that the goal is to label each input symbol
resulting in the output sequence y  y1y2    yl of the same length as the input se-
quence x
What kind of applications can you think of that returns the output sequence as long
as the input sequence One of the most widely studied problems in natural language
processing is a problem of classifying each word in a sentence into one of part-of-
speech tags often called POS tagging see Sec 31 of 77 Unfortunately in my
personal opinion this is perhaps the least interesting problem of all time in natural
language understanding but perhaps the most well suited problem for this section
In its simplest form we can view this problem of POS tagging as classifying each
word in a sentence as one of noun verb adjective and others As an example given
the following input sentence x
the goal is to output
x  Childreneatsweetcandy
y  nounverbadjectivenoun
This task can be solved by a recurrent neural network from the preceding section
Sec 411 after a quite trivial modication Instead of waiting until the end of the
sentence to get the last memory state of the recurrent neural network we will use the
immediate memory state to predict the label at each time step t
At each time t we get the immediate memory state ht by
ht  f xt ht1
where f is from Eq 41 Instead of continuing on to processing the next word we
will rst predict the label of the t-th input word xt
5 For those eager to learn more see httpdeeplearningnettutoriallstmhtml in
advance of the lab session
This can be done by
cid62
t  t1 t2 t3 t4
 softmaxVht 
Four tis correspond to the probabilities of the four categories 1 noun 2 verb 3
adjective and 4 others
From this output distribution at time step t we can dene a per-step per-sample
cost function
Cxt     K
Ikytk
where K is the number of categories four in this case We discussed earlier in Eq 36
Naturally a per-sample cost function is dened as the sum of these per-step per-sample
cost functions
Cx    l
Ikytk
Incorporating the Output Structures This formulation of the cost function is equiv-
alent to maximizing the log-probability of the correct output sequence given an input
sequence where the conditional log-probability is dened as
cid124
log pyx 
cid124
cid123cid122
cid123cid122
Eq 47
Eq 48
log pytx1    xt 
cid125
cid125
This means that the network is predicting the label of the t-th input symbol using only
the input symbols read up to that point ie x1x2    xt
In other words this means that the recurrent neural network is not taking into ac-
count the structure of the output sequence For instance even without looking at the
input sequence in English it is well known that the probability of the next word being a
noun increases if the current word is an adjective6 This kind of structures in the output
are effectively ignored in this formulation
Why is this so in this formulation Because we have made an assumption that
the output symbols y1y2    yl are mutually independent conditioned on the input se-
quence This is clear from Eq 49 and the denition of the conditional independence
Y1 and Y2 are conditionally independent dependent on X
 pY1Y2X  pY1XpY2x
If the underlying true conditional distribution obeyed this assumption of condi-
tional independence there is no worry However this is a very strong assumption for
6 Okay this requires a more thorough analysis but for the sake of the argument which does not have to
do anything with actual POS tags lets believe that this is indeed the case
many of the tasks we run into apparently from the example of POS tagging Then
how can we exploit the structure in the output sequence
One simple way is to make a less strong assumption about the conditional proba-
bility of the output sequence y given x For instance we can assume that
log pyx 
log pyiyixi
where yi and xi denote all the output symbols before the i-th one and all the input
symbols up to the i-th one respectively
Now the question is how we can incorporate this into the existing formulation of
a recurrent neural network from Eq 45 It turned out that the answer is extremely
simple All we need to do is to compute the memory state of the recurrent neural
network based not only on the current input symbol xt and the previous memory state
ht1 but also on the previous output symbol yt1 such that
ht  f xt yt1ht1
Similarly to Eq 41 we can think of implementing f as
f xt yt1ht1  gWxxxt   Wyyyt1  Whht1
There are two questions naturally arising from this formulation First what do we
do when computing h1 This is equivalent to saying what yy0 is There are two
potential answers to this question
1 Fix yy0 to an all-zero vector
2 Consider yy0 as an additional parameter
In the latter case yy0 will be estimated together with all the other parameters such
as those weight matrices Wx Wy Wh and V
Inference The second question involves how to handle yt1 During training it is
quite straightforward as our cost function KL-divergence between the underlying
true distribution and the parametric conditional distribution pyx approximated by
Monte Carlo method says that we use the groundtruth value for yt1s
It is however not clear what we should do when we test the trained network because
then we are not given the groundtruth output sequence This process of nding an
output that maximizes the conditional log-probability is called inference7
y  argmax
log pyx
7 Okay I confess The term inference refers to a much larger class of problems even if we consider only
machine learning However let me simply use this term to refer to a task of nding the most likely output of
a function
The exact inference is quite straightforward One can simply evaluate log pyx for
every possible output sequence and choose the one with the highest conditional proba-
bility Unfortunately this is almost always intractable as the number of every possible
output sequence grows exponentially with respect to the length of the sequence
Y   Kl
where Y  K and l are the set of all possible output sequences the number of labels and
the length of the sequence respectively Thus this is necessary to resort to approximate
search over the set Y 
The most naive approach to approximate inference is a greedy one With the trained
model you predict the rst output symbol y1 based on the rst input symbol x1 by
selecting the category of the highest probability py1x1 Now given y1 x1 and x2
we compute py2x1x2y1 from which we select the next output symbol y2 with the
highest probability We continue this process iteratively until the last output symbol yl
is selected
This is greedy in the sense that any early choice with a high conditional probability
may turn out to be unlikely one due to extremely low conditional probabilities later on
It is highly related to the so-called garden path sentence problem To know more about
this read for instance Sec 324 of 77
It is possible to alleviate this issue by considering N  K best hypotheses of the
output sequence at each time step This procedure is called beam search and we will
discuss more about this in a later lecture on neural machine translation
42 Gated Recurrent Units
421 Making Simple Recurrent Neural Networks Realistic
Let us get back to the analogy we made in Sec 41 We compared a recurrent neural
network to how CPU works Executing a recurrent function f is equivalent to executing
one of the instructions on CPU and the memory state of the recurrent neural network is
equivalent to the registers of the CPU This analogy does sound plausible except that
it is not
In fact how a simple recurrent neural network works is far from being similar to
how CPU works I am now talking about how they are implemented in practice but
rather Im talking at the conceptual level What is it at the conceptual level that makes
the simple recurrent neural network unrealistic
An important observation we make about the simple recurrent neural network is
that it refreshes the whole memory state at each time step This is almost opposite to
how the registers on a CPU are maintained Each time an instruction is executed the
CPU does not clear up the whole registers and repopulate them Rather it works only
on a small number of registers All the other registers values are stored as they were
before the execution of the instruction
Lets try to write this procedure mathematically Each time based on the choice
of instruction to be executed a subset of the registers of a CPU or a subset of the
elements in the memory state of a recurrent neural network is selected This can be
written down as a binary vector u  01nh
cid26 0
if the registers value does not change
if the registers value will change
With this binary vector which I will call an update gate a new memory state or a
new register value at time t can be computed as a convex interpolation such that
ht  1 ucid12 ht1  ucid12 ht 
where cid12 is as usual an element-wise multiplication ht denotes a new memory state or
a new register value after executing the instruction at time t
Another unrealistic point about the simple recurrent neural network is that each
execution considers the whole registers It is almost impossible to imagine designing
an instruction on a CPU that requires to read the values of all the registers Instead
what almost always happens is that each instruction will consider only a small subset
of the registers which again we can use a binary vector to represent Let me call it a
reset gate r  01nh
cid26 0
if the registers value will not be used
if the registers value will be used
This reset gate can be multiplied to the register values before being used by the
instruction at time t8 If we use a recursive function f from Eq 41 it means that
ht  f xt rcid12 ht1  gW xt   Urcid12 ht1
Now let us put these two gates that are necessary to make the simple recurrent
neural network more realistic into one piece At each time step the candidate memory
state is computed based on a subset of the elements of the previous memory state
ht  gW xt   Urcid12 ht1
A new memory state is computed as a linear interpolation between the previous mem-
ory state and this candidate memory state using the update gate
ht  1 ucid12 ht1  ucid12 ht
See Fig 42 for the graphical illustration
422 Gated Recurrent Units
Now here goes a big question How are the update u and reset r gates computed
If we stick to our analogy to the CPU those gates must be pre-congured per
instruction Those binary gates are dependent on the instruction Again however this
8 It is important to note that this is not resetting the actual values of the registers but only the input to the
instructionrecursive function
Figure 42 A graphical illustration of a
gated recurrent unit 29
is not what we want to do in our case There is no set of predened instructions but the
execution of any instruction corresponds to computing a recurrent function based on the
input symbol and the memory state from the previous time step see eg Eq 41
Similarly to this what we want with the update and reset gates is that they are computed
by a function which depends on the input symbol and the previous memory state
This sounds like quite straightforward except that we dened the gates to be binary
This means that whatever the function we use to compute those gates the function will
be a discontinuous function with zero derivative almost everywhere except at the point
where a sharp transition from 0 to 1 happens We discussed the consequence of having
an activation function with zero derivative almost everywhere in Sec 341 and the
conclusion was that it becomes very difcult to compute the gradient of the cost func-
tion efciently and exactly with these discrete activation functions in a computational
One simple solution which turned out to be extremely efcient is to consider those
gates not as binary vectors but as real-valued coefcient vectors In other words we
redene the update and reset gates to be
u  01nh r  01nh 
This approach makes these gates leaky in the sense that they always allow some leak
of information through the gate
In the case of the reset gate rather than making a hard decision on which subset
of the registers or the elements of the memory state will be used it now decides how
much information from the previous memory state will be used The update gate on
the other hand now controls how much content in the memory state will be replaced
which is equivalent to saying that it controls how much information will be kept from
the previous memory state
Under this denition we can simply use a sigmoid function from Eq 311 to
compute these gates
r  Wr xt   Urht1
u  Wu xt   Uurcid12 ht1
where Wr Ur Wu and Uu are the additional parameters9 Since the sigmoid function
is differentiable everywhere we can use the backpropagation algorithm see Sec 34
9 Note that this is not the formulation available for computing the reset and update gates For instance
urhhxto compute the derivatives of the cost function with respect to these parameters and
estimate them together with all the other parameters
We call this recurrent activation function with the reset and update gates a gated
recurrent unit GRU and a recurrent neural network having this GRU as a gated re-
current network
423 Long Short-Term Memory
The gated recurrent unit GRU is highly motivated by a much earlier work on long
short-term memory LSTM units 5310 The LSTM was proposed in 1997 with
the goal of building a recurrent neural network that can learn long-term dependen-
cies across many number of timsteps which was deemed to be difcult to do so with a
simple recurrent neural network
Unlike the element-wise nonlinearity of the simple recurrent neural network and the
gated recurrent unit the LSTM explicitly separates the memory state ct and the output
ht The output is a small subset of the hidden memory state and only this subset of the
memory state is visibly exposed to any other part of the whole network
How does a recurrent neural network with LSTM units decide how much of the
memory state it will reveal As perhaps obvious at this point the LSTM uses a so-
called output gate o to achieve this goal Similarly to the reset and update gates of the
GRU the output gate is computed by
o   Wo xt   Uoht1
This output vector is multiplied to the memory state ct point-wise to result in the output
ht  ocid12 tanhct 
Updating the memory state ct closely resembles how it is updated in the GRU see
Eq 410 A major difference is that instead of using a single update gate the LSTM
uses two gates forget and input gates such that
ct  fcid12 ct1  icid12 ct 
where f  Rnh i  Rnh and ct are the forget gate input gate and the candidate memory
state respectively
The roles of those two gates are quite clear from their names The forget gate
decides how much information from the memory state will be forgotten while the
input gate controls how much informationa about the new input consisting of the input
one can use the following denitions of the reset and update gates
r  Wr xt   Urht1
u  Wu xt   Uuht1
which is more parallelizable than the original formulation from 29 This is because there is no more direct
dependency between r and u which makes it possible to compute them in parallel
10 Okay let me confess here I was not well aware of long short-term memory when I was designing the
gated recurrent unit together with Yoshua Bengio and Caglar Gulcehre in 2014
symbol and the previous output will be inputted to the memory They are computed
f  W f  xt   U f ht1
i  Wi xt   Uiht1
The candidate memory state is computed similarly to how it was done with the
GRU in Eq 411
ct  gWc xt   Ucht1
where g is often an element-wise tanh
All the additional parameters specic to the LSTMWoUoW f U f WiUiWc
and Uc are estimated together with all the other parameters Again every function
inside the LSTM is differentiable everywhere and we can use the backpropagation
algorithm to efcient compute the gradient of the cost function with respect to all the
parameters
Although I have described one formulation of the long short-term memory unit
here there are many other variants proposed over more than a decade since it was rst
proposed For instance the forget gate in Eq 412 was not present in the original
work 53 but was xed to 1 Gers et al 45 proposed the forget gate few years after
the LSTM was originally proposed and it turned out to be one of the most crucial
component in the LSTM For more variants of the LSTM I suggest you to read 49
43 Why not Rectiers
431 Rectiers Explode
Let us go back to the simple recurrent neural network which uses the simple transfor-
mation layer from Eq 41
f xt ht1  gW xt   Uht1
where g is an element-wise nonlinearity
One of the most widely used nonlinearities is a hyperbolic tangent function tanh
This is unlike the case in feedforward neural networks multilayer perceptrons where a
unbounded piecewise linear function such as a rectier and maxout has become stan-
dard In the case of feedforward neural networks you can safely assume that everyone
uses some kind of piecewise linear function as an activation function in the network
This has become pretty much standard since Krizhevsky et al 67 shocked the com-
puter vision research community by outperforming all the more traditional computer
vision teams in the ImageNet Large Scale Visual Recognition Challenge 201212
11 Interestingly based on the observation in 58 it seems like the plain LSTM with a forget gate and the
GRU seem to be close to the optimal gated unit we can nd
12 httpimage-netorgchallengesLSVRC2012resultshtml
The main difference between logistic functions tanh and sigmoid function and
piecewise linear functions rectiers and maxout is that the former is bounded from
both above and below while the latter is bounded only from below or in some cases
not bounded at all 5013
This unbounded nature of piece-wise linear functions makes it difcult for them to
be used in recurrent neural networks Why is this so
Let us consider the simplest case of unbounded element-wise nonlinearity a linear
function
ga  a
The hidden state after l symbols is
hl UUUU   W xl3  W xl2  W xl1  W xl
cid32 l2
cid33
lcid481
cid32 l1
cid33
cid32 lt
lcid481
cid124
lcid481
W x1 
W xt 
cid125
cid33
cid123cid122
W x2   UW xl1  W xl
where l is the length of the input sequence
Let us assume that
 U is a full rank matrix
 The input sequence is sparse l
 W xi  0 for all i
and consider Eq 414 a
t1 I xt cid540  c where c  O1
cid32ltcid48
lcid481
htcid48
cid33
W xtcid48
Now lets look at what happens to Eq 415 First the eigendecomposition of the
matrix U
U  QSQ1
where S is a diagonal matrix whose non-zero entries are eigenvalues Q is an orthogo-
nal matrix Then
13 A parametric rectier or PReLU is dened as
ltcid48
lcid481
U  QSltcid48Q1
cid26 x
if x  0
otherwise 
where a is a parameter to be estimated together with all the other parameters of a network
cid33
cid32ltcid48
lcid481
W xtcid48  diagSltcid48
cid124 cid123cid122 cid125
cid12 QQ1
W xtcid48 
where cid12 is an element-wise product
What happens if the largest eigenvalue emax  maxdiagS is larger than 1 the
norm of hl will explode ie cid107hlcid107   Furthermore due to the assumption that
W xtcid48  0 each element of hl will explode to innity as well The rate of growth is
exponentially with respect to the length of the input sequence meaning that even when
the input sequence is not too long the norm of the memory state grows quickly if emax
is reasonably larger than 1
This happens because the nonlinearity g is unbounded If g is bounded from both
above and below such as the case with tanh the norm of the memory state is also
bounded In the case of tanh  R  11
cid107hlcid107  dimhl
This is one reason why a logistic function such as tanh and  is most widely used
with recurrent neural networks compared to piecewise linear functions14 I will call
this recurrent neural network with tanh as an element-wise nonlinear function a simple
recurrent neural network
Is tanh a Blessing
Now the argument in the previous section may sound like tanh and  are the nonlinear
functions that one should use This seems quite convincing for recurrent neural net-
works and perhaps so for feedforward neural networks as well if the network is deep
enough
Here let me try to convince you otherwise by looking at how the norm of backprop-
agated derivative behaves Again this is much easier to see if we assume the following
 U is a full rank matrix
 The input sequence is sparse l
Similarly to Eq 414 let us consider a forward computational path until hl how-
t1 I xt cid540  c where c  O1
ever without assuming a linear activation function
hl  g Ug Ug Ug U    W xl3  W xl2  W xl1  W xl 
We will consider a subsequence of this process in which all the input symbols are 0
except for the rst symbol
hl1  gcid0Ugcid0Ucid0gcid0Uhl0  Wcid0xl01
cid1cid1cid1cid1cid1 
14 However it is not to say that piecewise linear functions are never used for recurrent neural networks
See for instance 69 6
It should be noted that as l approaches innity there will be at least one such sub-
sequence whose length also approaches innity due to the sparsity of the input we
assumed
From this equation lets look at
cid0xl01
cid1 
This measures the effect of the l0 1-th input symbol xl01 on the l1-th memory state
of the simple recurrent neural network This is also the crucial derivative that needs to
be computed in order to compute the gradient of the cost function using the automated
backpropagation procedure described in Sec 34
This derivative can be rewritten as
Among these three terms in the left hand side we will focus on the rst one a which
can be further expanded as
cid0xl01

cid124 cid123cid122 cid125
hl11
cid1 
hl01
hl01
hl01
cid124 cid123cid122 cid125
hl11
cid124 cid123cid122 cid125
hl11
hl11
hl12
cid124 cid123cid122 cid125
cid1 
hl01
cid0xl01
hl02

cid124 cid123cid122 cid125
hl02
hl1
cid124cid123cid122cid125
 
hl02
hl01
cid124 cid123cid122 cid125
hl01
Because this is a recurrent neural network we can see that the analytical forms for
the terms grouped by the parentheses in the above equation are identical except for the
subscripts indicating the time index In other words we can simply only on one of
those groups and the resulting analytical form will be generally applicable to all the
other groups
First we look at Eq 416 b which is nothing but a derivative of a nonlinear
activation function used in this simple recurrent neural network The derivatives of the
widely used logistic functions are
cid48x  x1  x
tanhcid48x 1 tanh2x
as described earlier in Sec 341 Both of these functions derivatives are bounded
In the simplest case in which g is a linear function ie x  gx we do not even
need to look at
from Eq 416
0  cid48x  025
0  tanhcid48x  1
cid13cid13cid13 ht
cid13cid13cid13 We simply ignore all the ht
Next consider Eq 416 c In this case of simple recurrent neural network we
notice that we have already learned how to compute this derivative earlier in Sec 332
From these two we get
hl01
ht1
cid19cid18hl11
cid18hl1
cid18ht
cid19
hl11
cid19
cid18hl02
hl02
cid19
Do you see how similar it looks like Eq 415 If the recurrent activation function
f is linear this whole term reduces to
hl01
 Ul1l01
which according to Sec 431 will explode as l   if
emax  1
where emax is the largest eigenvalue of U When emax  1 it will vanish ie cid107 hl1
hl01
0 exponentially fast
What if the recurrent activation function f is not linear at all Lets look at ht
cid107

cid124
f cid48
f cid48
cid18
f cid48
cid19
cid123cid122
cid17
cid16 ht
cid18ht
cid0QSQ1cid1 

cid125
cid19
cid12 S
where we used the eigendecomposition of U  QSQ1 This can be re-written into
This means that the eigenvalue of U will be scaled by the derivative of the recurrent ac-
tivation function at each timestep In this case we can bound the maximum eigenvalue
of ht
max   emax
where  is the upperbound on gcid48  ht
 See Eqs 417418 for the upperbounds of
the sigmoid and hyperbolic tangent functions
In other words if the largest eigenvalue of U is larger than 1
  it is likely that this
temporal derivative of hl1 with respect to hl01 will explode meaning that its norm will
grow exponentially large In the opposite case of emax  1
  the norm of the temporal
derivative likely shrinks toward 0 The former case is referred to as exploding gradient
and the latter vanishing gradient These cases were studied already at the very early
years of research in recurrent neural networks 9 52
Using tanh is a blessing in recurrent neural networks when running the network
forward as I described in the previous section This is however not necessarily true in
the case of backpropagating derivaties Especially because there is a higher chance of
vanishing gradient with tanh or even worse with  Why Because 1
  1 for almost
everywhere
433 Are We Doomed
Exploding Gradient Fortunately it turned out that the phenomenon of exploding
gradient is quite easy to address First it is straightforward to detect whether the ex-
ploding gradient happened by inspecting the norm of the gradient fo the cost with
respect to the parameterscid13cid13 Ccid13cid13 If the gradients norm is larger than some predened
threhold   0 we can simply renormalize the norm of the gradient to be  Otherwise
we leave it as it is
In mathematics
cid40
 
 cid107cid107 
if cid107cid107  
otherwise
 is a rescaled gradient update
where we used the shorthand notiation  for  C
direction which will be used by the stochastic gradient descent SGD algorithm from
Sec 222 This algorithm is referred to as gradient clipping 83
Vanishing Gradient What about vanishing gradient But rst what does vanishing
gradient mean We need to understand the meaning of this phenomenon in order to tell
whether this is a problem at all from the beginning
Let us consider a case the variable-length output where x  y from Sec 414
Lets assume that there exists a clear dependency between the output label yt and the
input symbol xtcid48 where tcid48 cid28 t This means that the empirical cost will decrease when
the weights are adjusted such that
log pyt  y
t      xtcid48   
is maximized where y
has great inuence on the t-th output yt and the inuence can be measured by
t is the ground truth output label at time t The value of  xtcid48
 log pyt  y
 xtcid48
Instead of exactly computing  log pyt y
t 
 xtcid48 
t    
 we can approximate it by the nite
difference method Let   Rdim xtcid48  be a vector of which each element is a very
small real value   0 Then
t    
 log pyt  y
 xtcid48
log pyt  y
 log pyt  y
t      xtcid48     
t      xtcid48     cid11 
where cid11 is an element-wise division This shows that  log pyt y
computes the
 xtcid48 
difference in the t-th output probability with respect to the change in the value of the
tcid48-th input
t 
In other words  log pyt y
 xtcid48 
directly reects the degree to which the t-th output
yt depends on the tcid48-th input xtcid48 according to the network To put it in another way
 log pyt y
reects how much dependency the recurrent neural network has cap-
 xtcid48 
tured the dependency between yt and xtcid48
t 
t 
Lets rewrite
 log pyt  y
 xtcid48
t    
 log pyt  y
t    
ht1
cid124
cid123cid122
cid125
 htcid481
htcid48
htcid48
 xt 
The terms marked with a looks exactly identical to Eq 416 We have already seen
that this term can easily vanish toward zero with a high probability see Sec 432
This means that the recurrent neural network is unlikely to capture this dependency
This is especially true when the temporal distance between the output and input ie
t tcid48 cid29 0
The biggest issue with this vanishing behaviour is that there is no straightforward
way to avoid it We cannot tell whether  log pyt y
 0 is due to the lack of this
 xtcid48 
dependency in the true underlying function or due to the wrong conguration param-
eter setting of the recurrent neural network If we are certain that there are indeed
these long-term dependencies we may simultaneously minimize the following auxil-
iary term together with the cost function
t 
1
cid13cid13cid13  C
cid13cid13cid13  C
ht1
ht1
cid13cid13cid13
ht1
cid13cid13cid13
2
This term which was introduced in 83 is minimized when the norm of the derivative
does not change as it is being backpropagated effectively forcing the gradient not to
vanish
This term however was found to help signicantly only when the target task or the
underlying function does indeed exhibit long-term dependencies How can we know
in advance Pascanu et al 83 showed this with the well-known toy tasks which were
specically designed to exhibit long-term dependencies 52
434 Gated Recurrent Units Address Vanishing Gradient
Will the same problems of vanishing gradient happen with the gated recurrent units
GRU or the long short-term memory units LSTM Let us write the memory state at
time t
ht ut cid12 ht  1 ut cid12cid0ut1 cid12 ht1  1 ut1cid12cid0ut2 cid12 ht2  1 ut2cid12  cid1cid1
ut cid12 ht  1 ut cid12 ut1 cid12 ht1  1 ut cid12 1 ut1cid12 ut2 cid12 ht2 
Lets be more specic and see what happens to this with respect to xtcid48
ht ut cid12 ht  1 ut cid12 ut1 cid12 ht1  1 ut cid12 1 ut1cid12 ut2 cid12 ht2 
cid32
cid124
kttcid481
cid33
1 uk
cid123cid122
cid12tanh W xtcid48  U rtcid48 cid12 htcid481 
cid12 utcid48
cid125
where  is for element-wise multiplication
What this implies is that the GRU effectively introduces a shortcut from time tcid48 to
t The change in xtcid48 will directly inuence the value of ht and subsequently the t-th
output symbol yt In other words all the issue with the simple recurrent neural network
we discussed earlier in Sec 433
The update gate controls the strength of these shortcuts Lets assume for now that
the update gate is xed to some predened value between 0 and 1 This effectively
makes the GRU a leaky integration unit 6 However as it is perhaps clear from
Eq 419 that we will inevitably run into an issue Why is this so
Lets say we are sure that there are many long-term dependencies in the data It is
natural to choose a large coefcient for the leaky integration unit meaning the update
gate is close to 1 This will denitely help carrying the dependency across many time
steps but this inevitably carries unnecessary information as well This means that
much of the representational power of the output function goutht  is wasted in ignoring
those unnecessary information
If the update gate is xed to something substantially smaller than 1 all the shortcuts
see Eq 419 a will exponentially vanish Why Because it is a repeated multipli-
cation of a scalar small than 1 In other words it does not really help to have a leaky
integration unit in the place of a simple tanh unit
This is however not the case with the actual GRU or LSTM because those update
gates are not xed but are adaptive with respect to the input If the network detects
that there is an important dependency being captured the update gate will be closed
u j  0 This will effectively strengthen the shortcut connection see Eq 419 a
When the network detects that there is no dependency anymore it will open the update
gate u j  1 which effectively cuts off the shortcut How does the network know or
detect the existence or lack of these dependencies Do we need to manually code this
up I will leave these questions for you to gure out
Chapter 5
Neural Language Models
51 Language Modeling First Step
What does it mean for a machine to understand natural language In other words how
can we tell that the machine understood natural language These are the two equivalent
questions that are at the core of this course
One of the most basic capability of a machine that can signal us that it indeed
understands natural language is for the machine to tell how likely a given sentence
is Of course this is extremely ill-dened as we probably cannot dene the likeliness
of a sentence because there are many different types of unlikeliness For instance
a sentence Colorless green ideas sleep furiously from Chomskys 32 is unlikely
according to our common sense because
1 An object idea cannot be both colorless and green
2 An object cannot sleep furiously
3 An idea does not sleep
On the other hand this sentence is a grammatically correct sentence
Lets take a look at another sentence Jane and me went to see a movie yesterday
Grammatically this is not the most correct sentence one can make It should be Jane
and I went to see a movie yesterday Even with a grammatical error in the original
sentence however the meaning of the sentence is clear to me and perhaps is much more
understandable than the sentence colorless green ideas sleep furiously Furthermore
many people likely say this saying me instead of I quite often This sentence is
thus likely according to our common sense but is not likely according to the grammar
This observation makes us wonder what is the criterion to use Is it correct for a
machine to tell whether the sentence is likely by analyzing its grammatical correctness
Or is it possible that the machine should deem a sentence likely only when its meaning
agrees well with common sense regardless of its grammatical correctness in the most
strict sense
As we discussed in the rst lecture of the course we are more interested in ap-
proaching natural language as a means for one to communicate ideas to a listener In
this sense language use is a function which takes as input the surrounding environ-
ment including the others speech and returns linguistic response and this function
is not given but learned via observing others use of language and the reinforcement
by the surrounding environment 97 Also throughout this course we are not con-
cerned too much about the existing syntactic or grammatical structures underlying
natural language which makes it difcult for us to say anything about the grammatical
correctness of a given sentence
In short we take the route here that the likeliness of a sentence be determined
based on its agreement with common sense The common sense here is captured by
everyday use of natural language which consequently implies that the statistics of
natural language use can be a strong indicator for determining the likely of a natural
language sentence
511 What if those linguistic structures do exist
Of course as we discussed earlier in Sec 11 and in this section not everyone agrees
This is due to the fact that a perfect grammatical sentence may be considered unlikely
just because it does not happen often In other words statistical approaches to language
modeling may conclude that a sentence with perfectly valid grammatical construction
is unlikely Is this a problem
This problem of telling how likely a given sentence is can be viewed very naturally
as building a probabilistic model of sentences In other words given a sentence S what
is the probability pS of S Let us briey talk about what this means for the case of
viewing the likeliness of a sentence as equivalent to its grammatical correctness1
We rst assume that there is an underlying linguistic structure G which has gener-
ated the observed sentence S Of course we do not know the correct G in advance and
unfortunately no one will tell us what the correct G is2 Thus G is a hidden variable
in this case This hidden structure G generates the observed sentence S according to an
unknown conditional distribution pSG Each and every grammatical structure G is
assigned a prior probability which is also unknown in advance3
With the conditional distribution SG and the prior distribution G we easily get the
joint distribution SG by
pSG  pSGpG
from the denition of conditional probability4 From this joint distribution we get the
1 Why briey and why here Because we will not pursue this line at all after this section
2 Here the correct G means the G that generated S not the whole structure of G which is assumed to
exist according to a certain set of rules
3 This is not necessarily true If we believe that each and every grammatical correct sentence is equally
likely and that each correct grammatical structure generates a single corresponding sentence the prior dis-
tribution over the hidden linguistic structure is such that any correct structure is given an equal probability
while any incorrect structure is given a zero probability But of course if we think about it there are clearly
certain structures that are more prevalent and others that are not
4 A conditional probability of A given B is dened as
pAB 
distribution over a given sentence S by marginalizing out G
pS  
pSG
This means that we should compute how likely a given sentence S is with respect to all
possible underlying linguistic structure This is very likely intractable because there
must be innite possible such structures
Instead of computing pS exactly we can simply look at its lowerbound For in-
stance one simplest and probably not the best way to do so is
pS  
pSG  pS G
where G  argmaxG pSG  argmaxG pGS5
This lowerbound is tight ie pS  pS G when there is only a single true un-
derlying linguistic structure G given S What this says is that there is no other possible
linguistic structure possible for a single observed sentence ie no ambiguity in infer-
ring the correct linguistic structure In other words we can compute the probability or
likeliness of a given sentence by inferring its correct underlying linguistic structure
However there are a few issues here First it is not clear which formalism G
follows and we have briey discussed about this at the very beginning of this course
Second it is quite well known that most of the formalisms do indeed have uncertainty
in inference Again we looked at one particular example in Sec 112 These two
issues make many people including myself quite uneasy about this type of model-
based approaches
In the remaining of this chapter I will thus talk about model-free approaches as
opposed to these model-based approaches
512 Quick Note on Linguistic Units
Before continuing there is one question that must be bugging you or at least has
bugged me a lot what is the minimal linguistic unit
If we think about written text the minimal unit does seem like a character With
spoken language the minimal unit seems to be a phoneme But is this the level at
which we want to model the process of understanding natural language In fact to
most of the existing natural language processing researchers as well as some or most
linguists the answer to this question is a hard no
The main reason is that these low-level units both characters and phonemes do not
convey any meaning themselves Does a Latin alphabet q have its own meaning The
answer by most of the people will be no Then starting from this alphabet q how
far should we climb up in the hierarchy of linguistic units to reach a level at which the
unit begins to convey its own meaning qu does not seem to have its own meaning
still qui in French means who but in English it does not really say much quit
in English is a valid word that has its own meaning and similarly quiet is a valid
word that has its own meaning quite apart from that of quit
5 This inequality holds due to the denition of probability which states that pX  0 and X pX  1
It looks like a word is the level at which meaning begins to form itself However
this raises a follow-up question on the denition of a word What is a word
It is tempting to say that a sequence of non-blank characters is a word This makes
everyones life so much easier because we can simply split each sentence by a blank
space to get a sequence of words Unfortunately this is a very bad strategy The sim-
plest counter example to this denition of words is a token which I will use to refer to
a sequence of non-blank characters consisting of a word followed by a punctuation
If we simply split a sentence into words by blank spaces we will get a bunch of re-
dundant words For instance llama llama llama llama llama llama
and llama will all be distinct words We will run into an issue of exploding vocab-
ulary with any morphologically rich language Furthermore in some languages such
as Chinese there is no blank space at all inside a sentence in which case this simple
strategy will completely fail to give us any meaningful small linguistic unit other than
sentences
Now at this point it almost seems like the best strategy is to use each character
as a linguistic unit This is not necessarily true due to the highly nonlinear nature of
orthography6 There are many examples in which this nonlinear nature shows its dif-
culty One such example is to consider the following three words quite quiet
and quit7 All three character sequences have near identical forms but their corre-
sponding meanings differ from each other substantially In other words any function
that maps from these character sequences to the corresponding meanings will have to
be extremely nonlinear and thus difcult to be learned from data Of course this is an
area with active research and I hope I am not giving you an impression that characters
are not the units to use see eg 61
Now then the question is whether there is some middle ground between characters
and words or blank-space-separated tokens that are more suitable to be used as ele-
mentary linguistic units see eg 93 Unfortunately this is again an area with active
research Hopefully we will have time later in the course to discuss this issue further
For now we will simply use blank-space-separated tokens as our linguistic units
52 Statistical Language Modeling
Regardless of which linguistic unit we use any natural language sentence S can be
represented as a sequence of T discrete symbols such that
S  w1w2    wT 
Each symbol is one element from a vocabulary V which contains all possible symbols
V cid8v1v2    vVcid9 
where V is used to mean the size of the vocabulary or the number of all symbols
6 Orthography is dened as the study of spelling and how letters combine to represent sounds and form
words
7 I would like to thank Bart van Merrienboer for this example
The problem of language modeling is equivalent to nding a model that assigns a
probability pS to a sentence
pS  pw1w2    wT 
Of course we are not given this distribution and need to learn this from data
Lets say we are given data D which contains N sentences such that
D cid8S1S2    SNcid9 
where each sentence Sn is
2    wn
meaning that each sentence has a different length
Sn  wn
Given this data D let us estimate the probability of a certain sentence S This is
quite straightforward
where I is the indicator function dened earlier in Eq 37 which is dened as
n1 ISSn
cid26 1
ISSn 
if S  Sn
0 otherwise
This is equivalent to counting how many times S occurs in the data8
521 Data SparsityScarcity
Has this solved the whole problem of language model No unfortunately not The
very major issue here is that however large your corpus is it is unlikely to contain all
reasonable sentences in the world Lets do simple counting here
There are V symbols in a vocabulary Each sentence can be as long as T symbols
Then there are VT possible sentences A reasonable range for the sentence length T
is roughly between 1 to 50 meaning that there are
possible sentences As its quite clear this is a huge space of sentences
Of course not all those sentences are plausible This is however conceivable that
even the fraction of that space will be gigantic especially considering that the size of
vocabulary often goes up to 100k to 1M words Many of the plausible sentences will
not appear in the corpus Is this true In fact yes it is
It is quite easy to nd such an example For instance Google Books Ngram
Viewer9 lets you search for a sentence or a sequence of up to ve English words from
8 A data set consisting of written text is often referred to as a corpus
9 httpsbooksgooglecomngrams
Figure 51 A picture of a llama lying down From httpsenwikipedia
orgwikiLlama
the gigantic corpus of Google Books Let me try to search for a very plausible sen-
tence I like llama and the Google Books Ngram10 Viewer returns an error saying
that Ngrams not found I like llama see Fig 51 in the case you are not familiar
with a llama See Fig 52 as an evidence
Figure 52 A resulting page of Google Books Ngram Viewer for the query I like
llama
What does this mean for the estimate in Eq 52 It means that this estimator will
be too harsh for many of the plausible sentences that do not occur in the data As soon
as a given sentence does not appear exactly as it is in the corpus this estimator will
say that there is a zero probability of the given sentence Although the sentence I like
llama is a likely sentence according to this estimator in Eq 52 it will be deemed
extremely unlikely
This problem is due to the issue of data sparsity Data sparsity here refers to the
10 We will discuss what Ngrams are in the later sections
phenomenon where a training set does not cover the whole space of input sufciently
In more concrete terms most of the points in the input space which have non-zero
probabilities according to the true underlying distribution do not appear in the training
set If the size of a training set is assumed to be xed the severity of data sparsity
increases as the average or maximum length of the sentences This follows from the
fact that the size of the input space the set of all possible sentences grows with respect
to the maximum possible length of a sentence
In the next section we will discuss the most straightforward approach to addressing
this issue of data sparsity
n-Gram Language Model
The fact that the issue of data sparsity worsens as the maximum length of sentences
grows hints us a straightforward approach to addressing this limit the maximum length
of phrasessentences we estimate a probability on This idea is a foundation on which
a so-called n-gram language model is based
In the n-gram language model we rst rewrite the probability of a given sentence
S from Eq 51 into
pS  pw1w2    wT   pw1pw2w1 pwkwk
 pwTwT 
cid124
cid123cid122
cid125
where wk denotes all the symbols before the k-th symbol wk From this the n-
gram language model makes an important assumption that each conditional probability
Eq 53 a is only conditioned on the n 1 preceding symbols only meaning
pwkwk  pwkwknwkn1    wk1
This results in
pS  T
pwtwtn    wt1
What does this mean Under this assumption we are saying that any symbol in a
sentence is predictable based on the n 1 preceding symbols This is in fact a quite
reasonable assumption in many languages For instance let us consider a phrase I am
from Even without any more context information surrounding this phrase such as
surrounding words and the identity of a speaker we know that the word following this
phrase will be likely a name of place or country In other words the probability of a
name of place or country given the three preceding words I am from is higher than
that of any other words
But of course this assumption does not always work For instance consider a
 Let us focus on
phrase In Korea more than half of all the residents speak Korean
cid124 cid123cid122 cid125
the last word Korean marked with a We immediately see that it will be useful
to condition its conditional probability on the second word Korea Why is this so
Because the conditional probability of Korean following speak should signicantly
increase over all the other words that correspond to other languages knowing the fact
that the sentence is talking about the residents of Korea This requires the conditional
distribution to be conditioned on at least 10 words  is considered a separate word
and this certainly will not be captured by n-gram language model with n  9
From these examples it is clear that theres a natural trade-off between the quality
of probability estimate and statistical efciency based on the choice of n in n-gram
language modeling The higher n the longer context the conditional distribution has
leading to a better modelestimate second example however resulting in a situation
of more sever data sparsity see Sec 521 On the other hand the lower n leads to
the worse language modeling second example but this will avoid the issue of data
sparsity
n-gram Probability Estimation We can estimate the n-gram conditional probability
pwkwkn    wk1 from the training corpus Since it is a conditional probability we
need to rewrite it according to the denition of the conditional probability
pwkwkn    wk1 
pwkn    wk1wk
pwkn    wk1
This rewrite implies that the n-gram probability is equivalent to counting the occur-
rences of the n-gram wkn    wk among all n-grams starting with wkn    wk1
Let us consider the denominator rst The denominator can be computed by the
marginalizing the k-th word wcid48 below
pwkn    wk1  
wcid48V
pwkn    wk1wcid48
From Eq 52 we know how to estimate pwkn    wk1wcid48
pwkn    wk1wcid48  cwkn    wk1wcid48
where c is the number of occurrences of the given n-gram in the training corpus and
Nn is the number of all n-grams in the training corpus
Now lets plug Eq 56 into Eqs 5455
pwkwkn    wk1 
cwkn    wk1wk
Nn wcid48V cwkn    wk1wcid48
531 Smoothing and Back-Off
Note that I am missing many references this section as I am writing this on my travel
I will ll in missing references once Im back from my travel
The biggest issue of having an n-gram that never occurs in the training corpus is
that any sentence containing the n-gram will be given a zero probability regardless
of how likely all the other n-grams are Let us continue with the example of I like
llama With an n-gram language model built using all the books in Google Books the
following totally valid sentence11 will be given a zero probability
 I like llama which is a domesticated South American camelid12
Why is this so Because the probability of this sentence is given as a product of all
possible trigrams
pI like llama which is a domesticated South American camelid
pIplikeI pllamaIlike
 pcamelidSouthAmerican
cid125
cid124
cid123cid122
One may mistakenly believe that we can simply increase the size of corpus col-
lecting even more data to avoid this issue However remember that data sparsity is
almost always an issue in statistical modeling 24 which means that more data call
for better statistical models with often more parameters leading to the issue of data
sparsity
One way to alleviate this problem is to assign a small probability to all unseen
n-grams At least in this case we will assign some small non-zero probability to
any sentence thereby avoiding a valid but zero-probability sentence under the n-gram
language model One simplest implementation of this approach is to assume that each
and every n-gram occurs at least  times and any occurrence in the training corpus is
in addition to this background occurrence
In this case the estimate of an n-gram becomes
pwkwkn    wk1 
  cwknwkn1    wk
wcid48V   cwknwkn1    wcid48
V  wcid48V cwknwkn1    wcid48
  cwknwkn1    wk
where cwknwkn1    wk is the number of occurrences of the given n-gram in the
training corpus cwknwkn1    wcid48 is the number of occurrences of the given n-
gram if the last word wk is substituted with a word wcid48 from the vocabulary V   is often
set to be a scalar such that 0    1 See the difference from the original estimate in
Eq 57
It is quite easy to see that this is a quite horrible estimator how does it make sense
to say that every unseen n-gram occurs with the same frequency Also knowing that
this is a horrible approach what can we do about this
One possibility is to smooth the n-gram probability by interpolating between the
estimate of the n-gram probability in Eq 57 and the estimate of the n 1-gram
probability This can written down as
pSwkwkn    wk1  wkn    wk1pwkwkn    wk1
 1  wkn    wk1pSwkwkn1    wk1
11 This is not strictly true as I should put a in front of the llama
12 The description of a llama taken from Wikipedia httpsenwikipediaorgwikiLlama
This implies that the n-gram smoothed probability is computed recursively by the
lower-order n-gram probabilities This is clearly an effective strategy considering that
falling off to the lower-order n-grams contains at least some information of the original
n-gram unlike the previous approach of adding a scalar  to every possible n-gram
Now a big question here is how the interpolation coefcient  is computed The
simplest approach we can think of is to t it to the data as well However the situ-
ation is not that easy as using the same training corpus which was used to estimate
pwkwkn    wk1 according to Eq 57 will lead to a degenerate case What is
this degenerate case If the same corpus is used to t both the non-smoothed n-gram
probability and  s the optimal solution is to simply set all  s to 1 as that will assign
the high probabilities to all the n-grams Therefore one needs to use a separate corpus
to t  s
More generally we may rewrite Eq 58 as
pSwkwkn    wk1 
cid26 wkwkn    wk1 if cwkn    wk1wk  0
wkn1    wkpSwkwkn1    wk1 otherwise
following the notation introduced in 63 Specic choices of  and  lead to a number
of different smoothing techniques For an extensive list of these smoothing techniques
see 24
Before ending this section on smoothing techniques for n-gram language modeling
let me briey describe one of the most widely used smoothing technique called the
modied Kneser-Ney smoothing KN smoothing described in 24 This modied
KN smoothing is efciently implemented in the open-source software package called
KenLM 51
First let us dene some quantities We will use nk to denote the total number of
n-grams that occur exactly k times in the training corpus With this we dene the
following so-called discounting factors
n1  2n2
D1 1 2Y
D2 2 3Y
D3 3 4Y
Also let us dene the following quantities describing the number of all possible words
following a given n-gram with a specied frequency l
Nlwkn    wk1  cwkn    wk1wk  l
The modied KN smoothing then denes  in Eq 59 to be
wkwkn    wk1 
cwkn    wk1wk Dcwkn    wk1wk
wcid48V cwkn    wk1wcid48
where D is
And  is dened as
wkn    wk1 

if c  0
if c  1
if c  2
if c  3
D1N1wkn    wk1  D2N2wkn    wk1  D3N3wkn    wk1
wcid48V cwkn    wk1wcid48
For details on how this modied KN smoothing has been designed see 24
532 Lack of Generalization
Although n-gram language modelling works like a charm in many cases This is still
not totally satisfactory because of the lack of generalization What do I mean by
generalization here
Consider an example where three trigrams13 were observed from a training corpus
chases a cat chases a dog and chases a rabbit There is a clear pattern here The
pattern is that it is highly likely that chases a will be followed by an animal
How do we know this This is a trivial example of humans generalization abil-
ity We have noticed a higher-level concept in this case an animal from observing
words such as cat dog and rabbit and based on this concept we generalize this
knowledge that chases a is followed by an animal to unseen trigrams in the form of
chases a animal
This however does not happen with n-gram language model As an example lets
consider a trigram chases a llama Unless this specic trigram occurred more than
once in the training corpus the conditional probability given by n-gram language mod-
eling will be zero14 This issue is closely related to data sparsity but the main differ-
ence is that it is not the lack of data or n-grams but the lack of world knowledge In
other words there exist relevant n-grams in the training corpus but n-gram language
modelling is not able to exploit these
At this point it almost seems trivial to address this issue by incorporating existing
knowledge into language modelling For instance one can think of using a dictionary
to nd the denition of a word in interest continuing on from the previous example
the denition of llama and letting the language model notice that llama is a a
13 Is trigram a proper term Certainly not but it is widely accepted by the whole community of natural
language processing researchers Heres an interesting discussion on how n-grams should be referred to
as from 77 these alternatives are usually referred to as a bigram a trigram and a four-gram model
respectively Revealing this will surely be enough to cause any Classicists who are reading this book to stop
and to leave the eld to uneducated engineering sorts  with the declining levels of education in recent
decades  some people do make an attempt at appearing educated by saying quadgram 
14 Here we assume that no smoothing or backoff is used However even when these techniques are used
we cannot be satised since the probability assigned to this trigram will be at best reasonable up to the point
that the n-gram language model is giving as high probability as the bigram chases a In other words we do
not get any generalization based on the fact that a llama is an animal similar to a cat dog or rabbit
domesticated pack animal of the camel family found in the Andes valued for its soft
woolly eece Based on this the language model should gure out that the probability
of chases a llama should be similar to chases a cat chases a dog or chases a
rabbit because all cat dog and rabbit are animals according to the dictionary
This is however not satisfactory for us First those denitions are yet another
natural language text and letting the model understand it becomes equivalent to nat-
ural language understanding which is the end-goal of this whole course Second
a dictionary or any human-curated knowledge base is an inherently limited resource
These are limited in the sense that they are often static not changing rapidly to reect
the changes in language use and are often too generic potentially not capturing any
domain-specic knowledge
In the next section I will describe an approach purely based on statistics of natural
language that is able to alleviate this lack of generalization
54 Neural Language Model
One thing we notice from n-gram language modelling is that this boils down to com-
puting the conditional distribution of a next word wk given n  1 preceding words
In other words the goal of n-gram language modeling is to nd a
wkn    wk1
function that takes as input n 1 words and returns a conditional probability of a next
pwkwkn    wk1  f wk
 wkn    wk1
This is almost exactly what we have learned in Chapter 2
First we should dene the input to this language modelling function Clearly the
input will be a sequence of n 1 words but the question is how each of these words
will be represented Since our goal is to put the least amount of prior knowledge we
want to represent each word such that each and every word in the vocabulary is equi-
distant away from the others One encoding scheme that achieves this goal is 1-of-K
coding
In this 1-of-K coding scheme each word i in the vocabulary V is represented as a
binary vector wi whose sum equals 1 To denote the i-th word with the vector wi we
set the i-th element of the vector wi to be 1 and consequently all the other elements
are set to zero Mathematically
wi  00    
    0cid62  01V
1cid124cid123cid122cid125
i-th element
cid26 1
This kind of vector is often called a one-hot vector
It is easy to see that this encoding scheme perfectly ts our goal of having minimal
prior because
wi  w j 
if i cid54 j
otherwise
Now the input to our function is a sequence of n  1 such vectors which I will
denote by w1w2    wn1 As we will use a neural network as a function approx-
imator here15 these vectors will be multiplied with a weight matrix E After this we
get a sequence of continuous vectors p1p2    pn1 where
p j  Ecid62w j
and E  RVd
Before continuing to build this function let us see what it means to multiply the
transpose of a matrix with an one-hot vector from left Since only one of the elements
of the one-hot vector is non-zero all the rows of the matrix will be ignored except for
the row corresponding to the index of the non-zero element of the one-hot vector This
row is multiplied by 1 which simply gives us the same row as the result of this whole
matrixvector multiplication In short the multiplication of the transpose of a matrix
with an one-hot vector is equivalent to slicing out a single row from the matrix
In other words let
 

where ei  Rd Then
Ecid62wi  ei
This view has two consequences First in practice it will be much more efcient
computationally to implement this multiplication as a simple table look-up For in-
stance in Python with NumPy do
p  Ei
instead of
p  numpydotET wi
Second from this perspective we can see each row of the matrix E as a continuous-
space representation of a corresponding word ei will be a vector representation of the
i-th word in the vocabulary V  This representation is often called a word embedding
and should reect the underlying meaning of the word We will discuss this further
shortly
Closely following 8 we will simply concatenate the continuous-space represen-
tations of the input words such that
p cid2p1p2   pn1cid3cid62
15 Obviously this does not have to be true but at the end of the day it is unclear if there is any parametric
function approximation other than neural networks
This vector p is a representation of n 1 input words in a continuous vector space and
often referred to as a context vector
This context vector is fed through a composition of nonlinear feature extraction
layers We can for instance apply the simple transformation layer from Eq 38 such
h  tanhWp  b
where W and b are the parameters
Once a set of nonlinear layers has been applied to the context vector its time to
compute the output probability distribution In this case of language modelling the
distribution outputted by the function is a categorical distribution We discussed how
we can build a function to return a categorical distribution already in Sec 312
As a recap a categorical distribution denes a probability of one event happening
among K discrete events The probability of the k-th event happening is often denoted
as k and
k  1
Therefore the function needs to return a K-dimensional vector 1 2     K In this
case of language modelling K  V and i corresponds to the probability of the i-th
word in the vocabulary for the next word
As discussed earlier in Sec 312 we can use softmax to compute each of those
output probabilities
pwn  kw1w2    wn1  k 
k h  ck
expucid62
kcid481 expucid62
kcid48h  ckcid48
where uk  Rdimh
This whole function is called a neural language model See Fig 53 a for the
graphical illustration of neural language model
541 How does Neural Language Model Generalize to Unseen n-
Grams  Distributional Hypothesis
Now that we have described neural language model let us take a look into what hap-
pens inside Especially we will focus on how the model generalizes to unseen n-grams
The previously described neural language model can be thought of as a composite
of two function g  f  The rst stage f projects a sequence of context words or
preceding n 1 words to a continuous vector space
f  01Vn1  Rd
We will call the resulting vector h a context vector The second stage g maps this
continuous vector h to the target word probability by applying afne transformation to
the vector h followed by softmax normalization
Figure 53 a Schematics of neural language model
language model generalizes to an unseen n-gram
b Example of how neural
Let us look more closely at what g does in Eq 514 If we ignore the effect of
the bias ck for now we can clearly see that the probability of the k-th word in the
vocabulary is large when the output vector uk or the k-th row of the output matrix U
is well aligned with the context vector h In other words the probability of the next
word being the k-th word in the vocabulary is roughly proportional to the inner product
between the context vector h and the corresponding target word vector uk
Now let us consider two context vectors h j and hk These contexts are followed by
a similar set of words meaning that the conditional distributions of the next word are
similar to each other Although these distributions are dened over all possibility target
words let us look at the probabilities of only one of the target words wl
cid16
cid16
cid17
cid17
j pwlh j 
k pwlhk 
cid16
k is then16
wcid62
wcid62
cid17
wcid62
l h j  hk
The ratio between pl
j and pl
From this we can clearly see that in order for the ratio pl
to be 1 ie pl
wcid62
l h j  hk  0
16 Note that both pl
j and pl
k are positive due to our use of softmax
1-of-K codingContinuous-spaceWord RepresentationSoftmaxNonlinear projectionthreefourteamsgroupsNow let us assume that wl is not an all-zero vector as otherwise it will be too dull
a case In this case the way to achieve the equality in Eq 515 is to drive the context
vectors h j and hk to each other In other words the context vectors must be similar
to each other in terms of Euclidean distance in order to result in similar conditional
distributions of the next word
What does this mean This means that the neural language model must project
n 1-grams that are followed by the same word to nearby points in the context vec-
tor space while keeping the other n-grams away from that neighbourhood This is
necessary in order to give a similar probability to the same word If two n 1-grams
which are followed by the same word in the training corpus are projected to far away
points in the context vector space it naturally follows from this argument that the prob-
ability over the next word will differ substantially resulting in a bad language model
Let us consider an extreme example where we do bigram modeling with the train-
ing corpus comprising only three sentences
 There are three teams left for the qualication
 four teams have passed the rst round
 four groups are playing in the eld
We will focus on the bold-faced phrases three teams four teams and four group
The rst word of each of these bigrams is a context word and neural language model
is asked to compute the probability of the word following the context word
It is important to notice that neural language model must project three and four
to nearby points in the context space see Eq 513 This is because the context
vectors from these two words need to give a similar probability to the word teams
This naturally follows from our discussion earlier on how dot product preserves the
ordering in the space And from these two context vectors which are close to each
other the model assigns similar probabilities to teams and groups because they
occur in the training corpus In other words the target word vector uteams and ugroups
will also be similar to each other because otherwise the probability of teams given
four pteamsfour and groups given four pgroupsfour will be very differ-
ent despite the fact that they occurred equally likely in the training corpus
Now lets assume the case where we use the neural language model trained on
this tiny training corpus to assign a probability to an unseen bigram three groups
The neural language model will project the context word three to a point in the con-
text space close to the point of four From this context vector the neural language
model will have to assign a high probability to the word groups because the context
vector hthree and the target word vector ugroups well align Thereby even without ever
seeing the bigram three groups the neural language model can assign a reasonable
probability See Fig 53 b for graphical illustration
What this example shows is that neural language model automatically learns the
similarity among different context words via context vectors h and also among dif-
ferent target words via target word vectors uk by exploiting co-occurrences of words
In this example the neural language model learned that four and three are similar
from the fact that both of them occur together with teams Similarly in the target
side the neural language model was able to capture the similarity between teams
and groups by noticing that they both follow a common word four
This is a clear real-world demonstration of the so-called distributional hypothe-
sis Distributional hypothesis states that words which are similar in meaning appear
in similar distributional contexts 41 By observing which words a given word co-
occurs together it is possible to peek into the words underlying meaning Of course
this is only a partial picture17 into the underlying meaning of each word or as a mat-
ter of fact a phrase but surely still a very interesting property that is being naturally
exploited by neural language model
In neural language model the most direct way to observe the effect of this dis-
tributional hypothesisstructure is to investigate the rst layers weight matrix E in
Eq 512 This weight matrix can be considered as a set of dense vectors of the
words in the input vocabularycid8e1e2    eVcid9 and any visualization technique such
as principal component analysis PCA or t-SNE 104 can be used to project each
high-dimensional word vector into a lower-dimensional space often 2-D or 3-D
542 Continuous Bag-of-Words Language Model
Maximum PseudoLikelihood Approach
This is about time someone asks a question why we are only considering the preceding
words when doing language modelling Is it a good assumption that the conditional
distribution over a word is only dependent on preceding words
In fact we do not have to do so We can certainly model a natural language sentence
such that each word in a sentence is conditioned on 2n surrounding words n words to
the left and n words to the right In this case we get a Markov random eld MRF
language model 56
Figure 54 An example Markov random eld language model MRF-LM with the
order n  1
In a Markov random eld MRF language model MRF-LM we say each word in
a given sentence is a random variable wi We connect each word with its 2n surrounding
words with undirected edges and these edges represent the conditional dependency
structure of the whole MRF-LM An example of an MRF-LM with n  1 is shown in
Fig 54
A probability over a Markov random eld is dened as a product of clique po-
tentials A potential is dened for each clique as a positive function whose input is
the values of the random variables in the clique
In the case of MRF-LM we will
assign 1 as a potential to every clique except for cliques of two random variables in
17 We will discuss why this is only a partial picture later on
other words we only use pairwise potentials only The pairwise potential between the
words i and j is dened as
cid16
Ecid62wicid62Ecid62w jcid17
cid16
cid17
ecid62
 wiw j  exp
where E is from Eq 512 and wi is the one-hot vector of the i-th word One must
note that this is one possible implementation of the pairwise potential and there may be
other possibilities such as to replace the dot product between the word vectors ecid62
wiew j
with a deeper network
With this pairwise potential the probability over the whole sentence is dened as
pw1w2    wT  
 wt w j 
cid32Tn
cid33
ecid62
wt ew j
where Z is the normalization constant This normalization constant makes the product
of the potentials to be a probability and often is at the core of computational intractabil-
ity in Markov random elds
Figure 55 Gray nodes indicate the Markov blank of the fourth word
Although compute the full sentence probability is intractable in this MRF-LM it is
quite straightforward to compute the conditional probability of each word wi given all
the other words When computing the conditional probability we must rst notice that
the conditional probability of wi only depends on the values of other words included
in its Markov blanket In the case of Markov random elds the Markov blanket of
a random variable is dened as a set of all immediate neighbours and it implies that
the conditional probability of wi is dependent only on n preceding words and the n
following words See Fig 55 for an example
Keeping this in mind we can easily see that
pwiwin    wi1wi1    win 
Zcid48 exp
where Zcid48 is a normalization constant computed by
cid32
cid32 n
Zcid48  
ecid62
ewik 
cid33cid33
ewik 
cid32 n
cid32
ecid62
cid33cid33
Do you see a stark similarity to neural language model we discussed earlier This
conditional probability is a shallow neural network with a single linear hidden layer
Figure 56 Continuous Bag-of-Words model approximates the conditional distribution
over the j-th word w j under the MRF-LM
whose input are the context words n preceding and n following words and the output
is the conditional distribution of the center word wi We will talk about this shortly in
more depth See Fig 56 for graphical illustration
Now we know that it is often difcult to compute the full sentence probability
pw1    wT  due to the intractable normalization constant Z We however know how
to compute the conditional probabilities for all words quite tractably The former
fact implies that it is perhaps not the best idea to maximize log-likelihood to train this
model18 The latter however sheds a bit of light because we can train a model to
maximize pseudolikelihood 11 instead19
Pseudolikelihood of the MRF-LM is dened as
logPL 
log pwiwin    wi1wi1    win
Maximizing this pseudolikelihood is equivalent to training a neural network in Fig 56
which approximates each conditional distribution pwiwin    wi1wi1    win
to give a higher probability to the ground-truth center word in the training corpus
Unfortunately even after training the model by maximizing the pseudolikelihood
in Eq 516 we do not have a good way to compute the full sentence probability under
this model Under certain conditions maximizing pseudolikelihood indeed converges
to the maximum likelihood solution but this does not mean that we can use the product
of all the conditionals as a replacement of the full sentence probability However this
does not mean that we cannot use this MRF-LM as a language model since given
a xed model the pseudoprobability the product of all the conditionals can score
different sentences
18 However this is not to say maximum likelihood in this case is impossible There are different ways to
approximate the full sentence probability under this model See 56 for one such approach
19 See the note by Amir Globerson later modied by David Sontag available at httpcsnyu
edudsontagcoursesinference14slidespseudolikelihoodnotespdf
1-of-K codingSoftmaxContinuous Bag-of-WordsThis is in contrast to the neural language model we discussed earlier in Sec 54 In
the case of neural language model we were able to compute the probability of a given
sentence by computing the conditional probability of each word reading from left until
the end of the sentence This is perhaps one of the reasons why the MRF-LM is not
often used in practice as a language model Then you must ask why I even bothered
to explain this MRF-LM in the rst place
This approach which was proposed in 79 as a continuous bag-of-words CBoW
model20 was found to exhibit an interesting property That is the word embedding
matrix E learned as a part of this CBoW model very well reects underlying structures
of words and this has become one of the darling models by natural language processing
researchers in recent years We will discuss further in the next section
Skip-Gram and Implicit Matrix Factorization In 79 another model called skip-
gram is proposed The skip-gram model is built by ipping the continuous bag-of-
words model Instead of trying to predict the middle word given 2n surrounding words
the skip-gram model tries to predict randomly chosen one of the 2n surrounding words
given the middle word From this description alone it is quite clear that this skip-gram
model is not going to be great as a language model However it turned out that the
word vectors obtained by training a skip-gram model were as good as those obtained by
either a continuous bag-of-words model or any other neural language model Of course
it is debatable which criterion be used to determine the goodness of word vectors but in
many of the existing so-called intrinsic evaluations those obtained from a skip-gram
model have been shown to excel
The authors of 72 recently showed that training a skip-gram model with negative
sampling see 79 is equivalent to factorizing a positive point-wise mutual informa-
tion matrix PPMI into two lower-dimensional matrices The left lower-dimensional
matrix corresponds to the input word embedding matrix E in a skip-gram model In
other words training a skip-gram model implicitly factorizes a PPMI matrix
Their work drew a nice connection between the existing works on distributional
word representations from natural language processing or even computational linguis-
tics and these more recent neural approaches I will not go into any further detail in
this course but I encourage readers to read 72
543 Semi-Supervised Learning with Pretrained Word Embeddings
One thing I want to emphasize in these language models including n-gram language
model neural language model and continuous bag-of-words model is that they are
purely unsupervised meaning that all we need is a large corpus of unannotated text
This is one thing that makes this statistical approach to language modelling much more
appealing than any other approach based on linguistic structures see Sec 511 for a
brief discussion
20 One difference between the model we derived in this section starting from the MRF-LM and the one
proposed in 79 is that in our derivation the neural network shares a single weight matrix E for both the
input and output layers
When it comes to neural language model and continuous bag-of-words model we
now know that these networks learn continuous vector representations of input words
target words and the context phrase h from Eq 513 We also discussed how these
vector representations encode similarities among different linguistic units be it a word
or a phrase
What this implies is that once we train this type of language model on a large or
effectively innite21 corpus of unlabelled text we get good vectors for those linguistic
units for free Among these word vectors the rows of the input weight matrix E in
Eq 512 have been extensively used in many natural language processing applica-
tions in recent years since 103 33 79
Let us consider an extreme example of classifying each English word as either
positive or negative For instance happy is positive while sad is negative A
training set of 2 examples1 positive and 1 negative words is given How would one
build a classier22
There are two issues here First it is unclear how we should represent the input in
this case a word A good reader who has read this note so far will be clearly ready to
use an one-hot vector and use a softmax layer in the output and I commend you for
that However this still does not solve a more serious issue which is that we have only
two training examples All the word vectors save for two vectors corresponding to the
words in the training set will not be updated at all
One way to overcome these two issues is to make somewhat strong but reasonable
assumption that similar input will have similar sentiments This assumption is at the
heart of semi-supervised learning 23 It says that high-dimensional data points in
effect lies on a lower-dimensional manifold and the target values of the points on this
manifold change smoothly Under this assumption if we can well model this lower-
dimensional data manifold using unlabelled training examples we can train a good
classier23
And guess what We have access to this lower-dimensional manifold which is
represented by the set of pretrained word vectors E Believing that similar words have
similar sentiment and that these pretrained word vectors indeed well reect similarities
among words let me build a simple nearest neighbour NN classier which uses the
pretrained word vectors
cid26 positive
NNw 
if cosewehappy  cosewebad
otherwise
where cos is a cosine similarity dened as
negative
coseie j 
ecid62
cid107eicid107cid107e jcid107 
21 Why Because of almost universal broadband access to the Internet
22 Although the setting of 2 training examples is extreme but the task itself turned out to be not-so-
extreme In fact there is multiple dictionaries of words sentiment maintained For instance check http
sentiwordnetisticnritsearchphpqllama
23 What do I mean by a good classier A good classier is a classier that classies unseen test examples
well See Sec 23
This use of a term similarity almost makes this set of pretrained word vectors
look like some kind of magical wand that can solve everything24 This is however not
true and using pretrained word vectors must be done with caution
Why should we be careful in using these pretrained word vectors We must remem-
ber that these word vectors were obtained by training a neural network to maximize a
certain objective or to minimize a certain cost function This means that these word
vectors capture certain aspects of words underlying structures that are necessary to
achieve the training objective and that there is no reason for these word vectors to
capture any other properties of the words that are not necessary for maximizing the
training objective In other words similarity among multiple words has many dif-
ferent aspects and these word vectors will capture only a few of these many aspects
Which few aspects will be determined by the choice of training objective
The hope is that language modelling is a good training objective that will encourage
the word vectors to capture as many aspects of similarity as possible25 But is this true
in general
Lets consider an example of words describing emotions such as happy sad
and angry in the context of a continuous bag-of-words model These emotion-
describing words often follow some forms of a verb feel such as feel feels
felt and feeling This means that those emotion-describing words will have to be
projected nearby in the context space in order to give a high probability to those forms
of feel as a middle word This is understandable and agrees quite well with our in-
tuition All those emotion-describing words are similar to each other in the sense that
they all describe emotion But wait this aspect of similarity is not going to help sen-
timent classication of words In fact this aspect of similarity will hurt the sentiment
classier because a positive word happy will be close to negative words sad and
angry in this word vector space
The lesson here is that when you are solving a language-related task with very little
data it is a good idea to consider using a set of pretrained word vectors from neural
language models However you must do so in caution and perhaps try to pretrain your
own word vectors by training a neural network to maximize a certain objective that
better suits your nal task
But then what other training objectives are there We will get to that later
55 Recurrent Language Model
Neural language model indeed avoids the lack of generalization in the conventional n-
gram language modeling It still assumes the n-th order Markov property meaning that
it looks only as far back into the past as n 1 words In Sec 53 I gave an example of
In Korea more than half of all the residents speak Korean In this example the con-
ditional distribution over the last word in the sentence clearly will be better estimated
24 For future reference I must say there were many papers claiming that the pretrained word vectors are
indeed magic wands at three top-tier natural language processing conferences ACL EMNLP NAACL in
2014 and 2015
25 Some may ask how a single vector which is a point in a space can capture multiple aspects of similarity
This is possible because these word vectors are high-dimensional
Figure 57
network language model
a A recurrent neural network from Sec 414 b A recurrent neural
if it is conditioned on the second word of the sentence which is more than 10 words
back in the past
Let us recall what we learned in Sec 414 There we learn how to build a recurrent
neural network to read a variable-length sequence and return a variable-length output
sequence An example we considered back then was a task of part-of-speech tagging
where the input is a sentence such as
x  Childreneatsweetcandy
and the target output is a sequence of part-of-speech tags such as
y  nounverbadjectivenoun
In order to make less of an assumption on the conditional independence of the
predicted tags we made a small adjustment such that the prediction Yt at each timestep
was fed back into the recurrent neural network in the next timestep together with the
input Xt1 See Fig 57 a for graphical illustration
Why am I talking about this again after saying that the task of part-of-speech tag-
ging is not even going to be considered as a valid topic for the nal project Because
the very same model for part-of-speech tagging will be turned into the very recurrent
neural network language model in this section
Let us start by considering a single conditional distribution marked a below from
the full sentence probability
pw1w2    wT  
cid124
pwtw1    wt1
cid123cid122
cid125
This conditional probability can be approximated by a neural network as weve been
doing over and over again throughout this course that takes as input w1    wt1 and
returns the probability over all possible words in the vocabulary V  This is not unlike
neural language model we discussed earlier in Sec 54 except that the input is now a
variable-length sequence
Figure 58 A recurrent neural network language model
htcid48
In this case we can use a recurrent neural network which is capable of summariz-
ingmemorizing a variable-length input sequence A recurrent neural network summa-
rizes a given input sequence w1    wt1 into a memory state ht1
cid26 0
f ewtcid48 htcid481
where tcid48 runs from 0 to t  1
f is a recurrent function which can be any of a naive
transition function from Eq 41 a gated recurrent unit or a long short-term memory
unit from Sec 422 ewtcid48 is a word vector corresponding to the word wtcid48
This summary ht1 is afne-transformed followed by a softmax nonlinear function
to compute the conditional probability of wt Hopefully everyone remembers how it is
done As in Eq 46
if tcid48  0
otherwise 
  softmaxVht1
where  is a vector of probabilities of all the words in the vocabulary
One thing to notice here is that the iteration procedure in Eq 517 computes a
sequence of every memory state vector ht by simply reading the input sentence once
In other words we can let the recurrent neural network read one word wt at a time
update the memory state ht and compute the conditional probability of the next word
pwt1wt 
This procedure is illustrated in Fig 57 b26 This language model is called a
recurrent neural network language model RNN-LM 80
But wait from looking at Figs 57 ab there is a clear difference between
the recurrent neural networks for part-of-speech tagging and language model That is
there is no feedback connection from the output of the previous time step back into the
recurrent neural network in the RNN-LM This is simply an illusion from the limitation
in the graphical illustration because the input wt1 in the next time step is in fact the
output wt1 at the current time step This becomes clearer by drawing the same gure
in a slightly different way as in Fig 58
26 In the gure you should notice the beginning-of-the-sentence symbol cid104scid105 This is necessary in order to
use the very same recurrent function f to compute the conditional probability of the rst word in the input
sentence
56 How do n-gram language model neural language
model and RNN-LM compare
Now the question is which one of these language models we should use in practice In
order to answer this we must rst discuss the metric most commonly used for evaluat-
ing language models
The most commonly used metric is a perplexity In the context of language mod-
elling the perplexity PPL of a model M is computed by
n1 logb pM wnwn
PPL  b 1
where N is the number of all the words in the validationtest corpus and b is some
constant that is often 2 or 10 in practice
What is this perplexed metric I totally agree with you on this one Of course there
is a quite well principled way to explain what this perplexity is based on information
theory This is however not necessary for us to understand this metric called perplexity
As the exponential function with base b in the case of perplexity in Eq 518
is a monotonically increasing function we see that the ordering of different language
models based on the perplexity will not change even if we only consider the exponent
logb pM wnwn
Furthermore assuming that b  1 we can simply replace logb with log natural loga-
rithm without changing the order of different language models
log pM wnwn
Now this looks awfully similar to the cost function or negative log-likelihood we
minimize in order to train a neural network see Chapter 2
Lets take a look at a single term inside the summation above
log pM wnwn
This is simply measuring how high a probability the language model M is assigning to
a correct next word given all the previous words Again because log is a monotonically
increasing function
In summary the inverse perplexity measures how high a probability the language
model M assigns to correct next words in the testvalidation corpus on average There-
fore a better language model is the one with a lower perplexity There is nothing so
perplexing about the perplexity once we start viewing it from this perspective
We are now ready to compare different language models or to be more precise
three different classes of language modelscount-based n-gram language model neural
n-gram language model and recurrent neural network language model The biggest
challenge in doing so is that this comparison will depend on many factors that are not
easy to control To list a few of them
 Language
 GenreTopic of training validation and test corpora
 Size of a training corpus
 Size of a language model
Figure 59 The perplexity word error rate WER and character error rate CER of
an automatic speech recognition system using different language models Note that all
the results by neural or recurrent language models are by interpolating these models
with the count-based n-gram language model Reprinted from 100
Because of this difculty this kind of comparison has often been done in the con-
text of a specic downstream application This choice of a downstream application
often puts rough constraints on the size of available or commonly used corpus target
language and reasonably accepted size of language models For instance the authors
of 3 compared the conventional n-gram language model and neural language model
with various approximation techniques with machine translation as a nal task In
100 the authors compared all the three classes of language model in the context of
automatic speech recognition
First let us look at one observation made in 100 From Fig 59 we can see that
it is benecial to use a recurrent neural network language model RNN-LM compared
to a usual neural language model Especially when long short-term memory units were
Figure 510 The trend of perplexity as the size of language model changes Reprinted
from 100
used the improvement over the neural language model was signicant Furthermore
we see that it is possible to improve these language models by simply increasing their
Similarly in Fig 510 from the same paper 100 it is observed that larger language
models tend to get betterlower perplexity and that RNN-LM in general outperforms
neural language models
These two observations do seem to suggest that neural and recurrent language mod-
els are better candidates as language model However this is not to be taken as an
evidence for choosing neural or recurrent language models It has been numerously
observed over years that the best performance both in terms of perplexity and in terms
of performance in the downstream applications such as machine translation and auto-
matic speech recognition is achieved by combining a count-based n-gram language
model and a neural or recurrent language model See for instance 92
This superiority of combined or hybrid language model suggests that the count-
based or conventional n-gram language model neural language model and recurrent
neural network language model are capturing underlying structures of natural language
sentences that are complement to each other However it is not crystal clear how these
captured structures differ from each other
Chapter 6
Neural Machine Translation
Finally we have come to the point in this course where we discuss an actual natural
language task In this chapter we will discuss how translation from one language to
another can be done with statistical methods more specically neural networks
61 Statistical Approach to Machine Translation
Lets rst think of what it means to translate one sentence X in a source language to an
equivalent sentence Y in a target language which is different from the source language
A process of translation is a function that takes as input the source sentence X and
returns a correct translation Y  and it is clear that there may be more than one correct
translations The latter fact implies that this function of translation should return not a
single correct translation but a probability distribution that assigns high probabilities
to more than one likely translations
Now let us write it in a more formal way First the input is a sequence of words
where Tx is the length of the source sentence A target sentence is
X  x1x2    xTx 
Y  y1y2    yTy
Similarly Ty is the length of the target sentence
The translation function f then reads the input sequence X and computes the prob-
ability over target sentences In other words
f  V 
x  CVy1
is a set of all possible source sentences of any
where Vx is a source vocabulary and V 
length Tx  0 Vy is a target vocabulary and Ck is a standard k-simplex
What is a standard k-simplex It is a set dened by
cid12cid12cid12cid12cid12 k
cid40
cid41
t0    tk  Rk1
tk  1 and ti  0 for all i
Figure 61 Graphical illustration of statistical machine translation
In short this set contains all possible settings for categorical distributions of k  1
possible outcomes This means that the translation function f returns a probability
distribution PYX over all possible translations of length Ty  1
Given a source sentence X this translation function f returns the conditional prob-
ability of a translation Y  PYX Let us rewrite this conditional probability according
to what we have discussed in Chapter 5
PYX 
cid124
Pyty1    yt1
Xcid124cid123cid122cid125
conditional
cid125
cid123cid122
language modelling
Looking at it in this way it is clear that this is nothing but conditional language mod-
elling This means that we can use any of the techniques we have used earlier in
Chapter 5 for statistical machine translation
Training can be trivially done by maximizing the log-likelihood or equivalently
minimizing the negative log-likelihood see Sec 31
given a training set
C    1
log pyn
t X n
D cid8X 1Y 1 X 2Y 2     X NY Ncid9
consisting of N training pairs
All these look extremely straightforward and do not deviate too much from what we
have learned so far in this course A big picture on this process translation is shown in
Fig 61 More specically building a statistical machine translation model is simple
because we have learned how to
1 Assign a probability to a sentence in Sec 52
2 Handle variable-length sequences with recurrent neural networks in Sec 41
3 Compute the gradient of an empirical cost function C with respect to the param-
eters  of a recurrent neural network in Sec 412 and Sec 34
Corporaf  La croissance conomique sest ralentie ces dernires annes e  Economic growth has slowed down in recent years 4 Use stochastic gradient descent to minimize the cost function in Sec 222
Of course simply knowing all these does not get you a working neural network that
translates from one language to another We will discuss in detail how we can build
such a neural network in the next section Before going to the next section we must
rst discuss two issues 1 where do we get training data 2 how do we evaluate
machine translation systems
611 Parallel Corpora Training Data for Machine Translation
First let us consider again what the problem were trying to solve here It is machine
translation and from the description in the previous section and from Eqs 6162
it is a sentence-to-sentence translation task We approach this problem by building a
model that takes as input a source sentence S and computes the probability PYX of
a target sentence Y  equivalently a translation In order for this model to translate we
must train it with a training set of pairs of a source sentence and its correct translation
The very rst problem we run into is where we can nd this training set which is
often called a parallel corpus It is not easy to think of documents which have been
translated into multiple languages Lets take for instance all the books that are being
translated each year According to 86 approximately 3 of titles published each year
in English are translations from another language1 A few international news agencies
publish some of their news articles in multiple languages For instance AFP publishes
1500 stories in French 700 stories in English 400 stories in Spanish 250 stories in
Arabic 200 stories in German and 150 stories in Portuguese each day and there are
some overlapping stories across these six languages2 Online commerce sites such as
eBay often list their products in international sites with their descriptions in multiple
languages3
Unfortunately these sources of multiple languages of the same content are not suit-
able for our purpose Why is this so Most importantly they are often copy-righted
and sold for personal use only We cannot buy more than 14400 books in order to
train a translation model We will likely go broke before completing the purchase
and even if so it is unclear whether it is acceptable under copyright to use these text
to build a translation model Because we are mixing multiple sources of which each
is protected under copyright is the translation model trained from a mix of all these
materials considered a derivative work4
This issue is nothing new and has been there since the very rst statistical machine
translation system was proposed in 19 Fortunately it turned out that there are a
number of legitimate sources where we can get documents translated in more than
one languages often very faithfully to their content These sources are parliamentary
proceedings of bilingual or multilingual countries
1 According to the information Bowker released in October of 2005 in 2004 there were 375000 new
books published in English  Of that total approx 14440 were new translations which is slightly more
than 3 of all books published 86
2 httpwwwafpcomenproductsservicestext
3 httpsellercentreebaycoukinternational-selling-tools
4 httpcopyrightgovcircscirc14pdf
Brown et al 19 used the proceedings from the Canadian parliament which are by
law kept in both French and English All of these proceedings are digitally available
and called Hansards You can check it yourself online at httpwwwparlgc
ca and heres an excerpt from the Prayers of the 2nd Session 41st Parliament Issue
 French ELIZABETH DEUX par la Grace de Dieu REINE du Royaume-
Uni du Canada et de ses autres royaumes et territoires Chef du Commonwealth
Defenseur de la Foi
 English ELIZABETH THE SECOND by the Grace of God of the United
Kingdom Canada and Her other Realms and Territories QUEEN Head of the
Commonwealth Defender of the Faith
Every single word spoken in the Canadian parliament is translated either into French
or into English A more recent version of Hansards preprocessed for research can be
found at httpwwwisiedunatural-languagedownloadhansard
Similarly the European parliament used to provided the parliamentary proceedings
in all 23 ofcial languages6 This is a unique data in the sense that each and every
sentence is translated into either 11 or 26 ofcial languages For instance here is one
example 65
 Danish det er nsten en personlig rekord for mig dette efterar
 German das ist fur mich fast personlicher rekord in diesem herbst 
 Greek omitted
 English that is almost a personal record for me this autumn 
 Spanish es la mejor marca que he alcanzado este otono 
 Finnish se on melkein minun ennatykseni tana syksyna 
 French c  est pratiquement un record personnel pour moi  cet automne 
 Italian e  quasi il mio record personale dell  autunno 
 Dutch dit is haast een persoonlijk record deze herfst 
 Portuguese e quase o meu recorde pessoal deste semestre 
 Swedish det ar nastan personligt rekord for mig denna host 
The European proceedings has been an invaluable resource for machine translation
research At least the existing multilingual proceedings up to 2011 can be still used
and it is known in the eld as the Europarl corpus 65 and can be downloaded from
httpwwwstatmtorgeuroparl
These proceedings-based parallel corpora have two distinct advantages First in
many cases the sentences in those corpora are well-formed and their translations are
5 This is one political lesson here Canada is still headed by the Queen of the United Kingdom
6 Unfortunately the European parliament decided to stop translating its proceedings into all 23 of-
cial languages on 21 Nov 2011 as an effort toward budget cut See httpwwweuractivcom
cultureparliament-cuts-translation-budg-news-516201
done by professionals meaning the quality of the corpora is guaranteed Second sur-
prisingly the topics discussed in those proceedings are quite diverse Clearly the mem-
bers of the parliament do not often chitchat too often but they do discuss a diverse set
of topics Heres one such example from the Europarl corpus
 English Although there are now two Finnish channels and one Portuguese one
there is still no Dutch channel which is what I had requested because Dutch
people here like to be able to follow the news too when we are sent to this place
of exile every month
 French Il y a bien deux chanes nnoises et une chane portugaise mais il
ny a toujours aucune chane neerlandaise Pourtant je vous avais demande une
chane neerlandaise car les Neerlandais aussi desirent pouvoir suivre les actu-
alites chaque mois lorsquils sont envoyes en cette terre dexil
One apparent limitation is that these proceedings cover only a handful of languages
in the world mostly west European languages This is not desirable Why According
to Ethnologue 20147 the top-ve most spoken languages in the world are
1 Chinese approx 12 billion
2 Spanish approx 414 million
3 English approx 335 million
4 Hindi approx 260 million
5 Arabic approx 237 million
There are only two European languages in this list
So then where can we get all data for all these non-European languages There
are a number of resources you can use and let me list a few of them here
You can nd the translated subtitle of the TED talks at the Web Inventory of
Transcribed and Translated Talks WIT3 httpswit3fbkeu 22
a quite small corpus but includes 104 languages For RussianEnglish data Yandex
released a parallel corpus of one million sentence pairs You can get it at https
translateyandexrucorpuslangen You can continue with other
languages by googling very hard but eventually you run into a hard wall
This hard wall is not only the lack of any resource but also lack of enough resource
For instance I quickly googled for KoreanEnglish parallel corpora and found the
following resources
 SWRC English-Korean multilingual corpus 60000 sentence pairs http
semanticwebkaistackrhomeindexphpCorpus10
 Jungyeuls English-Korean parallel corpus 94123 sentence pairs https
githubcomjungyeulkorean-parallel-corpora
This is just not large enough
One way to avoid this or mitigate this problem is to automatically mine parallel
corpora from the Internet There have been quite some work in this direction as a way
7 httpwwwethnologuecomworld
to increase the size of parallel corpora 87 112 The idea is to build an algorithm that
crawls the Internet and nd a pair of corresponding pages in two different languages
One of the largest preprocessed corpus of multiple languages from the Internet is the
Common Crawl Parallel Corpus created by Smith et al 98 available at http
wwwstatmtorgwmt13training-parallel-commoncrawltgz
612 Automatic Evaluation Metric
Lets say we have trained a machine translation model on a training corpus A big
question follows how do we evaluate this model
In the case of classication evaluation is quite straightforward All we need to do is
to classify held-out test examples with a trained classier and see how many examples
were correctly classied This is however not true in the case of translation
There are a number of issues but let us discuss two most important problems here
First there may be many correct translations given a single source sentence For in-
stance the following three sentences are the translations made by a human translator
given a single Chinese sentence 82
 It is a guide to action that ensures that the military will forever heed Party com-
 It is the guiding principle which guarantees the military forces always being
under the command of the Party
 It is the practical guide for the army always to heed the directions of the party
They all clearly differ from each other although they are the translations of a single
source sentence
Second the quality of translation cannot be measured as either success or failure
It is rather a smooth measure between success and failure Let us consider an English
translation of a French sentence Jaime un llama qui est un animal mignon qui vit en
Amerique du Sud8
One possible English translation of this French sentence is I like a llama which is a
cute animal living in South America Lets give this translation a score 100 success
According to Google translate the French sentence above is I like a llama a cute
animal that lives in South America I see that Google translate has omitted qui est
from the original sentence but the whole meaning has well been captured Let us give
this translation a slightly lower score of 90
Then how about I like a llama from South America This is certainly not a
correct translation but except for the part about a llama being cute this sentence does
communicate most of what the original French sentence tried to communicate Maybe
we can give this translation a score of 50
How about I do not like a llama which is an animal from South America This
translation correctly describes the characteristics of llama exactly as described in the
source sentence However this translation incorrectly states that I do not like a llama
when I like a llama according to the original French sentence What kind of score
would you give this translation
8 I would like to thank Laurent Dinh for the French translation
Even worse we want an automated evaluation algorithm We cannot look at thou-
sands of validation or test sentence pairs to tell how well a machine translation model
does Even if we somehow did it for a single model in order to compare this translation
model against others we must do it for every single machine translation model under
comparison We must have an automatic evaluation metric in order to efciently test
and compare different machine translation models
BLEU One of the most widely used automatic evaluation metric for assessing the
quality of translations is BLEU proposed in 82 BLEU computes the geometric mean
of the modied n-gram precision scores multiplied by brevity penalty Let me describe
this in detail here
First we dene the modied n-gram precision pn of a translation Y as
SC ngramS cngram
SC ngramS cngram
where C is a corpus of all the sentencestranslations and S is a set of all unique n-grams
in one sentence in C cngram is the count of the n-gram and cngram is
cngram  mincngramcrefngram
crefngram is the count of the n-gram in reference sentences
What does this modied n-gram precision measure It measures the ratio between
the number of n-grams in the translation and the number of those n-grams actually
occurred in a reference ground-truth translation If there is no n-gram from the trans-
lation in the reference this modied precision will be zero because cref will be zero
all the time
It is common to use the geometric average of modied 1- 2- 3- and 4-gram preci-
sions which is computed by
cid33
cid32
1  exp
If we use this geometric average P as it is there is a big loophole One can get
a high average modied precision by making as short a translation as possible For
instance a reference translation is
 I like a llama a cute animal that lives in South America 
and a translation we are trying to evaluate is
 cute animal that lives
This is clearly a very bad translation but the modied 1- 2- 3- and 4-gram precisions
will be high The modied precisions are
1  1  1  1
1  1  1  1
1  1  1
1  1  1
cid19
0  0  0  0
Their geometric average is then
1  exp
cid181
which is the maximum modied precision you can get
In order to avoid this behaviour BLEU penalizes the geometric average of the
modied n-gram precisions by the ratio of the lengths between the reference r and
translation l This is done by rst computing a brevity penalty
cid26 1
expcid01 r
cid1
 if l  r
 if l  r
If the translation is longer than the reference it uses the geometric average of the
modied n-gram precisions as it is Otherwise it will penalize it by multiplying the
average precision with a scalar less than 1 In the case of the example above the brevity
penalty is 0064 and the nal BLEU score is 0064
Figure 62 a BLEU vs bilingual and monolingual judgements of three machine
translation systems S1 S2 and S3 and two humans H1 and H2 Reprinted from
b BLEU vs human judgement adequacy and uency separately of three
machine translation systems two statistical and one rule-based systems Reprinted
from 20
The BLEU was shown to correlate well with human judgements in the original
article 82 Fig 62 a shows how BLEU correlates with the human judgements in
comparing different translation systems
This is however not to be taken as a message saying that the BLEU is the perfect
automatic evaluation metric
It has been shown that the BLEU is only adequate in
comparing two similar machine translation systems but not too much so in comparing
two very different systems For instance Callison-Burch et al 20 observed that the
BLEU underestimates the quality of the machine translation system that is not a phrase-
based statistical system See Fig 62 b for an example
BLEU is denitely not a perfect metric and many researchers strive to build a better
evaluation metric for machine translation systems Some of the alternatives available
at the moment are METEOR 36 and TER 99
62 Neural Machine Translation
Simple Encoder-Decoder Model
From the previous section and from Eq 62 it is clear that we need to model each
conditional distribution inside the product as a function This function will take as
input all the previous words in the target sentence Y  y1    yt1 and the whole
source sentence X  x1    xTx  Given these inputs the function will compute the
probabilities of all the words in the target vocabulary Vy In this section I will describe
an approach that was proposed multiple times independently over 17 years in 43 28
Let us start by tackling how to handle the source sentence X  x1    xTx  Since
this is a variable-length sequence we can readily use a recurrent neural network from
Chapter 4 However unlike the previous examples there is no explicit targetoutput in
this case All we need is a vector summary of the source sentence
We call this recurrent neural network an encoder as it encodes the source sentence
into a continuous vector code It is implemented as
ht1Ecid62
ht  enc
cid16
cid17
As usual enc can be any recurrent activation function but it is highly recommended to
use either gated recurrent units see Sec 422 or long short-term memory units see
Sec 423 Ex  RVxd is an input weight matrix containing word vectors as its rows
see Eq 512 in Sec 54 and xt is an one-hot vector representation of the word xt
see Eq 510 in Sec 54 h0 is initialized as an all-zero vector
After reading the whole sentence up to xTx the last memory state hTx of the encoder
summarizes the whole source sentence into a single vector as shown in Fig  a
Thanks to this encoder we can now work with a single vector instead of a whole
sequence of source words Let us denote this vector as c and call it a context vector
We now need to design a decoder again using a recurrent neural network As I
mentioned earlier the decoder is really nothing but a language model except that it is
conditioned on the source sentence X What this means is that we can build a recurrent
neural network language model from Sec 55 but feeding also the context vector at
each time step In other words
zt  dec
zt1
Ecid62
y yt1c
cid105cid17
cid16
cid104
Figure 63 a The encoder and b the decoder of a simple neural machine translation
Do you see the similarity and dissimilarity to Eq 517 from Sec 55 Its essentially
same except that the input at time t is a concatenated vector of the word vector of the
previous word yt1 and the context vector c
Once the decoders memory state is updated we can compute the probabilities of
all possible target words by
cid16
cid17
pyt  wcid48yt X  exp
ecid62
wcid48zt
where ewcid48 is the target word vector associated the word wcid48 This is equivalent to afne-
transforming zt followed by a softmax function from Eq 35 from Sec 31
Now should we again initialize z0 to be an all-zero vector Maybe or maybe not
One way to view what this decoder does is that the decoder models a trajectory in
a continuous vector space and each point in the trajectory is zt Then z0 acts as a
starting point of this trajectory and it is natural to initialize this starting point to be a
point relevant to the source sentence Because we have access to the source sentences
content via c we can again use it to initialize z0 as
z0  init c 
See Fig 63 b for the graphical illustration of the decoder
Although I have used c as if it is a separate variable this is not true c is simply
a shorthand notation of the last memory state of the encoder which is a function of
the whole source sentence What does this mean It means that we can compute the
gradient of the empirical cost function in Eq 63 with respect to all the parameters of
both the encoder and decoder and maximize the cost function using stochastic gradient
descent just like any other neural network we have learned so far in this course
621 Sampling vs Decoding
Sampling We are ready to compute the conditional distribution PYX over all pos-
sible translations given a source sentence When we have a distribution the rst thing
we can try is to sample from this distribution Often it is not straightforward to gen-
erate samples from a distribution but fortunately in this case we can readily generate
exact samples from the distribution PYX
We simply iterate over the following steps until a token indicating the end of a
sentence cid104eoscid105
1 Compute c Eq 65
2 Initialize z0 with c Eq 68
3 Compute zt given zt1 yt1 and c Eq 66
4 Compute pytyt X Eq 67
5 Sample yt from the compute distribution
6 Repeat 35 until yt  cid104eoscid105
After taking these steps we get a sample Y 
cid16
cid17
y1     y Y
given a source sentence
X Of course there is no guarantee that this will be a good translation of X In order to
nd a good translation meaning a translation with a high probability P YX we need
to repeatedly sample multiple translations from PYX and choose one with the high
probability
This is not too desirable as it is not clear how many translations we need to sample
from PYX and also it will likely be computationally expensive We must wonder
whether we can solve the following optimization problem directly
Y  argmax
logPYX
Unfortunately the exact solution to this requires evaluating PYX for every possible
Y  Even if we limit our search space of Y to consist of only sentences of length up
to a nite number it will likely become too large the cardinality of the set grows
exponentially with respect to the number of words in a translation Thus it only
makes sense to solving the optimization problem above approximately
Approximate Decoding Beamsearch Although it is quite clear that nding a trans-
lation Y that maximizes the log-probability logP YX is extremely expensive we will
regardlessly try it here
One very natural way to enumerate all possible target sentences and simultaneously
computing the log-probability of each and every one of them is to start from all possible
rst word compute the probabilities of them and from each potential rst word branch
into all possible second words and so on This procedure forms a tree and any path
from the root of this tree to any intermediate node is a valid but perhaps very unlikely
sentence See Fig 64 for the illustration The conditional probabilities of all these
paths or sentences can be computed as we expand this tree down by simply following
Eq 62
Of course we cannot compute the conditional probabilities of all possible sen-
tences Hence we must resort to some kind of approximate search Wait search Yes
this whole procedure of nding the most likely translation is equivalent to searching
through a space in this case a tree of all possible sentences for one sentence that has
the highest conditional probability
Figure 64 a Search space depicted as a tree b Greedy search
The most basic approach to approximately searching for the most likely translation
is to choose only a single branch at each time step t In other words
yt  argmax
wcid48V
log pyt  wcid48 yt X
where the conditional probability is dened in Eq 67 and yt   y1 y2     yt1 is
a sequence of greedily-selected target words up to the t  1-th step This procedure
is repeated until the selected yt is a symbol corresponding to the end of the translation
often denoted as cid104eoscid105 See Fig 64 b for illustration
There is a big problem of this greedy search That is as soon as it makes one
mistake at one time step there is no way for this search procedure to recover from this
mistake This happens because the conditional distributions at later steps depend on
the choices made earlier
Consider the following two sequences w1w2 and wcid48
1w2 These sequences
probabilities are
pw1w2  pw1pw2w1
pwcid48
1pw2wcid48
1w2  pwcid48
Lets assume that
where 0    1 meaning that pw1  pwcid48
choose w1 over wcid48
1 and ignore wcid48
 pw1  pwcid48
Now we can see that theres a problem with this Lets assume that
pw2wcid48
 pw2w1  pw2wcid48
1  pw2w1 
1 In this case the greedy search will
where  was dened earlier In this case
pw1w2 pw1pw2w1   pwcid48
1  pwcid48
 pwcid48
pw2wcid48
1pw2w1
1pw2wcid48
1  pwcid48
In short
It means that the sequence wcid48
algorithm is unable to notice this because simply pw1  pwcid48
pw1w2  pwcid48
1w2 is more likely than w1w2 but the greedy search
Unfortunately the only way to completely avoid this undesirable situation is to
consider all the possible paths starting from the very rst time step This is exactly the
reason why we introduced the greedy search in the rst place but the greedy search
is too greedy The question is then whether there is something in between the exact
search and the greedy search
Beam Search Let us start from the very rst position t  1 First we compute the
conditional probabilities of all the words in the vocabulary
py1  wX for all w  V
Among these we choose the K most likely words and initialize the K hypotheses
1 w1
2     w1
We use the subscript to denote the hypothesis and the subscript the time step As an
example w1
1 is the rst hypothesis at time step 1
For each hypothesis we compute the next conditional probabilities of all the words
in the vocabulary
py2  wy1  w1
i X for all w  V
where i  1    K We then have K V candidates with the corresponding probabil-

cid124
cid125
   
   
cid123cid122
   
Figure 65 Beam search with the beam width set to 3
Among these K V candidates we choose the K most likely candidates
1 w1
2     w1
Starting from these K new hypotheses we repeat the process of computing the proba-
bilities of all K V possible candidates and choosing among them the K most likely
new hypotheses
It should be clear that this procedure called beam search and shown in Fig 65
becomes equivalent to the exact search as K   Also when K  1 this procedure is
equivalent to the greedy search In other words this beam search interpolates between
the exact search which is computationally intractable but exact and the greedy search
which is computationally very cheap but probably quite inexact by changing the size
K of hypotheses maintained throughout the search procedure
How do we choose K One might mistakenly think that we can simply use as large
K as possible given the constraints on computation and memory Unfortunately this is
not necessarily true as this interpolation by K is not monotonic That is the quality of
the translation found by the beam search with a larger K is not necessarily better than
the translation found with a smaller K
Let us consider the case of vocabulary having three symbols abc and any valid
translation being of a length 3 In the rst step we have
pa  05 pb  015 pc  045
In the case of K  1 ie greedy search we choose a If K  2 we will keep a and
Given a as the rst symbol we have
paa  04 pba  03 pca  03
in which case we keep aa with K  1 With K  2 we should check also
pac  045 pbc  045 pcc  01
from which we maintain the hypotheses ca and cb 045 045 and 045 045
respectively Note that with K  2 we have discarded aa
Now the greedy search ends by computing the last conditional probabilities
paaa  09 pbaa  005 pcaa  005
The nal verdict from the greedy search is therefore aaa with its probability being
05 04 09  018
What happens with the beam search having K  2 We need to check the following
conditional probabilities
paca  07 pbca  02 pcca  01
pacb  04 pbcb  00 pccb  06
From here we consider caa and cbc with the corresponding probabilities 045
045 07  014175 and 045 045 06  01215 Among these two caa is
nally chosen due to its higher probability than that of cbc
In summary the greedy search found aaa whose probability is
paaa  018
and the beam search with K  2 found caa whose probability is
pcaa  014175
Even with a larger K the beam search found a worse translation
Now clearly what one can do is to set the maximum beam width K and try with
all possible 1  K  K Among the translations given by K beam search procedures
the best translation can be selected based on their corresponding probabilities From
the point of view of computational complexity this is perhaps the best approach to
upper-bound the worst-case memory consumption Doing the beam search once with
K or multiple beam searches with K  1     K are equivalent in terms of memory con-
sumption ie both are OKV Furthermore the worst-case computation is OKV
assuming a constant time computation for computing each conditional probability In
practice however the constant in front of KV does matter and we often choose K
based on the translation quality of the validation set after trying a number of values
124816
If youre interested in how to improve beam search by backtracking so that the
beam search becomes complete refer to eg 44 113 If youre interested in general
search strategies refer to 90 Also in the context of statistical machine translation it
is useful to read 64
63 Attention-based Neural Machine Translation
One important property of the simple encoder-decoder model for neural machine trans-
lation from Sec 62 is that a whole source sentence is compressed into a single real-
valued vector c This sounds okay since the space of all possible source sentences is
countable while the context vector space 11d is uncountable There exists a map-
ping from this sentence space to the context vector space and all we need to ensure is
that training the simple encoder-decoder model nds this mapping This is conditioned
on the assumption that the hypothesis space9 dened by the model architecturethe
number of hidden units and parameters includes this mapping from any source sen-
tence to a context vector
Unfortunately considering the complexity of any natural language sentence it is
quite easy to guess that this mapping must be highly nonlinear and will require a huge
encoder and consequently a huge decoder to map back from a context vector to a target
sentence In fact this fact was empirically validated last year 2014 when the almost
identical models from two groups 101 27 showed vastly different performances on
the same EnglishFrench translation task The only difference there was that the au-
thors of 101 used a much larger model than the authors of 27 did
At a more fundamental level theres a question of whether a natural language sen-
tence should be fully represented as a single vector For instance there is now a famous
quote by Prof Raymond Mooney10 of the University of Texas at Austin You cant
cram the meaning of a whole  sentence into a single  vector11 Though
our goal is not in answering this fundamental question from linguistics
Our goal is rather to investigate the possibility of avoiding this situation of having
to learn a highly nonlinear complex mapping from a source sentence to a single vector
The question we are more interested in is whether there exists a neural network that
can handle a variable-length sentence by building a variable-length representation of
it Especially we are interested in whether we can build a neural machine translation
system that can exploit a variable-length context representation
Variable-length Context Representation In the simple encoder-decoder model a
source sentence regardless of its length was mapped to a single context vector by a
recurrent neural network
cid16
ht  enc
cid17
ht1Ecid62
See Eq 65 and the surrounding text for more details
Instead here we will encode a source sentence X  x1x2    xTx  with a set C of
context vectors hts This is achieved by having two recurrent neural networks rather
than a single recurrent neural networks as in the simple encoder-decoder model The
rst recurrent neural network to which we will refer as a forward recurrent neural
network reads the source sentence as usual and results in a set of forward memory
9 See Sec 232
10 httpswwwcsutexasedumooney
11 httpnlpersblogspotcom201409amr-not-semantics-but-close-maybe
Figure 66 An encoder with a bidirectional recurrent neural network
h t for t  1    Tx The second recurrent neural network a reverse recurrent
neural network reads the source sentence in a reverse order starting from xTx to x1
h t for t 
This reverse network will output a sequence of reverse memory states
1    Tx
For each xt we will concatenate
h t and
cid35
cid34 
h t to form a context-dependent vector ht
h t
We will form a context set with these context-dependent vectors c  h1h2    hTx
See Fig 66 for the graphical illustration of this process
Now why is ht a context-dependent vector We should look at what the input was
to a function that computed ht The rst half of ht
cid16
h t  fenc
h t was computed by
cid17
cid17
x xt1
Ecid62
h t was
cid16
cid105cid62
where fenc is a forward recurrent activation function From this we see that
computed by all the source words up to t ie xt Similarly
Ecid62
h t depends on all the source
h t  renc
cid17
cid17
where renc is a reverse recurrent activation function and
words from t to the end ie xt
h cid62
In summary ht 
h cid62
cid104
is a vector representation of the t-th word xt with
respect to all the other words in the source sentence This is why ht is a context-
dependent representation But then what is the difference among all those context-
dependent representations h1    hTx We will discuss this shortly
cid105cid17cid104
Decoder with Attention Mechanism Before anything let us think of what the mem-
ory state zt of the decoder from Eq 66 does
cid105cid17
cid105cid17
cid16
cid16
cid104
cid104
Ecid62
y yt3c
Ecid62
y yt2c
Ecid62
y yt1c
zt  dec
cid16 
cid16 Ecid62
cid16 Ecid62
Figure 67 Illustration of how the relevance score e23 of the second context vector h2
at time step 3 dashed curves and box
It is computed based on all the generated target words so far  y1 y2     yt1 and
the context vector12 c which is the summary of the source sentence The very reason
why I designed the decoder in this way is so that the memory state zt is informative of
which target word should be generated at time t after generating the rst t  1 target
words given the source sentence In order to do so zt must encode what have been
translated so far among the words that are supposed to be translated which is encoded
in the context vector c Lets keep this in mind
In order to compute the new memory state zt with a context set C h1h2    hTx
we must rst get one vector out of Tx context vectors Why is this necessary Because
we cannot have an innitely large number of parameters to cope with any number of
context vectors Then how can we get a single vector from an unspecied number of
context vectors hts
First let us score each context vector h j  j  1    Tx based on how relevant it is
for translating a next target word This scoring needs to be based on 1 the previous
memory state zt1 which summarizes what has been translated up to the t  2-th
word13 2 the previously generated target word yt1 and 3 the j-th context vector
e jt  fscorezt1Ecid62
y yt1h j
Conceptually the score e jt will be computed by comparing zt1 yt1 with the con-
text vector c j See Fig 67 for graphical illustration
12 We will shortly switch to using a context set instead
13 Think of why this is only up to the t  2-th word not up to the t  1-th one
Figure 68 Computing the new memory state zt of the decoder based on the previous
memory state zt1 the previous target word yt1 and the weighted average of context
vectors according to the attention weights
Once the scores for all the context vectors h js  j  1    Tx are computed by
fscore we normalize them with a softmax function
expe jt 
jcid481 expe jcid48t 
 jt 
We call these normalized scores the attention weights as they correspond to how much
the decoder attends to each of the context vectors This whole process of computing
the attention weights is often referred to as an attention mechanism see eg 26
We take the weighted average of the context vectors with these attention weights
 jth j
This weighted average is used to compute the new memory state zt of the decoder
which is identical to the decoders update equation from the simple encoder-decoder
model see Eq 66 except that ct is used instead of c a in the equation below
zt1
Ecid62
y yt1 ctcid124cid123cid122cid125


zt  dec
See Fig 68 for the graphical illustration of how it works
Given the new memory state zt of the decoder the output probabilities of all the
target words in a vocabulary happen without any change from the simple encoder-
decoder model in Sec 62
We will call this model which has a bidirectional recurrent neural network as an en-
coder and a decoder with the attention mechanism an attention-based encoder-decoder
model This approach was proposed last year 2014 in the context of machine transla-
tion in 2 and has been studied extensively in 76
631 What does the Attention Mechanism do
One important thing to notice is that this attention-based encoder-decoder model can be
reduced to the simple encoder-decoder model easily This happens when the attention
mechanism fscore in Eq 610 returns a constant regardless of its input When this
happens the context vector ct at each time step t see Eq 612 is same for all the
time steps t  1    Ty
The encoder effectively maps the whole input sentence into a single vector which was
at the core of the simple encoder-decoder model from Sec 62
This is not the only situation in which this type of behaviour happens Another
h 1 of
possible scenario is for the encoder to make the last memory states
the forward and reverse recurrent neural networks to have a special mark telling that
these are the last states The attention mechanism then can exploit this to assign a large
score to these two memory states but still constant across time t This will become
even closer to the simple encoder-decoder model
h Tx and
The question is how we can avoid these degenerate cases Or is it necessary for us
to explicitly make these degenerate cases unlikely Of course there is no single answer
to this question Let me give you my answer which may differ from others answer
The goal of introducing a novel network architecture is to guide a model according
to our intuition or scientic observation so that it will do a better job at a target task In
our case the attention mechanism was introduced based on our observation and some
intuition that it is not desirable to ask the encoder to compress a whole source sentence
into a single vector
This incorporation of prior knowledge however should not put a hard constraint
We give a model a possibility of exploiting this prior knowledge but should not force
the model to use this prior knowledge exclusively As this prior knowledge based
on our observation of a small portion of data is not likely to be true in general the
model must be able to ignore this if the data does not exhibit the underlying structure
corresponding to this prior knowledge In this case of attention-based encoder-decoder
model the existence of those degenerate cases above is a direct evidence of what this
attention-based model can do if there is no such underlying structure present in the
Then a natural next question is whether there are such structures that can be well
exploited by this attention mechanism in real data
If we train this attention-based
encoder-decoder model on the parallel corpora we discussed earlier in Sec 611 what
kind of structure does this attention-based model learn
In order to answer this question we must rst realize that we can easily visualize
what is happening inside this attention-based model First note that given a pair of
source X and target Y sentences14 the attention-based model computes an alignment
matrix A  01
XY

X1 X2
1Y
2Y
 XY
 
where  jt is dened in Eq 611
Each column at of this alignment matrix A is how well each source word based
on its context-dependent vector representation from Eq 69 is aligned to the t-th
target word Each row b j similarly shows how well each target word is aligned to the
content-dependent vector of the j-th source word In other words we can simply draw
the alignment matrix A as if it were a gray scale 2-D image
In Fig 69 the visualization of four alignment matrices is presented It is quite
clear especially to a French-English bilingual speaker that the model indeed captured
the underlying structure of wordphrase mapping between two languages For instance
focus on European Economic Area in Fig 69 a The model correctly noticed
that Area corresponds to zone Economic to economique and European to
europeenne without any supervision about this type of alignment
This is nice to see that the model was able to notice these regularities from data
without any explicit supervision However the goal of introducing the attention mech-
anism was not to get these pretty gures After all our goal is not to build an inter-
pretable model but a model that is predictive of the correct output given an input see
Chapter 1 and 16 In this regard how much does the introduction of the attention
mechanism help
In 2 the attention-based encoder-decoder model was compared against the sim-
ple encoder-decoder model in the task of English-French translation They observed
the relative improvement of up to 60 in terms of BLEU see Sec 612 as shown in
Table 61 Furthermore by using some of the latest techniques such as handling large
vocabularies 55 building a vocabulary of subword units 93 and variants of the atten-
tion mechanism 76 it has been found possible to achieve a better translation quality
with neural machine translation than the existing state-of-the-art translation systems
14 Note that if youre given only a source sentence you can let the model translate and align simultane-
Simple EncDec
Attention-based EncDec
Attention-based EncDec LV
Attention-based EncDec LVcid63
State-of-the-art SMT
BLEU Rel Improvement
1060
Table 61 The translation performances and the relative improvements over the simple
encoder-decoder model on an English-to-French translation task WMT14 measured
by BLEU 2 55 cid63 an ensemble of multiple attention-based models  the state-of-
the-art phrase-based statistical machine translation system 39
64 Warren Weavers Memorandum
In 1949 Warren Weaver15 wrote a memorandum titled cid104Translationcid105 on machine trans-
lation 108 Although this text was written way before computers have become ubiq-
uitous16 there are many interesting ideas that are closely related to what we have dis-
cussed so far in this chapter Let us go over some parts of the Weavers memorandum
and see how the ideas there corresponds to modern-day machine translation
Necessity of Linguistic Knowledge Weaver talks about a distinguished mathemati-
cian P who was surprised by his colleague His colleague had an amateur interest in
cryptography and one day presented P his method to decipher an encrypted Turkish
text successfully The most important point according to Weaver from this instance
is that the decoding was done by someone who did not know Turkish Now this
sounds familiar doesnt it
As long as there was a parallel corpus we are able to use neural machine transla-
tion models described throughout this chapter without ever caring about which lan-
guages we are training a model to translate between Especially if we decide to consider
each sentence as a sequence of characters17 there is almost no need for any linguistic
knowledge when building these neural machine translation systems
This lack of necessity for linguistic knowledge is not new In fact the most widely
studied and used machine translation approach which is count-based statistical ma-
chine translation 19 66 does not require any prior knowledge about source and target
languages All it needs is a large corpus
Importance of Context Recall from Sec 63 that the encoder of an attention-based
neural machine translation uses a bidirectional recurrent neural network in order to ob-
tain a context set Each vector in the context set was considered a context-dependent
15 Yes this is the very same Weaver after which the building of the Courant Institute of Mathematical
Sciences has been named
16 Although Weaver talks about modern computers over and over in his memorandum what he refers to
is not exactly what we think of computers as these days
17 In fact only very recently people have started investigating the possibility of building a machine trans-
lation system based on character sequences 73 This has been made possible due to the recent success of
neural machine translation
vector as it represents what the center word means with respect to all the surround-
ing words This context dependency is a necessary component in making the whole
attention-based neural machine translation as it helps disambiguating the meaning of
each word and also distinguishing multiple occurrences of a single word by their con-
Weaver discusses this extensively in Sec 34 in his memorandum First to Weaver
it was amply clear that a translation procedure that does little more than handle a one-
to-one correspondence of words can not hope to be useful 
in which the problems
of  multiple meanings  are frequent In other words it is simply not possible to
look at each word separately from surrounding words or context and translate it to a
corresponding target word because there is uncertainty in the meaning of the source
word which can only be resolved by taking into account its context
So what does Weaver propose in order to address this issue He proposes in Sec 5
that if one can see not only the central word in question but also say N words on
either side then if sic N is large enough one can unambiguously decide the meaning
of the central word If we consider only a single sentence and take the innite limit of
N   we see that what Weaver refers to is exactly the bidirectional recurrent neural
network used by the encoder of the attention-based translation system Furthermore
we see that the continuous bag-of-words language model or Markov random eld
based language model from Sec 542 exactly does what Weaver proposed by setting
N to a nite number
In Sec 521 we talked about the issue of data sparsity and how it is desirable to
have a larger N but its often not a good idea statistically to do so Weaver was also
worried about this by saying that it would hardly be practical to do this by means of
a generalized dictionary which contains all possible phases sic 2N  1 words long
for the number of such phases sic is horrifying We learned that this issue of data
sparsity can be largely avoided by adopting a fully parametric approach instead of a
table-based approach in Sec 54
Common base of human communications Weaver suggested in the last section of
his memorandum that perhaps the way for translation is to descend from each lan-
guage down to the common base of human communication  the real but as yet undis-
covered universal language  and then re-emerge by whatever particular route is conve-
nient He specically talked about a universal language and this makes me wonder
if we can consider the memory state of the recurrent neural networks both of the en-
coder and decoder as this kind of intermediate language This intermediate language
radically departs from our common notion of natural languages Unlike conventional
languages it does not use discrete symbols but uses continuous vectors This use of
continuous vectors allows us to use simple arithmetics to manipulate the meaning as
well as its surface realization18
This view may sound radical considering that what weve discussed so far has been
conned to translating from one language to another After all this universal language
18 If you nd this view too radical or fascinating I suggest you to look at the presentation slides by
Geoff Hinton at httpsdrivegooglecomfiled0B16RwCMQqrtdMWFaeThBTC1mZkk
viewuspsharing
of ours is very specic to only a single source language with respect to a single target
language This is however not a constraint on the neural machine translation by design
but simply a consequence of our having focused on this specic case
Indeed in this year 2015 researchers have begun to report that it is possible to
build a neural machine translation model that considers multiple languages and even
further multiple tasks 38 75 More works in this line are expected and it will be
interesting to see if Weavers prediction again turns out to be true
Figure 69 Visualizations of the four sample alignment matrices The alignment
matrices were computed from an attention-based translation model trained to translate
a sentence in English to French Reprinted from 2
TheagreementontheEuropeanEconomicAreawassignedinAugust1992endLaccordsurlazoneconomiqueeuropenneatsignenaot1992endItshouldbenotedthatthemarineenvironmentistheleastknownofenvironmentsendIlconvientdenoterquelenvironnementmarinestlemoinsconnudelenvironnementendDestructionoftheequipmentmeansthatSyriacannolongerproducenewchemicalweaponsendLadestructiondelquipementsignifiequelaSyrienepeutplusproduiredenouvellesarmeschimiquesendThiswillchangemyfuturewithmyfamilythemansaidendCelavachangermonaveniravecmafamilleaditlhommeendChapter 7
Final Words
Let me wrap up this lecture note by describing some aspects of natural language under-
standing with distributed representations that I have not discussed in this course These
are the topics that I would have spent time on had the course been scheduled to last
twice the duration as it is now Afterward I will nalize this whole lecture note with a
short summary
71 Multimedia Description Generation as Translation
Those who have followed this course closely so far must have noticed that the neural
machine translation model described in the previous chapter is quite general in the
sense that the input to the model does not have to be a sentence In the case of the
simple encoder-decoder model from Sec 62 it is clear that any type of input X can be
used instead of a sentence as long as there is a feature extractor that returns the vector
representation c of the input
And fortunately we already learned how to build a feature extractor throughout
this course Almost every single model that is a neural network in our case converts
an input into a continuous vector Let us take a multilayer perceptron from Sec 33
as an example Any classier built as a multilayer perceptron can be considered as a
two-stage process see Sec 332 First the feature vector of the input is extracted see
Eq 39
 x   ux  c
The extracted feature vector  x is then afne-transformed followed by softmax func-
tion This results in a conditional distribution over all possible labels see Eq 44
This means that we can make the simple encoder-decoder model to work with non-
language input simply by replacing the recurrent neural network based encoder with
the feature extraction stage of the multilayer perceptron Furthermore it is possible to
pretrain this feature extractor by training the whole multilayer perceptron on a separate
classication dataset1
1 This way of using a feature extractor pretrained from another network has become a de facto standard
This approach of using the encoder-decoder model for describing non-language
input has become popular in recent years especially 2014 and 2015 and has been
applied to many applications including imagevideo description generation and speech
recognition For an extensive list of these applications I refer the readers to a recent
review article by Cho et al 26
Example Image Caption Generation Let me take as an example the task of im-
age caption generation The possibility of using the encoder-decoder model for image
caption generation was noticed by several research groups almost simultaneously last
year 2014 62 106 59 78 37 40 252 The success of neural machine translation
in 101 and earlier success of deep convolutional network on object recognition see
eg 67 96 102 inspired them the idea to use the deep convolutional networks fea-
ture extractor together with the recurrent neural network decoder for the task of image
caption generation
Right after these Xu et al 111
realized that it is possible to use the
attention-based encoder-decoder model
from Sec 63 for image caption gen-
eration Unlike the simple model the
attention-based model requires a context
set instead of a context vector The con-
text set should contain multiple context
vectors and each vector should repre-
sent a spatial location with respect to
the whole image meaning each context
vector is a spatially-localized context-
dependent image descriptor This was
achieved by using the last convolutional
layers activations of the pretrained deep
convolutional network instead of the last
fully-connected layers See Fig 71 for
graphical illustration of this approach
Figure 71 Image caption generation with
the attention-based encoder-decoder model
These approaches based on neural
networks or in other words based on dis-
tributed representations have been suc-
cessful at image caption generation Four out of ve top rankers in the recent Microsoft
CoCo Image Captioning Challenge 20153 were using variants of the neural encoder-
decoder model based on human evaluation of the captions
in many of the computer vision tasks 94 This is also closely related to semi-supervised learning with
pretrained word embeddings which we discussed in Sec 543 In that case it was only the rst input layer
that was pretrained and used later see Eqs 511512
2 I must however make a note that Kiros et al 62 proposed a fully neural network based image caption
generation earlier than all the others cited here did
3 httpmscocoorgdatasetcaptions-leaderboard
AnnotationVectorsWord SsampleuiRecurrentStatezif  a   man   is   jumping   into   a   lake   hjAttentionMechanismaAttention        weightjaj1Convolutional Neural Network72 Language Understanding with World Knowledge
In Sec 12 we talked about how we view natural languages as a function This function
of natural language maps from a tuple of a speakers speech a listeners mental state
and the surrounding world to the listeners reaction often as a form of natural language
response Unfortunately in order to make it manageable we decided to build a model
that approximates only a part of this true function
Immediate state of the surrounding world In this course of action one thing we
have dropped out is the surrounding world The surrounding world may mean many
different things One of them is the current state of the surrounding world As an
example when I say look at this cute llama it is quite likely that the surrounding
world at the current state contains either an actual llama or at least a picture of a llama
A listener then understands easily what a llama is even without having known what a
llama is in advance By looking at the picture of llama the listener makes a mental note
that the llama looks similar to a camel and therefore must be a four-legged animal
If the surrounding world is not taken into account as weve been doing so far
the listener can only generalize based on the context words Just like how the neural
language model from Sec 54 generalized to unseen or rarely seen words the listener
can infer that llama must be a type of animal by remembering that the phrase look
at this cute has mainly been followed by an animal such as cat or dog However
it is quite clear that look at this cute is also followed by many other nouns including
baby book and so on
The question is then how to exploit this How can we incorporate for instance
vision information from the surrounding world into natural language understanding
The simplest approach is to simply concatenate a word embedding vector see
Eq 512 and a corresponding image vector obtained from an existing feature ex-
tractor see above 60 This can be applied to any existing language models such as
neural language model see Sec 54 and neural machine translation model see Chap-
ter 6 This approach gives a strong signal to the model the similarities among different
words based on the corresponding objects appearances This approach of concatenat-
ing vectors of two different modalities eg language and vision was earlier proposed
in 109
A more sophisticated approach is to design and train a model to solve a task that
requires tight interaction between language and other modalities As our original goal
is to build a natural language function all we need to do is to build a function approxi-
mator that takes as input both language and other modalities Recently Antol et al 1
built a large-scale dataset of question-answer-image triplets called visual question an-
swering VQA for this specic purpose They have carefully built the dataset such
that many if not most questions can only be answered when the accompanying image
is taken into consideration Any model thats able to solve the questions in this dataset
well will have to consider both language and vision
Knowledge base Lost in a library So far we have talked about incorporating an
immediate state of the surrounding world However our use of languages is more
sophisticated This is especially apparent in written languages Let us take an example
of me writing this lecture note It is not the case where I simply sit and start writing
the whole text based purely on my mental state with memory of my past research and
the immediate surrounding world state which has almost nothing to do with Rather
a large part of this writing process is spent on going through various research articles
and books written by others in order to nd relevant details of the topic
In this case the surrounding world is a database in which human knowledge is
stored You can think of a library or the Internet As the amount of knowledge is
simply too large to be memorized in the entirety it is necessary for a person to be able
to search through the vast knowledge base But wait what does it have to do with
natural language understanding
Consider the case where the context phrase is Llama is a domesticated camelid
from Without access to the knowledge base or in this specic instance access to
Wikipedia any language model can only say as much as that this context phrase is
likely followed by a name of some place This is especially true if we assume that the
training corpus did not mention llama at all However if the language model is able
to search Wikipedia and condition on its search result it suddenly becomes so obvious
that this context phrase is followed by South America or the name of any region on
Andean mountain rages
Although this may sound too complicated a task to incorporate into a neural net-
work the concept of how to incorporate this is not necessarily complicated In fact we
can use the attention mechanism discussed in Sec 63 almost as it is Let us describe
here a conceptual picture of how this can be done
Let D  d1d2    dM be a set of knowledge vectors Each knowledge vector
di is a vector representation of a piece of knowledge For instance di can be a vector
representation of one Wikipedia article It is certainly unclear what is the best way to
obtain this vector representation of an entire article but let us assume that an oracle
gave us a means to do so
Let us focus on recurrent language modelling from Sec 554 At each time step
we have access to the following vectors
1 Context vector ht1 the summary all the preceding words
2 Current word wt the current input word
Similarly to what we have done in Sec 63 we will dene a scoring function fscore
which scores each knowledge vector di with respect to the context vector and the cur-
rent word
it  exp  fscorediht1ewt  
where ewt is a vector representation of the current word wt
This score reects the relevance of the knowledge in predicting the next word and
once it is computed for every knowledge vector we compute the weighted sum of all
4 This approach of using attention mechanism for external knowledge pieces has been proposed recently
in 14 in the context of question-answering Here we stick to language modelling as the course has not
dealt with question-answering tasks
the knowledge
itdi
This vector dt is a vector summary of the knowledge relevant to the next word taking
into account the context phrase In the case of an earlier example the scoring function
gives a high score to the Wikipedia article on llama based on the history of preceding
words Llama is a domesticated camelid from
This knowledge vector is used when updating the memory state of the recurrent
neural network
cid0ht1ewt  dt
cid1 
ht  frec
From this updated memory state which also contains the knowledge extracted from
the selected knowledge vector the next words distribution is computed according to
Eq 46
One important issue with this approach is that the size of knowledge set D is often
extremely large For instance English Wikipedia contains more than 5M articles as of
23 Nov 20155 It easily becomes impossible to score each and every knowledge vector
not to mention to extract knowledge vectors of all the articles6 It is an open question
how this unreasonable amount of computation needed for search can be avoided
Why is this any signicant One may naively think that if we train a large enough
network with a large enough data which contains all those world knowledge a trained
network will be able to contain all those world knowledge likely in a compressed
form in its parameters together with its network architecture This is true up to a
certain level but there are many issues here
First the world knowledge were talking about here contains all the knowledge
accumulated so far Even a human brain arguably the best working neural network
to date cannot store all the world knowledge and must resort to searching over the
external database of knowledge It is no wonder we have libraries where people can go
and look for relevant knowledge
Second the world knowledge is dynamic Every day some parts of the world
knowledge become obsolete and at the same time previously unknown facts are added
to the world knowledge If anyone looked up Facebook before 2004 they wouldve
ended up with yearly facebooks from American universities Nowadays it is almost
certain that when a person looks up Facebook they will nd information on Face-
book the social network site Having all the current world knowledge encoded in the
models parameters is not ideal in this sense
5 httpsenwikipediaorgwikiWikipediaStatistics
6 This is true especially when those knowledge vectors are also updated during training
73 Larger-Context Language Understanding
Beyond Sentences and Beyond Words
If we view natural language as a function it becomes clear that what weve discussed
so far throughout the course is heavily restrictive There are two reasons behind this
restriction
First what we have discussed so far has narrowly focused on handling a sentence
In Sec 52 I have described language model as a way to model a sentence probability
pS This is a bit weird in the sense that weve been using a term language modelling
not sentence modelling Keeping it in mind we can start looking at a probability of a
document or discourse D as a whole rather than as a product of sentence probabilities
pSkSk
where the document D consists of N sentences This approach is readily integrated into
the language modelling approaches we discussed earlier in Chapter 5 by
pw jw jSk
This is applicable to any language-related models we have discussed so far includ-
ing neural language model from Sec 54 recurrent language model from Sec 55
Markov random eld language model from Sec 542 and neural machine translation
from Chapter 6
In the context of language modelling two recent articles proposed to explore this
direction I refer the readers to 107 and 57
Second we have stuck to representing a sentence as a sequence of words so far
despite a short discussion in Sec 512 where I strongly claim that this does not have
to be This is indeed true and in fact even if we replace most occurrence of word
in this course with for instance character all the arguments stand Of course by
using smaller units than words we run into many practical and theoretical issues One
most severe practical issue is that each sentence suddenly becomes much longer One
most sever theoretical issue is that it is a highly nonlinear mapping from a sequence
of characters to its meaning as we discussed earlier in Sec 512 Nevertheless the
advance in computing and deep neural networks which are capable of learning such
a highly nonlinear mapping have begun to let researchers directly work on this prob-
lem of using subword units see eg 61 73 Note that I am not trying to say that
characters are the only possible sub-word units and recently an effective statistical ap-
proach to deriving sub-word units off-line was proposed and applied to neural machine
translation in 93
74 Warning and Summary
Before I nish this lecture note with the summary of what we have discussed through-
out this course let me warn you by quoting Claude Shannon 957
It will be all too easy for our somewhat articial prosperity to collapse
overnight when it is realized that the use of a few exciting words like in-
formation entropy redundancy do not solve all our problems
Natural language understanding with distributed representation is a fascinating topic
that has recently gathered large interest from both machine learning and natural lan-
guage processing communities This may give a wrong sign that this approach with
neural networks is an ultimate winner in natural language understandingprocessing
though without any ill intention As Shannon pointed out this prosperity of distributed
representation based natural language understanding may collapse overnight as can
any other approaches out there8 Therefore I warn the readers especially students to
keep this quote in their mind and remember that it is not a few recent successes of this
approach to natural language understanding but the fundamental ideas underlying this
approach that matter and should be remembered after this course
Summary Finally here goes the summary of what we have learned throughout this
semester We began our journey by a brief discussion on how we view human language
as and we decided to stick to the idea that a language is a function not an entity existing
independent of the surrounding world including speakers and listeners Is this a correct
way to view a human language Maybe maybe not I will leave it up to you to decide
In order to build a machine that can approximate this language function in Chap-
ter 2 we studied basic ideas behind supervised learning in machine learning We de-
ned what a cost function is how we can minimize it using an iterative optimization
algorithm specically stochastic gradient descent and learned the importance of hav-
ing a validation set for both early-stopping and model selection These are all basic
topics that are dealt in almost any basic machine learning course and the only thing
that I would like to emphasize is the importance of not looking at a held-out test set
One must always select anything related to learning eg hyperparameters networks
architectures and so on based solely on a validation set As soon as one tunes any
of those based on the test set performance any result from this tuning easily becomes
invalid or at least highly disputable
In Chapter 3 we nally talked about deep neural networks or more traditionally
called multilayer perceptron9 I tried to go over basic but important details as slowly as
possible including how to build a deep neural network based classier how to dene
a cost function and how to compute the gradient wrt the parameters of the network
However I must confess that there are better materials for this topic than this lecture
7 I would like to thank Adam Lopez for pointing me to this quote
8 Though it is interesting to note that information theory never really collapsed overnight Rather its
prosperity has been continuing for more than half a century since Shannon warned us about its potential
overnight collapse in 1956
9 I personally prefer multilayer perceptron but it seems like it has gone out of fashion
We then moved on to recurrent neural networks in Chapter 4 This was a necessary
step in order to build a neural network based model that can handle both variable-length
input and output Again my goal here was to take as much time as it is needed to moti-
vate the need of recurrent networks and to give you basic ideas underlying them Also
I spent quite some time on why it has been considered difcult to train recurrent neural
networks by stochastic gradient descent like algorithms and as a remedy introduced
gated recurrent units and long short-term memory units
Only after these long four to ve weeks have I started talking about how to handle
language data in Chapter 5 I motivated neural language models by the lack of general-
ization and the curse of data sparsity It is my regret that I have not spent much time on
discussing the existing techniques for count-based n-gram language models but again
there are much better materials and better lecturers for these techniques already Af-
ter the introduction of neural language model I spent some time on describing how
this neural language model is capable of generalizing to unseen phrases Continuing
from this neural language model in Sec 55 language modelling using recurrent neu-
ral networks was introduced as a way to avoid Markov assumption of n-gram language
This discussion on neural language model naturally continued on to neural machine
translation in Chapter 6 Rather than going directly into describing neural machine
translation models I have spent a full week on two issues that are often overlooked
data preparation in Sec 611 and evaluation in Sec 612 I wish the discussion of these
two topics has reminded students that machine learning is not only about algorithms
and models but is about a full pipeline starting from data collection to evaluation often
with loops here and there This chapter nished with where we are in 2015 compared
to what Weaver predicted in 1949
Of course there are so many interesting topics in this area of natural language
understanding I am not qualied nor knowledgeable to teach many if not most of
those topics unfortunately and have focused on those few topics that I have worked on
myself I hope this lecture note will serve at least as a useful starting point into more
advanced topics in natural language understanding with distributed representations
Bibliography
1 S Antol A Agrawal J Lu M Mitchell D Batra C L Zitnick and D Parikh
Vqa Visual question answering In International Conference on Computer Vi-
sion ICCV 2015
2 D Bahdanau K Cho and Y Bengio Neural machine translation by jointly
learning to align and translate arXiv preprint arXiv14090473 2014
3 P Baltescu and P Blunsom Pragmatic neural language modelling in machine
translation arXiv preprint arXiv14127119 2014
4 F Bastien P Lamblin R Pascanu J Bergstra I Goodfellow A Bergeron
N Bouchard D Warde-Farley and Y Bengio Theano new features and speed
improvements arXiv preprint arXiv12115590 2012
5 A G Baydin B A Pearlmutter and A A Radul Automatic differentiation in
machine learning a survey arXiv preprint arXiv150205767 2015
6 Y Bengio N Boulanger-Lewandowski and R Pascanu Advances in optimiz-
ing recurrent networks In Acoustics Speech and Signal Processing ICASSP
2013 IEEE International Conference on pages 86248628 IEEE 2013
7 Y Bengio N Leonard and A Courville Estimating or propagating gradi-
ents through stochastic neurons for conditional computation arXiv preprint
arXiv13083432 2013
8 Y Bengio H Schwenk J-S Senecal F Morin and J-L Gauvain Neural
probabilistic language models In Innovations in Machine Learning pages 137
186 Springer Berlin Heidelberg 2006
9 Y Bengio P Simard and P Frasconi Learning long-term dependencies with
gradient descent is difcult Neural Networks IEEE Transactions on 52157
166 1994
10 J Bergstra O Breuleux F Bastien P Lamblin R Pascanu G Desjardins
J Turian D Warde-Farley and Y Bengio Theano a cpu and gpu math expres-
sion compiler In Proceedings of the Python for scientic computing conference
SciPy volume 4 page 3 Austin TX 2010
11 J Besag Statistical analysis of non-lattice data The statistician pages 179195
12 C M Bishop Mixture density networks 1994
13 C M Bishop Pattern recognition and machine learning springer 2006
14 A Bordes N Usunier S Chopra and J Weston Large-scale simple question
answering with memory networks arXiv preprint arXiv150602075 2015
15 L Bottou Online algorithms and stochastic approximations In D Saad edi-
tor Online Learning and Neural Networks Cambridge University Press Cam-
bridge UK 1998
16 L Breiman et al Statistical modeling The two cultures with comments and a
rejoinder by the author Statistical Science 163199231 2001
17 J S Bridle Training stochastic model recognition algorithms as networks can
lead to maximum mutual information estimation of parameters In D Touretzky
editor Advances in Neural Information Processing Systems 2 pages 211217
Morgan-Kaufmann 1990
18 E Brochu V M Cora and N de Freitas A tutorial on Bayesian optimization
of expensive cost functions with application to active user modeling and hierar-
chical reinforcement learning arXiv10122599 csLG Dec 2010
19 P F Brown J Cocke S A D Pietra V J D Pietra F Jelinek J D Lafferty
R L Mercer and P S Roossin A statistical approach to machine translation
Computational linguistics 1627985 1990
20 C Callison-Burch M Osborne and P Koehn Re-evaluation the role of bleu in
machine translation research In EACL volume 6 pages 249256 2006
21 A Carnie Syntax A generative introduction John Wiley  Sons 2013
22 M Cettolo C Girardi and M Federico Wit3 Web inventory of transcribed
and translated talks In Proceedings of the 16th Conference of the European As-
sociation for Machine Translation EAMT pages 261268 Trento Italy May
23 O Chapelle B Scholkopf and A Zien editors Semi-Supervised Learning
MIT Press Cambridge MA 2006
24 S F Chen and J Goodman An empirical study of smoothing techniques for
language modeling In Proceedings of the 34th annual meeting on Association
for Computational Linguistics pages 310318 Association for Computational
Linguistics 1996
25 X Chen and C L Zitnick Learning a recurrent visual representation for image
caption generation arXiv14115654 2014
26 K Cho A Courville and Y Bengio Describing multimedia content using
attention-based encoderdecoder networks 2015
27 K Cho B van Merrienboer D Bahdanau and Y Bengio On the properties
of neural machine translation Encoder-decoder approaches arXiv preprint
arXiv14091259 2014
28 K Cho B Van Merrienboer C Gulcehre D Bahdanau F Bougares
H Schwenk and Y Bengio Learning phrase representations using rnn encoder-
decoder for statistical machine translation arXiv preprint arXiv14061078
29 K Cho B van Merrienboer C Gulcehre F Bougares H Schwenk and Y Ben-
gio Learning phrase representations using RNN encoder-decoder for statistical
machine translation In Proceedings of the Empiricial Methods in Natural Lan-
guage Processing EMNLP 2014 Oct 2014
30 N Chomsky A review of B F skinners verbal behavior Language 35126
58 1959
31 N Chomsky Linguistic contributions to the study of mind future Language
and thinking pages 323364 1968
32 N Chomsky Syntactic structures Walter de Gruyter 2002
33 R Collobert J Weston L Bottou M Karlen K Kavukcuoglu and P Kuksa
Natural language processing almost from scratch The Journal of Machine
Learning Research 1224932537 2011
34 T M Cover Geometrical and statistical properties of systems of linear inequal-
ities with applications in pattern recognition IEEE Transactions on Electronic
Computers EC-143326334 1965
35 J Denker and Y Lecun Transforming neural-net output levels to probability
distributions In Advances in Neural Information Processing Systems 3 Citeseer
36 M Denkowski and A Lavie Meteor universal Language specic translation
evaluation for any target language In Proceedings of the EACL 2014 Workshop
on Statistical Machine Translation 2014
37 J Donahue L A Hendricks S Guadarrama M Rohrbach S Venugopalan
K Saenko and T Darrell Long-term recurrent convolutional networks for vi-
sual recognition and description arXiv14114389 2014
38 D Dong H Wu W He D Yu and H Wang Multi-task learning for multiple
language translation ACL 2015
39 N Durrani B Haddow P Koehn and K Heaeld Edinburghs phrase-based
machine translation systems for WMT-14 In Proceedings of the Ninth Work-
shop on Statistical Machine Translation pages 97104 Association for Com-
putational Linguistics Baltimore MD USA 2014
40 H Fang S Gupta F Iandola R Srivastava L Deng P Dollar J Gao X He
M Mitchell J C Platt C L Zitnick and G Zweig From captions to visual
concepts and back arXiv14114952 2014
41 J R Firth A synopsis of linguistic theory 1930-1955 Oxford Philological
Society 1957
42 R Fletcher Practical Methods of Optimization Wiley-Interscience New York
NY USA 2nd edition 1987
43 M L Forcada and R P Neco Recursive hetero-associative memories for trans-
lation In Biological and Articial Computation From Neuroscience to Tech-
nology pages 453462 Springer 1997
44 D Furcy and S Koenig Limited discrepancy beam search
125131 2005
In IJCAI pages
45 F A Gers J Schmidhuber and F Cummins Learning to forget Continual
prediction with lstm Neural computation 121024512471 2000
46 X Glorot A Bordes and Y Bengio Deep sparse rectier neural networks
In International Conference on Articial Intelligence and Statistics pages 315
323 2011
47 Y Goldberg A primer on neural network models for natural language process-
ing arXiv preprint arXiv151000726 2015
48 I Goodfellow D Warde-farley M Mirza A Courville and Y Bengio Maxout
In Proceedings of the 30th International Conference on Machine
networks
Learning ICML-13 pages 13191327 2013
49 K Greff R K Srivastava J Koutnk B R Steunebrink and J Schmidhuber
Lstm A search space odyssey arXiv preprint arXiv150304069 2015
50 K He X Zhang S Ren and J Sun Delving deep into rectiers Sur-
passing human-level performance on imagenet classication arXiv preprint
arXiv150201852 2015
51 K Heaeld I Pouzyrevsky J H Clark and P Koehn Scalable modied
In Proceedings of the 51st Annual
Kneser-Ney language model estimation
Meeting of the Association for Computational Linguistics pages 690696 Soa
Bulgaria August 2013
52 S Hochreiter Y Bengio P Frasconi and J Schmidhuber Gradient ow in
recurrent nets the difculty of learning long-term dependencies volume 1 A
eld guide to dynamical recurrent neural networks IEEE Press 2001
53 S Hochreiter and J Schmidhuber Long short-term memory Neural computa-
tion 9817351780 1997
54 G-B Huang Q-Y Zhu and C-K Siew Extreme learning machine Theory
and applications Neurocomputing 7013489501 2006
55 S Jean K Cho R Memisevic and Y Bengio On using very large target
vocabulary for neural machine translation In ACL 2015 2014
56 Y Jernite A M Rush and D Sontag A fast variational approach for learn-
ing markov random eld language models 32nd International Conference on
Machine Learning ICML 2015
57 Y Ji T Cohn L Kong C Dyer and J Eisenstein Document context language
models arXiv preprint arXiv151103962 2015
58 R Jozefowicz W Zaremba and I Sutskever An empirical exploration of recur-
rent network architectures In Proceedings of the 32nd International Conference
on Machine Learning ICML-15 pages 23422350 2015
59 A Karpathy and F-F Li Deep visual-semantic alignments for generating image
descriptions arXiv14122306 2014
60 D Kiela and L Bottou Learning image embeddings using convolutional neural
networks for improved multi-modal semantics Proceedings of EMNLP 2014
61 Y Kim Y Jernite D Sontag and A M Rush Character-aware neural language
models arXiv preprint arXiv150806615 2015
62 R Kiros R Salakhutdinov and R Zemel Multimodal neural language models
In ICML2014 2014
63 R Kneser and H Ney Improved backing-off for m-gram language modeling
In Acoustics Speech and Signal Processing 1995 ICASSP-95 1995 Interna-
tional Conference on volume 1 pages 181184 IEEE 1995
64 P Koehn Pharaoh a beam search decoder for phrase-based statistical machine
translation models In Machine translation From real users to research pages
115124 Springer 2004
65 P Koehn Europarl A parallel corpus for statistical machine translation In MT
summit volume 5 pages 7986 Citeseer 2005
66 P Koehn F J Och and D Marcu Statistical phrase-based translation In Pro-
ceedings of the 2003 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Language Technology-Volume 1
pages 4854 Association for Computational Linguistics 2003
67 A Krizhevsky I Sutskever and G E Hinton Imagenet classication with deep
convolutional neural networks In Advances in neural information processing
systems pages 10971105 2012
68 T S Kuhn The structure of scientic revolutions University of Chicago press
69 Q V Le N Jaitly and G E Hinton A simple way to initialize recurrent net-
works of rectied linear units arXiv preprint arXiv150400941 2015
70 Y LeCun Y Bengio and G Hinton Deep learning Nature 5217553436
444 2015
71 Y LeCun L Bottou G Orr and K R Muller Efcient BackProp In G Orr
and K Muller editors Neural Networks Tricks of the Trade volume 1524 of
Lecture Notes in Computer Science pages 550 Springer Verlag 1998
72 O Levy and Y Goldberg Neural word embedding as implicit matrix factor-
ization In Z Ghahramani M Welling C Cortes N Lawrence and K Wein-
berger editors Advances in Neural Information Processing Systems 27 pages
21772185 Curran Associates Inc 2014
73 W Ling I Trancoso C Dyer and A W Black Character-based neural machine
translation arXiv preprint arXiv151104586 2015
74 D G Lowe Object recognition from local scale-invariant features In Computer
vision 1999 The proceedings of the seventh IEEE international conference on
volume 2 pages 11501157 Ieee 1999
75 M-T Luong Q V Le I Sutskever O Vinyals and L Kaiser Multi-task
sequence to sequence learning arXiv preprint arXiv151106114 2015
76 M-T Luong H Pham and C D Manning Effective approaches to attention-
based neural machine translation arXiv preprint arXiv150804025 2015
77 C D Manning and H Schutze Foundations of statistical natural language
processing MIT press 1999
78 J Mao W Xu Y Yang J Wang and A L Yuille Explain images with multi-
modal recurrent neural networks arXiv14101090 2014
79 T Mikolov K Chen G Corrado and J Dean Efcient estimation of word
representations in vector space arXiv preprint arXiv13013781 2013
80 T Mikolov M Karaat L Burget J Cernocky and S Khudanpur Recurrent
neural network based language model In INTERSPEECH 2010 pages 1045
1048 2010
81 V Nair and G E Hinton Rectied linear units improve restricted boltzmann
In Proceedings of the 27th International Conference on Machine
machines
Learning ICML-10 pages 807814 2010
82 K Papineni S Roukos T Ward and W-J Zhu Bleu a method for automatic
evaluation of machine translation In Proceedings of the 40th annual meeting
on association for computational linguistics pages 311318 Association for
Computational Linguistics 2002
83 R Pascanu T Mikolov and Y Bengio On the difculty of training recurrent
neural networks In Proceedings of The 30th International Conference on Ma-
chine Learning pages 13101318 2013
84 A Perfors J Tenenbaum and T Regier Poverty of the stimulus a rational
approach In Annual Conference 2006
85 K B Petersen M S Pedersen et al The matrix cookbook Technical University
of Denmark 715 2008
86 C W Post The Three Percent Problem Rants and Responses on Publishing
Translation and the Future of Reading Open Letter 2011
87 P Resnik and N A Smith The web as a parallel corpus Computational Lin-
guistics 293349380 2003
88 H Robbins and S Monro A stochastic approximation method The Annals of
Mathematical Statistics 223400407 1951
89 F Rosenblatt Principles of neurodynamics perceptrons and the theory of brain
mechanisms Report Cornell Aeronautical Laboratory Spartan Books 1962
90 S Russell and P Norvig Articial intelligence a modern approach 1995
91 J Schmidhuber Deep learning in neural networks An overview Neural Net-
works 6185117 2015
92 H Schwenk Continuous space language models Computer Speech  Lan-
guage 213492518 2007
93 R Sennrich B Haddow and A Birch Neural machine translation of rare words
with subword units arXiv preprint arXiv150807909 2015
94 P Sermanet D Eigen X Zhang M Mathieu R Fergus and Y LeCun Over-
feat Integrated recognition localization and detection using convolutional net-
works arXiv preprint arXiv13126229 2013
95 C Shannon The bandwagon edtl IRE Transactions on Information Theory
123 1956
96 K Simonyan and A Zisserman Very deep convolutional networks for large-
scale image recognition arXiv preprint arXiv14091556 2014
97 B F Skinner Verbal behavior BF Skinner Foundation 2014
98 J R Smith H Saint-Amand M Plamada P Koehn C Callison-Burch and
A Lopez Dirt cheap web-scale parallel text from the common crawl In ACL
1 pages 13741383 2013
99 M Snover B Dorr R Schwartz L Micciulla and J Makhoul A study of trans-
lation edit rate with targeted human annotation In Proceedings of association
for machine translation in the Americas pages 223231 2006
100 M Sundermeyer H Ney and R Schluter From feedforward to recurrent lstm
neural networks for language modeling Audio Speech and Language Process-
ing IEEEACM Transactions on 233517529 2015
101 I Sutskever O Vinyals and Q V Le Sequence to sequence learning with
neural networks In Advances in neural information processing systems pages
31043112 2014
102 C Szegedy W Liu Y Jia P Sermanet S Reed D Anguelov D Erhan V Van-
houcke and A Rabinovich Going deeper with convolutions arXiv preprint
arXiv14094842 2014
103 J Turian L Ratinov and Y Bengio Word representations a simple and general
method for semi-supervised learning In Proceedings of the 48th annual meeting
of the association for computational linguistics pages 384394 Association for
Computational Linguistics 2010
104 L van der Maaten and G E Hinton Visualizing data using t-SNE Journal of
Machine Learning Research 925792605 November 2008
105 V Vapnik The Nature of Statistical Learning Theory Springer-Verlag New
York Inc New York NY USA 1995
106 O Vinyals A Toshev S Bengio and D Erhan Show and tell A neural image
caption generator arXiv preprint arXiv14114555 2014
107 T Wang and K Cho Larger-context language modelling arXiv preprint
arXiv151103729 2015
108 W Weaver Translation Machine translation of languages 141523 1955
109 J Weston S Bengio and N Usunier Large scale image annotation learning to
rank with joint word-image embeddings Machine learning 8112135 2010
110 T Winograd Understanding natural language Cognitive psychology 311
191 1972
111 K Xu J Ba R Kiros K Cho A Courville R Salakhutdinov R Zemel and
Y Bengio Show attend and tell Neural image caption generation with visual
attention In International Conference on Machine Learning 2015
112 Y Zhang K Wu J Gao and P Vines Automatic acquisition of chineseenglish
parallel corpus from the web In Advances in Information Retrieval pages 420
431 Springer 2006
113 R Zhou and E A Hansen Beam-stack search Integrating backtracking with
beam search In ICAPS pages 9098 2005
Natural Language Understanding with
Distributed Representation
Kyunghyun Cho
Courant Institute of Mathematical Sciences and
Center for Data Science
New York University
November 26 2015
Abstract
This is a lecture note for the course DS-GA 3001 cid104Natural Language Understanding
with Distributed Representationcid105 at the Center for Data Science1 New York University
in Fall 2015 As the name of the course suggests this lecture note introduces readers
to a neural network based approach to natural language understandingprocessing In
order to make it as self-contained as possible I spend much time on describing basics of
machine learning and neural networks only after which how they are used for natural
languages is introduced On the language front I almost solely focus on language
modelling and machine translation two of which I personally nd most fascinating
and most fundamental to natural language understanding
After about a month of lectures and about 40 pages of writing this lecture note I
found this fascinating note 47 by Yoav Goldberg on neural network models for natural
language processing This note deals with wider topics on natural language processing
with distributed representations in more details and I highly recommend you to read it
hopefully along with this lecture note I seriously wish Yoav had written it earlier so
that I couldve simply used his excellent note for my course
This lecture note had been written quite hastily as the course progressed meaning
that I could spare only about 100 hours in total for this note This is my lame excuse
for likely many mistakes in this lecture note and I kindly ask for your understanding
in advance Again how grateful I wouldve been had I found Yoavs note earlier
I am planning to update this lecture note gradually over time hoping that I will
be able to convince the Center for Data Science to let me teach the same course next
year The latest version will always be available both in pdf and in latex source code
from httpsgithubcomnyu-dlNLPDLLectureNote The arXiv
version will be updated whenever a major revision is made
I thank all the students and non-students who took2 this course and David Rosen-
berg for feedback
1 httpcdsnyuedu
2 In fact they are still taking the course as of 24 Nov 2015 They have two guest lectures and a nal exam
left until the end of the course
Contents
Introduction
11 Route we will not take
     
            
111 What is Language 
                     
112 Language Understanding                    
               
121 Language as a Function                    
122 Language Understanding as a Function Approximation    
     
12 Road we will take 
2 Function Approximation as Supervised Learning
21 Function Approximation Parametric Approach            
211 Expected Cost Function                    
212 Empirical Cost Function                    
22 Learning as Optimization                        
221 Gradient-based Local Iterative Optimization          
                 
23 When do we stop learning                       
231 Early Stopping                         
                     
232 Model Selection  
24 Evaluation 
                        
25 Linear Regression for Non-Linear Functions              
Feature Extraction                       
Stochastic Gradient Descent
3 Neural Networks and Backpropagation Algorithm
31 Conditional Distribution Approximation                
311 Why do we want to do this                  
312 Other Distributions                       
32 Feature Extraction is also a Function                  
33 Multilayer Perceptron                          
331 Example Binary classication with a single hidden unit
332 Example Binary classication with more than one hidden units 29
34 Automating Backpropagation                      
341 What if a Function is not Differentiable
    
4 Recurrent Neural Networks and Gated Recurrent Units
   
      
Fixed-Size Output y
42 Gated Recurrent Units
41 Recurrent Neural Networks                       
       
412 Multiple Child Nodes and Derivatives             
413 Example Sentiment Analysis
                
414 Variable-Length Output y x  y
   
                       
421 Making Simple Recurrent Neural Networks Realistic    
422 Gated Recurrent Units                     
423 Long Short-Term Memory                   
                         
                     
431 Rectiers Explode 
                     
Is tanh a Blessing 
433 Are We Doomed  
                     
434 Gated Recurrent Units Address Vanishing Gradient      
43 Why not Rectiers 
    
    
5 Neural Language Models
51 Language Modeling First Step                     
511 What if those linguistic structures do exist           
512 Quick Note on Linguistic Units
               
52 Statistical Language Modeling                     
521 Data SparsityScarcity                     
n-Gram Language Model  
                     
Smoothing and Back-Off                    
532 Lack of Generalization                     
                       
541 How does Neural Language Model Generalize to Unseen n-
Grams  Distributional Hypothesis              
54 Neural Language Model
542 Continuous Bag-of-Words Language Model
Maximum PseudoLikelihood Approach           
Semi-Supervised Learning with Pretrained Word Embeddings
55 Recurrent Language Model                       
56 How do n-gram language model neural language model and RNN-LM
                      
compare 
6 Neural Machine Translation
61 Statistical Approach to Machine Translation              
Parallel Corpora Training Data for Machine Translation   
612 Automatic Evaluation Metric                  
62 Neural Machine Translation
Simple Encoder-Decoder Model
                   
Sampling vs Decoding                     
63 Attention-based Neural Machine Translation              
          
64 Warren Weavers Memorandum                    
631 What does the Attention Mechanism do
7 Final Words
71 Multimedia Description Generation as Translation           
72 Language Understanding with World Knowledge           
73 Larger-Context Language Understanding
Beyond Sentences and Beyond Words                 
74 Warning and Summary                         
Chapter 1
Introduction
This lecture is going to be the only one where I discuss some philosophical meaning
nonpractical arguments because according to Chris Manning and Hinrich Schuetze
even practically-minded people have to confront the issue of what prior knowledge to
try to build into their model 77
11 Route we will not take
111 What is Language
The very rst question we must ask ourselves before starting this course is the ques-
tion of what natural language is Of course the rest of this course does not in any
way require us to know what natural language is but it is a philosophical question I
recommend everyone including myself to ponder upon once a while
When I start talking about languages with anyone there is a single person who
never misses to be mentioned that is Noam Chomsky His view has greatly inuenced
the modern linguistics and although many linguists I have talked to claim that their
work and eld have long moved on from Chomskys I can feel his shadow all over
My rst encounter with Chomsky was at the classroom of Automata from my
early undergrad years I was not the most attentive student back then and all I can
remember is Chomskys hierarchy and how it has shaped our view on languages in this
context programmingcomputer languages A large part of the course was dedicated
to explaining which class of languages emerges given a set of constraints on a set of
generating rules or production rules
For instance if we are given a set of generating rules that do not depend on the con-
textmeaning of non-terminal symbols context-free grammar CFG we get a context-
free language If we put a bit of constraints to CFG that each generating rule is such
that a non-terminal symbol is replaced by either a terminal symbol a terminal symbol
by a non-terminal symbol or an empty symbol then we get a regular grammar Sim-
ilarly to CFG we get a regular language from the regular grammar and the regular
language is a subset of the context-free language
What Chomsky believes is that this kind of approach applies also to human lan-
guages or natural languages There exists a set of generating rules that generates a
natural language But then the obvious question to follow is where those generating
rules are Where are they stored How are they stored Do we have separate generating
rules for different languages
112 Language Understanding
Understanding Human Language Those questions are interesting but out of scope
for this course Those questions are the ones linguists try to answer Generative linguis-
tics aims at guring out what those rules are how they are combined to form a valid
sentence how they are adapted to different languages and so on We will leave these to
linguists and continue on to our journey of building a machine that understands human
languages
Natural Language Understanding So lets put these questions aside and trust Chom-
sky that we humans are specially designed to store those generating rules somewhere
in the brain 30 21 Or better yet lets trust Chomsky that theres a universal gram-
mar built in our brain In other words lets say we were born with this set of generating
rules for natural languages and while growing we have adapted this universal gram-
mar toward our native tongue language variation
When we decide to speak of something whatever that is and however implausi-
ble that is our brain quickly picks up a sequence of some of those generating rules
and starts generating a sentence accordingly Of course those rules do not generate a
sentence directly but generates a sequence of control signals to move our muscles to
make sound When heard by other people who understand your language the sound
becomes a sentence
In our case we are more interested in a machine hearing that sound or a sentence
from here on When a machine heard this sentence what wouldshould a language un-
derstanding machine do to understand a language or more simply a sentence Again
we are assuming that this sentence was generated from applying a sequence of the
existing generating rules
Under our assumption a natural rst step that comes to my mind is to gure out
that sequence of the generating rules which led to the sentence Once the sequence is
found or in a fancier term inferred the next step will be to gure out what kind of
mental state of the speaker led to those generating rules
Lets take an example sentence Our company is training workers from Sec 13
of 77 which is a horrible choice because this was used as an example of ambiguity
in parsing Regardless a speaker obviously has an awesome image of her company
which trains its workers and wants to tell a machine about this This mental state is
used to select the following generating rules assuming a phrase structure grammar1
1 Stanford Parser httpnlpstanfordedu8080parser
NP PRP Our NN company
VP VBZ is
VP VBG training
NP NNS workers
Figure 11 A parse of Our company is training workers
The machine hears the sentence Our company is training workers and infers
the parse in Fig 11 Then we can make a simple set of rules again
to let the
machine answer questions about this sentence kinds of questions that imply that the
machine has understood the sentence language For instance given a question Who
is training workers the machine can answer by noticing that the question is asking
for the subject of the verb phrase is training acted on the object workers and that
the subject is Our company
Side Note Bayesian Language Understanding This generative view of languages
ts quite well with Bayesian modelling see eg 84 There exists a hidden mecha-
nism or a set of generating rules and a rule governing their composition which can be
modelled as a latent variable Z Given these rules a language or a sentence X is gen-
erated according to the conditional distribution PXZ Then understanding language
by humans is equivalent to computing the posterior distribution over all possible sets
of generating rules and their compositional rules ie PZX This answers the ques-
tion of what is the most likely mechanism underlying the observed language
Furthermore from the perspective of machines Bayesian approach is attractive In
this case we assume to know the set of rules in advance and let the latent variable Z
denote the specic conguration use of those rules Given this sequence of applying
the rules a sentence X is generated via the conditional distribution PXZ Machine
understanding of language is equivalent to inferring the posterior distribution over Z
given X
For more details about Bayesian approaches in the context of machine learning
please refer to 13 or take the course DS-GA 1005 Inference and Representation by
Prof David Sontag
pa13TheAmbiguityofLanguageWhyNLPIsDicult17PhilosophicallythisbringsusclosetothepositionadoptedinthelaterwritingsofWittgensteinthatisWittgenstein1968wherethemean-ingofawordisdenedbythecircumstancesofitsuseausetheoryofusetheoryofmeaningmeaningseethequotationsatthebeginningofthechapterUnderthisconceptionmuchofStatisticalNLPresearchdirectlytacklesquestionsofmeaning13TheAmbiguityofLanguageWhyNLPIsDicultAnNLPsystemneedstodeterminesomethingofthestructureoftextnormallyatleastenoughthatitcananswerWhodidwhattowhomConventionalparsingsystemstrytoanswerthisquestiononlyintermsofpossiblestructuresthatcouldbedeemedgrammaticalforsomechoiceofwordsofacertaincategoryForexamplegivenareasonablegrammarastandardNLPsystemwillsaythatsentence110has3syntacticanal-ysesoftencalledparses110OurcompanyistrainingworkersThethreedieringparsesmightberepresentedasin111111aSNPOurcompanyVPAuxisVPVtrainingNPworkersbSNPOurcompanyVPVisNPVPVtrainingNPworkersUnderstanding vs Using Whats clear from this example is that in this generative
view of languages there is a clear separation between understanding and using In-
ferring the generating rules from a given sentence is understanding and answering a
question based on this understanding using is a separate activity Understanding part
is done when the underlying true structure has been determined regardless of how
this understanding be used
To put it in a slightly different wording language understanding does not require its
use or downstream tasks In this road that we will not take in this course understanding
exists as it is regardless of what the understood insightknowledge will be used for
And this is the reason why we do not walk down this road
12 Road we will take
121 Language as a Function
In this course we will view a naturalhuman language as a system intended to com-
municate ideas from a speaker to a hearer 110 What this means is that we do not
view a language as a separate entity that exists on its own Rather we view a whole
system or behaviour of communication as a language Furthermore this view dictates
that we must take into account the world surrounding a speaker and a hearer in order
to understand language
Under this view of language language or rather its usage become somewhat similar
to action or behaviour Speaking of something is equivalent to acting on a listener as
both of them inuence the listener in one way or another The purpose of language
is then to inuence another by efciently communicate ones will or intention2 This
hints at how language came to be or may have come to be evolution language
has evolved to facilitate the exchange of ideas among people learning humans learn
language by being either encouraged or punished for the use of language This latter
view on how language came to be is similar in spirit to the behaviourism of B F
Skinner necessary mediation of reinforcement by another organism 97
This is a radical departure from the generative view of human language where
language existed on its own and its understanding does not necessarily require the
existence of the outside world nor the existence of a listener
It is no wonder why
Chomsky was so harsh in criticizing Skinners work in 30 This departure as I see
it is the departure toward a functional view of language Language is a function of
communication
122 Language Understanding as a Function Approximation
Lets make a large jump here such that we consider this function as a mathematical
function This function called language takes as input the state of the surrounding
world the speakers speech either written spoken or signed and the listeners mental
2 Chomsky does not agree it is wrong to think of human use of language as characteristically informa-
tive in fact or in intention 31
state3 Inside the function the listeners mental state is updated to incorporate the new
idea from the speakers speech The function then returns a response by the listener
which may include no response as well and a set of non-verbal action sequences
what would be the action sequence if the speaker insulted the listener
In this case language understanding both from humans and machines perspec-
tive boils down to guring out the internal working of this function In other words we
understand language by learning the internal mechanism of the function Furthermore
this view suggests that the underlying structures of language are heavily dependent on
the surrounding environment context as well as on the target task The former con-
text dependence is quite clear as the function takes as input the context but the latter
may be confusing now Hopefully this will become clearer later in the course
How can we approximate this function How can we gure out the internal working
mechanism of this function What tools do we have
Language Understanding by Machine Learning This functional view of languages
suddenly makes machine learning a very appealing tool for understanding human lan-
guages After all function approximation is the core of machine learning Classica-
tion is a classical example of function approximation clustering is a function approxi-
mation where the target is not given generative modeling learns a function that returns
a probability of an input and so on
When we approximate a function in machine learning the prime ingredient is data
We are given data which was either generated from this function unsupervised learn-
ing or well t this function supervised learning based on which we adjust our ap-
proximation to the function often iteratively to best t the data But I must note here
that it does not matter how well the approximated function ts the data it was tted to
but matters how well this approximation ts unseen data4
In language understanding this means that we collect a large data set of input and
output pairs or conversations together with the recording of the surrounding environ-
ment and t some arbitrary function to well predict the output given an input We
probably want to evaluate this approximation in a novel conversation If this function
makes a conversation just like a person voila we made a machine that passed the
Turing test Simple right
Problem Unfortunately as soon as we try to do this we run into a big problem This
problem is not from machine learning nor languages but the denition of this function
of language
Properly approximating this function requires us to either simulate or record the
whole world in fact the whole universe For this function takes as input and main-
tains as internal state the surrounding world context and the mental state of the in-
dividual speaker This is unavoidable if we wanted to very well approximate this
function as a whole
It is unclear however whether we want to approximate the full function For a
human to survive yes it is likely that the full function is needed But if our goal is
3 We assume here that a such thing exists however it is represented in our brain
4 This is a matter of generalization and we will talk about this more throughout the course
restricted to a certain task such as translation language modelling and so on we may
not want to approximate this function fully We probably want to approximate only a
subset of this whole function For instance if our goal is to understand the process
of translation from one language to another we can perhaps ignore all but the speech
input to the function and all but the speech output from the function because often a
trained person can translate a sentence in one language to another without knowing
the whole context
This latter approach to language understandingapproximating a partial function
of languages will be at the core of this course We will talk about various language
tasks that are a part of this whole function of language These tasks will include but
are not limited to language modelling machine translation imagevideo description
generation and question answering For these tasks and potentially more we will study
how to use machine learning or more specically deep learning to solve these tasks
by approximating sub-functions of language
Chapter 2
Function Approximation as
Supervised Learning
Throughout this course we will extensively use articial neural networks1 to approx-
imate a part of the function of natural language This makes it necessary for us to
study the basics of neural networks rst and this lecture and a couple of subsequent
ones are designed to serve this purpose
21 Function Approximation Parametric Approach
211 Expected Cost Function
Let us start by dening a data distribution pdata pdata is dened over a pair of input
and output vectors x  Id and y  Ok respectively I and O are respectively sets of
all possible input and output values such as R 01 and 01    L This data
distribution is not known to us
The goal is to nd a relationship between x and y More specically we are in-
terested in nding a function f  Rd  Ok that generates the output y given its corre-
sponding input x The very rst thing we should do is to put some constraints on the
function f to make our search for the correct f a bit less impossible In this lecture
and throughout the course I will consider only a parametric function f  in which case
the function is fully specied with a set of parameters 
Next we must dene a way to measure how well the function f approximates
the underlying mechanism of generation x  y Lets denote by y the output of the
function with a particular set  of parameters and a given input x
1 From here on I will simply drop articial and call them neural networks Whenever I say neural
network it refers to articial neural networks
y  f x
How well f approximates the true generating function is equivalent to how far y is from
the correct output y Lets use Dyy for now call this distance2 between y and y
It is clear that we want to nd  that minimizes Dyy for every pair in the space
RRd Ok But wait every pair equally likely Probably not for we do not care how
well f approximates the true function when a pair of input x and output y is unlikely
meaning we do not care how bad the approximation is if pdataxy is small However
this is a bit difcult to take into account as we must decided on the threshold below
which we consider any pair irrelevant
Hence we weight the distance between the approximated y and the correct y of
each pair xy in the space by its probability pxy Mathematically saying we want
to nd
where the integral cid82 should be replaced with the summation  if any of x and y is
pdataxyDyydxdy
cid90
cid90
discrete
We call this quantity being minimized with respect to the parameters  a cost func-
tion C  This is equivalent to computing the expected distance between the predicted
output y and the correct one y
C  
pdataxyDyydxdy
cid90
cid90
Exypdata Dyy
This is often called an expected loss or risk and minimizing this cost function is re-
ferred to as expected risk minimization 105
Unfortunately C  cannot be exactly computed for a number of reasons The
most important reason among them is simply that we dont know what the data distri-
bution pdata is Even if we have access to pdata we can exactly compute C  only with
heavy assumptions on both the data distribution and the distance function3
212 Empirical Cost Function
This does not mean that we are doomed from the beginning Instead of the full-blown
description of the data distribution pdata we will assume that someone miraculously
gave us a nite set of pairs drawn from the data distribution We will call this a training
cid8x1y1     xNyNcid9 
As we have access to the samples from the data distribution we can use Monte
Carlo method to approximate the expected cost function C  such that
C   C  
D ynyn
2 Note that we do not require this distance to satisfy the triangular inequality meaning that it does not
have to be a distance However I will just call it distance for now
We call this approximate C  of the expected cost function an empirical cost function
or empirical risk or empirical loss
Because empirical cost function is readily computable we will mainly work with
the empirical cost function not with the expected cost function However keep in mind
that at the end of the day the goal is to nd a set of parameters that minimizes the
expected cost
22 Learning as Optimization
We often call this process of nding a good set of parameters that minimizes the ex-
pected cost learning This term is used from the perspective of a machine which imple-
ments the function f  as it learns to approximate the true generating function f from
training data
From what I have described so far it may have become clear even without me men-
tioning that learning is optimization We have a clearly dened function the empirical
cost function C which needs to be minimized with respect to its input 
221 Gradient-based Local Iterative Optimization
There are many optimization algorithms one can use to nd a set of parameters that
minimizes C Sometimes you can even nd the optimal set of parameters in a closed
form equation4 In most cases because there is no known closed-form solution it is
typical to use an iterative optimization algorithm see 42 for in-depth discussion on
optimization
By an iterative optimization I mean an algorithm which renes its estimate of the
optimal set of parameters little by little until the values of the parameters converge to
the optimal expected cost function Also it is worthwhile to note that most iterative
optimization algorithms are local in the sense that they do not require us to evaluate
the whole parameter space but only a small subset along the path from the starting
point to the convergence point5
Here I will describe the simplest one among those local iterative optimization algo-
rithms called gradient descent GD algorithm As the name suggests this algorithm
depends entirely on the gradient of the cost function6
4 One such example is a linear regression where
 f Wx  Wx
 Dyy  1
2cid107y ycid1072
In this case the optimal W is
W  YXcid62XXcid621
X cid2x1   xNcid3 Y cid2y1   yNcid3 
Try it yourself
5 There are global optimization algorithms but they are out of scope for this course See for instance
18 for one such algorithm called Bayesian optimization
6 From here on I will use the cost function to refer to the empirical cost function
blue
Figure 21
sin10x  x
x  06
gradient at x  06
f x 
red a gradient at
magenta a negative
The gradient of a function  C is a vector whose direction points to the direction of
the greatest rate of increase in the functions value and whose magnitude measures this
rate At each point  t in the parameter space the gradient of the cost function  C t 
is the opposite direction toward which we want to move the parameters See Fig 21
for graphical illustration
One important point of GD that needs to be mentioned here is on how large a
step one takes each time As clear from the magenta line the direction opposite to
the direction given by the gradient in Fig 21 if too large a step is taken toward the
negative gradient direction the optimization process will overshoot and miss the local
minimum around x  08 This step size or sometimes called learning rate  is one
most important hyperparameter of the GD algorithm
Now we have all the ingredients for the GD algorithm  C and  The GD algo-
rithm iterates the following step
     C 
The iteration continues until a certain stopping criterion is met which we will discuss
shortly
222 Stochastic Gradient Descent
This simple GD algorithm works surprisingly quite well and it is a fundamental basis
upon which many advanced optimization algorithms have been built I will present a
list of few of those advanced algorithms later on and discuss them briey But before
going into those advanced algorithms lets solve one tiny but signicant issue of the
GD algorithm
This tiny but signicant issue arises especially often in machine learning That is
it is computationally very expensive to compute C and consequently its gradient  C
thanks to the ever increasing size of the training set D
Why is the growing size of the training set making it more and more computation-
ally demanding to compute C and  C This is because both of them are essentially
the sum of as many per-sample costs as there are examples in the training set In other
Cxnyn 
C  
 C  
 Cxnyn 
And N goes up to millions or billions very easily these days
This enormous computational cost involved in each GD step has motivated the
stochastic gradient descent SGD algorithm 88 15
First recall from Eq 23 that the cost function we minimize is the empirical
cost function C which is the sample-based approximation to the expected cost function
C This approximation was done by assuming that the training examples were drawn
randomly from the data distribution pdata
C   C  
D ynyn
In fact as long as this assumption on the training set holds we can always approximate
the expected cost function with a fewer number of training examples
C   CM   
M 
D ymym
where M cid28 N and M is the indices of the examples in this much smaller subset of the
training set We call this small subset a minibatch
Similarly this leads to a minibatch-based estimate of the gradient as well
 CM   
M 
D ymym
It must now be clear to you where I am headed toward At each GD step instead
of using the full training set we will use a small subset M which is randomly selected
to compute the gradient estimate In other words we use CM instead of C and  CM
instead of  C in Eq 25
Because computing CM and  CM is independent of the size of the training set we
can use SGD to make as many steps as we want without worrying about the growing
size of training examples This is highly benecial as regardless of how many train-
ing examples you used to compute the gradient we can only take a tiny step toward
that descending direction Furthermore the increased level of noisy in the gradient
estimate due to the small sample size has been suspected to help reaching a better so-
lution in high-dimensional non-convex problems such as those in training deep neural
networks 717
7 Why would this be the case It is worth thinking about this issue further
We can set M to be any constant and in an extreme we can set it to 1 as well In
this case we call it online SGD8 Surprisingly already in 1951 it was shown that using
a single example each time is enough for the SGD to converge to a minimum under
certain conditions obviously 88
This SGD algorithm will be at the core of this course and will be discussed further
in the future lectures
23 When do we stop learning
From here on I assume that we approximate the ground truth function by iteratively
rening its set of parameters in most cases using stochastic gradient descent In other
words learning of a machine that approximates the true generating function f happens
gradually as the machine goes over the training examples little by little over time
Let us go over again what kind of constraintsissue we have rst
1 Lack of access to the expected cost function C 
2 Computationally expensive empirical cost function C 
3 Potential non-convexity of the empirical cost function C 
The most severe issue is that we do not have access to the expected cost function
which is the one we want to minimize in order to work well with any pair of input x
and output y Instead we have access to the empirical cost function which is a nite
sample approximation to the expected cost function
Why is this a problem Because we do not have a guarantee that the local mini-
mum of the empirical cost function corresponds to the local minimum of the expected
cost function An example of this mismatch between the expected and empirical cost
functions is shown in Fig 22
As in the case shown in Fig 22 it is not desirable to minimize the empirical cost
function perfectly The parameters that perfectly minimize the empirical cost function
in the case of Fig 22 the slope a of a linear function f x  ax will likely be a
sub-optimal cost for the expected cost function about which we really care
231 Early Stopping
What should we do There are many ways to avoid this weird contradiction where
we want to optimize the cost function well but not too well Among those one most
important trick is early stopping which is only applicable when iterative optimization
is used
First we will split the training set D into two partitions Dtrain and Dval9 We call
them a training set and a validation set respectively In practice it is a good idea to
keep D much larger than Dcid48 because of the reasons that will become clear shortly
8 Okay this is not true in a strict sense SGD is an online algorithm with M  1 originally and using
M  1 is a variant of SGD often called minibatch SGD However as using minibatches M  1 is almost
always the case in practice I will refer to minibatch SGD as SGD and to the original SGD as online SGD
9 Later on we will split it further into three partitions
Figure 22 blue Expected cost
function C 
red Empirical
cost function C 
The un-
derlying true generating function
was f x  sin10x  x The
cost function uses the squared Eu-
clidean distance
The empiri-
cal cost function was computed
based on 10 noisy examples of
which xs were sampled from the
uniform distribution between 0
and 1 For each sample input x
noise from zero-mean Gaussian
distribution with standard devia-
tion 001 was added to f x to
emulate the noisy measurement
channel
Further let us dene the training cost as
C   Ctrain  
Dtrain 
xyDtrain
Dtrain yy
and the validation cost as
Cval  
Dval 
xyDval
D yy
With these two cost functions we are all ready to use early stopping now
After every few updates using SGD or GD the validation cost function is evalu-
ated with the current set of parameters The parameters are updated ie the training
cost function is optimized until the validation cost does not decrease or starts to in-
crease instead of decreasing
Thats it It is almost free as long as the size of the validation set is reasonable
since each evaluation is at most as expensive as computing the gradient of the empirical
cost function Because of the simplicity and effectiveness this early stopping strategy
has become de facto standard in deep learning and in general machine learning
The question that needs to be asked here is what the validation cost function does
here Clearly it approximates the expected cost function C similarly to the empirical
cost function C as well as the training cost function Ctrain In the innite limit of the
size of either training or validation set they should coincide but in the case of a nite
set those two cost functions differ by the noise in sampling sampling pairs from the
data distribution and observation noise in y  f x
The fact that we explicitly optimize the training cost function implies that there is
a possibility in fact almost surely in practice that the set of parameters found by this
optimization process may capture not only the underlying generating function but also
noise in the observation and sampling procedure This is an issue because we want our
machine to approximate the true generating function not the noise process involved
The validation cost function measures both the true generating structure as well as
noise injected during sampling and observation However assuming that noise is not
correlated with the underlying generating function noise introduced in the validation
cost function differs from that in the training cost function In other words the set
of parameters that perfectly minimizes the training cost function thereby capturing
even noise in the training set will be penalized when measured by the validation cost
function
232 Model Selection
In fact the use of the validation cost does not stop at the early stopping Rather it has a
more general role in model selection First we must talk about model selection itself
This whole procedure of optimization or learning can be cast as a process of
searching for the best hypothesis over the entire space H of hypotheses Here each
hypothesis corresponds to each possible function with a unique set of parameters and
a unique functional form that takes the input x and output y In the case of regression
x  Rd and y  R the hypothesis space includes an n-th order polynomial function
f x 
k1 iknik0
ai1i2ik
kcid481
kcid48
where ai1i2iks are the coefcients and any other functional form that you can imag-
ine as long as it can process x and return a real-valued scalar In the case of neural
networks this space includes all the possible model architectures which are dened by
the number of layers the type of nonlinearities the number of hidden units in each
layer and so on
Let us use M  H to denote one hypothesis10 One important thing to remember is
that the parameter space is only a subset of the hypothesis space because the parameter
space is dened by a family of hypotheses the parameter space of a linear function
cannot include a set of parameters for a second-order polynomial function
Given a denition of expected cost function we can score each hypothesis M by
the corresponding cost CM Then the whole goal of function approximation boils down
to the search for a hypothesis M with the minimal expected cost function C But of
course we do not have access to the expected cost function and resort to the empirical
cost function based on a given training set
The optimization-based approach we discussed so far searches for the best hypoth-
esis based on the empirical cost iteratively However because of the issue of overtting
which means that the optimization algorithm overshot and missed the local minimum
of the expected cost function because it was aimed at the local minimum of the empir-
ical cost function I introduced the concept of early stopping based on the validation
10 M because each hypothesis corresponds to one learning machine
This is unfortunately not satisfactory as we have only searched for the best hypoth-
esis inside a small subset of the whole hypothesis space H  What if another subset
of the hypothesis space includes a function that better suits the underlying generating
function f  Are we doomed
It is clearly better to try more than one subsets of the hypothesis space For in-
stance for a regression task we can try linear functions H1 quadratic second-order
polynomial functions H2 and sinusoidal functions H3 Lets say for each of these
subsets we found the best hypothesis using iterative optimization and early stopping
MH1 MH2 and MH3 Then the question is how we should choose one of those hy-
potheses
Similar to what weve done with early stopping we can use the validation cost to
compare these hypotheses Among those three we choose one that has the smallest
validation cost CvalM
This is one way to do model selection and we will talk about another way to do
this later
24 Evaluation
But wait if this is an argument for using the validation cost to early stop the optimiza-
tion or learning one needs to notice something weird What is it
Because we used the validation cost to stop the optimization there is a chance
that the set of parameters we found is optimal for the validation set whose structure
consists of both the true generating function and samplingobservation noise but not
to the general data distribution This means that we cannot tell whether the function
estimate f approximating the true generating function f is a good t by simply early
stopping based on the validation cost Once the optimization is done we need yet
another metric to see how well the learned function estimate f approximates f 
Therefore we need to split the training set not into two partitions but into three
partitions We call them a training set Dtrain a validation set Dval and a test set Dtest
Consequently we will have three cost functions a training cost function Ctrain a vali-
dation cost function Cval and a test cost function Ctest similarly to Eqs 2627
This test cost function is the one we use to compare different hypotheses or models
fairly Any hypothesis that worked best in terms of the test cost is the one that you
choose
Lets not Cheat One most important lesson here is that you must never look at a test
set As soon as you take a peak at the test set it will inuence your choice in the model
structure as well as any other hyperparameters biasing toward a better test cost The
best option is to never ever look at the test set until it is absolutely needed eg need
to present your result
25 Linear Regression for Non-Linear Functions
Let us start with a simple linear function to approximate a true generating function such
y  f x  Wcid62x
where W  Rdl is the weight matrix
parameter ie   W
The empirical cost function is then
C  
The gradient of the empirical cost function is
 C    1
In this case this weight matrix is the only
cid13cid13cid13yn  Wcid62xncid13cid13cid132
cid16
yn  Wcid62xncid17cid62
With these two well dened we can use the iterative optimization algorithm such
as GD or SGD to nd the best W that minimizes the empirical cost function11 Or
better is to use a validation set to stop the optimization algorithm at the point of the
minimal validation cost function remember early stopping
Now but we are not too satised with a linear network are we
251 Feature Extraction
Why are we not satised
First we are not sure whether the true generating function f was a linear function
If it is not can we expect linear regression to approximate the true function well Of
course not We will talk about this shortly
Second because we were given x meaning we did not have much control over what
we want to measure as x it is unclear how well x represents the input For instance
consider doing a sales forecast of air conditioner at one store which opened ve years
ago The input x is the number of days since the opening date of the store 1 Jan 2009
and the output y is the number of units sold on each day
Clearly in this example the relationship between x and y is not linear Furthermore
perhaps the most important feature for predicting the sales of air conditioners is missing
from the input x which is a month or a season if you prefer It is likely that the
sales bottoms out during the winter perhaps sometime around December January and
February and it hits the peak during summer months around May June and July
In other words if we look at how far the month is away from July we can predict the
sales quite well even with linear regression
11 In fact looking at Eq 28 its quite clear that you can compute the optimal W analytically See
Eq 24
Let us call this quantity  x or equivalent feature such that
 x  mx  
where mx  12    12 is the month of x and   55 With this feature we can t
linear regression to better approximate the sales gure of air conditioners Furthermore
we can add yet another feature to improve the predictive performance For instance
one such feature can be which day of week x is
This whole process of extracting a good set of features that will make our choice
of parametric function family such as linear regression in this case is called feature
extraction This feature extraction is an important step in machine learning and has
often been at the core of many applications such as computer vision the representative
example is SIFT 74
Feature extraction often requires heavy knowledge of the domain in which this
function approximation is applied To use linear regression for computer vision it is
a good idea to use computer vision knowledge to extract a good set of features If we
want to use it for environmental problems we must rst notice which features must be
important and how they should be represented for linear regression to work
This is okay for a machine learning practitioner in a particular eld because the
person has in-depth knowledge about the eld There are however many cases where
theres simply not enough domain knowledge to exploit To make the matter worse it
is likely that the domain knowledge is not correct making the whole business of using
manually extracted features futile
Chapter 3
Neural Networks and
Backpropagation Algorithm
31 Conditional Distribution Approximation
I have mainly described so far as if the function we approximate or the function we
use to approximate returns only a constant value as in one point y in the output space
This is however not true and in fact the function can return anything including a
distribution 17 35 12
Lets rst decompose the data distribution pdata into the product of two terms
pdataxy  pdataxpdatayx
It becomes clear that one way to sample from pdata is to sample an input xn from
pdatax and subsequently sample the corresponding output yn from the conditional
distribution pdatayxn
This implies that the function approximation of the generating function  f  x  y
is effectively equivalent to approximating the conditional distribution pdatayx This
may suddenly sound much more complicated but it should not alarm you at all As
long as we choose to use a distribution parametrized by a small number of param-
eters to approximate the conditional distribution pdatayx this is quite manageable
without almost any modication to the expected and empirical cost functions we have
discussed
approximating the true underlying probability distribution pdatayx As the notation
suggests the function now returns the parameters of the distribution  x given the
input x
For example lets say y  01k is a binary vector and we chose to use inde-
pendent Bernoulli distribution to approximate the conditional distribution pdatayx
In this case the parameters that dene the conditional distribution are the means of k
Let us use  x to denote a set of parameters for the probability distribution pyx x
dimensions
pyx 
kcid481
pykcid48x 
kcid481
kcid48 1 kcid481ykcid48 
ykcid48
Then the function  x should output a k-dimensional vector of which each element is
between 0 and 1
Another example lets say y  Rk is a real-valued vector It is quite natural to use a
Gaussian distribution with a diagonal covariance matrix to approximate the conditional
distribution pyx
cid32
cid33
pyx 
kcid481
2kcid48
ykcid48  kcid482
kcid48
The parameters for this conditional distribution are  x 1 2     k12    k
where k  R and k  R0
In this case of probability approximation it is natural to use Kullback-Leibler KL
divergence to measure the distance1 The KL divergence from one distribution P to the
other Q is dened2 by
KLPcid107Q 
Pxlog
In our case of functiondistribution approximation we want to minimize the KL di-
vergence from the data distribution pdatayx to the approximate distribution pyx
averaged over the data distribution pdatax
cid90
cid90
cid90
C  
pdataxKLpdatacid107 pdx 
pdatax
cid90
pdatayxlog
pdatayx
pyx
But again we do not have access to pdata and cannot compute this expected cost func-
Similarly to how we dened the empirical cost function earlier we must approxi-
mate this expected KL divergence using the training set
C  
log pynxn
As an example if we choose to return the binary vector y as in Eq 31 the empirical
cost function will be
C    1
ykcid48 log kcid48  1 ykcid48log1 kcid48
kcid481
1 Again we use a loose denition of the distance where triangular inequality is not enforced
2 Why dont I say the KL divergence between two distributions here Because the KL divergence is not
a symmetric measure ie KLPcid107Q cid54 KLQcid107P
which is often called a cross entropy cost In the case of Eq 32
C    1
kcid481
ykcid48  kcid482
kcid48
 logkcid48
Do you see something interesting in Eq 34 If we assume that the function
outputs 1 for all kcid48s we see that this cost function reduces to that using the Euclidean
distance between the true output y and the mean  What does this mean
There will be many occasions later on to discuss more about this perspective when
we discuss language modelling However one thing we must keep in our mind is that
there is nothing different between approximating a function and a distribution
311 Why do we want to do this
Before we move on to the main topic of todays lecture lets try to understand why
we want to output the distribution Unlike returning a single point in the space the
distribution returned by the function f incorporates both the most likely outcome y as
well as the uncertainty associated with this value
In the case of the Gaussian output in Eq 32 the standard deviation kcid48 or the
variance  2
kcid48 indicates how uncertain the function is about the output centered at kcid48
Similarly the mean kcid48 of the Bernoulli output in Eq 31 is directly proportional to
the functions condence in predicting that the kcid48-th dimension of the output is 1
Figure 31 Is this a duck or a rab-
bit 68 At the end of the day
we want our function f to return
a conditional distribution saying
that pduckx  prabbitx in-
stead of returning the answer out
of these two possible answers
This is useful in many aspects but one important aspect is that it reects the natural
uncertainty of the underlying generating function One input x may be interpreted in
more than one ways leading to two possible outputs which happens more often than
not in the real world For instance the famous picture in Fig 31 can be viewed as a
picture of a duck or a picture of a rabbit in which case the function needs to output the
probability distribution by which the same probability mass is assigned to both a duck
and a rabbit Furthermore there is observational noise that cannot easily be identied
and ignored by the function in which case the function should return the uncertainty
due to the observational noise along with the most likely or the average prediction
312 Other Distributions
I have described two distributions densities that are widely used
 Bernoulli distribution binary classication
 Gaussian distribution real value regression
Here let me present one more distribution which we will use almost everyday through
this course
Categorical Distribution Multi-Class Classication Multi-class classication is a
task in which each example belongs to one of K classes For each input x the problem
reduces to nd a probability pkx of the k-th class under the constraint that
pkx  1
It is clear that in this case the function f returns K values 1 2     K each
of which is between 0 and 1 Furthermore the sum of ks must sum to 1 This can be
achieved easily by letting f to compute afne transformation of x or  x to return K
unbounded real values followed by a so called softmax function 17
expwcid62
kcid481 expwcid62
k  x  bk
kcid48 x  bk
where wk  Rdim x and bk  R are the parameters of afne transformation
In this case the empirical cost function based on the KL divergence is
Ikyn 
C    1
Ikynk
cid26 1
if k  yn
0 otherwise
32 Feature Extraction is also a Function
We talked about the manual feature extraction in the previous lecture see Sec 251
But this is quite unsatisfactory because this whole process of manual feature extraction
is heavily dependent on the domain knowledge meaning that we cannot have a generic
principle on which we design features This raises a question instead of manually
designing features ourselves is it possible for this to happen automatically
One thing we notice is that the feature extraction process  x is nothing but a
function A function of a function is a function right In other words we will extend
our denition of the function to include the feature extraction function
y  f  x
We will assume that the feature extraction function  is also parametrized and its
parameters are included in the set of parameters which includes those of f  As an
example  in Eq 29 is a parameter of the feature extraction 
A natural next question is which family of parametric functions we should use for
 We run into the same issue we talked about earlier in Sec 23 the size of hypothesis
space is simply too large
Instead of choosing one great feature extraction function we can go for a stack of
simple transformations which are all learned3 Each transformation can be as simple
as afne transformation followed by a simple point-wise nonlinearity
where W0 is the weight matrix b0 is the bias and g is a point-wise nonlinearity such
as tanh4
0x  gW0x  b0
One interesting thing is that if the dimensionality of the transformed feature vector
0x is much larger than that of x the function f 0x can approximate any func-
tion from x to y under some assumptions even when the parameters W0 and b0 are
randomly selected 34
The problem solved right We just put a huge matrix W0 apply some nonlinear
function g to it and t linear regression as I described earlier We dont even need to
touch W0 and b0 All we need to do is replace the input xn of all the pairs in the training
set to 0xn
In fact there is a group of researchers claiming to have gured this out by them-
selves less than a decade ago as of 2015 who call this model an extreme learning
machine 54 There have been some debates about this so-called extreme learning
machine Here I will not make any comment myself but would be a good exercise for
you to gure out why there has been debates about this
But regardlessly this is not what we want5 What we want is to fully tune the
whole thing
33 Multilayer Perceptron
The basic idea of multilayer perceptron is to stack a large number of those feature
extraction layers in Eq 38 between the input and the output This idea is as old as
the whole eld of neural network research dating back to early 1960s 89 However
it took many more years for people to gure out a way to tune the whole network both
f and s together See 91 and 70 if you are interested in the history
3 A great article about
this was posted recently in httpcolahgithubioposts
2014-03-NN-Manifolds-Topology
4 Some of the widely used nonlinearities are
 Sigmoid  x 
 Hyperbolic function tanhx  1exp2x
1exp2x
 Rectied linear unit rectx  max0x
1expx
5 And more importantly I will not accept any nal project proposal whose main model is based on the
331 Example Binary classication with a single hidden unit
Let us start with the simplest example The input x  R is a real-valued scalar and
the output y  01 is a binary value corresponding to the inputs label The feature
extractor  is dened as
 x   ux  c
where u and c are the parameters The function f returns the mean of the Bernoulli
conditional distribution pyx
  f x   w x  b
In both of these equations  is a sigmoid function
 x 
1  expx
We use the KL divergence to measure the distance between the true conditional
distribution pyx and the predicted conditional distribution pyx
KLpcid107 p  
y01
y01
pyxlog
pyx
pyxlog pyx pyxlog pyx
Note that the rst term in the summation pyxlog pyx can be safely ignored in our
case Why Because this does not concern p which is one we change in order to
minimize this KL divergence
Lets approximate this KL divergence with a single sample from pyx and leave
only the relevant part We will call this a per-sample cost
Cx  log pyx
 log y1 1y
 ylog   1 ylog1 
where  is from Eq 310
It is okay to work with this per-sample cost function
instead of the full cost function because the full cost function is almost always the
unweighted sum of these per-sample cost functions See Eq 23
We now need to compute the gradient of this cost function Cx with respect to all the
parameters w b u and c First lets start with w
 w 
 w 
which is a simple application of chain rule of derivatives Compare this to
 b 
 b 
In both equations   w x  b which is the input to f 
Both of these derivatives share Cx
   where
y  y    y
  y
cid48 
1 
cid48 
1 
 cid124cid123cid122cid125
cid48
cid48 
  y
1 
cid48    y
because the derivative of the sigmoid function  
  is
cid48  1 
Note that this corresponds to computing the difference between the correct label y and
the predicted label probability 
Given this output derivative Cx
   all we need to compute are
 w   x
 b  1
From these computations we see that
 w    y x
 b    y
Let us continue on to u and c We can again rewrite the derivatives wrt these into
 u 
 c 
 c 
where  is the input to  similarly to  was to the input to 
There are two things to notice here First we already have Cx
derivatives wrt w and b meaning there is no need to re-compute it Second  
shared between the derivatives wrt u and c
  from computing the
 is
Therefore we rst compute  
 
cid124cid123cid122cid125
cid48
 wcid48  w x1  x
Next we compute
 u  x
 c  1
Now all the ingredients are there
 u   yw x1  xx
 c   yw x1  x
The most important lession to learn from here is that most of the computations
needed to get the derivatives in this seemingly complicated multilayered computational
graph multilayer perceptron are shared At the end of the day the amount of compu-
tation needed to compute the gradient of the cost function wrt all the parameters in
the network is only as expensive as computing the cost function itself
332 Example Binary classication with more than one hidden
Let us try to generalize this simple or rather simplest model into a slightly more
general setting We will still look at the binary classication but with multiple hidden
units and a multidimensional input such that
 x  Ux  c
where U  Rld and c  Rl Consequently w will be a l-dimensional vector
The output derivative Cx
  stays same as before See Eq 315 However we
note that the derivative of  with respect to w should now differ because its a vector6
Lets look at what this means
The  can be expressed as
  wcid62 x  b 
wiix  b
In this case we can start computing the derivative with respect to each element of wi
separately
 ix
6 The Matrix Cookbook 85 is a good reference for this section
and will put them into a vector
cid20  
 w 
    
cid21cid62
cid62
 1x2x    lx
  x
Then the derivative of the cost function Cy with respect to w can be written as
 w    y x
Now lets look at Cy
in which case nothing really changed from the case of a single hidden unit in Eq 316
  Again because  x is now a vector there has to be some
  In fact the
 w due to the symmetry
changes Because Cy
procedure for computing this is identical to that for computing  
in Eq 318 That is
  is already computed we only need to look at  
  w
Next what about 
  Because the nonlinear activation function  is applied
element-wise we can simply compute this derivative for each element in  x such
cid16cid2cid48
  diag
1xcid48
2x    cid48
l xcid3cid62cid17
where diag returns a diagonal matrix of the input vector In short we will denote this
as cid48
Overall so far we have got
    ywcid62cid48x    ywcid12 diagcid48x
where cid12 is an element-wise multiplication
Now it is time to compute 
Ucid62x
U  x
according to the Matrix Cookbook 85 Then lets look at the whole derivative wrt
U    ywcid12 diagcid48xxcid62
Note that all the vectors in this lecture note are column vectors
For c its straightforward since
 c  1
34 Automating Backpropagation
This procedure presented as two examples is called a backpropagation algorithm If
you read textbooks on neural networks you see a fancier way to explain this back-
propagation algorithm by introducing a lot of fancy terms such as local error  and
so on But personally I nd it much easier to understand backpropagation as a clever
application of the chain rule of derivatives to a directed acyclic graph DAG in which
each node computes a certain function  using the output of the previous nodes I will
refer to this DAG as a computational graph from here on
Figure 32 a A graphical representation of the computational graph of the example
network from Sec 332 b A graphical illustration of a function node  forward
pass  backward pass
A typical computational graph looks like the one in Fig 32 a This computational
graph has two types of nodes 1 function node cid13 and 2 variable node 2 There
are four different types of function nodes 1 MatMulAB  AB 2 MatSumAB 
AB 3  element-wise sigmoid function and 4 Cy cost node The variables nodes
correspond to either parameters or data x and y Each function node has a number
associated with it to distinguish between the nodes of the same function
Now in this computational graph let us start computing the gradient using the
 y and Cy
 1 
 MatSum1 and multiply it
backpropagation algorithm We start from the last code Cy by computing Cy
Then the function node  1 will compute its own derivative
with Cy
 1 passed back from the function node Cy So far weve computed
 1
 MatSum1 
 1
 1
 MatSum1
The function node MatSum1 has two inputs b and the output of MatMul1 Thus
 MatMul1  Each of these is multiplied
 MatSum1 from Eq 319 At this point we already
this node computes two derivatives  MatSum1
with the backpropagated derivative
have the derivative of the cost function Cy wrt one of the parameters b
and  MatSum1
 b 
 MatSum1
 MatSum1
111222This process continues mechanically until the very beginning of the graph a set
of root variable nodes is reached All we need in this process of backpropagating the
derivatives is that each function node implements both forward computation as well
as backward computation In the backward computation the function node received
the derivative from the next function node evaluates its own derivative with respect to
the inputs at the point of the forward activation and passes theses derivatives to the
corresponding previous nodes See Fig 32 b for the graphical illustration
Importantly the inner mechanism of a function node does not change depending on
its context or equivalently where the node is placed in a computational graph In other
words if each type of function nodes is implemented in advance it becomes trivial to
build a complicated neural network including multilayer perceptrons and compute
the gradient of the cost function which is one such function node in the graph with
respect to all the parameters as well as all the inputs
This is a special case called the reverse mode of automatic differentiation7 It
is probably the most valuable tool in deep learning and fortunately many widely used
toolkits such as Theano 10 4 have implemented this reverse mode of automatic differ-
entiation with an extensive number of function nodes used in deep learning everyday
Before nishing this discussion on automating backpropagation Id like you to
think of pushing this even further For instance you can think of each function node
returning not its numerical derivative on its backward pass but a computational sub-
graph computing its derivative This means that it will return a computational graph
of gradient where the output is the derivatives of all the variable nodes or a subset
of them Then we can use the same facility to compute the second-order derivatives
341 What if a Function is not Differentiable
From the description so far one thing we notice is that backpropagation works only
when each and every function node in a computational graph is differentiable In
other words the nonlinear activation function must be chosen such that almost every-
where it is differentiable All three activation functions I have presented so far have
this property
Logistic Functions A sigmoid function is dened as
and its derivative is
 x 
1  expx
cid48x   x1  x
A hyperbolic tangent function is
tanhx 
exp2x 1
exp2x  1
7 If anyones interested in digging more into the whole eld of automatic differentiation try to Google it
and youll nd tons of materials One such reference is 5
and its derivative is
cid18
tanhcid48x 
cid192
expx  expx
Piece-wise Linear Functions
81 46 earlier
I described a rectied linear unit rectier or ReLU
rectx  max0x
It is clear that this function is not strictly differentiable because of the discontinuity
at x  0 However the chance of the input to this rectier lands exactly at 0 has
zero probability meaning that we can forget about this extremely unlikely event The
derivative of the rectier in this case is
rectcid48x 
cid26 1
if x  0
if x  0
Although the rectier has become the most widely used nonlinearity especially
in deep learnings applications to computer vision8 there is a small issue with the
rectier That is for a half of the input space the derivative is zero meaning that the
error the output derivative from Eq 315 will be not well propagated through the
rectier function node
In 48 the rectier was extended to a maxout unit so as to avoid this issue of the
existence of zero-derivative region in the input to the rectier The maxout unit of rank
k is dened as
maxoutx1    xk  maxx1    xk
and its derivative as
 maxout
x1    xk 
cid26 1
if maxx1    xk  xi
0 otherwise
This means that the derivative is backpropagated only through one of the k inputs
Stochastic Variables These activation functions work well with the backpropagation
algorithm because they are differentiable almost everywhere in the input space How-
ever what happens if a function is non-differentiable at all One such example is a
binary stochastic node which is computed by
1 Compute p   x where x is the input to the function node
2 Consider p as a mean of a Bernoulli distribution ie Bp
3 Generate one sample s  01 from the Bernoulli distribution
4 Output s
8 Almost all the winning entries in ImageNet Large Scale Visual Recognition Challenges ILSVRC use a
convolutional neural network with rectiers See httpimage-netorgchallengesLSVRC
Clearly there is no derivative of this function node
Does it mean that were doomed in this case Fortunately no Although I will not
discuss about this any further in this course Bengio et al 7 provide an extensive list
of approaches we can take in order to compute the derivative of the stochastic function
Chapter 4
Recurrent Neural Networks and
Gated Recurrent Units
After the last lecture I hope that it has become clear how to build a multilayer percep-
tron Of course there are so many details that I did not mention but are extremely im-
portant in practice For instance how many layers of simple transformations Eq 38
should a multilayer perceptron have for a certain task How wide equiv dim0x
should each transformation be What other transformation layers are there What kind
of learning rate  see Eq 25 should we use How should we schedule this learning
rate over training Answers to many of these questions are unfortunately heavily task-
data- and model-dependent and I cannot provide any general answer to them
41 Recurrent Neural Networks
Instead I will move on to describing how we can build a neural network1 to handle
a variable length input Until now the input x was assumed to be either a scalar or
a vector of the xed number of dimensions From here on however we remove this
assumption of a xed size input and consider the case of having a variable length input
What do I mean by a variable length input A variable length input x is a sequence
where each input x has a different number of elements For instance the rst training
examples input x1 may consist of l1 elements such that
x1  x1
2    x1
Meanwhile another examples input xn may be a sequence of ln cid54 l1 elements
xn  xn
2    xn
Lets go back to very basic about dealing with these kinds of sequences Further-
more let us assume that each element xi is binary meaning that it is either 0 or 1 What
1 Now let me begin using a term neural network instead of a general function
would be the most natural way to write a function that returns the number of 1s in
an input sequence x  x1x2    xl My answer is to rst build a recursive function
called ADD1 shown in Alg 1 This function ADD1 will be called for each element of
the input x as in Alg 2
Algorithm 1 A function ADD1
s  0
function ADD1vs
if v  0 then return s
else return s  1
end function
Algorithm 2 A function ADD1
s  0
for i  12    l do s  ADD1xis
end for
There are two important components in this implementation First there is a mem-
ory s which counts the number of 1s in the input sequence x Second a single function
ADD1 is applied to each symbol in the sequence one at a time together with the mem-
ory s Thanks to these two properties our implementation of the function ADD1 can be
used with the input sequence of any length
Now let us generalize this idea of having a memory and a recursive function that
works over a variable length sequence One likely most general case of this idea is
a digital computer we use everyday A computer program is a sequence x of instruc-
tions xi A central processing unit CPU reads each instruction of this program and
manipulates its registers according to what the instruction says Manipulating registers
is often equivalent to manipulating any inputoutput IO device attached to the CPU
Once one instruction is executed the CPU moves on to the next instruction which will
be executed with the content of the registers from the previous step In other words
these registers work as a memory in this case s from Alg 2 and the execution of an
instruction by the CPU corresponds to a recursive function ADD1 from Alg 1
Both ADD1 and CPU are hard coded in the sense that they do what they have been
designed and manufactured to do Clearly this is not what we want because nobody
knows how to design a CPU or a recursive function for natural language understanding
which is our ultimate goal Instead what we want is to have a parametric recursive
function that is able to read a sequence of linguistic symbols and use a memory in
order to understand natural languages
To build this parametric recursive function2 that works on a variable-length input
sequence x  x1x2    xl we now know that there needs to be a memory We will
use one vector h  Rdh as this memory vector As is clear from Alg 1 this recursive
function takes as input both one input symbol xt and the memory vector h and it
2 In neural network research we call this function a recurrent neural network
returns the updated memory vector It often helps to time index the memory vector
as well such that the input to this function is ht1 the memory after processing the
previous symbol xt1 and we use ht to denote the memory vector returned by the
function This function is then
ht  f xt ht1
Now the big question is what kind of parametric form this recursive function f
takes We will follow the simple transformation layer from Eq 38 in which case we
f xt ht1  gW xt   Uht1
where  xt  is a function that transforms the input symbol often discrete into a d-
dimensional real-valued vector W  Rdhd and Udhdh are parameters of this function
A nonlinear activation function g can be any function but for now we will assume that
it is an element-wise nonlinear function such as tanh
411 Fixed-Size Output y
Because our goal is to approximate an underlying true function we now need to think
of how we use this recursive function to return an output y As with the case of variable-
length sequence input x y can only be either a xed-size output such as a category to
which the input x belongs or a variable-length sequence output Here let us discuss the
case of having a xed-size output y
The most natural approach is to use the last memory vector hl to produce the output
or more often output distribution Consider a task of binary classication where y is
either positive 1 or negative 0 in which case a Bernoulli distribution ts perfectly
A Bernoulli distribution is fully characterized by a single parameter  Hence
   vcid62hl
where v  Rdh is a weight vector and  is a sigmoid function
This now looks very much like the multilayer perceptron from Sec 33 The whole
function given an input sequence x computes
cid125
   vcid62 gW xl  UgW xl1  UgW xl2 gW x1  Uh0 
cid123cid122
cid124
a recurrence
where h0 is an initial memory state which can be simply set to an all-zero vector
The main difference is that the input is not given only to the rst simple trans-
formation layer but is given to all those transformation layers one at a time Also
each transformation layer shares the parameters W and U3 The rst two steps of the
3 Note that for brevity I have omitted bias vectors This should not matter much as having a bias vector
is equivalent to augmenting the input with a constant element whose value is xed at 1 Why Because
cid21
cid20 x
 Wx  b
Note that as I have declared before all vectors are column vectors
recurrence part a of Eq 42 are shown as a computational graph in Fig 41
Figure 41 Sample computational graph of the recurrence in Eq 42
As this is not any special computational graph the whole discussion on how to au-
tomate backpropagation computing the gradient of the cost function wrt the parame-
ters in Sec 34 applies to recurrent neural networks directly except for one potentially
confusing point
412 Multiple Child Nodes and Derivatives
It may be confusing how to handle those parameters that are shared across multiple
time steps W and U in Fig 41 In fact in the earlier section Sec 34 we did not
discuss about what to do when the output of one node is fed into multiple function
nodes Mathematically saying what do we do in the case of
c  g f1x f2x     fnx
g can be any function but let us look at two widely used cases
 Addition g f1x     fnx  n
 x 
 Multiplication g f1x     fnx  n
i12n
i1 fix
 g 
cid32
jcid54i
i1 fix
 x 
cid33
 x 
 g 
i12n
 x 
From these two cases we can see that in general
 x 
 g 
i12n
 x 
This means that when multiple derivatives are backpropagated into a single node the
node should rst sum them and multiply its summed derivative with its own derivative
What does this mean for the shared parameters of the recurrent neural network In
an equation
 MatSuml
 MatSuml
 MatMull
 MatMull
cid124
cid124
cid124
cid123cid122
cid123cid122
cid123cid122
cid125
cid125
cid125
cid124
cid124
 
cid123cid122
cid123cid122
cid125
cid125
cid124
 MatSuml
 MatSuml1
 MatSuml1
 MatMull1
 MatMull1
 MatSuml
 MatSuml
 MatSuml1
 MatSuml1
 MatSuml2
 MatSuml2
 MatMull2
 MatMull2
 MatSuml
cid123cid122
cid125
where the superscript l of each function node denotes the layer at which the function
node resides
Similarly to what weve observed in Sec 34 many derivatives are shared across
the terms inside the summation in Eq 43 This allows us to compute the derivative
of the cost function wrt the parameter W efciently by simply running the recurrent
neural network backward
413 Example Sentiment Analysis
There is a task in natural language processing called sentiment analysis As the name
suggests the goal of this task is to predict the sentiment of a given text This is de-
nitely one function that a human can do fairly well when you read a critiques review
of a movie you can easily tell whether the critique likes hates or is neutral to the
movie Also even without a star rating of a product on Amazon you can quite easily
tell whether a user like it by reading herhis review of the product
In this task an input sequence x is a given text and the xed-size output is its label
which is almost always one of positive negative or neutral Let us assume for now
that the input is a sequence of words where each word xi is represented as a so-called
one-hot vector4 In this case we can use
in Eq 41
 xt   xt
4 A one-hot vector is a way to represent a discrete symbol as a binary vector The one-hot vector vi of a
symbol i  V  12    V is
cid124 cid123cid122 cid125
vi  0    0
1i1
 1cid124cid123cid122cid125
cid124 cid123cid122 cid125
 0    0
i1V
cid62
Once the input sequence or paragraph in this specic example is read we get
the last memory state hl of the recurrent neural network We will afne-transform hl
followed by the softmax function to obtain the conditional distribution of the output
y  123 1 positive 2 neutral and 3 negative
cid62
  1 2 3
 softmaxVhl
where 1 2 and 3 are the probabilities of positive neural and negative See
Eq 35 for more details on the softmax function
Because this network returns a categorial distribution it is natural to use the cate-
gorical cross entropy as the cost function See Eq 36 A working example of this
sentiment analyzer based on recurrent neural networks will be introduced and discussed
during the lab session5
414 Variable-Length Output y x  y
Lets generalize what we have discussed so far to recurrent neural networks here In-
stead of a xed-size output y we will assume that the goal is to label each input symbol
resulting in the output sequence y  y1y2    yl of the same length as the input se-
quence x
What kind of applications can you think of that returns the output sequence as long
as the input sequence One of the most widely studied problems in natural language
processing is a problem of classifying each word in a sentence into one of part-of-
speech tags often called POS tagging see Sec 31 of 77 Unfortunately in my
personal opinion this is perhaps the least interesting problem of all time in natural
language understanding but perhaps the most well suited problem for this section
In its simplest form we can view this problem of POS tagging as classifying each
word in a sentence as one of noun verb adjective and others As an example given
the following input sentence x
the goal is to output
x  Childreneatsweetcandy
y  nounverbadjectivenoun
This task can be solved by a recurrent neural network from the preceding section
Sec 411 after a quite trivial modication Instead of waiting until the end of the
sentence to get the last memory state of the recurrent neural network we will use the
immediate memory state to predict the label at each time step t
At each time t we get the immediate memory state ht by
ht  f xt ht1
where f is from Eq 41 Instead of continuing on to processing the next word we
will rst predict the label of the t-th input word xt
5 For those eager to learn more see httpdeeplearningnettutoriallstmhtml in
advance of the lab session
This can be done by
cid62
t  t1 t2 t3 t4
 softmaxVht 
Four tis correspond to the probabilities of the four categories 1 noun 2 verb 3
adjective and 4 others
From this output distribution at time step t we can dene a per-step per-sample
cost function
Cxt     K
Ikytk
where K is the number of categories four in this case We discussed earlier in Eq 36
Naturally a per-sample cost function is dened as the sum of these per-step per-sample
cost functions
Cx    l
Ikytk
Incorporating the Output Structures This formulation of the cost function is equiv-
alent to maximizing the log-probability of the correct output sequence given an input
sequence where the conditional log-probability is dened as
cid124
log pyx 
cid124
cid123cid122
cid123cid122
Eq 47
Eq 48
log pytx1    xt 
cid125
cid125
This means that the network is predicting the label of the t-th input symbol using only
the input symbols read up to that point ie x1x2    xt
In other words this means that the recurrent neural network is not taking into ac-
count the structure of the output sequence For instance even without looking at the
input sequence in English it is well known that the probability of the next word being a
noun increases if the current word is an adjective6 This kind of structures in the output
are effectively ignored in this formulation
Why is this so in this formulation Because we have made an assumption that
the output symbols y1y2    yl are mutually independent conditioned on the input se-
quence This is clear from Eq 49 and the denition of the conditional independence
Y1 and Y2 are conditionally independent dependent on X
 pY1Y2X  pY1XpY2x
If the underlying true conditional distribution obeyed this assumption of condi-
tional independence there is no worry However this is a very strong assumption for
6 Okay this requires a more thorough analysis but for the sake of the argument which does not have to
do anything with actual POS tags lets believe that this is indeed the case
many of the tasks we run into apparently from the example of POS tagging Then
how can we exploit the structure in the output sequence
One simple way is to make a less strong assumption about the conditional proba-
bility of the output sequence y given x For instance we can assume that
log pyx 
log pyiyixi
where yi and xi denote all the output symbols before the i-th one and all the input
symbols up to the i-th one respectively
Now the question is how we can incorporate this into the existing formulation of
a recurrent neural network from Eq 45 It turned out that the answer is extremely
simple All we need to do is to compute the memory state of the recurrent neural
network based not only on the current input symbol xt and the previous memory state
ht1 but also on the previous output symbol yt1 such that
ht  f xt yt1ht1
Similarly to Eq 41 we can think of implementing f as
f xt yt1ht1  gWxxxt   Wyyyt1  Whht1
There are two questions naturally arising from this formulation First what do we
do when computing h1 This is equivalent to saying what yy0 is There are two
potential answers to this question
1 Fix yy0 to an all-zero vector
2 Consider yy0 as an additional parameter
In the latter case yy0 will be estimated together with all the other parameters such
as those weight matrices Wx Wy Wh and V
Inference The second question involves how to handle yt1 During training it is
quite straightforward as our cost function KL-divergence between the underlying
true distribution and the parametric conditional distribution pyx approximated by
Monte Carlo method says that we use the groundtruth value for yt1s
It is however not clear what we should do when we test the trained network because
then we are not given the groundtruth output sequence This process of nding an
output that maximizes the conditional log-probability is called inference7
y  argmax
log pyx
7 Okay I confess The term inference refers to a much larger class of problems even if we consider only
machine learning However let me simply use this term to refer to a task of nding the most likely output of
a function
The exact inference is quite straightforward One can simply evaluate log pyx for
every possible output sequence and choose the one with the highest conditional proba-
bility Unfortunately this is almost always intractable as the number of every possible
output sequence grows exponentially with respect to the length of the sequence
Y   Kl
where Y  K and l are the set of all possible output sequences the number of labels and
the length of the sequence respectively Thus this is necessary to resort to approximate
search over the set Y 
The most naive approach to approximate inference is a greedy one With the trained
model you predict the rst output symbol y1 based on the rst input symbol x1 by
selecting the category of the highest probability py1x1 Now given y1 x1 and x2
we compute py2x1x2y1 from which we select the next output symbol y2 with the
highest probability We continue this process iteratively until the last output symbol yl
is selected
This is greedy in the sense that any early choice with a high conditional probability
may turn out to be unlikely one due to extremely low conditional probabilities later on
It is highly related to the so-called garden path sentence problem To know more about
this read for instance Sec 324 of 77
It is possible to alleviate this issue by considering N  K best hypotheses of the
output sequence at each time step This procedure is called beam search and we will
discuss more about this in a later lecture on neural machine translation
42 Gated Recurrent Units
421 Making Simple Recurrent Neural Networks Realistic
Let us get back to the analogy we made in Sec 41 We compared a recurrent neural
network to how CPU works Executing a recurrent function f is equivalent to executing
one of the instructions on CPU and the memory state of the recurrent neural network is
equivalent to the registers of the CPU This analogy does sound plausible except that
it is not
In fact how a simple recurrent neural network works is far from being similar to
how CPU works I am now talking about how they are implemented in practice but
rather Im talking at the conceptual level What is it at the conceptual level that makes
the simple recurrent neural network unrealistic
An important observation we make about the simple recurrent neural network is
that it refreshes the whole memory state at each time step This is almost opposite to
how the registers on a CPU are maintained Each time an instruction is executed the
CPU does not clear up the whole registers and repopulate them Rather it works only
on a small number of registers All the other registers values are stored as they were
before the execution of the instruction
Lets try to write this procedure mathematically Each time based on the choice
of instruction to be executed a subset of the registers of a CPU or a subset of the
elements in the memory state of a recurrent neural network is selected This can be
written down as a binary vector u  01nh
cid26 0
if the registers value does not change
if the registers value will change
With this binary vector which I will call an update gate a new memory state or a
new register value at time t can be computed as a convex interpolation such that
ht  1 ucid12 ht1  ucid12 ht 
where cid12 is as usual an element-wise multiplication ht denotes a new memory state or
a new register value after executing the instruction at time t
Another unrealistic point about the simple recurrent neural network is that each
execution considers the whole registers It is almost impossible to imagine designing
an instruction on a CPU that requires to read the values of all the registers Instead
what almost always happens is that each instruction will consider only a small subset
of the registers which again we can use a binary vector to represent Let me call it a
reset gate r  01nh
cid26 0
if the registers value will not be used
if the registers value will be used
This reset gate can be multiplied to the register values before being used by the
instruction at time t8 If we use a recursive function f from Eq 41 it means that
ht  f xt rcid12 ht1  gW xt   Urcid12 ht1
Now let us put these two gates that are necessary to make the simple recurrent
neural network more realistic into one piece At each time step the candidate memory
state is computed based on a subset of the elements of the previous memory state
ht  gW xt   Urcid12 ht1
A new memory state is computed as a linear interpolation between the previous mem-
ory state and this candidate memory state using the update gate
ht  1 ucid12 ht1  ucid12 ht
See Fig 42 for the graphical illustration
422 Gated Recurrent Units
Now here goes a big question How are the update u and reset r gates computed
If we stick to our analogy to the CPU those gates must be pre-congured per
instruction Those binary gates are dependent on the instruction Again however this
8 It is important to note that this is not resetting the actual values of the registers but only the input to the
instructionrecursive function
Figure 42 A graphical illustration of a
gated recurrent unit 29
is not what we want to do in our case There is no set of predened instructions but the
execution of any instruction corresponds to computing a recurrent function based on the
input symbol and the memory state from the previous time step see eg Eq 41
Similarly to this what we want with the update and reset gates is that they are computed
by a function which depends on the input symbol and the previous memory state
This sounds like quite straightforward except that we dened the gates to be binary
This means that whatever the function we use to compute those gates the function will
be a discontinuous function with zero derivative almost everywhere except at the point
where a sharp transition from 0 to 1 happens We discussed the consequence of having
an activation function with zero derivative almost everywhere in Sec 341 and the
conclusion was that it becomes very difcult to compute the gradient of the cost func-
tion efciently and exactly with these discrete activation functions in a computational
One simple solution which turned out to be extremely efcient is to consider those
gates not as binary vectors but as real-valued coefcient vectors In other words we
redene the update and reset gates to be
u  01nh r  01nh 
This approach makes these gates leaky in the sense that they always allow some leak
of information through the gate
In the case of the reset gate rather than making a hard decision on which subset
of the registers or the elements of the memory state will be used it now decides how
much information from the previous memory state will be used The update gate on
the other hand now controls how much content in the memory state will be replaced
which is equivalent to saying that it controls how much information will be kept from
the previous memory state
Under this denition we can simply use a sigmoid function from Eq 311 to
compute these gates
r  Wr xt   Urht1
u  Wu xt   Uurcid12 ht1
where Wr Ur Wu and Uu are the additional parameters9 Since the sigmoid function
is differentiable everywhere we can use the backpropagation algorithm see Sec 34
9 Note that this is not the formulation available for computing the reset and update gates For instance
urhhxto compute the derivatives of the cost function with respect to these parameters and
estimate them together with all the other parameters
We call this recurrent activation function with the reset and update gates a gated
recurrent unit GRU and a recurrent neural network having this GRU as a gated re-
current network
423 Long Short-Term Memory
The gated recurrent unit GRU is highly motivated by a much earlier work on long
short-term memory LSTM units 5310 The LSTM was proposed in 1997 with
the goal of building a recurrent neural network that can learn long-term dependen-
cies across many number of timsteps which was deemed to be difcult to do so with a
simple recurrent neural network
Unlike the element-wise nonlinearity of the simple recurrent neural network and the
gated recurrent unit the LSTM explicitly separates the memory state ct and the output
ht The output is a small subset of the hidden memory state and only this subset of the
memory state is visibly exposed to any other part of the whole network
How does a recurrent neural network with LSTM units decide how much of the
memory state it will reveal As perhaps obvious at this point the LSTM uses a so-
called output gate o to achieve this goal Similarly to the reset and update gates of the
GRU the output gate is computed by
o   Wo xt   Uoht1
This output vector is multiplied to the memory state ct point-wise to result in the output
ht  ocid12 tanhct 
Updating the memory state ct closely resembles how it is updated in the GRU see
Eq 410 A major difference is that instead of using a single update gate the LSTM
uses two gates forget and input gates such that
ct  fcid12 ct1  icid12 ct 
where f  Rnh i  Rnh and ct are the forget gate input gate and the candidate memory
state respectively
The roles of those two gates are quite clear from their names The forget gate
decides how much information from the memory state will be forgotten while the
input gate controls how much informationa about the new input consisting of the input
one can use the following denitions of the reset and update gates
r  Wr xt   Urht1
u  Wu xt   Uuht1
which is more parallelizable than the original formulation from 29 This is because there is no more direct
dependency between r and u which makes it possible to compute them in parallel
10 Okay let me confess here I was not well aware of long short-term memory when I was designing the
gated recurrent unit together with Yoshua Bengio and Caglar Gulcehre in 2014
symbol and the previous output will be inputted to the memory They are computed
f  W f  xt   U f ht1
i  Wi xt   Uiht1
The candidate memory state is computed similarly to how it was done with the
GRU in Eq 411
ct  gWc xt   Ucht1
where g is often an element-wise tanh
All the additional parameters specic to the LSTMWoUoW f U f WiUiWc
and Uc are estimated together with all the other parameters Again every function
inside the LSTM is differentiable everywhere and we can use the backpropagation
algorithm to efcient compute the gradient of the cost function with respect to all the
parameters
Although I have described one formulation of the long short-term memory unit
here there are many other variants proposed over more than a decade since it was rst
proposed For instance the forget gate in Eq 412 was not present in the original
work 53 but was xed to 1 Gers et al 45 proposed the forget gate few years after
the LSTM was originally proposed and it turned out to be one of the most crucial
component in the LSTM For more variants of the LSTM I suggest you to read 49
43 Why not Rectiers
431 Rectiers Explode
Let us go back to the simple recurrent neural network which uses the simple transfor-
mation layer from Eq 41
f xt ht1  gW xt   Uht1
where g is an element-wise nonlinearity
One of the most widely used nonlinearities is a hyperbolic tangent function tanh
This is unlike the case in feedforward neural networks multilayer perceptrons where a
unbounded piecewise linear function such as a rectier and maxout has become stan-
dard In the case of feedforward neural networks you can safely assume that everyone
uses some kind of piecewise linear function as an activation function in the network
This has become pretty much standard since Krizhevsky et al 67 shocked the com-
puter vision research community by outperforming all the more traditional computer
vision teams in the ImageNet Large Scale Visual Recognition Challenge 201212
11 Interestingly based on the observation in 58 it seems like the plain LSTM with a forget gate and the
GRU seem to be close to the optimal gated unit we can nd
12 httpimage-netorgchallengesLSVRC2012resultshtml
The main difference between logistic functions tanh and sigmoid function and
piecewise linear functions rectiers and maxout is that the former is bounded from
both above and below while the latter is bounded only from below or in some cases
not bounded at all 5013
This unbounded nature of piece-wise linear functions makes it difcult for them to
be used in recurrent neural networks Why is this so
Let us consider the simplest case of unbounded element-wise nonlinearity a linear
function
ga  a
The hidden state after l symbols is
hl UUUU   W xl3  W xl2  W xl1  W xl
cid32 l2
cid33
lcid481
cid32 l1
cid33
cid32 lt
lcid481
cid124
lcid481
W x1 
W xt 
cid125
cid33
cid123cid122
W x2   UW xl1  W xl
where l is the length of the input sequence
Let us assume that
 U is a full rank matrix
 The input sequence is sparse l
 W xi  0 for all i
and consider Eq 414 a
t1 I xt cid540  c where c  O1
cid32ltcid48
lcid481
htcid48
cid33
W xtcid48
Now lets look at what happens to Eq 415 First the eigendecomposition of the
matrix U
U  QSQ1
where S is a diagonal matrix whose non-zero entries are eigenvalues Q is an orthogo-
nal matrix Then
13 A parametric rectier or PReLU is dened as
ltcid48
lcid481
U  QSltcid48Q1
cid26 x
if x  0
otherwise 
where a is a parameter to be estimated together with all the other parameters of a network
cid33
cid32ltcid48
lcid481
W xtcid48  diagSltcid48
cid124 cid123cid122 cid125
cid12 QQ1
W xtcid48 
where cid12 is an element-wise product
What happens if the largest eigenvalue emax  maxdiagS is larger than 1 the
norm of hl will explode ie cid107hlcid107   Furthermore due to the assumption that
W xtcid48  0 each element of hl will explode to innity as well The rate of growth is
exponentially with respect to the length of the input sequence meaning that even when
the input sequence is not too long the norm of the memory state grows quickly if emax
is reasonably larger than 1
This happens because the nonlinearity g is unbounded If g is bounded from both
above and below such as the case with tanh the norm of the memory state is also
bounded In the case of tanh  R  11
cid107hlcid107  dimhl
This is one reason why a logistic function such as tanh and  is most widely used
with recurrent neural networks compared to piecewise linear functions14 I will call
this recurrent neural network with tanh as an element-wise nonlinear function a simple
recurrent neural network
Is tanh a Blessing
Now the argument in the previous section may sound like tanh and  are the nonlinear
functions that one should use This seems quite convincing for recurrent neural net-
works and perhaps so for feedforward neural networks as well if the network is deep
enough
Here let me try to convince you otherwise by looking at how the norm of backprop-
agated derivative behaves Again this is much easier to see if we assume the following
 U is a full rank matrix
 The input sequence is sparse l
Similarly to Eq 414 let us consider a forward computational path until hl how-
t1 I xt cid540  c where c  O1
ever without assuming a linear activation function
hl  g Ug Ug Ug U    W xl3  W xl2  W xl1  W xl 
We will consider a subsequence of this process in which all the input symbols are 0
except for the rst symbol
hl1  gcid0Ugcid0Ucid0gcid0Uhl0  Wcid0xl01
cid1cid1cid1cid1cid1 
14 However it is not to say that piecewise linear functions are never used for recurrent neural networks
See for instance 69 6
It should be noted that as l approaches innity there will be at least one such sub-
sequence whose length also approaches innity due to the sparsity of the input we
assumed
From this equation lets look at
cid0xl01
cid1 
This measures the effect of the l0 1-th input symbol xl01 on the l1-th memory state
of the simple recurrent neural network This is also the crucial derivative that needs to
be computed in order to compute the gradient of the cost function using the automated
backpropagation procedure described in Sec 34
This derivative can be rewritten as
Among these three terms in the left hand side we will focus on the rst one a which
can be further expanded as
cid0xl01

cid124 cid123cid122 cid125
hl11
cid1 
hl01
hl01
hl01
cid124 cid123cid122 cid125
hl11
cid124 cid123cid122 cid125
hl11
hl11
hl12
cid124 cid123cid122 cid125
cid1 
hl01
cid0xl01
hl02

cid124 cid123cid122 cid125
hl02
hl1
cid124cid123cid122cid125
 
hl02
hl01
cid124 cid123cid122 cid125
hl01
Because this is a recurrent neural network we can see that the analytical forms for
the terms grouped by the parentheses in the above equation are identical except for the
subscripts indicating the time index In other words we can simply only on one of
those groups and the resulting analytical form will be generally applicable to all the
other groups
First we look at Eq 416 b which is nothing but a derivative of a nonlinear
activation function used in this simple recurrent neural network The derivatives of the
widely used logistic functions are
cid48x  x1  x
tanhcid48x 1 tanh2x
as described earlier in Sec 341 Both of these functions derivatives are bounded
In the simplest case in which g is a linear function ie x  gx we do not even
need to look at
from Eq 416
0  cid48x  025
0  tanhcid48x  1
cid13cid13cid13 ht
cid13cid13cid13 We simply ignore all the ht
Next consider Eq 416 c In this case of simple recurrent neural network we
notice that we have already learned how to compute this derivative earlier in Sec 332
From these two we get
hl01
ht1
cid19cid18hl11
cid18hl1
cid18ht
cid19
hl11
cid19
cid18hl02
hl02
cid19
Do you see how similar it looks like Eq 415 If the recurrent activation function
f is linear this whole term reduces to
hl01
 Ul1l01
which according to Sec 431 will explode as l   if
emax  1
where emax is the largest eigenvalue of U When emax  1 it will vanish ie cid107 hl1
hl01
0 exponentially fast
What if the recurrent activation function f is not linear at all Lets look at ht
cid107

cid124
f cid48
f cid48
cid18
f cid48
cid19
cid123cid122
cid17
cid16 ht
cid18ht
cid0QSQ1cid1 

cid125
cid19
cid12 S
where we used the eigendecomposition of U  QSQ1 This can be re-written into
This means that the eigenvalue of U will be scaled by the derivative of the recurrent ac-
tivation function at each timestep In this case we can bound the maximum eigenvalue
of ht
max   emax
where  is the upperbound on gcid48  ht
 See Eqs 417418 for the upperbounds of
the sigmoid and hyperbolic tangent functions
In other words if the largest eigenvalue of U is larger than 1
  it is likely that this
temporal derivative of hl1 with respect to hl01 will explode meaning that its norm will
grow exponentially large In the opposite case of emax  1
  the norm of the temporal
derivative likely shrinks toward 0 The former case is referred to as exploding gradient
and the latter vanishing gradient These cases were studied already at the very early
years of research in recurrent neural networks 9 52
Using tanh is a blessing in recurrent neural networks when running the network
forward as I described in the previous section This is however not necessarily true in
the case of backpropagating derivaties Especially because there is a higher chance of
vanishing gradient with tanh or even worse with  Why Because 1
  1 for almost
everywhere
433 Are We Doomed
Exploding Gradient Fortunately it turned out that the phenomenon of exploding
gradient is quite easy to address First it is straightforward to detect whether the ex-
ploding gradient happened by inspecting the norm of the gradient fo the cost with
respect to the parameterscid13cid13 Ccid13cid13 If the gradients norm is larger than some predened
threhold   0 we can simply renormalize the norm of the gradient to be  Otherwise
we leave it as it is
In mathematics
cid40
 
 cid107cid107 
if cid107cid107  
otherwise
 is a rescaled gradient update
where we used the shorthand notiation  for  C
direction which will be used by the stochastic gradient descent SGD algorithm from
Sec 222 This algorithm is referred to as gradient clipping 83
Vanishing Gradient What about vanishing gradient But rst what does vanishing
gradient mean We need to understand the meaning of this phenomenon in order to tell
whether this is a problem at all from the beginning
Let us consider a case the variable-length output where x  y from Sec 414
Lets assume that there exists a clear dependency between the output label yt and the
input symbol xtcid48 where tcid48 cid28 t This means that the empirical cost will decrease when
the weights are adjusted such that
log pyt  y
t      xtcid48   
is maximized where y
has great inuence on the t-th output yt and the inuence can be measured by
t is the ground truth output label at time t The value of  xtcid48
 log pyt  y
 xtcid48
Instead of exactly computing  log pyt y
t 
 xtcid48 
t    
 we can approximate it by the nite
difference method Let   Rdim xtcid48  be a vector of which each element is a very
small real value   0 Then
t    
 log pyt  y
 xtcid48
log pyt  y
 log pyt  y
t      xtcid48     
t      xtcid48     cid11 
where cid11 is an element-wise division This shows that  log pyt y
computes the
 xtcid48 
difference in the t-th output probability with respect to the change in the value of the
tcid48-th input
t 
In other words  log pyt y
 xtcid48 
directly reects the degree to which the t-th output
yt depends on the tcid48-th input xtcid48 according to the network To put it in another way
 log pyt y
reects how much dependency the recurrent neural network has cap-
 xtcid48 
tured the dependency between yt and xtcid48
t 
t 
Lets rewrite
 log pyt  y
 xtcid48
t    
 log pyt  y
t    
ht1
cid124
cid123cid122
cid125
 htcid481
htcid48
htcid48
 xt 
The terms marked with a looks exactly identical to Eq 416 We have already seen
that this term can easily vanish toward zero with a high probability see Sec 432
This means that the recurrent neural network is unlikely to capture this dependency
This is especially true when the temporal distance between the output and input ie
t tcid48 cid29 0
The biggest issue with this vanishing behaviour is that there is no straightforward
way to avoid it We cannot tell whether  log pyt y
 0 is due to the lack of this
 xtcid48 
dependency in the true underlying function or due to the wrong conguration param-
eter setting of the recurrent neural network If we are certain that there are indeed
these long-term dependencies we may simultaneously minimize the following auxil-
iary term together with the cost function
t 
1
cid13cid13cid13  C
cid13cid13cid13  C
ht1
ht1
cid13cid13cid13
ht1
cid13cid13cid13
2
This term which was introduced in 83 is minimized when the norm of the derivative
does not change as it is being backpropagated effectively forcing the gradient not to
vanish
This term however was found to help signicantly only when the target task or the
underlying function does indeed exhibit long-term dependencies How can we know
in advance Pascanu et al 83 showed this with the well-known toy tasks which were
specically designed to exhibit long-term dependencies 52
434 Gated Recurrent Units Address Vanishing Gradient
Will the same problems of vanishing gradient happen with the gated recurrent units
GRU or the long short-term memory units LSTM Let us write the memory state at
time t
ht ut cid12 ht  1 ut cid12cid0ut1 cid12 ht1  1 ut1cid12cid0ut2 cid12 ht2  1 ut2cid12  cid1cid1
ut cid12 ht  1 ut cid12 ut1 cid12 ht1  1 ut cid12 1 ut1cid12 ut2 cid12 ht2 
Lets be more specic and see what happens to this with respect to xtcid48
ht ut cid12 ht  1 ut cid12 ut1 cid12 ht1  1 ut cid12 1 ut1cid12 ut2 cid12 ht2 
cid32
cid124
kttcid481
cid33
1 uk
cid123cid122
cid12tanh W xtcid48  U rtcid48 cid12 htcid481 
cid12 utcid48
cid125
where  is for element-wise multiplication
What this implies is that the GRU effectively introduces a shortcut from time tcid48 to
t The change in xtcid48 will directly inuence the value of ht and subsequently the t-th
output symbol yt In other words all the issue with the simple recurrent neural network
we discussed earlier in Sec 433
The update gate controls the strength of these shortcuts Lets assume for now that
the update gate is xed to some predened value between 0 and 1 This effectively
makes the GRU a leaky integration unit 6 However as it is perhaps clear from
Eq 419 that we will inevitably run into an issue Why is this so
Lets say we are sure that there are many long-term dependencies in the data It is
natural to choose a large coefcient for the leaky integration unit meaning the update
gate is close to 1 This will denitely help carrying the dependency across many time
steps but this inevitably carries unnecessary information as well This means that
much of the representational power of the output function goutht  is wasted in ignoring
those unnecessary information
If the update gate is xed to something substantially smaller than 1 all the shortcuts
see Eq 419 a will exponentially vanish Why Because it is a repeated multipli-
cation of a scalar small than 1 In other words it does not really help to have a leaky
integration unit in the place of a simple tanh unit
This is however not the case with the actual GRU or LSTM because those update
gates are not xed but are adaptive with respect to the input If the network detects
that there is an important dependency being captured the update gate will be closed
u j  0 This will effectively strengthen the shortcut connection see Eq 419 a
When the network detects that there is no dependency anymore it will open the update
gate u j  1 which effectively cuts off the shortcut How does the network know or
detect the existence or lack of these dependencies Do we need to manually code this
up I will leave these questions for you to gure out
Chapter 5
Neural Language Models
51 Language Modeling First Step
What does it mean for a machine to understand natural language In other words how
can we tell that the machine understood natural language These are the two equivalent
questions that are at the core of this course
One of the most basic capability of a machine that can signal us that it indeed
understands natural language is for the machine to tell how likely a given sentence
is Of course this is extremely ill-dened as we probably cannot dene the likeliness
of a sentence because there are many different types of unlikeliness For instance
a sentence Colorless green ideas sleep furiously from Chomskys 32 is unlikely
according to our common sense because
1 An object idea cannot be both colorless and green
2 An object cannot sleep furiously
3 An idea does not sleep
On the other hand this sentence is a grammatically correct sentence
Lets take a look at another sentence Jane and me went to see a movie yesterday
Grammatically this is not the most correct sentence one can make It should be Jane
and I went to see a movie yesterday Even with a grammatical error in the original
sentence however the meaning of the sentence is clear to me and perhaps is much more
understandable than the sentence colorless green ideas sleep furiously Furthermore
many people likely say this saying me instead of I quite often This sentence is
thus likely according to our common sense but is not likely according to the grammar
This observation makes us wonder what is the criterion to use Is it correct for a
machine to tell whether the sentence is likely by analyzing its grammatical correctness
Or is it possible that the machine should deem a sentence likely only when its meaning
agrees well with common sense regardless of its grammatical correctness in the most
strict sense
As we discussed in the rst lecture of the course we are more interested in ap-
proaching natural language as a means for one to communicate ideas to a listener In
this sense language use is a function which takes as input the surrounding environ-
ment including the others speech and returns linguistic response and this function
is not given but learned via observing others use of language and the reinforcement
by the surrounding environment 97 Also throughout this course we are not con-
cerned too much about the existing syntactic or grammatical structures underlying
natural language which makes it difcult for us to say anything about the grammatical
correctness of a given sentence
In short we take the route here that the likeliness of a sentence be determined
based on its agreement with common sense The common sense here is captured by
everyday use of natural language which consequently implies that the statistics of
natural language use can be a strong indicator for determining the likely of a natural
language sentence
511 What if those linguistic structures do exist
Of course as we discussed earlier in Sec 11 and in this section not everyone agrees
This is due to the fact that a perfect grammatical sentence may be considered unlikely
just because it does not happen often In other words statistical approaches to language
modeling may conclude that a sentence with perfectly valid grammatical construction
is unlikely Is this a problem
This problem of telling how likely a given sentence is can be viewed very naturally
as building a probabilistic model of sentences In other words given a sentence S what
is the probability pS of S Let us briey talk about what this means for the case of
viewing the likeliness of a sentence as equivalent to its grammatical correctness1
We rst assume that there is an underlying linguistic structure G which has gener-
ated the observed sentence S Of course we do not know the correct G in advance and
unfortunately no one will tell us what the correct G is2 Thus G is a hidden variable
in this case This hidden structure G generates the observed sentence S according to an
unknown conditional distribution pSG Each and every grammatical structure G is
assigned a prior probability which is also unknown in advance3
With the conditional distribution SG and the prior distribution G we easily get the
joint distribution SG by
pSG  pSGpG
from the denition of conditional probability4 From this joint distribution we get the
1 Why briey and why here Because we will not pursue this line at all after this section
2 Here the correct G means the G that generated S not the whole structure of G which is assumed to
exist according to a certain set of rules
3 This is not necessarily true If we believe that each and every grammatical correct sentence is equally
likely and that each correct grammatical structure generates a single corresponding sentence the prior dis-
tribution over the hidden linguistic structure is such that any correct structure is given an equal probability
while any incorrect structure is given a zero probability But of course if we think about it there are clearly
certain structures that are more prevalent and others that are not
4 A conditional probability of A given B is dened as
pAB 
distribution over a given sentence S by marginalizing out G
pS  
pSG
This means that we should compute how likely a given sentence S is with respect to all
possible underlying linguistic structure This is very likely intractable because there
must be innite possible such structures
Instead of computing pS exactly we can simply look at its lowerbound For in-
stance one simplest and probably not the best way to do so is
pS  
pSG  pS G
where G  argmaxG pSG  argmaxG pGS5
This lowerbound is tight ie pS  pS G when there is only a single true un-
derlying linguistic structure G given S What this says is that there is no other possible
linguistic structure possible for a single observed sentence ie no ambiguity in infer-
ring the correct linguistic structure In other words we can compute the probability or
likeliness of a given sentence by inferring its correct underlying linguistic structure
However there are a few issues here First it is not clear which formalism G
follows and we have briey discussed about this at the very beginning of this course
Second it is quite well known that most of the formalisms do indeed have uncertainty
in inference Again we looked at one particular example in Sec 112 These two
issues make many people including myself quite uneasy about this type of model-
based approaches
In the remaining of this chapter I will thus talk about model-free approaches as
opposed to these model-based approaches
512 Quick Note on Linguistic Units
Before continuing there is one question that must be bugging you or at least has
bugged me a lot what is the minimal linguistic unit
If we think about written text the minimal unit does seem like a character With
spoken language the minimal unit seems to be a phoneme But is this the level at
which we want to model the process of understanding natural language In fact to
most of the existing natural language processing researchers as well as some or most
linguists the answer to this question is a hard no
The main reason is that these low-level units both characters and phonemes do not
convey any meaning themselves Does a Latin alphabet q have its own meaning The
answer by most of the people will be no Then starting from this alphabet q how
far should we climb up in the hierarchy of linguistic units to reach a level at which the
unit begins to convey its own meaning qu does not seem to have its own meaning
still qui in French means who but in English it does not really say much quit
in English is a valid word that has its own meaning and similarly quiet is a valid
word that has its own meaning quite apart from that of quit
5 This inequality holds due to the denition of probability which states that pX  0 and X pX  1
It looks like a word is the level at which meaning begins to form itself However
this raises a follow-up question on the denition of a word What is a word
It is tempting to say that a sequence of non-blank characters is a word This makes
everyones life so much easier because we can simply split each sentence by a blank
space to get a sequence of words Unfortunately this is a very bad strategy The sim-
plest counter example to this denition of words is a token which I will use to refer to
a sequence of non-blank characters consisting of a word followed by a punctuation
If we simply split a sentence into words by blank spaces we will get a bunch of re-
dundant words For instance llama llama llama llama llama llama
and llama will all be distinct words We will run into an issue of exploding vocab-
ulary with any morphologically rich language Furthermore in some languages such
as Chinese there is no blank space at all inside a sentence in which case this simple
strategy will completely fail to give us any meaningful small linguistic unit other than
sentences
Now at this point it almost seems like the best strategy is to use each character
as a linguistic unit This is not necessarily true due to the highly nonlinear nature of
orthography6 There are many examples in which this nonlinear nature shows its dif-
culty One such example is to consider the following three words quite quiet
and quit7 All three character sequences have near identical forms but their corre-
sponding meanings differ from each other substantially In other words any function
that maps from these character sequences to the corresponding meanings will have to
be extremely nonlinear and thus difcult to be learned from data Of course this is an
area with active research and I hope I am not giving you an impression that characters
are not the units to use see eg 61
Now then the question is whether there is some middle ground between characters
and words or blank-space-separated tokens that are more suitable to be used as ele-
mentary linguistic units see eg 93 Unfortunately this is again an area with active
research Hopefully we will have time later in the course to discuss this issue further
For now we will simply use blank-space-separated tokens as our linguistic units
52 Statistical Language Modeling
Regardless of which linguistic unit we use any natural language sentence S can be
represented as a sequence of T discrete symbols such that
S  w1w2    wT 
Each symbol is one element from a vocabulary V which contains all possible symbols
V cid8v1v2    vVcid9 
where V is used to mean the size of the vocabulary or the number of all symbols
6 Orthography is dened as the study of spelling and how letters combine to represent sounds and form
words
7 I would like to thank Bart van Merrienboer for this example
The problem of language modeling is equivalent to nding a model that assigns a
probability pS to a sentence
pS  pw1w2    wT 
Of course we are not given this distribution and need to learn this from data
Lets say we are given data D which contains N sentences such that
D cid8S1S2    SNcid9 
where each sentence Sn is
2    wn
meaning that each sentence has a different length
Sn  wn
Given this data D let us estimate the probability of a certain sentence S This is
quite straightforward
where I is the indicator function dened earlier in Eq 37 which is dened as
n1 ISSn
cid26 1
ISSn 
if S  Sn
0 otherwise
This is equivalent to counting how many times S occurs in the data8
521 Data SparsityScarcity
Has this solved the whole problem of language model No unfortunately not The
very major issue here is that however large your corpus is it is unlikely to contain all
reasonable sentences in the world Lets do simple counting here
There are V symbols in a vocabulary Each sentence can be as long as T symbols
Then there are VT possible sentences A reasonable range for the sentence length T
is roughly between 1 to 50 meaning that there are
possible sentences As its quite clear this is a huge space of sentences
Of course not all those sentences are plausible This is however conceivable that
even the fraction of that space will be gigantic especially considering that the size of
vocabulary often goes up to 100k to 1M words Many of the plausible sentences will
not appear in the corpus Is this true In fact yes it is
It is quite easy to nd such an example For instance Google Books Ngram
Viewer9 lets you search for a sentence or a sequence of up to ve English words from
8 A data set consisting of written text is often referred to as a corpus
9 httpsbooksgooglecomngrams
Figure 51 A picture of a llama lying down From httpsenwikipedia
orgwikiLlama
the gigantic corpus of Google Books Let me try to search for a very plausible sen-
tence I like llama and the Google Books Ngram10 Viewer returns an error saying
that Ngrams not found I like llama see Fig 51 in the case you are not familiar
with a llama See Fig 52 as an evidence
Figure 52 A resulting page of Google Books Ngram Viewer for the query I like
llama
What does this mean for the estimate in Eq 52 It means that this estimator will
be too harsh for many of the plausible sentences that do not occur in the data As soon
as a given sentence does not appear exactly as it is in the corpus this estimator will
say that there is a zero probability of the given sentence Although the sentence I like
llama is a likely sentence according to this estimator in Eq 52 it will be deemed
extremely unlikely
This problem is due to the issue of data sparsity Data sparsity here refers to the
10 We will discuss what Ngrams are in the later sections
phenomenon where a training set does not cover the whole space of input sufciently
In more concrete terms most of the points in the input space which have non-zero
probabilities according to the true underlying distribution do not appear in the training
set If the size of a training set is assumed to be xed the severity of data sparsity
increases as the average or maximum length of the sentences This follows from the
fact that the size of the input space the set of all possible sentences grows with respect
to the maximum possible length of a sentence
In the next section we will discuss the most straightforward approach to addressing
this issue of data sparsity
n-Gram Language Model
The fact that the issue of data sparsity worsens as the maximum length of sentences
grows hints us a straightforward approach to addressing this limit the maximum length
of phrasessentences we estimate a probability on This idea is a foundation on which
a so-called n-gram language model is based
In the n-gram language model we rst rewrite the probability of a given sentence
S from Eq 51 into
pS  pw1w2    wT   pw1pw2w1 pwkwk
 pwTwT 
cid124
cid123cid122
cid125
where wk denotes all the symbols before the k-th symbol wk From this the n-
gram language model makes an important assumption that each conditional probability
Eq 53 a is only conditioned on the n 1 preceding symbols only meaning
pwkwk  pwkwknwkn1    wk1
This results in
pS  T
pwtwtn    wt1
What does this mean Under this assumption we are saying that any symbol in a
sentence is predictable based on the n 1 preceding symbols This is in fact a quite
reasonable assumption in many languages For instance let us consider a phrase I am
from Even without any more context information surrounding this phrase such as
surrounding words and the identity of a speaker we know that the word following this
phrase will be likely a name of place or country In other words the probability of a
name of place or country given the three preceding words I am from is higher than
that of any other words
But of course this assumption does not always work For instance consider a
 Let us focus on
phrase In Korea more than half of all the residents speak Korean
cid124 cid123cid122 cid125
the last word Korean marked with a We immediately see that it will be useful
to condition its conditional probability on the second word Korea Why is this so
Because the conditional probability of Korean following speak should signicantly
increase over all the other words that correspond to other languages knowing the fact
that the sentence is talking about the residents of Korea This requires the conditional
distribution to be conditioned on at least 10 words  is considered a separate word
and this certainly will not be captured by n-gram language model with n  9
From these examples it is clear that theres a natural trade-off between the quality
of probability estimate and statistical efciency based on the choice of n in n-gram
language modeling The higher n the longer context the conditional distribution has
leading to a better modelestimate second example however resulting in a situation
of more sever data sparsity see Sec 521 On the other hand the lower n leads to
the worse language modeling second example but this will avoid the issue of data
sparsity
n-gram Probability Estimation We can estimate the n-gram conditional probability
pwkwkn    wk1 from the training corpus Since it is a conditional probability we
need to rewrite it according to the denition of the conditional probability
pwkwkn    wk1 
pwkn    wk1wk
pwkn    wk1
This rewrite implies that the n-gram probability is equivalent to counting the occur-
rences of the n-gram wkn    wk among all n-grams starting with wkn    wk1
Let us consider the denominator rst The denominator can be computed by the
marginalizing the k-th word wcid48 below
pwkn    wk1  
wcid48V
pwkn    wk1wcid48
From Eq 52 we know how to estimate pwkn    wk1wcid48
pwkn    wk1wcid48  cwkn    wk1wcid48
where c is the number of occurrences of the given n-gram in the training corpus and
Nn is the number of all n-grams in the training corpus
Now lets plug Eq 56 into Eqs 5455
pwkwkn    wk1 
cwkn    wk1wk
Nn wcid48V cwkn    wk1wcid48
531 Smoothing and Back-Off
Note that I am missing many references this section as I am writing this on my travel
I will ll in missing references once Im back from my travel
The biggest issue of having an n-gram that never occurs in the training corpus is
that any sentence containing the n-gram will be given a zero probability regardless
of how likely all the other n-grams are Let us continue with the example of I like
llama With an n-gram language model built using all the books in Google Books the
following totally valid sentence11 will be given a zero probability
 I like llama which is a domesticated South American camelid12
Why is this so Because the probability of this sentence is given as a product of all
possible trigrams
pI like llama which is a domesticated South American camelid
pIplikeI pllamaIlike
 pcamelidSouthAmerican
cid125
cid124
cid123cid122
One may mistakenly believe that we can simply increase the size of corpus col-
lecting even more data to avoid this issue However remember that data sparsity is
almost always an issue in statistical modeling 24 which means that more data call
for better statistical models with often more parameters leading to the issue of data
sparsity
One way to alleviate this problem is to assign a small probability to all unseen
n-grams At least in this case we will assign some small non-zero probability to
any sentence thereby avoiding a valid but zero-probability sentence under the n-gram
language model One simplest implementation of this approach is to assume that each
and every n-gram occurs at least  times and any occurrence in the training corpus is
in addition to this background occurrence
In this case the estimate of an n-gram becomes
pwkwkn    wk1 
  cwknwkn1    wk
wcid48V   cwknwkn1    wcid48
V  wcid48V cwknwkn1    wcid48
  cwknwkn1    wk
where cwknwkn1    wk is the number of occurrences of the given n-gram in the
training corpus cwknwkn1    wcid48 is the number of occurrences of the given n-
gram if the last word wk is substituted with a word wcid48 from the vocabulary V   is often
set to be a scalar such that 0    1 See the difference from the original estimate in
Eq 57
It is quite easy to see that this is a quite horrible estimator how does it make sense
to say that every unseen n-gram occurs with the same frequency Also knowing that
this is a horrible approach what can we do about this
One possibility is to smooth the n-gram probability by interpolating between the
estimate of the n-gram probability in Eq 57 and the estimate of the n 1-gram
probability This can written down as
pSwkwkn    wk1  wkn    wk1pwkwkn    wk1
 1  wkn    wk1pSwkwkn1    wk1
11 This is not strictly true as I should put a in front of the llama
12 The description of a llama taken from Wikipedia httpsenwikipediaorgwikiLlama
This implies that the n-gram smoothed probability is computed recursively by the
lower-order n-gram probabilities This is clearly an effective strategy considering that
falling off to the lower-order n-grams contains at least some information of the original
n-gram unlike the previous approach of adding a scalar  to every possible n-gram
Now a big question here is how the interpolation coefcient  is computed The
simplest approach we can think of is to t it to the data as well However the situ-
ation is not that easy as using the same training corpus which was used to estimate
pwkwkn    wk1 according to Eq 57 will lead to a degenerate case What is
this degenerate case If the same corpus is used to t both the non-smoothed n-gram
probability and  s the optimal solution is to simply set all  s to 1 as that will assign
the high probabilities to all the n-grams Therefore one needs to use a separate corpus
to t  s
More generally we may rewrite Eq 58 as
pSwkwkn    wk1 
cid26 wkwkn    wk1 if cwkn    wk1wk  0
wkn1    wkpSwkwkn1    wk1 otherwise
following the notation introduced in 63 Specic choices of  and  lead to a number
of different smoothing techniques For an extensive list of these smoothing techniques
see 24
Before ending this section on smoothing techniques for n-gram language modeling
let me briey describe one of the most widely used smoothing technique called the
modied Kneser-Ney smoothing KN smoothing described in 24 This modied
KN smoothing is efciently implemented in the open-source software package called
KenLM 51
First let us dene some quantities We will use nk to denote the total number of
n-grams that occur exactly k times in the training corpus With this we dene the
following so-called discounting factors
n1  2n2
D1 1 2Y
D2 2 3Y
D3 3 4Y
Also let us dene the following quantities describing the number of all possible words
following a given n-gram with a specied frequency l
Nlwkn    wk1  cwkn    wk1wk  l
The modied KN smoothing then denes  in Eq 59 to be
wkwkn    wk1 
cwkn    wk1wk Dcwkn    wk1wk
wcid48V cwkn    wk1wcid48
where D is
And  is dened as
wkn    wk1 

if c  0
if c  1
if c  2
if c  3
D1N1wkn    wk1  D2N2wkn    wk1  D3N3wkn    wk1
wcid48V cwkn    wk1wcid48
For details on how this modied KN smoothing has been designed see 24
532 Lack of Generalization
Although n-gram language modelling works like a charm in many cases This is still
not totally satisfactory because of the lack of generalization What do I mean by
generalization here
Consider an example where three trigrams13 were observed from a training corpus
chases a cat chases a dog and chases a rabbit There is a clear pattern here The
pattern is that it is highly likely that chases a will be followed by an animal
How do we know this This is a trivial example of humans generalization abil-
ity We have noticed a higher-level concept in this case an animal from observing
words such as cat dog and rabbit and based on this concept we generalize this
knowledge that chases a is followed by an animal to unseen trigrams in the form of
chases a animal
This however does not happen with n-gram language model As an example lets
consider a trigram chases a llama Unless this specic trigram occurred more than
once in the training corpus the conditional probability given by n-gram language mod-
eling will be zero14 This issue is closely related to data sparsity but the main differ-
ence is that it is not the lack of data or n-grams but the lack of world knowledge In
other words there exist relevant n-grams in the training corpus but n-gram language
modelling is not able to exploit these
At this point it almost seems trivial to address this issue by incorporating existing
knowledge into language modelling For instance one can think of using a dictionary
to nd the denition of a word in interest continuing on from the previous example
the denition of llama and letting the language model notice that llama is a a
13 Is trigram a proper term Certainly not but it is widely accepted by the whole community of natural
language processing researchers Heres an interesting discussion on how n-grams should be referred to
as from 77 these alternatives are usually referred to as a bigram a trigram and a four-gram model
respectively Revealing this will surely be enough to cause any Classicists who are reading this book to stop
and to leave the eld to uneducated engineering sorts  with the declining levels of education in recent
decades  some people do make an attempt at appearing educated by saying quadgram 
14 Here we assume that no smoothing or backoff is used However even when these techniques are used
we cannot be satised since the probability assigned to this trigram will be at best reasonable up to the point
that the n-gram language model is giving as high probability as the bigram chases a In other words we do
not get any generalization based on the fact that a llama is an animal similar to a cat dog or rabbit
domesticated pack animal of the camel family found in the Andes valued for its soft
woolly eece Based on this the language model should gure out that the probability
of chases a llama should be similar to chases a cat chases a dog or chases a
rabbit because all cat dog and rabbit are animals according to the dictionary
This is however not satisfactory for us First those denitions are yet another
natural language text and letting the model understand it becomes equivalent to nat-
ural language understanding which is the end-goal of this whole course Second
a dictionary or any human-curated knowledge base is an inherently limited resource
These are limited in the sense that they are often static not changing rapidly to reect
the changes in language use and are often too generic potentially not capturing any
domain-specic knowledge
In the next section I will describe an approach purely based on statistics of natural
language that is able to alleviate this lack of generalization
54 Neural Language Model
One thing we notice from n-gram language modelling is that this boils down to com-
puting the conditional distribution of a next word wk given n  1 preceding words
In other words the goal of n-gram language modeling is to nd a
wkn    wk1
function that takes as input n 1 words and returns a conditional probability of a next
pwkwkn    wk1  f wk
 wkn    wk1
This is almost exactly what we have learned in Chapter 2
First we should dene the input to this language modelling function Clearly the
input will be a sequence of n 1 words but the question is how each of these words
will be represented Since our goal is to put the least amount of prior knowledge we
want to represent each word such that each and every word in the vocabulary is equi-
distant away from the others One encoding scheme that achieves this goal is 1-of-K
coding
In this 1-of-K coding scheme each word i in the vocabulary V is represented as a
binary vector wi whose sum equals 1 To denote the i-th word with the vector wi we
set the i-th element of the vector wi to be 1 and consequently all the other elements
are set to zero Mathematically
wi  00    
    0cid62  01V
1cid124cid123cid122cid125
i-th element
cid26 1
This kind of vector is often called a one-hot vector
It is easy to see that this encoding scheme perfectly ts our goal of having minimal
prior because
wi  w j 
if i cid54 j
otherwise
Now the input to our function is a sequence of n  1 such vectors which I will
denote by w1w2    wn1 As we will use a neural network as a function approx-
imator here15 these vectors will be multiplied with a weight matrix E After this we
get a sequence of continuous vectors p1p2    pn1 where
p j  Ecid62w j
and E  RVd
Before continuing to build this function let us see what it means to multiply the
transpose of a matrix with an one-hot vector from left Since only one of the elements
of the one-hot vector is non-zero all the rows of the matrix will be ignored except for
the row corresponding to the index of the non-zero element of the one-hot vector This
row is multiplied by 1 which simply gives us the same row as the result of this whole
matrixvector multiplication In short the multiplication of the transpose of a matrix
with an one-hot vector is equivalent to slicing out a single row from the matrix
In other words let
 

where ei  Rd Then
Ecid62wi  ei
This view has two consequences First in practice it will be much more efcient
computationally to implement this multiplication as a simple table look-up For in-
stance in Python with NumPy do
p  Ei
instead of
p  numpydotET wi
Second from this perspective we can see each row of the matrix E as a continuous-
space representation of a corresponding word ei will be a vector representation of the
i-th word in the vocabulary V  This representation is often called a word embedding
and should reect the underlying meaning of the word We will discuss this further
shortly
Closely following 8 we will simply concatenate the continuous-space represen-
tations of the input words such that
p cid2p1p2   pn1cid3cid62
15 Obviously this does not have to be true but at the end of the day it is unclear if there is any parametric
function approximation other than neural networks
This vector p is a representation of n 1 input words in a continuous vector space and
often referred to as a context vector
This context vector is fed through a composition of nonlinear feature extraction
layers We can for instance apply the simple transformation layer from Eq 38 such
h  tanhWp  b
where W and b are the parameters
Once a set of nonlinear layers has been applied to the context vector its time to
compute the output probability distribution In this case of language modelling the
distribution outputted by the function is a categorical distribution We discussed how
we can build a function to return a categorical distribution already in Sec 312
As a recap a categorical distribution denes a probability of one event happening
among K discrete events The probability of the k-th event happening is often denoted
as k and
k  1
Therefore the function needs to return a K-dimensional vector 1 2     K In this
case of language modelling K  V and i corresponds to the probability of the i-th
word in the vocabulary for the next word
As discussed earlier in Sec 312 we can use softmax to compute each of those
output probabilities
pwn  kw1w2    wn1  k 
k h  ck
expucid62
kcid481 expucid62
kcid48h  ckcid48
where uk  Rdimh
This whole function is called a neural language model See Fig 53 a for the
graphical illustration of neural language model
541 How does Neural Language Model Generalize to Unseen n-
Grams  Distributional Hypothesis
Now that we have described neural language model let us take a look into what hap-
pens inside Especially we will focus on how the model generalizes to unseen n-grams
The previously described neural language model can be thought of as a composite
of two function g  f  The rst stage f projects a sequence of context words or
preceding n 1 words to a continuous vector space
f  01Vn1  Rd
We will call the resulting vector h a context vector The second stage g maps this
continuous vector h to the target word probability by applying afne transformation to
the vector h followed by softmax normalization
Figure 53 a Schematics of neural language model
language model generalizes to an unseen n-gram
b Example of how neural
Let us look more closely at what g does in Eq 514 If we ignore the effect of
the bias ck for now we can clearly see that the probability of the k-th word in the
vocabulary is large when the output vector uk or the k-th row of the output matrix U
is well aligned with the context vector h In other words the probability of the next
word being the k-th word in the vocabulary is roughly proportional to the inner product
between the context vector h and the corresponding target word vector uk
Now let us consider two context vectors h j and hk These contexts are followed by
a similar set of words meaning that the conditional distributions of the next word are
similar to each other Although these distributions are dened over all possibility target
words let us look at the probabilities of only one of the target words wl
cid16
cid16
cid17
cid17
j pwlh j 
k pwlhk 
cid16
k is then16
wcid62
wcid62
cid17
wcid62
l h j  hk
The ratio between pl
j and pl
From this we can clearly see that in order for the ratio pl
to be 1 ie pl
wcid62
l h j  hk  0
16 Note that both pl
j and pl
k are positive due to our use of softmax
1-of-K codingContinuous-spaceWord RepresentationSoftmaxNonlinear projectionthreefourteamsgroupsNow let us assume that wl is not an all-zero vector as otherwise it will be too dull
a case In this case the way to achieve the equality in Eq 515 is to drive the context
vectors h j and hk to each other In other words the context vectors must be similar
to each other in terms of Euclidean distance in order to result in similar conditional
distributions of the next word
What does this mean This means that the neural language model must project
n 1-grams that are followed by the same word to nearby points in the context vec-
tor space while keeping the other n-grams away from that neighbourhood This is
necessary in order to give a similar probability to the same word If two n 1-grams
which are followed by the same word in the training corpus are projected to far away
points in the context vector space it naturally follows from this argument that the prob-
ability over the next word will differ substantially resulting in a bad language model
Let us consider an extreme example where we do bigram modeling with the train-
ing corpus comprising only three sentences
 There are three teams left for the qualication
 four teams have passed the rst round
 four groups are playing in the eld
We will focus on the bold-faced phrases three teams four teams and four group
The rst word of each of these bigrams is a context word and neural language model
is asked to compute the probability of the word following the context word
It is important to notice that neural language model must project three and four
to nearby points in the context space see Eq 513 This is because the context
vectors from these two words need to give a similar probability to the word teams
This naturally follows from our discussion earlier on how dot product preserves the
ordering in the space And from these two context vectors which are close to each
other the model assigns similar probabilities to teams and groups because they
occur in the training corpus In other words the target word vector uteams and ugroups
will also be similar to each other because otherwise the probability of teams given
four pteamsfour and groups given four pgroupsfour will be very differ-
ent despite the fact that they occurred equally likely in the training corpus
Now lets assume the case where we use the neural language model trained on
this tiny training corpus to assign a probability to an unseen bigram three groups
The neural language model will project the context word three to a point in the con-
text space close to the point of four From this context vector the neural language
model will have to assign a high probability to the word groups because the context
vector hthree and the target word vector ugroups well align Thereby even without ever
seeing the bigram three groups the neural language model can assign a reasonable
probability See Fig 53 b for graphical illustration
What this example shows is that neural language model automatically learns the
similarity among different context words via context vectors h and also among dif-
ferent target words via target word vectors uk by exploiting co-occurrences of words
In this example the neural language model learned that four and three are similar
from the fact that both of them occur together with teams Similarly in the target
side the neural language model was able to capture the similarity between teams
and groups by noticing that they both follow a common word four
This is a clear real-world demonstration of the so-called distributional hypothe-
sis Distributional hypothesis states that words which are similar in meaning appear
in similar distributional contexts 41 By observing which words a given word co-
occurs together it is possible to peek into the words underlying meaning Of course
this is only a partial picture17 into the underlying meaning of each word or as a mat-
ter of fact a phrase but surely still a very interesting property that is being naturally
exploited by neural language model
In neural language model the most direct way to observe the effect of this dis-
tributional hypothesisstructure is to investigate the rst layers weight matrix E in
Eq 512 This weight matrix can be considered as a set of dense vectors of the
words in the input vocabularycid8e1e2    eVcid9 and any visualization technique such
as principal component analysis PCA or t-SNE 104 can be used to project each
high-dimensional word vector into a lower-dimensional space often 2-D or 3-D
542 Continuous Bag-of-Words Language Model
Maximum PseudoLikelihood Approach
This is about time someone asks a question why we are only considering the preceding
words when doing language modelling Is it a good assumption that the conditional
distribution over a word is only dependent on preceding words
In fact we do not have to do so We can certainly model a natural language sentence
such that each word in a sentence is conditioned on 2n surrounding words n words to
the left and n words to the right In this case we get a Markov random eld MRF
language model 56
Figure 54 An example Markov random eld language model MRF-LM with the
order n  1
In a Markov random eld MRF language model MRF-LM we say each word in
a given sentence is a random variable wi We connect each word with its 2n surrounding
words with undirected edges and these edges represent the conditional dependency
structure of the whole MRF-LM An example of an MRF-LM with n  1 is shown in
Fig 54
A probability over a Markov random eld is dened as a product of clique po-
tentials A potential is dened for each clique as a positive function whose input is
the values of the random variables in the clique
In the case of MRF-LM we will
assign 1 as a potential to every clique except for cliques of two random variables in
17 We will discuss why this is only a partial picture later on
other words we only use pairwise potentials only The pairwise potential between the
words i and j is dened as
cid16
Ecid62wicid62Ecid62w jcid17
cid16
cid17
ecid62
 wiw j  exp
where E is from Eq 512 and wi is the one-hot vector of the i-th word One must
note that this is one possible implementation of the pairwise potential and there may be
other possibilities such as to replace the dot product between the word vectors ecid62
wiew j
with a deeper network
With this pairwise potential the probability over the whole sentence is dened as
pw1w2    wT  
 wt w j 
cid32Tn
cid33
ecid62
wt ew j
where Z is the normalization constant This normalization constant makes the product
of the potentials to be a probability and often is at the core of computational intractabil-
ity in Markov random elds
Figure 55 Gray nodes indicate the Markov blank of the fourth word
Although compute the full sentence probability is intractable in this MRF-LM it is
quite straightforward to compute the conditional probability of each word wi given all
the other words When computing the conditional probability we must rst notice that
the conditional probability of wi only depends on the values of other words included
in its Markov blanket In the case of Markov random elds the Markov blanket of
a random variable is dened as a set of all immediate neighbours and it implies that
the conditional probability of wi is dependent only on n preceding words and the n
following words See Fig 55 for an example
Keeping this in mind we can easily see that
pwiwin    wi1wi1    win 
Zcid48 exp
where Zcid48 is a normalization constant computed by
cid32
cid32 n
Zcid48  
ecid62
ewik 
cid33cid33
ewik 
cid32 n
cid32
ecid62
cid33cid33
Do you see a stark similarity to neural language model we discussed earlier This
conditional probability is a shallow neural network with a single linear hidden layer
Figure 56 Continuous Bag-of-Words model approximates the conditional distribution
over the j-th word w j under the MRF-LM
whose input are the context words n preceding and n following words and the output
is the conditional distribution of the center word wi We will talk about this shortly in
more depth See Fig 56 for graphical illustration
Now we know that it is often difcult to compute the full sentence probability
pw1    wT  due to the intractable normalization constant Z We however know how
to compute the conditional probabilities for all words quite tractably The former
fact implies that it is perhaps not the best idea to maximize log-likelihood to train this
model18 The latter however sheds a bit of light because we can train a model to
maximize pseudolikelihood 11 instead19
Pseudolikelihood of the MRF-LM is dened as
logPL 
log pwiwin    wi1wi1    win
Maximizing this pseudolikelihood is equivalent to training a neural network in Fig 56
which approximates each conditional distribution pwiwin    wi1wi1    win
to give a higher probability to the ground-truth center word in the training corpus
Unfortunately even after training the model by maximizing the pseudolikelihood
in Eq 516 we do not have a good way to compute the full sentence probability under
this model Under certain conditions maximizing pseudolikelihood indeed converges
to the maximum likelihood solution but this does not mean that we can use the product
of all the conditionals as a replacement of the full sentence probability However this
does not mean that we cannot use this MRF-LM as a language model since given
a xed model the pseudoprobability the product of all the conditionals can score
different sentences
18 However this is not to say maximum likelihood in this case is impossible There are different ways to
approximate the full sentence probability under this model See 56 for one such approach
19 See the note by Amir Globerson later modied by David Sontag available at httpcsnyu
edudsontagcoursesinference14slidespseudolikelihoodnotespdf
1-of-K codingSoftmaxContinuous Bag-of-WordsThis is in contrast to the neural language model we discussed earlier in Sec 54 In
the case of neural language model we were able to compute the probability of a given
sentence by computing the conditional probability of each word reading from left until
the end of the sentence This is perhaps one of the reasons why the MRF-LM is not
often used in practice as a language model Then you must ask why I even bothered
to explain this MRF-LM in the rst place
This approach which was proposed in 79 as a continuous bag-of-words CBoW
model20 was found to exhibit an interesting property That is the word embedding
matrix E learned as a part of this CBoW model very well reects underlying structures
of words and this has become one of the darling models by natural language processing
researchers in recent years We will discuss further in the next section
Skip-Gram and Implicit Matrix Factorization In 79 another model called skip-
gram is proposed The skip-gram model is built by ipping the continuous bag-of-
words model Instead of trying to predict the middle word given 2n surrounding words
the skip-gram model tries to predict randomly chosen one of the 2n surrounding words
given the middle word From this description alone it is quite clear that this skip-gram
model is not going to be great as a language model However it turned out that the
word vectors obtained by training a skip-gram model were as good as those obtained by
either a continuous bag-of-words model or any other neural language model Of course
it is debatable which criterion be used to determine the goodness of word vectors but in
many of the existing so-called intrinsic evaluations those obtained from a skip-gram
model have been shown to excel
The authors of 72 recently showed that training a skip-gram model with negative
sampling see 79 is equivalent to factorizing a positive point-wise mutual informa-
tion matrix PPMI into two lower-dimensional matrices The left lower-dimensional
matrix corresponds to the input word embedding matrix E in a skip-gram model In
other words training a skip-gram model implicitly factorizes a PPMI matrix
Their work drew a nice connection between the existing works on distributional
word representations from natural language processing or even computational linguis-
tics and these more recent neural approaches I will not go into any further detail in
this course but I encourage readers to read 72
543 Semi-Supervised Learning with Pretrained Word Embeddings
One thing I want to emphasize in these language models including n-gram language
model neural language model and continuous bag-of-words model is that they are
purely unsupervised meaning that all we need is a large corpus of unannotated text
This is one thing that makes this statistical approach to language modelling much more
appealing than any other approach based on linguistic structures see Sec 511 for a
brief discussion
20 One difference between the model we derived in this section starting from the MRF-LM and the one
proposed in 79 is that in our derivation the neural network shares a single weight matrix E for both the
input and output layers
When it comes to neural language model and continuous bag-of-words model we
now know that these networks learn continuous vector representations of input words
target words and the context phrase h from Eq 513 We also discussed how these
vector representations encode similarities among different linguistic units be it a word
or a phrase
What this implies is that once we train this type of language model on a large or
effectively innite21 corpus of unlabelled text we get good vectors for those linguistic
units for free Among these word vectors the rows of the input weight matrix E in
Eq 512 have been extensively used in many natural language processing applica-
tions in recent years since 103 33 79
Let us consider an extreme example of classifying each English word as either
positive or negative For instance happy is positive while sad is negative A
training set of 2 examples1 positive and 1 negative words is given How would one
build a classier22
There are two issues here First it is unclear how we should represent the input in
this case a word A good reader who has read this note so far will be clearly ready to
use an one-hot vector and use a softmax layer in the output and I commend you for
that However this still does not solve a more serious issue which is that we have only
two training examples All the word vectors save for two vectors corresponding to the
words in the training set will not be updated at all
One way to overcome these two issues is to make somewhat strong but reasonable
assumption that similar input will have similar sentiments This assumption is at the
heart of semi-supervised learning 23 It says that high-dimensional data points in
effect lies on a lower-dimensional manifold and the target values of the points on this
manifold change smoothly Under this assumption if we can well model this lower-
dimensional data manifold using unlabelled training examples we can train a good
classier23
And guess what We have access to this lower-dimensional manifold which is
represented by the set of pretrained word vectors E Believing that similar words have
similar sentiment and that these pretrained word vectors indeed well reect similarities
among words let me build a simple nearest neighbour NN classier which uses the
pretrained word vectors
cid26 positive
NNw 
if cosewehappy  cosewebad
otherwise
where cos is a cosine similarity dened as
negative
coseie j 
ecid62
cid107eicid107cid107e jcid107 
21 Why Because of almost universal broadband access to the Internet
22 Although the setting of 2 training examples is extreme but the task itself turned out to be not-so-
extreme In fact there is multiple dictionaries of words sentiment maintained For instance check http
sentiwordnetisticnritsearchphpqllama
23 What do I mean by a good classier A good classier is a classier that classies unseen test examples
well See Sec 23
This use of a term similarity almost makes this set of pretrained word vectors
look like some kind of magical wand that can solve everything24 This is however not
true and using pretrained word vectors must be done with caution
Why should we be careful in using these pretrained word vectors We must remem-
ber that these word vectors were obtained by training a neural network to maximize a
certain objective or to minimize a certain cost function This means that these word
vectors capture certain aspects of words underlying structures that are necessary to
achieve the training objective and that there is no reason for these word vectors to
capture any other properties of the words that are not necessary for maximizing the
training objective In other words similarity among multiple words has many dif-
ferent aspects and these word vectors will capture only a few of these many aspects
Which few aspects will be determined by the choice of training objective
The hope is that language modelling is a good training objective that will encourage
the word vectors to capture as many aspects of similarity as possible25 But is this true
in general
Lets consider an example of words describing emotions such as happy sad
and angry in the context of a continuous bag-of-words model These emotion-
describing words often follow some forms of a verb feel such as feel feels
felt and feeling This means that those emotion-describing words will have to be
projected nearby in the context space in order to give a high probability to those forms
of feel as a middle word This is understandable and agrees quite well with our in-
tuition All those emotion-describing words are similar to each other in the sense that
they all describe emotion But wait this aspect of similarity is not going to help sen-
timent classication of words In fact this aspect of similarity will hurt the sentiment
classier because a positive word happy will be close to negative words sad and
angry in this word vector space
The lesson here is that when you are solving a language-related task with very little
data it is a good idea to consider using a set of pretrained word vectors from neural
language models However you must do so in caution and perhaps try to pretrain your
own word vectors by training a neural network to maximize a certain objective that
better suits your nal task
But then what other training objectives are there We will get to that later
55 Recurrent Language Model
Neural language model indeed avoids the lack of generalization in the conventional n-
gram language modeling It still assumes the n-th order Markov property meaning that
it looks only as far back into the past as n 1 words In Sec 53 I gave an example of
In Korea more than half of all the residents speak Korean In this example the con-
ditional distribution over the last word in the sentence clearly will be better estimated
24 For future reference I must say there were many papers claiming that the pretrained word vectors are
indeed magic wands at three top-tier natural language processing conferences ACL EMNLP NAACL in
2014 and 2015
25 Some may ask how a single vector which is a point in a space can capture multiple aspects of similarity
This is possible because these word vectors are high-dimensional
Figure 57
network language model
a A recurrent neural network from Sec 414 b A recurrent neural
if it is conditioned on the second word of the sentence which is more than 10 words
back in the past
Let us recall what we learned in Sec 414 There we learn how to build a recurrent
neural network to read a variable-length sequence and return a variable-length output
sequence An example we considered back then was a task of part-of-speech tagging
where the input is a sentence such as
x  Childreneatsweetcandy
and the target output is a sequence of part-of-speech tags such as
y  nounverbadjectivenoun
In order to make less of an assumption on the conditional independence of the
predicted tags we made a small adjustment such that the prediction Yt at each timestep
was fed back into the recurrent neural network in the next timestep together with the
input Xt1 See Fig 57 a for graphical illustration
Why am I talking about this again after saying that the task of part-of-speech tag-
ging is not even going to be considered as a valid topic for the nal project Because
the very same model for part-of-speech tagging will be turned into the very recurrent
neural network language model in this section
Let us start by considering a single conditional distribution marked a below from
the full sentence probability
pw1w2    wT  
cid124
pwtw1    wt1
cid123cid122
cid125
This conditional probability can be approximated by a neural network as weve been
doing over and over again throughout this course that takes as input w1    wt1 and
returns the probability over all possible words in the vocabulary V  This is not unlike
neural language model we discussed earlier in Sec 54 except that the input is now a
variable-length sequence
Figure 58 A recurrent neural network language model
htcid48
In this case we can use a recurrent neural network which is capable of summariz-
ingmemorizing a variable-length input sequence A recurrent neural network summa-
rizes a given input sequence w1    wt1 into a memory state ht1
cid26 0
f ewtcid48 htcid481
where tcid48 runs from 0 to t  1
f is a recurrent function which can be any of a naive
transition function from Eq 41 a gated recurrent unit or a long short-term memory
unit from Sec 422 ewtcid48 is a word vector corresponding to the word wtcid48
This summary ht1 is afne-transformed followed by a softmax nonlinear function
to compute the conditional probability of wt Hopefully everyone remembers how it is
done As in Eq 46
if tcid48  0
otherwise 
  softmaxVht1
where  is a vector of probabilities of all the words in the vocabulary
One thing to notice here is that the iteration procedure in Eq 517 computes a
sequence of every memory state vector ht by simply reading the input sentence once
In other words we can let the recurrent neural network read one word wt at a time
update the memory state ht and compute the conditional probability of the next word
pwt1wt 
This procedure is illustrated in Fig 57 b26 This language model is called a
recurrent neural network language model RNN-LM 80
But wait from looking at Figs 57 ab there is a clear difference between
the recurrent neural networks for part-of-speech tagging and language model That is
there is no feedback connection from the output of the previous time step back into the
recurrent neural network in the RNN-LM This is simply an illusion from the limitation
in the graphical illustration because the input wt1 in the next time step is in fact the
output wt1 at the current time step This becomes clearer by drawing the same gure
in a slightly different way as in Fig 58
26 In the gure you should notice the beginning-of-the-sentence symbol cid104scid105 This is necessary in order to
use the very same recurrent function f to compute the conditional probability of the rst word in the input
sentence
56 How do n-gram language model neural language
model and RNN-LM compare
Now the question is which one of these language models we should use in practice In
order to answer this we must rst discuss the metric most commonly used for evaluat-
ing language models
The most commonly used metric is a perplexity In the context of language mod-
elling the perplexity PPL of a model M is computed by
n1 logb pM wnwn
PPL  b 1
where N is the number of all the words in the validationtest corpus and b is some
constant that is often 2 or 10 in practice
What is this perplexed metric I totally agree with you on this one Of course there
is a quite well principled way to explain what this perplexity is based on information
theory This is however not necessary for us to understand this metric called perplexity
As the exponential function with base b in the case of perplexity in Eq 518
is a monotonically increasing function we see that the ordering of different language
models based on the perplexity will not change even if we only consider the exponent
logb pM wnwn
Furthermore assuming that b  1 we can simply replace logb with log natural loga-
rithm without changing the order of different language models
log pM wnwn
Now this looks awfully similar to the cost function or negative log-likelihood we
minimize in order to train a neural network see Chapter 2
Lets take a look at a single term inside the summation above
log pM wnwn
This is simply measuring how high a probability the language model M is assigning to
a correct next word given all the previous words Again because log is a monotonically
increasing function
In summary the inverse perplexity measures how high a probability the language
model M assigns to correct next words in the testvalidation corpus on average There-
fore a better language model is the one with a lower perplexity There is nothing so
perplexing about the perplexity once we start viewing it from this perspective
We are now ready to compare different language models or to be more precise
three different classes of language modelscount-based n-gram language model neural
n-gram language model and recurrent neural network language model The biggest
challenge in doing so is that this comparison will depend on many factors that are not
easy to control To list a few of them
 Language
 GenreTopic of training validation and test corpora
 Size of a training corpus
 Size of a language model
Figure 59 The perplexity word error rate WER and character error rate CER of
an automatic speech recognition system using different language models Note that all
the results by neural or recurrent language models are by interpolating these models
with the count-based n-gram language model Reprinted from 100
Because of this difculty this kind of comparison has often been done in the con-
text of a specic downstream application This choice of a downstream application
often puts rough constraints on the size of available or commonly used corpus target
language and reasonably accepted size of language models For instance the authors
of 3 compared the conventional n-gram language model and neural language model
with various approximation techniques with machine translation as a nal task In
100 the authors compared all the three classes of language model in the context of
automatic speech recognition
First let us look at one observation made in 100 From Fig 59 we can see that
it is benecial to use a recurrent neural network language model RNN-LM compared
to a usual neural language model Especially when long short-term memory units were
Figure 510 The trend of perplexity as the size of language model changes Reprinted
from 100
used the improvement over the neural language model was signicant Furthermore
we see that it is possible to improve these language models by simply increasing their
Similarly in Fig 510 from the same paper 100 it is observed that larger language
models tend to get betterlower perplexity and that RNN-LM in general outperforms
neural language models
These two observations do seem to suggest that neural and recurrent language mod-
els are better candidates as language model However this is not to be taken as an
evidence for choosing neural or recurrent language models It has been numerously
observed over years that the best performance both in terms of perplexity and in terms
of performance in the downstream applications such as machine translation and auto-
matic speech recognition is achieved by combining a count-based n-gram language
model and a neural or recurrent language model See for instance 92
This superiority of combined or hybrid language model suggests that the count-
based or conventional n-gram language model neural language model and recurrent
neural network language model are capturing underlying structures of natural language
sentences that are complement to each other However it is not crystal clear how these
captured structures differ from each other
Chapter 6
Neural Machine Translation
Finally we have come to the point in this course where we discuss an actual natural
language task In this chapter we will discuss how translation from one language to
another can be done with statistical methods more specically neural networks
61 Statistical Approach to Machine Translation
Lets rst think of what it means to translate one sentence X in a source language to an
equivalent sentence Y in a target language which is different from the source language
A process of translation is a function that takes as input the source sentence X and
returns a correct translation Y  and it is clear that there may be more than one correct
translations The latter fact implies that this function of translation should return not a
single correct translation but a probability distribution that assigns high probabilities
to more than one likely translations
Now let us write it in a more formal way First the input is a sequence of words
where Tx is the length of the source sentence A target sentence is
X  x1x2    xTx 
Y  y1y2    yTy
Similarly Ty is the length of the target sentence
The translation function f then reads the input sequence X and computes the prob-
ability over target sentences In other words
f  V 
x  CVy1
is a set of all possible source sentences of any
where Vx is a source vocabulary and V 
length Tx  0 Vy is a target vocabulary and Ck is a standard k-simplex
What is a standard k-simplex It is a set dened by
cid12cid12cid12cid12cid12 k
cid40
cid41
t0    tk  Rk1
tk  1 and ti  0 for all i
Figure 61 Graphical illustration of statistical machine translation
In short this set contains all possible settings for categorical distributions of k  1
possible outcomes This means that the translation function f returns a probability
distribution PYX over all possible translations of length Ty  1
Given a source sentence X this translation function f returns the conditional prob-
ability of a translation Y  PYX Let us rewrite this conditional probability according
to what we have discussed in Chapter 5
PYX 
cid124
Pyty1    yt1
Xcid124cid123cid122cid125
conditional
cid125
cid123cid122
language modelling
Looking at it in this way it is clear that this is nothing but conditional language mod-
elling This means that we can use any of the techniques we have used earlier in
Chapter 5 for statistical machine translation
Training can be trivially done by maximizing the log-likelihood or equivalently
minimizing the negative log-likelihood see Sec 31
given a training set
C    1
log pyn
t X n
D cid8X 1Y 1 X 2Y 2     X NY Ncid9
consisting of N training pairs
All these look extremely straightforward and do not deviate too much from what we
have learned so far in this course A big picture on this process translation is shown in
Fig 61 More specically building a statistical machine translation model is simple
because we have learned how to
1 Assign a probability to a sentence in Sec 52
2 Handle variable-length sequences with recurrent neural networks in Sec 41
3 Compute the gradient of an empirical cost function C with respect to the param-
eters  of a recurrent neural network in Sec 412 and Sec 34
Corporaf  La croissance conomique sest ralentie ces dernires annes e  Economic growth has slowed down in recent years 4 Use stochastic gradient descent to minimize the cost function in Sec 222
Of course simply knowing all these does not get you a working neural network that
translates from one language to another We will discuss in detail how we can build
such a neural network in the next section Before going to the next section we must
rst discuss two issues 1 where do we get training data 2 how do we evaluate
machine translation systems
611 Parallel Corpora Training Data for Machine Translation
First let us consider again what the problem were trying to solve here It is machine
translation and from the description in the previous section and from Eqs 6162
it is a sentence-to-sentence translation task We approach this problem by building a
model that takes as input a source sentence S and computes the probability PYX of
a target sentence Y  equivalently a translation In order for this model to translate we
must train it with a training set of pairs of a source sentence and its correct translation
The very rst problem we run into is where we can nd this training set which is
often called a parallel corpus It is not easy to think of documents which have been
translated into multiple languages Lets take for instance all the books that are being
translated each year According to 86 approximately 3 of titles published each year
in English are translations from another language1 A few international news agencies
publish some of their news articles in multiple languages For instance AFP publishes
1500 stories in French 700 stories in English 400 stories in Spanish 250 stories in
Arabic 200 stories in German and 150 stories in Portuguese each day and there are
some overlapping stories across these six languages2 Online commerce sites such as
eBay often list their products in international sites with their descriptions in multiple
languages3
Unfortunately these sources of multiple languages of the same content are not suit-
able for our purpose Why is this so Most importantly they are often copy-righted
and sold for personal use only We cannot buy more than 14400 books in order to
train a translation model We will likely go broke before completing the purchase
and even if so it is unclear whether it is acceptable under copyright to use these text
to build a translation model Because we are mixing multiple sources of which each
is protected under copyright is the translation model trained from a mix of all these
materials considered a derivative work4
This issue is nothing new and has been there since the very rst statistical machine
translation system was proposed in 19 Fortunately it turned out that there are a
number of legitimate sources where we can get documents translated in more than
one languages often very faithfully to their content These sources are parliamentary
proceedings of bilingual or multilingual countries
1 According to the information Bowker released in October of 2005 in 2004 there were 375000 new
books published in English  Of that total approx 14440 were new translations which is slightly more
than 3 of all books published 86
2 httpwwwafpcomenproductsservicestext
3 httpsellercentreebaycoukinternational-selling-tools
4 httpcopyrightgovcircscirc14pdf
Brown et al 19 used the proceedings from the Canadian parliament which are by
law kept in both French and English All of these proceedings are digitally available
and called Hansards You can check it yourself online at httpwwwparlgc
ca and heres an excerpt from the Prayers of the 2nd Session 41st Parliament Issue
 French ELIZABETH DEUX par la Grace de Dieu REINE du Royaume-
Uni du Canada et de ses autres royaumes et territoires Chef du Commonwealth
Defenseur de la Foi
 English ELIZABETH THE SECOND by the Grace of God of the United
Kingdom Canada and Her other Realms and Territories QUEEN Head of the
Commonwealth Defender of the Faith
Every single word spoken in the Canadian parliament is translated either into French
or into English A more recent version of Hansards preprocessed for research can be
found at httpwwwisiedunatural-languagedownloadhansard
Similarly the European parliament used to provided the parliamentary proceedings
in all 23 ofcial languages6 This is a unique data in the sense that each and every
sentence is translated into either 11 or 26 ofcial languages For instance here is one
example 65
 Danish det er nsten en personlig rekord for mig dette efterar
 German das ist fur mich fast personlicher rekord in diesem herbst 
 Greek omitted
 English that is almost a personal record for me this autumn 
 Spanish es la mejor marca que he alcanzado este otono 
 Finnish se on melkein minun ennatykseni tana syksyna 
 French c  est pratiquement un record personnel pour moi  cet automne 
 Italian e  quasi il mio record personale dell  autunno 
 Dutch dit is haast een persoonlijk record deze herfst 
 Portuguese e quase o meu recorde pessoal deste semestre 
 Swedish det ar nastan personligt rekord for mig denna host 
The European proceedings has been an invaluable resource for machine translation
research At least the existing multilingual proceedings up to 2011 can be still used
and it is known in the eld as the Europarl corpus 65 and can be downloaded from
httpwwwstatmtorgeuroparl
These proceedings-based parallel corpora have two distinct advantages First in
many cases the sentences in those corpora are well-formed and their translations are
5 This is one political lesson here Canada is still headed by the Queen of the United Kingdom
6 Unfortunately the European parliament decided to stop translating its proceedings into all 23 of-
cial languages on 21 Nov 2011 as an effort toward budget cut See httpwwweuractivcom
cultureparliament-cuts-translation-budg-news-516201
done by professionals meaning the quality of the corpora is guaranteed Second sur-
prisingly the topics discussed in those proceedings are quite diverse Clearly the mem-
bers of the parliament do not often chitchat too often but they do discuss a diverse set
of topics Heres one such example from the Europarl corpus
 English Although there are now two Finnish channels and one Portuguese one
there is still no Dutch channel which is what I had requested because Dutch
people here like to be able to follow the news too when we are sent to this place
of exile every month
 French Il y a bien deux chanes nnoises et une chane portugaise mais il
ny a toujours aucune chane neerlandaise Pourtant je vous avais demande une
chane neerlandaise car les Neerlandais aussi desirent pouvoir suivre les actu-
alites chaque mois lorsquils sont envoyes en cette terre dexil
One apparent limitation is that these proceedings cover only a handful of languages
in the world mostly west European languages This is not desirable Why According
to Ethnologue 20147 the top-ve most spoken languages in the world are
1 Chinese approx 12 billion
2 Spanish approx 414 million
3 English approx 335 million
4 Hindi approx 260 million
5 Arabic approx 237 million
There are only two European languages in this list
So then where can we get all data for all these non-European languages There
are a number of resources you can use and let me list a few of them here
You can nd the translated subtitle of the TED talks at the Web Inventory of
Transcribed and Translated Talks WIT3 httpswit3fbkeu 22
a quite small corpus but includes 104 languages For RussianEnglish data Yandex
released a parallel corpus of one million sentence pairs You can get it at https
translateyandexrucorpuslangen You can continue with other
languages by googling very hard but eventually you run into a hard wall
This hard wall is not only the lack of any resource but also lack of enough resource
For instance I quickly googled for KoreanEnglish parallel corpora and found the
following resources
 SWRC English-Korean multilingual corpus 60000 sentence pairs http
semanticwebkaistackrhomeindexphpCorpus10
 Jungyeuls English-Korean parallel corpus 94123 sentence pairs https
githubcomjungyeulkorean-parallel-corpora
This is just not large enough
One way to avoid this or mitigate this problem is to automatically mine parallel
corpora from the Internet There have been quite some work in this direction as a way
7 httpwwwethnologuecomworld
to increase the size of parallel corpora 87 112 The idea is to build an algorithm that
crawls the Internet and nd a pair of corresponding pages in two different languages
One of the largest preprocessed corpus of multiple languages from the Internet is the
Common Crawl Parallel Corpus created by Smith et al 98 available at http
wwwstatmtorgwmt13training-parallel-commoncrawltgz
612 Automatic Evaluation Metric
Lets say we have trained a machine translation model on a training corpus A big
question follows how do we evaluate this model
In the case of classication evaluation is quite straightforward All we need to do is
to classify held-out test examples with a trained classier and see how many examples
were correctly classied This is however not true in the case of translation
There are a number of issues but let us discuss two most important problems here
First there may be many correct translations given a single source sentence For in-
stance the following three sentences are the translations made by a human translator
given a single Chinese sentence 82
 It is a guide to action that ensures that the military will forever heed Party com-
 It is the guiding principle which guarantees the military forces always being
under the command of the Party
 It is the practical guide for the army always to heed the directions of the party
They all clearly differ from each other although they are the translations of a single
source sentence
Second the quality of translation cannot be measured as either success or failure
It is rather a smooth measure between success and failure Let us consider an English
translation of a French sentence Jaime un llama qui est un animal mignon qui vit en
Amerique du Sud8
One possible English translation of this French sentence is I like a llama which is a
cute animal living in South America Lets give this translation a score 100 success
According to Google translate the French sentence above is I like a llama a cute
animal that lives in South America I see that Google translate has omitted qui est
from the original sentence but the whole meaning has well been captured Let us give
this translation a slightly lower score of 90
Then how about I like a llama from South America This is certainly not a
correct translation but except for the part about a llama being cute this sentence does
communicate most of what the original French sentence tried to communicate Maybe
we can give this translation a score of 50
How about I do not like a llama which is an animal from South America This
translation correctly describes the characteristics of llama exactly as described in the
source sentence However this translation incorrectly states that I do not like a llama
when I like a llama according to the original French sentence What kind of score
would you give this translation
8 I would like to thank Laurent Dinh for the French translation
Even worse we want an automated evaluation algorithm We cannot look at thou-
sands of validation or test sentence pairs to tell how well a machine translation model
does Even if we somehow did it for a single model in order to compare this translation
model against others we must do it for every single machine translation model under
comparison We must have an automatic evaluation metric in order to efciently test
and compare different machine translation models
BLEU One of the most widely used automatic evaluation metric for assessing the
quality of translations is BLEU proposed in 82 BLEU computes the geometric mean
of the modied n-gram precision scores multiplied by brevity penalty Let me describe
this in detail here
First we dene the modied n-gram precision pn of a translation Y as
SC ngramS cngram
SC ngramS cngram
where C is a corpus of all the sentencestranslations and S is a set of all unique n-grams
in one sentence in C cngram is the count of the n-gram and cngram is
cngram  mincngramcrefngram
crefngram is the count of the n-gram in reference sentences
What does this modied n-gram precision measure It measures the ratio between
the number of n-grams in the translation and the number of those n-grams actually
occurred in a reference ground-truth translation If there is no n-gram from the trans-
lation in the reference this modied precision will be zero because cref will be zero
all the time
It is common to use the geometric average of modied 1- 2- 3- and 4-gram preci-
sions which is computed by
cid33
cid32
1  exp
If we use this geometric average P as it is there is a big loophole One can get
a high average modied precision by making as short a translation as possible For
instance a reference translation is
 I like a llama a cute animal that lives in South America 
and a translation we are trying to evaluate is
 cute animal that lives
This is clearly a very bad translation but the modied 1- 2- 3- and 4-gram precisions
will be high The modied precisions are
1  1  1  1
1  1  1  1
1  1  1
1  1  1
cid19
0  0  0  0
Their geometric average is then
1  exp
cid181
which is the maximum modied precision you can get
In order to avoid this behaviour BLEU penalizes the geometric average of the
modied n-gram precisions by the ratio of the lengths between the reference r and
translation l This is done by rst computing a brevity penalty
cid26 1
expcid01 r
cid1
 if l  r
 if l  r
If the translation is longer than the reference it uses the geometric average of the
modied n-gram precisions as it is Otherwise it will penalize it by multiplying the
average precision with a scalar less than 1 In the case of the example above the brevity
penalty is 0064 and the nal BLEU score is 0064
Figure 62 a BLEU vs bilingual and monolingual judgements of three machine
translation systems S1 S2 and S3 and two humans H1 and H2 Reprinted from
b BLEU vs human judgement adequacy and uency separately of three
machine translation systems two statistical and one rule-based systems Reprinted
from 20
The BLEU was shown to correlate well with human judgements in the original
article 82 Fig 62 a shows how BLEU correlates with the human judgements in
comparing different translation systems
This is however not to be taken as a message saying that the BLEU is the perfect
automatic evaluation metric
It has been shown that the BLEU is only adequate in
comparing two similar machine translation systems but not too much so in comparing
two very different systems For instance Callison-Burch et al 20 observed that the
BLEU underestimates the quality of the machine translation system that is not a phrase-
based statistical system See Fig 62 b for an example
BLEU is denitely not a perfect metric and many researchers strive to build a better
evaluation metric for machine translation systems Some of the alternatives available
at the moment are METEOR 36 and TER 99
62 Neural Machine Translation
Simple Encoder-Decoder Model
From the previous section and from Eq 62 it is clear that we need to model each
conditional distribution inside the product as a function This function will take as
input all the previous words in the target sentence Y  y1    yt1 and the whole
source sentence X  x1    xTx  Given these inputs the function will compute the
probabilities of all the words in the target vocabulary Vy In this section I will describe
an approach that was proposed multiple times independently over 17 years in 43 28
Let us start by tackling how to handle the source sentence X  x1    xTx  Since
this is a variable-length sequence we can readily use a recurrent neural network from
Chapter 4 However unlike the previous examples there is no explicit targetoutput in
this case All we need is a vector summary of the source sentence
We call this recurrent neural network an encoder as it encodes the source sentence
into a continuous vector code It is implemented as
ht1Ecid62
ht  enc
cid16
cid17
As usual enc can be any recurrent activation function but it is highly recommended to
use either gated recurrent units see Sec 422 or long short-term memory units see
Sec 423 Ex  RVxd is an input weight matrix containing word vectors as its rows
see Eq 512 in Sec 54 and xt is an one-hot vector representation of the word xt
see Eq 510 in Sec 54 h0 is initialized as an all-zero vector
After reading the whole sentence up to xTx the last memory state hTx of the encoder
summarizes the whole source sentence into a single vector as shown in Fig  a
Thanks to this encoder we can now work with a single vector instead of a whole
sequence of source words Let us denote this vector as c and call it a context vector
We now need to design a decoder again using a recurrent neural network As I
mentioned earlier the decoder is really nothing but a language model except that it is
conditioned on the source sentence X What this means is that we can build a recurrent
neural network language model from Sec 55 but feeding also the context vector at
each time step In other words
zt  dec
zt1
Ecid62
y yt1c
cid105cid17
cid16
cid104
Figure 63 a The encoder and b the decoder of a simple neural machine translation
Do you see the similarity and dissimilarity to Eq 517 from Sec 55 Its essentially
same except that the input at time t is a concatenated vector of the word vector of the
previous word yt1 and the context vector c
Once the decoders memory state is updated we can compute the probabilities of
all possible target words by
cid16
cid17
pyt  wcid48yt X  exp
ecid62
wcid48zt
where ewcid48 is the target word vector associated the word wcid48 This is equivalent to afne-
transforming zt followed by a softmax function from Eq 35 from Sec 31
Now should we again initialize z0 to be an all-zero vector Maybe or maybe not
One way to view what this decoder does is that the decoder models a trajectory in
a continuous vector space and each point in the trajectory is zt Then z0 acts as a
starting point of this trajectory and it is natural to initialize this starting point to be a
point relevant to the source sentence Because we have access to the source sentences
content via c we can again use it to initialize z0 as
z0  init c 
See Fig 63 b for the graphical illustration of the decoder
Although I have used c as if it is a separate variable this is not true c is simply
a shorthand notation of the last memory state of the encoder which is a function of
the whole source sentence What does this mean It means that we can compute the
gradient of the empirical cost function in Eq 63 with respect to all the parameters of
both the encoder and decoder and maximize the cost function using stochastic gradient
descent just like any other neural network we have learned so far in this course
621 Sampling vs Decoding
Sampling We are ready to compute the conditional distribution PYX over all pos-
sible translations given a source sentence When we have a distribution the rst thing
we can try is to sample from this distribution Often it is not straightforward to gen-
erate samples from a distribution but fortunately in this case we can readily generate
exact samples from the distribution PYX
We simply iterate over the following steps until a token indicating the end of a
sentence cid104eoscid105
1 Compute c Eq 65
2 Initialize z0 with c Eq 68
3 Compute zt given zt1 yt1 and c Eq 66
4 Compute pytyt X Eq 67
5 Sample yt from the compute distribution
6 Repeat 35 until yt  cid104eoscid105
After taking these steps we get a sample Y 
cid16
cid17
y1     y Y
given a source sentence
X Of course there is no guarantee that this will be a good translation of X In order to
nd a good translation meaning a translation with a high probability P YX we need
to repeatedly sample multiple translations from PYX and choose one with the high
probability
This is not too desirable as it is not clear how many translations we need to sample
from PYX and also it will likely be computationally expensive We must wonder
whether we can solve the following optimization problem directly
Y  argmax
logPYX
Unfortunately the exact solution to this requires evaluating PYX for every possible
Y  Even if we limit our search space of Y to consist of only sentences of length up
to a nite number it will likely become too large the cardinality of the set grows
exponentially with respect to the number of words in a translation Thus it only
makes sense to solving the optimization problem above approximately
Approximate Decoding Beamsearch Although it is quite clear that nding a trans-
lation Y that maximizes the log-probability logP YX is extremely expensive we will
regardlessly try it here
One very natural way to enumerate all possible target sentences and simultaneously
computing the log-probability of each and every one of them is to start from all possible
rst word compute the probabilities of them and from each potential rst word branch
into all possible second words and so on This procedure forms a tree and any path
from the root of this tree to any intermediate node is a valid but perhaps very unlikely
sentence See Fig 64 for the illustration The conditional probabilities of all these
paths or sentences can be computed as we expand this tree down by simply following
Eq 62
Of course we cannot compute the conditional probabilities of all possible sen-
tences Hence we must resort to some kind of approximate search Wait search Yes
this whole procedure of nding the most likely translation is equivalent to searching
through a space in this case a tree of all possible sentences for one sentence that has
the highest conditional probability
Figure 64 a Search space depicted as a tree b Greedy search
The most basic approach to approximately searching for the most likely translation
is to choose only a single branch at each time step t In other words
yt  argmax
wcid48V
log pyt  wcid48 yt X
where the conditional probability is dened in Eq 67 and yt   y1 y2     yt1 is
a sequence of greedily-selected target words up to the t  1-th step This procedure
is repeated until the selected yt is a symbol corresponding to the end of the translation
often denoted as cid104eoscid105 See Fig 64 b for illustration
There is a big problem of this greedy search That is as soon as it makes one
mistake at one time step there is no way for this search procedure to recover from this
mistake This happens because the conditional distributions at later steps depend on
the choices made earlier
Consider the following two sequences w1w2 and wcid48
1w2 These sequences
probabilities are
pw1w2  pw1pw2w1
pwcid48
1pw2wcid48
1w2  pwcid48
Lets assume that
where 0    1 meaning that pw1  pwcid48
choose w1 over wcid48
1 and ignore wcid48
 pw1  pwcid48
Now we can see that theres a problem with this Lets assume that
pw2wcid48
 pw2w1  pw2wcid48
1  pw2w1 
1 In this case the greedy search will
where  was dened earlier In this case
pw1w2 pw1pw2w1   pwcid48
1  pwcid48
 pwcid48
pw2wcid48
1pw2w1
1pw2wcid48
1  pwcid48
In short
It means that the sequence wcid48
algorithm is unable to notice this because simply pw1  pwcid48
pw1w2  pwcid48
1w2 is more likely than w1w2 but the greedy search
Unfortunately the only way to completely avoid this undesirable situation is to
consider all the possible paths starting from the very rst time step This is exactly the
reason why we introduced the greedy search in the rst place but the greedy search
is too greedy The question is then whether there is something in between the exact
search and the greedy search
Beam Search Let us start from the very rst position t  1 First we compute the
conditional probabilities of all the words in the vocabulary
py1  wX for all w  V
Among these we choose the K most likely words and initialize the K hypotheses
1 w1
2     w1
We use the subscript to denote the hypothesis and the subscript the time step As an
example w1
1 is the rst hypothesis at time step 1
For each hypothesis we compute the next conditional probabilities of all the words
in the vocabulary
py2  wy1  w1
i X for all w  V
where i  1    K We then have K V candidates with the corresponding probabil-

cid124
cid125
   
   
cid123cid122
   
Figure 65 Beam search with the beam width set to 3
Among these K V candidates we choose the K most likely candidates
1 w1
2     w1
Starting from these K new hypotheses we repeat the process of computing the proba-
bilities of all K V possible candidates and choosing among them the K most likely
new hypotheses
It should be clear that this procedure called beam search and shown in Fig 65
becomes equivalent to the exact search as K   Also when K  1 this procedure is
equivalent to the greedy search In other words this beam search interpolates between
the exact search which is computationally intractable but exact and the greedy search
which is computationally very cheap but probably quite inexact by changing the size
K of hypotheses maintained throughout the search procedure
How do we choose K One might mistakenly think that we can simply use as large
K as possible given the constraints on computation and memory Unfortunately this is
not necessarily true as this interpolation by K is not monotonic That is the quality of
the translation found by the beam search with a larger K is not necessarily better than
the translation found with a smaller K
Let us consider the case of vocabulary having three symbols abc and any valid
translation being of a length 3 In the rst step we have
pa  05 pb  015 pc  045
In the case of K  1 ie greedy search we choose a If K  2 we will keep a and
Given a as the rst symbol we have
paa  04 pba  03 pca  03
in which case we keep aa with K  1 With K  2 we should check also
pac  045 pbc  045 pcc  01
from which we maintain the hypotheses ca and cb 045 045 and 045 045
respectively Note that with K  2 we have discarded aa
Now the greedy search ends by computing the last conditional probabilities
paaa  09 pbaa  005 pcaa  005
The nal verdict from the greedy search is therefore aaa with its probability being
05 04 09  018
What happens with the beam search having K  2 We need to check the following
conditional probabilities
paca  07 pbca  02 pcca  01
pacb  04 pbcb  00 pccb  06
From here we consider caa and cbc with the corresponding probabilities 045
045 07  014175 and 045 045 06  01215 Among these two caa is
nally chosen due to its higher probability than that of cbc
In summary the greedy search found aaa whose probability is
paaa  018
and the beam search with K  2 found caa whose probability is
pcaa  014175
Even with a larger K the beam search found a worse translation
Now clearly what one can do is to set the maximum beam width K and try with
all possible 1  K  K Among the translations given by K beam search procedures
the best translation can be selected based on their corresponding probabilities From
the point of view of computational complexity this is perhaps the best approach to
upper-bound the worst-case memory consumption Doing the beam search once with
K or multiple beam searches with K  1     K are equivalent in terms of memory con-
sumption ie both are OKV Furthermore the worst-case computation is OKV
assuming a constant time computation for computing each conditional probability In
practice however the constant in front of KV does matter and we often choose K
based on the translation quality of the validation set after trying a number of values
124816
If youre interested in how to improve beam search by backtracking so that the
beam search becomes complete refer to eg 44 113 If youre interested in general
search strategies refer to 90 Also in the context of statistical machine translation it
is useful to read 64
63 Attention-based Neural Machine Translation
One important property of the simple encoder-decoder model for neural machine trans-
lation from Sec 62 is that a whole source sentence is compressed into a single real-
valued vector c This sounds okay since the space of all possible source sentences is
countable while the context vector space 11d is uncountable There exists a map-
ping from this sentence space to the context vector space and all we need to ensure is
that training the simple encoder-decoder model nds this mapping This is conditioned
on the assumption that the hypothesis space9 dened by the model architecturethe
number of hidden units and parameters includes this mapping from any source sen-
tence to a context vector
Unfortunately considering the complexity of any natural language sentence it is
quite easy to guess that this mapping must be highly nonlinear and will require a huge
encoder and consequently a huge decoder to map back from a context vector to a target
sentence In fact this fact was empirically validated last year 2014 when the almost
identical models from two groups 101 27 showed vastly different performances on
the same EnglishFrench translation task The only difference there was that the au-
thors of 101 used a much larger model than the authors of 27 did
At a more fundamental level theres a question of whether a natural language sen-
tence should be fully represented as a single vector For instance there is now a famous
quote by Prof Raymond Mooney10 of the University of Texas at Austin You cant
cram the meaning of a whole  sentence into a single  vector11 Though
our goal is not in answering this fundamental question from linguistics
Our goal is rather to investigate the possibility of avoiding this situation of having
to learn a highly nonlinear complex mapping from a source sentence to a single vector
The question we are more interested in is whether there exists a neural network that
can handle a variable-length sentence by building a variable-length representation of
it Especially we are interested in whether we can build a neural machine translation
system that can exploit a variable-length context representation
Variable-length Context Representation In the simple encoder-decoder model a
source sentence regardless of its length was mapped to a single context vector by a
recurrent neural network
cid16
ht  enc
cid17
ht1Ecid62
See Eq 65 and the surrounding text for more details
Instead here we will encode a source sentence X  x1x2    xTx  with a set C of
context vectors hts This is achieved by having two recurrent neural networks rather
than a single recurrent neural networks as in the simple encoder-decoder model The
rst recurrent neural network to which we will refer as a forward recurrent neural
network reads the source sentence as usual and results in a set of forward memory
9 See Sec 232
10 httpswwwcsutexasedumooney
11 httpnlpersblogspotcom201409amr-not-semantics-but-close-maybe
Figure 66 An encoder with a bidirectional recurrent neural network
h t for t  1    Tx The second recurrent neural network a reverse recurrent
neural network reads the source sentence in a reverse order starting from xTx to x1
h t for t 
This reverse network will output a sequence of reverse memory states
1    Tx
For each xt we will concatenate
h t and
cid35
cid34 
h t to form a context-dependent vector ht
h t
We will form a context set with these context-dependent vectors c  h1h2    hTx
See Fig 66 for the graphical illustration of this process
Now why is ht a context-dependent vector We should look at what the input was
to a function that computed ht The rst half of ht
cid16
h t  fenc
h t was computed by
cid17
cid17
x xt1
Ecid62
h t was
cid16
cid105cid62
where fenc is a forward recurrent activation function From this we see that
computed by all the source words up to t ie xt Similarly
Ecid62
h t depends on all the source
h t  renc
cid17
cid17
where renc is a reverse recurrent activation function and
words from t to the end ie xt
h cid62
In summary ht 
h cid62
cid104
is a vector representation of the t-th word xt with
respect to all the other words in the source sentence This is why ht is a context-
dependent representation But then what is the difference among all those context-
dependent representations h1    hTx We will discuss this shortly
cid105cid17cid104
Decoder with Attention Mechanism Before anything let us think of what the mem-
ory state zt of the decoder from Eq 66 does
cid105cid17
cid105cid17
cid16
cid16
cid104
cid104
Ecid62
y yt3c
Ecid62
y yt2c
Ecid62
y yt1c
zt  dec
cid16 
cid16 Ecid62
cid16 Ecid62
Figure 67 Illustration of how the relevance score e23 of the second context vector h2
at time step 3 dashed curves and box
It is computed based on all the generated target words so far  y1 y2     yt1 and
the context vector12 c which is the summary of the source sentence The very reason
why I designed the decoder in this way is so that the memory state zt is informative of
which target word should be generated at time t after generating the rst t  1 target
words given the source sentence In order to do so zt must encode what have been
translated so far among the words that are supposed to be translated which is encoded
in the context vector c Lets keep this in mind
In order to compute the new memory state zt with a context set C h1h2    hTx
we must rst get one vector out of Tx context vectors Why is this necessary Because
we cannot have an innitely large number of parameters to cope with any number of
context vectors Then how can we get a single vector from an unspecied number of
context vectors hts
First let us score each context vector h j  j  1    Tx based on how relevant it is
for translating a next target word This scoring needs to be based on 1 the previous
memory state zt1 which summarizes what has been translated up to the t  2-th
word13 2 the previously generated target word yt1 and 3 the j-th context vector
e jt  fscorezt1Ecid62
y yt1h j
Conceptually the score e jt will be computed by comparing zt1 yt1 with the con-
text vector c j See Fig 67 for graphical illustration
12 We will shortly switch to using a context set instead
13 Think of why this is only up to the t  2-th word not up to the t  1-th one
Figure 68 Computing the new memory state zt of the decoder based on the previous
memory state zt1 the previous target word yt1 and the weighted average of context
vectors according to the attention weights
Once the scores for all the context vectors h js  j  1    Tx are computed by
fscore we normalize them with a softmax function
expe jt 
jcid481 expe jcid48t 
 jt 
We call these normalized scores the attention weights as they correspond to how much
the decoder attends to each of the context vectors This whole process of computing
the attention weights is often referred to as an attention mechanism see eg 26
We take the weighted average of the context vectors with these attention weights
 jth j
This weighted average is used to compute the new memory state zt of the decoder
which is identical to the decoders update equation from the simple encoder-decoder
model see Eq 66 except that ct is used instead of c a in the equation below
zt1
Ecid62
y yt1 ctcid124cid123cid122cid125


zt  dec
See Fig 68 for the graphical illustration of how it works
Given the new memory state zt of the decoder the output probabilities of all the
target words in a vocabulary happen without any change from the simple encoder-
decoder model in Sec 62
We will call this model which has a bidirectional recurrent neural network as an en-
coder and a decoder with the attention mechanism an attention-based encoder-decoder
model This approach was proposed last year 2014 in the context of machine transla-
tion in 2 and has been studied extensively in 76
631 What does the Attention Mechanism do
One important thing to notice is that this attention-based encoder-decoder model can be
reduced to the simple encoder-decoder model easily This happens when the attention
mechanism fscore in Eq 610 returns a constant regardless of its input When this
happens the context vector ct at each time step t see Eq 612 is same for all the
time steps t  1    Ty
The encoder effectively maps the whole input sentence into a single vector which was
at the core of the simple encoder-decoder model from Sec 62
This is not the only situation in which this type of behaviour happens Another
h 1 of
possible scenario is for the encoder to make the last memory states
the forward and reverse recurrent neural networks to have a special mark telling that
these are the last states The attention mechanism then can exploit this to assign a large
score to these two memory states but still constant across time t This will become
even closer to the simple encoder-decoder model
h Tx and
The question is how we can avoid these degenerate cases Or is it necessary for us
to explicitly make these degenerate cases unlikely Of course there is no single answer
to this question Let me give you my answer which may differ from others answer
The goal of introducing a novel network architecture is to guide a model according
to our intuition or scientic observation so that it will do a better job at a target task In
our case the attention mechanism was introduced based on our observation and some
intuition that it is not desirable to ask the encoder to compress a whole source sentence
into a single vector
This incorporation of prior knowledge however should not put a hard constraint
We give a model a possibility of exploiting this prior knowledge but should not force
the model to use this prior knowledge exclusively As this prior knowledge based
on our observation of a small portion of data is not likely to be true in general the
model must be able to ignore this if the data does not exhibit the underlying structure
corresponding to this prior knowledge In this case of attention-based encoder-decoder
model the existence of those degenerate cases above is a direct evidence of what this
attention-based model can do if there is no such underlying structure present in the
Then a natural next question is whether there are such structures that can be well
exploited by this attention mechanism in real data
If we train this attention-based
encoder-decoder model on the parallel corpora we discussed earlier in Sec 611 what
kind of structure does this attention-based model learn
In order to answer this question we must rst realize that we can easily visualize
what is happening inside this attention-based model First note that given a pair of
source X and target Y sentences14 the attention-based model computes an alignment
matrix A  01
XY

X1 X2
1Y
2Y
 XY
 
where  jt is dened in Eq 611
Each column at of this alignment matrix A is how well each source word based
on its context-dependent vector representation from Eq 69 is aligned to the t-th
target word Each row b j similarly shows how well each target word is aligned to the
content-dependent vector of the j-th source word In other words we can simply draw
the alignment matrix A as if it were a gray scale 2-D image
In Fig 69 the visualization of four alignment matrices is presented It is quite
clear especially to a French-English bilingual speaker that the model indeed captured
the underlying structure of wordphrase mapping between two languages For instance
focus on European Economic Area in Fig 69 a The model correctly noticed
that Area corresponds to zone Economic to economique and European to
europeenne without any supervision about this type of alignment
This is nice to see that the model was able to notice these regularities from data
without any explicit supervision However the goal of introducing the attention mech-
anism was not to get these pretty gures After all our goal is not to build an inter-
pretable model but a model that is predictive of the correct output given an input see
Chapter 1 and 16 In this regard how much does the introduction of the attention
mechanism help
In 2 the attention-based encoder-decoder model was compared against the sim-
ple encoder-decoder model in the task of English-French translation They observed
the relative improvement of up to 60 in terms of BLEU see Sec 612 as shown in
Table 61 Furthermore by using some of the latest techniques such as handling large
vocabularies 55 building a vocabulary of subword units 93 and variants of the atten-
tion mechanism 76 it has been found possible to achieve a better translation quality
with neural machine translation than the existing state-of-the-art translation systems
14 Note that if youre given only a source sentence you can let the model translate and align simultane-
Simple EncDec
Attention-based EncDec
Attention-based EncDec LV
Attention-based EncDec LVcid63
State-of-the-art SMT
BLEU Rel Improvement
1060
Table 61 The translation performances and the relative improvements over the simple
encoder-decoder model on an English-to-French translation task WMT14 measured
by BLEU 2 55 cid63 an ensemble of multiple attention-based models  the state-of-
the-art phrase-based statistical machine translation system 39
64 Warren Weavers Memorandum
In 1949 Warren Weaver15 wrote a memorandum titled cid104Translationcid105 on machine trans-
lation 108 Although this text was written way before computers have become ubiq-
uitous16 there are many interesting ideas that are closely related to what we have dis-
cussed so far in this chapter Let us go over some parts of the Weavers memorandum
and see how the ideas there corresponds to modern-day machine translation
Necessity of Linguistic Knowledge Weaver talks about a distinguished mathemati-
cian P who was surprised by his colleague His colleague had an amateur interest in
cryptography and one day presented P his method to decipher an encrypted Turkish
text successfully The most important point according to Weaver from this instance
is that the decoding was done by someone who did not know Turkish Now this
sounds familiar doesnt it
As long as there was a parallel corpus we are able to use neural machine transla-
tion models described throughout this chapter without ever caring about which lan-
guages we are training a model to translate between Especially if we decide to consider
each sentence as a sequence of characters17 there is almost no need for any linguistic
knowledge when building these neural machine translation systems
This lack of necessity for linguistic knowledge is not new In fact the most widely
studied and used machine translation approach which is count-based statistical ma-
chine translation 19 66 does not require any prior knowledge about source and target
languages All it needs is a large corpus
Importance of Context Recall from Sec 63 that the encoder of an attention-based
neural machine translation uses a bidirectional recurrent neural network in order to ob-
tain a context set Each vector in the context set was considered a context-dependent
15 Yes this is the very same Weaver after which the building of the Courant Institute of Mathematical
Sciences has been named
16 Although Weaver talks about modern computers over and over in his memorandum what he refers to
is not exactly what we think of computers as these days
17 In fact only very recently people have started investigating the possibility of building a machine trans-
lation system based on character sequences 73 This has been made possible due to the recent success of
neural machine translation
vector as it represents what the center word means with respect to all the surround-
ing words This context dependency is a necessary component in making the whole
attention-based neural machine translation as it helps disambiguating the meaning of
each word and also distinguishing multiple occurrences of a single word by their con-
Weaver discusses this extensively in Sec 34 in his memorandum First to Weaver
it was amply clear that a translation procedure that does little more than handle a one-
to-one correspondence of words can not hope to be useful 
in which the problems
of  multiple meanings  are frequent In other words it is simply not possible to
look at each word separately from surrounding words or context and translate it to a
corresponding target word because there is uncertainty in the meaning of the source
word which can only be resolved by taking into account its context
So what does Weaver propose in order to address this issue He proposes in Sec 5
that if one can see not only the central word in question but also say N words on
either side then if sic N is large enough one can unambiguously decide the meaning
of the central word If we consider only a single sentence and take the innite limit of
N   we see that what Weaver refers to is exactly the bidirectional recurrent neural
network used by the encoder of the attention-based translation system Furthermore
we see that the continuous bag-of-words language model or Markov random eld
based language model from Sec 542 exactly does what Weaver proposed by setting
N to a nite number
In Sec 521 we talked about the issue of data sparsity and how it is desirable to
have a larger N but its often not a good idea statistically to do so Weaver was also
worried about this by saying that it would hardly be practical to do this by means of
a generalized dictionary which contains all possible phases sic 2N  1 words long
for the number of such phases sic is horrifying We learned that this issue of data
sparsity can be largely avoided by adopting a fully parametric approach instead of a
table-based approach in Sec 54
Common base of human communications Weaver suggested in the last section of
his memorandum that perhaps the way for translation is to descend from each lan-
guage down to the common base of human communication  the real but as yet undis-
covered universal language  and then re-emerge by whatever particular route is conve-
nient He specically talked about a universal language and this makes me wonder
if we can consider the memory state of the recurrent neural networks both of the en-
coder and decoder as this kind of intermediate language This intermediate language
radically departs from our common notion of natural languages Unlike conventional
languages it does not use discrete symbols but uses continuous vectors This use of
continuous vectors allows us to use simple arithmetics to manipulate the meaning as
well as its surface realization18
This view may sound radical considering that what weve discussed so far has been
conned to translating from one language to another After all this universal language
18 If you nd this view too radical or fascinating I suggest you to look at the presentation slides by
Geoff Hinton at httpsdrivegooglecomfiled0B16RwCMQqrtdMWFaeThBTC1mZkk
viewuspsharing
of ours is very specic to only a single source language with respect to a single target
language This is however not a constraint on the neural machine translation by design
but simply a consequence of our having focused on this specic case
Indeed in this year 2015 researchers have begun to report that it is possible to
build a neural machine translation model that considers multiple languages and even
further multiple tasks 38 75 More works in this line are expected and it will be
interesting to see if Weavers prediction again turns out to be true
Figure 69 Visualizations of the four sample alignment matrices The alignment
matrices were computed from an attention-based translation model trained to translate
a sentence in English to French Reprinted from 2
TheagreementontheEuropeanEconomicAreawassignedinAugust1992endLaccordsurlazoneconomiqueeuropenneatsignenaot1992endItshouldbenotedthatthemarineenvironmentistheleastknownofenvironmentsendIlconvientdenoterquelenvironnementmarinestlemoinsconnudelenvironnementendDestructionoftheequipmentmeansthatSyriacannolongerproducenewchemicalweaponsendLadestructiondelquipementsignifiequelaSyrienepeutplusproduiredenouvellesarmeschimiquesendThiswillchangemyfuturewithmyfamilythemansaidendCelavachangermonaveniravecmafamilleaditlhommeendChapter 7
Final Words
Let me wrap up this lecture note by describing some aspects of natural language under-
standing with distributed representations that I have not discussed in this course These
are the topics that I would have spent time on had the course been scheduled to last
twice the duration as it is now Afterward I will nalize this whole lecture note with a
short summary
71 Multimedia Description Generation as Translation
Those who have followed this course closely so far must have noticed that the neural
machine translation model described in the previous chapter is quite general in the
sense that the input to the model does not have to be a sentence In the case of the
simple encoder-decoder model from Sec 62 it is clear that any type of input X can be
used instead of a sentence as long as there is a feature extractor that returns the vector
representation c of the input
And fortunately we already learned how to build a feature extractor throughout
this course Almost every single model that is a neural network in our case converts
an input into a continuous vector Let us take a multilayer perceptron from Sec 33
as an example Any classier built as a multilayer perceptron can be considered as a
two-stage process see Sec 332 First the feature vector of the input is extracted see
Eq 39
 x   ux  c
The extracted feature vector  x is then afne-transformed followed by softmax func-
tion This results in a conditional distribution over all possible labels see Eq 44
This means that we can make the simple encoder-decoder model to work with non-
language input simply by replacing the recurrent neural network based encoder with
the feature extraction stage of the multilayer perceptron Furthermore it is possible to
pretrain this feature extractor by training the whole multilayer perceptron on a separate
classication dataset1
1 This way of using a feature extractor pretrained from another network has become a de facto standard
This approach of using the encoder-decoder model for describing non-language
input has become popular in recent years especially 2014 and 2015 and has been
applied to many applications including imagevideo description generation and speech
recognition For an extensive list of these applications I refer the readers to a recent
review article by Cho et al 26
Example Image Caption Generation Let me take as an example the task of im-
age caption generation The possibility of using the encoder-decoder model for image
caption generation was noticed by several research groups almost simultaneously last
year 2014 62 106 59 78 37 40 252 The success of neural machine translation
in 101 and earlier success of deep convolutional network on object recognition see
eg 67 96 102 inspired them the idea to use the deep convolutional networks fea-
ture extractor together with the recurrent neural network decoder for the task of image
caption generation
Right after these Xu et al 111
realized that it is possible to use the
attention-based encoder-decoder model
from Sec 63 for image caption gen-
eration Unlike the simple model the
attention-based model requires a context
set instead of a context vector The con-
text set should contain multiple context
vectors and each vector should repre-
sent a spatial location with respect to
the whole image meaning each context
vector is a spatially-localized context-
dependent image descriptor This was
achieved by using the last convolutional
layers activations of the pretrained deep
convolutional network instead of the last
fully-connected layers See Fig 71 for
graphical illustration of this approach
Figure 71 Image caption generation with
the attention-based encoder-decoder model
These approaches based on neural
networks or in other words based on dis-
tributed representations have been suc-
cessful at image caption generation Four out of ve top rankers in the recent Microsoft
CoCo Image Captioning Challenge 20153 were using variants of the neural encoder-
decoder model based on human evaluation of the captions
in many of the computer vision tasks 94 This is also closely related to semi-supervised learning with
pretrained word embeddings which we discussed in Sec 543 In that case it was only the rst input layer
that was pretrained and used later see Eqs 511512
2 I must however make a note that Kiros et al 62 proposed a fully neural network based image caption
generation earlier than all the others cited here did
3 httpmscocoorgdatasetcaptions-leaderboard
AnnotationVectorsWord SsampleuiRecurrentStatezif  a   man   is   jumping   into   a   lake   hjAttentionMechanismaAttention        weightjaj1Convolutional Neural Network72 Language Understanding with World Knowledge
In Sec 12 we talked about how we view natural languages as a function This function
of natural language maps from a tuple of a speakers speech a listeners mental state
and the surrounding world to the listeners reaction often as a form of natural language
response Unfortunately in order to make it manageable we decided to build a model
that approximates only a part of this true function
Immediate state of the surrounding world In this course of action one thing we
have dropped out is the surrounding world The surrounding world may mean many
different things One of them is the current state of the surrounding world As an
example when I say look at this cute llama it is quite likely that the surrounding
world at the current state contains either an actual llama or at least a picture of a llama
A listener then understands easily what a llama is even without having known what a
llama is in advance By looking at the picture of llama the listener makes a mental note
that the llama looks similar to a camel and therefore must be a four-legged animal
If the surrounding world is not taken into account as weve been doing so far
the listener can only generalize based on the context words Just like how the neural
language model from Sec 54 generalized to unseen or rarely seen words the listener
can infer that llama must be a type of animal by remembering that the phrase look
at this cute has mainly been followed by an animal such as cat or dog However
it is quite clear that look at this cute is also followed by many other nouns including
baby book and so on
The question is then how to exploit this How can we incorporate for instance
vision information from the surrounding world into natural language understanding
The simplest approach is to simply concatenate a word embedding vector see
Eq 512 and a corresponding image vector obtained from an existing feature ex-
tractor see above 60 This can be applied to any existing language models such as
neural language model see Sec 54 and neural machine translation model see Chap-
ter 6 This approach gives a strong signal to the model the similarities among different
words based on the corresponding objects appearances This approach of concatenat-
ing vectors of two different modalities eg language and vision was earlier proposed
in 109
A more sophisticated approach is to design and train a model to solve a task that
requires tight interaction between language and other modalities As our original goal
is to build a natural language function all we need to do is to build a function approxi-
mator that takes as input both language and other modalities Recently Antol et al 1
built a large-scale dataset of question-answer-image triplets called visual question an-
swering VQA for this specic purpose They have carefully built the dataset such
that many if not most questions can only be answered when the accompanying image
is taken into consideration Any model thats able to solve the questions in this dataset
well will have to consider both language and vision
Knowledge base Lost in a library So far we have talked about incorporating an
immediate state of the surrounding world However our use of languages is more
sophisticated This is especially apparent in written languages Let us take an example
of me writing this lecture note It is not the case where I simply sit and start writing
the whole text based purely on my mental state with memory of my past research and
the immediate surrounding world state which has almost nothing to do with Rather
a large part of this writing process is spent on going through various research articles
and books written by others in order to nd relevant details of the topic
In this case the surrounding world is a database in which human knowledge is
stored You can think of a library or the Internet As the amount of knowledge is
simply too large to be memorized in the entirety it is necessary for a person to be able
to search through the vast knowledge base But wait what does it have to do with
natural language understanding
Consider the case where the context phrase is Llama is a domesticated camelid
from Without access to the knowledge base or in this specic instance access to
Wikipedia any language model can only say as much as that this context phrase is
likely followed by a name of some place This is especially true if we assume that the
training corpus did not mention llama at all However if the language model is able
to search Wikipedia and condition on its search result it suddenly becomes so obvious
that this context phrase is followed by South America or the name of any region on
Andean mountain rages
Although this may sound too complicated a task to incorporate into a neural net-
work the concept of how to incorporate this is not necessarily complicated In fact we
can use the attention mechanism discussed in Sec 63 almost as it is Let us describe
here a conceptual picture of how this can be done
Let D  d1d2    dM be a set of knowledge vectors Each knowledge vector
di is a vector representation of a piece of knowledge For instance di can be a vector
representation of one Wikipedia article It is certainly unclear what is the best way to
obtain this vector representation of an entire article but let us assume that an oracle
gave us a means to do so
Let us focus on recurrent language modelling from Sec 554 At each time step
we have access to the following vectors
1 Context vector ht1 the summary all the preceding words
2 Current word wt the current input word
Similarly to what we have done in Sec 63 we will dene a scoring function fscore
which scores each knowledge vector di with respect to the context vector and the cur-
rent word
it  exp  fscorediht1ewt  
where ewt is a vector representation of the current word wt
This score reects the relevance of the knowledge in predicting the next word and
once it is computed for every knowledge vector we compute the weighted sum of all
4 This approach of using attention mechanism for external knowledge pieces has been proposed recently
in 14 in the context of question-answering Here we stick to language modelling as the course has not
dealt with question-answering tasks
the knowledge
itdi
This vector dt is a vector summary of the knowledge relevant to the next word taking
into account the context phrase In the case of an earlier example the scoring function
gives a high score to the Wikipedia article on llama based on the history of preceding
words Llama is a domesticated camelid from
This knowledge vector is used when updating the memory state of the recurrent
neural network
cid0ht1ewt  dt
cid1 
ht  frec
From this updated memory state which also contains the knowledge extracted from
the selected knowledge vector the next words distribution is computed according to
Eq 46
One important issue with this approach is that the size of knowledge set D is often
extremely large For instance English Wikipedia contains more than 5M articles as of
23 Nov 20155 It easily becomes impossible to score each and every knowledge vector
not to mention to extract knowledge vectors of all the articles6 It is an open question
how this unreasonable amount of computation needed for search can be avoided
Why is this any signicant One may naively think that if we train a large enough
network with a large enough data which contains all those world knowledge a trained
network will be able to contain all those world knowledge likely in a compressed
form in its parameters together with its network architecture This is true up to a
certain level but there are many issues here
First the world knowledge were talking about here contains all the knowledge
accumulated so far Even a human brain arguably the best working neural network
to date cannot store all the world knowledge and must resort to searching over the
external database of knowledge It is no wonder we have libraries where people can go
and look for relevant knowledge
Second the world knowledge is dynamic Every day some parts of the world
knowledge become obsolete and at the same time previously unknown facts are added
to the world knowledge If anyone looked up Facebook before 2004 they wouldve
ended up with yearly facebooks from American universities Nowadays it is almost
certain that when a person looks up Facebook they will nd information on Face-
book the social network site Having all the current world knowledge encoded in the
models parameters is not ideal in this sense
5 httpsenwikipediaorgwikiWikipediaStatistics
6 This is true especially when those knowledge vectors are also updated during training
73 Larger-Context Language Understanding
Beyond Sentences and Beyond Words
If we view natural language as a function it becomes clear that what weve discussed
so far throughout the course is heavily restrictive There are two reasons behind this
restriction
First what we have discussed so far has narrowly focused on handling a sentence
In Sec 52 I have described language model as a way to model a sentence probability
pS This is a bit weird in the sense that weve been using a term language modelling
not sentence modelling Keeping it in mind we can start looking at a probability of a
document or discourse D as a whole rather than as a product of sentence probabilities
pSkSk
where the document D consists of N sentences This approach is readily integrated into
the language modelling approaches we discussed earlier in Chapter 5 by
pw jw jSk
This is applicable to any language-related models we have discussed so far includ-
ing neural language model from Sec 54 recurrent language model from Sec 55
Markov random eld language model from Sec 542 and neural machine translation
from Chapter 6
In the context of language modelling two recent articles proposed to explore this
direction I refer the readers to 107 and 57
Second we have stuck to representing a sentence as a sequence of words so far
despite a short discussion in Sec 512 where I strongly claim that this does not have
to be This is indeed true and in fact even if we replace most occurrence of word
in this course with for instance character all the arguments stand Of course by
using smaller units than words we run into many practical and theoretical issues One
most severe practical issue is that each sentence suddenly becomes much longer One
most sever theoretical issue is that it is a highly nonlinear mapping from a sequence
of characters to its meaning as we discussed earlier in Sec 512 Nevertheless the
advance in computing and deep neural networks which are capable of learning such
a highly nonlinear mapping have begun to let researchers directly work on this prob-
lem of using subword units see eg 61 73 Note that I am not trying to say that
characters are the only possible sub-word units and recently an effective statistical ap-
proach to deriving sub-word units off-line was proposed and applied to neural machine
translation in 93
74 Warning and Summary
Before I nish this lecture note with the summary of what we have discussed through-
out this course let me warn you by quoting Claude Shannon 957
It will be all too easy for our somewhat articial prosperity to collapse
overnight when it is realized that the use of a few exciting words like in-
formation entropy redundancy do not solve all our problems
Natural language understanding with distributed representation is a fascinating topic
that has recently gathered large interest from both machine learning and natural lan-
guage processing communities This may give a wrong sign that this approach with
neural networks is an ultimate winner in natural language understandingprocessing
though without any ill intention As Shannon pointed out this prosperity of distributed
representation based natural language understanding may collapse overnight as can
any other approaches out there8 Therefore I warn the readers especially students to
keep this quote in their mind and remember that it is not a few recent successes of this
approach to natural language understanding but the fundamental ideas underlying this
approach that matter and should be remembered after this course
Summary Finally here goes the summary of what we have learned throughout this
semester We began our journey by a brief discussion on how we view human language
as and we decided to stick to the idea that a language is a function not an entity existing
independent of the surrounding world including speakers and listeners Is this a correct
way to view a human language Maybe maybe not I will leave it up to you to decide
In order to build a machine that can approximate this language function in Chap-
ter 2 we studied basic ideas behind supervised learning in machine learning We de-
ned what a cost function is how we can minimize it using an iterative optimization
algorithm specically stochastic gradient descent and learned the importance of hav-
ing a validation set for both early-stopping and model selection These are all basic
topics that are dealt in almost any basic machine learning course and the only thing
that I would like to emphasize is the importance of not looking at a held-out test set
One must always select anything related to learning eg hyperparameters networks
architectures and so on based solely on a validation set As soon as one tunes any
of those based on the test set performance any result from this tuning easily becomes
invalid or at least highly disputable
In Chapter 3 we nally talked about deep neural networks or more traditionally
called multilayer perceptron9 I tried to go over basic but important details as slowly as
possible including how to build a deep neural network based classier how to dene
a cost function and how to compute the gradient wrt the parameters of the network
However I must confess that there are better materials for this topic than this lecture
7 I would like to thank Adam Lopez for pointing me to this quote
8 Though it is interesting to note that information theory never really collapsed overnight Rather its
prosperity has been continuing for more than half a century since Shannon warned us about its potential
overnight collapse in 1956
9 I personally prefer multilayer perceptron but it seems like it has gone out of fashion
We then moved on to recurrent neural networks in Chapter 4 This was a necessary
step in order to build a neural network based model that can handle both variable-length
input and output Again my goal here was to take as much time as it is needed to moti-
vate the need of recurrent networks and to give you basic ideas underlying them Also
I spent quite some time on why it has been considered difcult to train recurrent neural
networks by stochastic gradient descent like algorithms and as a remedy introduced
gated recurrent units and long short-term memory units
Only after these long four to ve weeks have I started talking about how to handle
language data in Chapter 5 I motivated neural language models by the lack of general-
ization and the curse of data sparsity It is my regret that I have not spent much time on
discussing the existing techniques for count-based n-gram language models but again
there are much better materials and better lecturers for these techniques already Af-
ter the introduction of neural language model I spent some time on describing how
this neural language model is capable of generalizing to unseen phrases Continuing
from this neural language model in Sec 55 language modelling using recurrent neu-
ral networks was introduced as a way to avoid Markov assumption of n-gram language
This discussion on neural language model naturally continued on to neural machine
translation in Chapter 6 Rather than going directly into describing neural machine
translation models I have spent a full week on two issues that are often overlooked
data preparation in Sec 611 and evaluation in Sec 612 I wish the discussion of these
two topics has reminded students that machine learning is not only about algorithms
and models but is about a full pipeline starting from data collection to evaluation often
with loops here and there This chapter nished with where we are in 2015 compared
to what Weaver predicted in 1949
Of course there are so many interesting topics in this area of natural language
understanding I am not qualied nor knowledgeable to teach many if not most of
those topics unfortunately and have focused on those few topics that I have worked on
myself I hope this lecture note will serve at least as a useful starting point into more
advanced topics in natural language understanding with distributed representations
Bibliography
1 S Antol A Agrawal J Lu M Mitchell D Batra C L Zitnick and D Parikh
Vqa Visual question answering In International Conference on Computer Vi-
sion ICCV 2015
2 D Bahdanau K Cho and Y Bengio Neural machine translation by jointly
learning to align and translate arXiv preprint arXiv14090473 2014
3 P Baltescu and P Blunsom Pragmatic neural language modelling in machine
translation arXiv preprint arXiv14127119 2014
4 F Bastien P Lamblin R Pascanu J Bergstra I Goodfellow A Bergeron
N Bouchard D Warde-Farley and Y Bengio Theano new features and speed
improvements arXiv preprint arXiv12115590 2012
5 A G Baydin B A Pearlmutter and A A Radul Automatic differentiation in
machine learning a survey arXiv preprint arXiv150205767 2015
6 Y Bengio N Boulanger-Lewandowski and R Pascanu Advances in optimiz-
ing recurrent networks In Acoustics Speech and Signal Processing ICASSP
2013 IEEE International Conference on pages 86248628 IEEE 2013
7 Y Bengio N Leonard and A Courville Estimating or propagating gradi-
ents through stochastic neurons for conditional computation arXiv preprint
arXiv13083432 2013
8 Y Bengio H Schwenk J-S Senecal F Morin and J-L Gauvain Neural
probabilistic language models In Innovations in Machine Learning pages 137
186 Springer Berlin Heidelberg 2006
9 Y Bengio P Simard and P Frasconi Learning long-term dependencies with
gradient descent is difcult Neural Networks IEEE Transactions on 52157
166 1994
10 J Bergstra O Breuleux F Bastien P Lamblin R Pascanu G Desjardins
J Turian D Warde-Farley and Y Bengio Theano a cpu and gpu math expres-
sion compiler In Proceedings of the Python for scientic computing conference
SciPy volume 4 page 3 Austin TX 2010
11 J Besag Statistical analysis of non-lattice data The statistician pages 179195
12 C M Bishop Mixture density networks 1994
13 C M Bishop Pattern recognition and machine learning springer 2006
14 A Bordes N Usunier S Chopra and J Weston Large-scale simple question
answering with memory networks arXiv preprint arXiv150602075 2015
15 L Bottou Online algorithms and stochastic approximations In D Saad edi-
tor Online Learning and Neural Networks Cambridge University Press Cam-
bridge UK 1998
16 L Breiman et al Statistical modeling The two cultures with comments and a
rejoinder by the author Statistical Science 163199231 2001
17 J S Bridle Training stochastic model recognition algorithms as networks can
lead to maximum mutual information estimation of parameters In D Touretzky
editor Advances in Neural Information Processing Systems 2 pages 211217
Morgan-Kaufmann 1990
18 E Brochu V M Cora and N de Freitas A tutorial on Bayesian optimization
of expensive cost functions with application to active user modeling and hierar-
chical reinforcement learning arXiv10122599 csLG Dec 2010
19 P F Brown J Cocke S A D Pietra V J D Pietra F Jelinek J D Lafferty
R L Mercer and P S Roossin A statistical approach to machine translation
Computational linguistics 1627985 1990
20 C Callison-Burch M Osborne and P Koehn Re-evaluation the role of bleu in
machine translation research In EACL volume 6 pages 249256 2006
21 A Carnie Syntax A generative introduction John Wiley  Sons 2013
22 M Cettolo C Girardi and M Federico Wit3 Web inventory of transcribed
and translated talks In Proceedings of the 16th Conference of the European As-
sociation for Machine Translation EAMT pages 261268 Trento Italy May
23 O Chapelle B Scholkopf and A Zien editors Semi-Supervised Learning
MIT Press Cambridge MA 2006
24 S F Chen and J Goodman An empirical study of smoothing techniques for
language modeling In Proceedings of the 34th annual meeting on Association
for Computational Linguistics pages 310318 Association for Computational
Linguistics 1996
25 X Chen and C L Zitnick Learning a recurrent visual representation for image
caption generation arXiv14115654 2014
26 K Cho A Courville and Y Bengio Describing multimedia content using
attention-based encoderdecoder networks 2015
27 K Cho B van Merrienboer D Bahdanau and Y Bengio On the properties
of neural machine translation Encoder-decoder approaches arXiv preprint
arXiv14091259 2014
28 K Cho B Van Merrienboer C Gulcehre D Bahdanau F Bougares
H Schwenk and Y Bengio Learning phrase representations using rnn encoder-
decoder for statistical machine translation arXiv preprint arXiv14061078
29 K Cho B van Merrienboer C Gulcehre F Bougares H Schwenk and Y Ben-
gio Learning phrase representations using RNN encoder-decoder for statistical
machine translation In Proceedings of the Empiricial Methods in Natural Lan-
guage Processing EMNLP 2014 Oct 2014
30 N Chomsky A review of B F skinners verbal behavior Language 35126
58 1959
31 N Chomsky Linguistic contributions to the study of mind future Language
and thinking pages 323364 1968
32 N Chomsky Syntactic structures Walter de Gruyter 2002
33 R Collobert J Weston L Bottou M Karlen K Kavukcuoglu and P Kuksa
Natural language processing almost from scratch The Journal of Machine
Learning Research 1224932537 2011
34 T M Cover Geometrical and statistical properties of systems of linear inequal-
ities with applications in pattern recognition IEEE Transactions on Electronic
Computers EC-143326334 1965
35 J Denker and Y Lecun Transforming neural-net output levels to probability
distributions In Advances in Neural Information Processing Systems 3 Citeseer
36 M Denkowski and A Lavie Meteor universal Language specic translation
evaluation for any target language In Proceedings of the EACL 2014 Workshop
on Statistical Machine Translation 2014
37 J Donahue L A Hendricks S Guadarrama M Rohrbach S Venugopalan
K Saenko and T Darrell Long-term recurrent convolutional networks for vi-
sual recognition and description arXiv14114389 2014
38 D Dong H Wu W He D Yu and H Wang Multi-task learning for multiple
language translation ACL 2015
39 N Durrani B Haddow P Koehn and K Heaeld Edinburghs phrase-based
machine translation systems for WMT-14 In Proceedings of the Ninth Work-
shop on Statistical Machine Translation pages 97104 Association for Com-
putational Linguistics Baltimore MD USA 2014
40 H Fang S Gupta F Iandola R Srivastava L Deng P Dollar J Gao X He
M Mitchell J C Platt C L Zitnick and G Zweig From captions to visual
concepts and back arXiv14114952 2014
41 J R Firth A synopsis of linguistic theory 1930-1955 Oxford Philological
Society 1957
42 R Fletcher Practical Methods of Optimization Wiley-Interscience New York
NY USA 2nd edition 1987
43 M L Forcada and R P Neco Recursive hetero-associative memories for trans-
lation In Biological and Articial Computation From Neuroscience to Tech-
nology pages 453462 Springer 1997
44 D Furcy and S Koenig Limited discrepancy beam search
125131 2005
In IJCAI pages
45 F A Gers J Schmidhuber and F Cummins Learning to forget Continual
prediction with lstm Neural computation 121024512471 2000
46 X Glorot A Bordes and Y Bengio Deep sparse rectier neural networks
In International Conference on Articial Intelligence and Statistics pages 315
323 2011
47 Y Goldberg A primer on neural network models for natural language process-
ing arXiv preprint arXiv151000726 2015
48 I Goodfellow D Warde-farley M Mirza A Courville and Y Bengio Maxout
In Proceedings of the 30th International Conference on Machine
networks
Learning ICML-13 pages 13191327 2013
49 K Greff R K Srivastava J Koutnk B R Steunebrink and J Schmidhuber
Lstm A search space odyssey arXiv preprint arXiv150304069 2015
50 K He X Zhang S Ren and J Sun Delving deep into rectiers Sur-
passing human-level performance on imagenet classication arXiv preprint
arXiv150201852 2015
51 K Heaeld I Pouzyrevsky J H Clark and P Koehn Scalable modied
In Proceedings of the 51st Annual
Kneser-Ney language model estimation
Meeting of the Association for Computational Linguistics pages 690696 Soa
Bulgaria August 2013
52 S Hochreiter Y Bengio P Frasconi and J Schmidhuber Gradient ow in
recurrent nets the difculty of learning long-term dependencies volume 1 A
eld guide to dynamical recurrent neural networks IEEE Press 2001
53 S Hochreiter and J Schmidhuber Long short-term memory Neural computa-
tion 9817351780 1997
54 G-B Huang Q-Y Zhu and C-K Siew Extreme learning machine Theory
and applications Neurocomputing 7013489501 2006
55 S Jean K Cho R Memisevic and Y Bengio On using very large target
vocabulary for neural machine translation In ACL 2015 2014
56 Y Jernite A M Rush and D Sontag A fast variational approach for learn-
ing markov random eld language models 32nd International Conference on
Machine Learning ICML 2015
57 Y Ji T Cohn L Kong C Dyer and J Eisenstein Document context language
models arXiv preprint arXiv151103962 2015
58 R Jozefowicz W Zaremba and I Sutskever An empirical exploration of recur-
rent network architectures In Proceedings of the 32nd International Conference
on Machine Learning ICML-15 pages 23422350 2015
59 A Karpathy and F-F Li Deep visual-semantic alignments for generating image
descriptions arXiv14122306 2014
60 D Kiela and L Bottou Learning image embeddings using convolutional neural
networks for improved multi-modal semantics Proceedings of EMNLP 2014
61 Y Kim Y Jernite D Sontag and A M Rush Character-aware neural language
models arXiv preprint arXiv150806615 2015
62 R Kiros R Salakhutdinov and R Zemel Multimodal neural language models
In ICML2014 2014
63 R Kneser and H Ney Improved backing-off for m-gram language modeling
In Acoustics Speech and Signal Processing 1995 ICASSP-95 1995 Interna-
tional Conference on volume 1 pages 181184 IEEE 1995
64 P Koehn Pharaoh a beam search decoder for phrase-based statistical machine
translation models In Machine translation From real users to research pages
115124 Springer 2004
65 P Koehn Europarl A parallel corpus for statistical machine translation In MT
summit volume 5 pages 7986 Citeseer 2005
66 P Koehn F J Och and D Marcu Statistical phrase-based translation In Pro-
ceedings of the 2003 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Language Technology-Volume 1
pages 4854 Association for Computational Linguistics 2003
67 A Krizhevsky I Sutskever and G E Hinton Imagenet classication with deep
convolutional neural networks In Advances in neural information processing
systems pages 10971105 2012
68 T S Kuhn The structure of scientic revolutions University of Chicago press
69 Q V Le N Jaitly and G E Hinton A simple way to initialize recurrent net-
works of rectied linear units arXiv preprint arXiv150400941 2015
70 Y LeCun Y Bengio and G Hinton Deep learning Nature 5217553436
444 2015
71 Y LeCun L Bottou G Orr and K R Muller Efcient BackProp In G Orr
and K Muller editors Neural Networks Tricks of the Trade volume 1524 of
Lecture Notes in Computer Science pages 550 Springer Verlag 1998
72 O Levy and Y Goldberg Neural word embedding as implicit matrix factor-
ization In Z Ghahramani M Welling C Cortes N Lawrence and K Wein-
berger editors Advances in Neural Information Processing Systems 27 pages
21772185 Curran Associates Inc 2014
73 W Ling I Trancoso C Dyer and A W Black Character-based neural machine
translation arXiv preprint arXiv151104586 2015
74 D G Lowe Object recognition from local scale-invariant features In Computer
vision 1999 The proceedings of the seventh IEEE international conference on
volume 2 pages 11501157 Ieee 1999
75 M-T Luong Q V Le I Sutskever O Vinyals and L Kaiser Multi-task
sequence to sequence learning arXiv preprint arXiv151106114 2015
76 M-T Luong H Pham and C D Manning Effective approaches to attention-
based neural machine translation arXiv preprint arXiv150804025 2015
77 C D Manning and H Schutze Foundations of statistical natural language
processing MIT press 1999
78 J Mao W Xu Y Yang J Wang and A L Yuille Explain images with multi-
modal recurrent neural networks arXiv14101090 2014
79 T Mikolov K Chen G Corrado and J Dean Efcient estimation of word
representations in vector space arXiv preprint arXiv13013781 2013
80 T Mikolov M Karaat L Burget J Cernocky and S Khudanpur Recurrent
neural network based language model In INTERSPEECH 2010 pages 1045
1048 2010
81 V Nair and G E Hinton Rectied linear units improve restricted boltzmann
In Proceedings of the 27th International Conference on Machine
machines
Learning ICML-10 pages 807814 2010
82 K Papineni S Roukos T Ward and W-J Zhu Bleu a method for automatic
evaluation of machine translation In Proceedings of the 40th annual meeting
on association for computational linguistics pages 311318 Association for
Computational Linguistics 2002
83 R Pascanu T Mikolov and Y Bengio On the difculty of training recurrent
neural networks In Proceedings of The 30th International Conference on Ma-
chine Learning pages 13101318 2013
84 A Perfors J Tenenbaum and T Regier Poverty of the stimulus a rational
approach In Annual Conference 2006
85 K B Petersen M S Pedersen et al The matrix cookbook Technical University
of Denmark 715 2008
86 C W Post The Three Percent Problem Rants and Responses on Publishing
Translation and the Future of Reading Open Letter 2011
87 P Resnik and N A Smith The web as a parallel corpus Computational Lin-
guistics 293349380 2003
88 H Robbins and S Monro A stochastic approximation method The Annals of
Mathematical Statistics 223400407 1951
89 F Rosenblatt Principles of neurodynamics perceptrons and the theory of brain
mechanisms Report Cornell Aeronautical Laboratory Spartan Books 1962
90 S Russell and P Norvig Articial intelligence a modern approach 1995
91 J Schmidhuber Deep learning in neural networks An overview Neural Net-
works 6185117 2015
92 H Schwenk Continuous space language models Computer Speech  Lan-
guage 213492518 2007
93 R Sennrich B Haddow and A Birch Neural machine translation of rare words
with subword units arXiv preprint arXiv150807909 2015
94 P Sermanet D Eigen X Zhang M Mathieu R Fergus and Y LeCun Over-
feat Integrated recognition localization and detection using convolutional net-
works arXiv preprint arXiv13126229 2013
95 C Shannon The bandwagon edtl IRE Transactions on Information Theory
123 1956
96 K Simonyan and A Zisserman Very deep convolutional networks for large-
scale image recognition arXiv preprint arXiv14091556 2014
97 B F Skinner Verbal behavior BF Skinner Foundation 2014
98 J R Smith H Saint-Amand M Plamada P Koehn C Callison-Burch and
A Lopez Dirt cheap web-scale parallel text from the common crawl In ACL
1 pages 13741383 2013
99 M Snover B Dorr R Schwartz L Micciulla and J Makhoul A study of trans-
lation edit rate with targeted human annotation In Proceedings of association
for machine translation in the Americas pages 223231 2006
100 M Sundermeyer H Ney and R Schluter From feedforward to recurrent lstm
neural networks for language modeling Audio Speech and Language Process-
ing IEEEACM Transactions on 233517529 2015
101 I Sutskever O Vinyals and Q V Le Sequence to sequence learning with
neural networks In Advances in neural information processing systems pages
31043112 2014
102 C Szegedy W Liu Y Jia P Sermanet S Reed D Anguelov D Erhan V Van-
houcke and A Rabinovich Going deeper with convolutions arXiv preprint
arXiv14094842 2014
103 J Turian L Ratinov and Y Bengio Word representations a simple and general
method for semi-supervised learning In Proceedings of the 48th annual meeting
of the association for computational linguistics pages 384394 Association for
Computational Linguistics 2010
104 L van der Maaten and G E Hinton Visualizing data using t-SNE Journal of
Machine Learning Research 925792605 November 2008
105 V Vapnik The Nature of Statistical Learning Theory Springer-Verlag New
York Inc New York NY USA 1995
106 O Vinyals A Toshev S Bengio and D Erhan Show and tell A neural image
caption generator arXiv preprint arXiv14114555 2014
107 T Wang and K Cho Larger-context language modelling arXiv preprint
arXiv151103729 2015
108 W Weaver Translation Machine translation of languages 141523 1955
109 J Weston S Bengio and N Usunier Large scale image annotation learning to
rank with joint word-image embeddings Machine learning 8112135 2010
110 T Winograd Understanding natural language Cognitive psychology 311
191 1972
111 K Xu J Ba R Kiros K Cho A Courville R Salakhutdinov R Zemel and
Y Bengio Show attend and tell Neural image caption generation with visual
attention In International Conference on Machine Learning 2015
112 Y Zhang K Wu J Gao and P Vines Automatic acquisition of chineseenglish
parallel corpus from the web In Advances in Information Retrieval pages 420
431 Springer 2006
113 R Zhou and E A Hansen Beam-stack search Integrating backtracking with
beam search In ICAPS pages 9098 2005
Natural Language Understanding with
Distributed Representation
Kyunghyun Cho
Courant Institute of Mathematical Sciences and
Center for Data Science
New York University
November 26 2015
Abstract
This is a lecture note for the course DS-GA 3001 cid104Natural Language Understanding
with Distributed Representationcid105 at the Center for Data Science1 New York University
in Fall 2015 As the name of the course suggests this lecture note introduces readers
to a neural network based approach to natural language understandingprocessing In
order to make it as self-contained as possible I spend much time on describing basics of
machine learning and neural networks only after which how they are used for natural
languages is introduced On the language front I almost solely focus on language
modelling and machine translation two of which I personally nd most fascinating
and most fundamental to natural language understanding
After about a month of lectures and about 40 pages of writing this lecture note I
found this fascinating note 47 by Yoav Goldberg on neural network models for natural
language processing This note deals with wider topics on natural language processing
with distributed representations in more details and I highly recommend you to read it
hopefully along with this lecture note I seriously wish Yoav had written it earlier so
that I couldve simply used his excellent note for my course
This lecture note had been written quite hastily as the course progressed meaning
that I could spare only about 100 hours in total for this note This is my lame excuse
for likely many mistakes in this lecture note and I kindly ask for your understanding
in advance Again how grateful I wouldve been had I found Yoavs note earlier
I am planning to update this lecture note gradually over time hoping that I will
be able to convince the Center for Data Science to let me teach the same course next
year The latest version will always be available both in pdf and in latex source code
from httpsgithubcomnyu-dlNLPDLLectureNote The arXiv
version will be updated whenever a major revision is made
I thank all the students and non-students who took2 this course and David Rosen-
berg for feedback
1 httpcdsnyuedu
2 In fact they are still taking the course as of 24 Nov 2015 They have two guest lectures and a nal exam
left until the end of the course
Contents
Introduction
11 Route we will not take
     
            
111 What is Language 
                     
112 Language Understanding                    
               
121 Language as a Function                    
122 Language Understanding as a Function Approximation    
     
12 Road we will take 
2 Function Approximation as Supervised Learning
21 Function Approximation Parametric Approach            
211 Expected Cost Function                    
212 Empirical Cost Function                    
22 Learning as Optimization                        
221 Gradient-based Local Iterative Optimization          
                 
23 When do we stop learning                       
231 Early Stopping                         
                     
232 Model Selection  
24 Evaluation 
                        
25 Linear Regression for Non-Linear Functions              
Feature Extraction                       
Stochastic Gradient Descent
3 Neural Networks and Backpropagation Algorithm
31 Conditional Distribution Approximation                
311 Why do we want to do this                  
312 Other Distributions                       
32 Feature Extraction is also a Function                  
33 Multilayer Perceptron                          
331 Example Binary classication with a single hidden unit
332 Example Binary classication with more than one hidden units 29
34 Automating Backpropagation                      
341 What if a Function is not Differentiable
    
4 Recurrent Neural Networks and Gated Recurrent Units
   
      
Fixed-Size Output y
42 Gated Recurrent Units
41 Recurrent Neural Networks                       
       
412 Multiple Child Nodes and Derivatives             
413 Example Sentiment Analysis
                
414 Variable-Length Output y x  y
   
                       
421 Making Simple Recurrent Neural Networks Realistic    
422 Gated Recurrent Units                     
423 Long Short-Term Memory                   
                         
                     
431 Rectiers Explode 
                     
Is tanh a Blessing 
433 Are We Doomed  
                     
434 Gated Recurrent Units Address Vanishing Gradient      
43 Why not Rectiers 
    
    
5 Neural Language Models
51 Language Modeling First Step                     
511 What if those linguistic structures do exist           
512 Quick Note on Linguistic Units
               
52 Statistical Language Modeling                     
521 Data SparsityScarcity                     
n-Gram Language Model  
                     
Smoothing and Back-Off                    
532 Lack of Generalization                     
                       
541 How does Neural Language Model Generalize to Unseen n-
Grams  Distributional Hypothesis              
54 Neural Language Model
542 Continuous Bag-of-Words Language Model
Maximum PseudoLikelihood Approach           
Semi-Supervised Learning with Pretrained Word Embeddings
55 Recurrent Language Model                       
56 How do n-gram language model neural language model and RNN-LM
                      
compare 
6 Neural Machine Translation
61 Statistical Approach to Machine Translation              
Parallel Corpora Training Data for Machine Translation   
612 Automatic Evaluation Metric                  
62 Neural Machine Translation
Simple Encoder-Decoder Model
                   
Sampling vs Decoding                     
63 Attention-based Neural Machine Translation              
          
64 Warren Weavers Memorandum                    
631 What does the Attention Mechanism do
7 Final Words
71 Multimedia Description Generation as Translation           
72 Language Understanding with World Knowledge           
73 Larger-Context Language Understanding
Beyond Sentences and Beyond Words                 
74 Warning and Summary                         
Chapter 1
Introduction
This lecture is going to be the only one where I discuss some philosophical meaning
nonpractical arguments because according to Chris Manning and Hinrich Schuetze
even practically-minded people have to confront the issue of what prior knowledge to
try to build into their model 77
11 Route we will not take
111 What is Language
The very rst question we must ask ourselves before starting this course is the ques-
tion of what natural language is Of course the rest of this course does not in any
way require us to know what natural language is but it is a philosophical question I
recommend everyone including myself to ponder upon once a while
When I start talking about languages with anyone there is a single person who
never misses to be mentioned that is Noam Chomsky His view has greatly inuenced
the modern linguistics and although many linguists I have talked to claim that their
work and eld have long moved on from Chomskys I can feel his shadow all over
My rst encounter with Chomsky was at the classroom of Automata from my
early undergrad years I was not the most attentive student back then and all I can
remember is Chomskys hierarchy and how it has shaped our view on languages in this
context programmingcomputer languages A large part of the course was dedicated
to explaining which class of languages emerges given a set of constraints on a set of
generating rules or production rules
For instance if we are given a set of generating rules that do not depend on the con-
textmeaning of non-terminal symbols context-free grammar CFG we get a context-
free language If we put a bit of constraints to CFG that each generating rule is such
that a non-terminal symbol is replaced by either a terminal symbol a terminal symbol
by a non-terminal symbol or an empty symbol then we get a regular grammar Sim-
ilarly to CFG we get a regular language from the regular grammar and the regular
language is a subset of the context-free language
What Chomsky believes is that this kind of approach applies also to human lan-
guages or natural languages There exists a set of generating rules that generates a
natural language But then the obvious question to follow is where those generating
rules are Where are they stored How are they stored Do we have separate generating
rules for different languages
112 Language Understanding
Understanding Human Language Those questions are interesting but out of scope
for this course Those questions are the ones linguists try to answer Generative linguis-
tics aims at guring out what those rules are how they are combined to form a valid
sentence how they are adapted to different languages and so on We will leave these to
linguists and continue on to our journey of building a machine that understands human
languages
Natural Language Understanding So lets put these questions aside and trust Chom-
sky that we humans are specially designed to store those generating rules somewhere
in the brain 30 21 Or better yet lets trust Chomsky that theres a universal gram-
mar built in our brain In other words lets say we were born with this set of generating
rules for natural languages and while growing we have adapted this universal gram-
mar toward our native tongue language variation
When we decide to speak of something whatever that is and however implausi-
ble that is our brain quickly picks up a sequence of some of those generating rules
and starts generating a sentence accordingly Of course those rules do not generate a
sentence directly but generates a sequence of control signals to move our muscles to
make sound When heard by other people who understand your language the sound
becomes a sentence
In our case we are more interested in a machine hearing that sound or a sentence
from here on When a machine heard this sentence what wouldshould a language un-
derstanding machine do to understand a language or more simply a sentence Again
we are assuming that this sentence was generated from applying a sequence of the
existing generating rules
Under our assumption a natural rst step that comes to my mind is to gure out
that sequence of the generating rules which led to the sentence Once the sequence is
found or in a fancier term inferred the next step will be to gure out what kind of
mental state of the speaker led to those generating rules
Lets take an example sentence Our company is training workers from Sec 13
of 77 which is a horrible choice because this was used as an example of ambiguity
in parsing Regardless a speaker obviously has an awesome image of her company
which trains its workers and wants to tell a machine about this This mental state is
used to select the following generating rules assuming a phrase structure grammar1
1 Stanford Parser httpnlpstanfordedu8080parser
NP PRP Our NN company
VP VBZ is
VP VBG training
NP NNS workers
Figure 11 A parse of Our company is training workers
The machine hears the sentence Our company is training workers and infers
the parse in Fig 11 Then we can make a simple set of rules again
to let the
machine answer questions about this sentence kinds of questions that imply that the
machine has understood the sentence language For instance given a question Who
is training workers the machine can answer by noticing that the question is asking
for the subject of the verb phrase is training acted on the object workers and that
the subject is Our company
Side Note Bayesian Language Understanding This generative view of languages
ts quite well with Bayesian modelling see eg 84 There exists a hidden mecha-
nism or a set of generating rules and a rule governing their composition which can be
modelled as a latent variable Z Given these rules a language or a sentence X is gen-
erated according to the conditional distribution PXZ Then understanding language
by humans is equivalent to computing the posterior distribution over all possible sets
of generating rules and their compositional rules ie PZX This answers the ques-
tion of what is the most likely mechanism underlying the observed language
Furthermore from the perspective of machines Bayesian approach is attractive In
this case we assume to know the set of rules in advance and let the latent variable Z
denote the specic conguration use of those rules Given this sequence of applying
the rules a sentence X is generated via the conditional distribution PXZ Machine
understanding of language is equivalent to inferring the posterior distribution over Z
given X
For more details about Bayesian approaches in the context of machine learning
please refer to 13 or take the course DS-GA 1005 Inference and Representation by
Prof David Sontag
pa13TheAmbiguityofLanguageWhyNLPIsDicult17PhilosophicallythisbringsusclosetothepositionadoptedinthelaterwritingsofWittgensteinthatisWittgenstein1968wherethemean-ingofawordisdenedbythecircumstancesofitsuseausetheoryofusetheoryofmeaningmeaningseethequotationsatthebeginningofthechapterUnderthisconceptionmuchofStatisticalNLPresearchdirectlytacklesquestionsofmeaning13TheAmbiguityofLanguageWhyNLPIsDicultAnNLPsystemneedstodeterminesomethingofthestructureoftextnormallyatleastenoughthatitcananswerWhodidwhattowhomConventionalparsingsystemstrytoanswerthisquestiononlyintermsofpossiblestructuresthatcouldbedeemedgrammaticalforsomechoiceofwordsofacertaincategoryForexamplegivenareasonablegrammarastandardNLPsystemwillsaythatsentence110has3syntacticanal-ysesoftencalledparses110OurcompanyistrainingworkersThethreedieringparsesmightberepresentedasin111111aSNPOurcompanyVPAuxisVPVtrainingNPworkersbSNPOurcompanyVPVisNPVPVtrainingNPworkersUnderstanding vs Using Whats clear from this example is that in this generative
view of languages there is a clear separation between understanding and using In-
ferring the generating rules from a given sentence is understanding and answering a
question based on this understanding using is a separate activity Understanding part
is done when the underlying true structure has been determined regardless of how
this understanding be used
To put it in a slightly different wording language understanding does not require its
use or downstream tasks In this road that we will not take in this course understanding
exists as it is regardless of what the understood insightknowledge will be used for
And this is the reason why we do not walk down this road
12 Road we will take
121 Language as a Function
In this course we will view a naturalhuman language as a system intended to com-
municate ideas from a speaker to a hearer 110 What this means is that we do not
view a language as a separate entity that exists on its own Rather we view a whole
system or behaviour of communication as a language Furthermore this view dictates
that we must take into account the world surrounding a speaker and a hearer in order
to understand language
Under this view of language language or rather its usage become somewhat similar
to action or behaviour Speaking of something is equivalent to acting on a listener as
both of them inuence the listener in one way or another The purpose of language
is then to inuence another by efciently communicate ones will or intention2 This
hints at how language came to be or may have come to be evolution language
has evolved to facilitate the exchange of ideas among people learning humans learn
language by being either encouraged or punished for the use of language This latter
view on how language came to be is similar in spirit to the behaviourism of B F
Skinner necessary mediation of reinforcement by another organism 97
This is a radical departure from the generative view of human language where
language existed on its own and its understanding does not necessarily require the
existence of the outside world nor the existence of a listener
It is no wonder why
Chomsky was so harsh in criticizing Skinners work in 30 This departure as I see
it is the departure toward a functional view of language Language is a function of
communication
122 Language Understanding as a Function Approximation
Lets make a large jump here such that we consider this function as a mathematical
function This function called language takes as input the state of the surrounding
world the speakers speech either written spoken or signed and the listeners mental
2 Chomsky does not agree it is wrong to think of human use of language as characteristically informa-
tive in fact or in intention 31
state3 Inside the function the listeners mental state is updated to incorporate the new
idea from the speakers speech The function then returns a response by the listener
which may include no response as well and a set of non-verbal action sequences
what would be the action sequence if the speaker insulted the listener
In this case language understanding both from humans and machines perspec-
tive boils down to guring out the internal working of this function In other words we
understand language by learning the internal mechanism of the function Furthermore
this view suggests that the underlying structures of language are heavily dependent on
the surrounding environment context as well as on the target task The former con-
text dependence is quite clear as the function takes as input the context but the latter
may be confusing now Hopefully this will become clearer later in the course
How can we approximate this function How can we gure out the internal working
mechanism of this function What tools do we have
Language Understanding by Machine Learning This functional view of languages
suddenly makes machine learning a very appealing tool for understanding human lan-
guages After all function approximation is the core of machine learning Classica-
tion is a classical example of function approximation clustering is a function approxi-
mation where the target is not given generative modeling learns a function that returns
a probability of an input and so on
When we approximate a function in machine learning the prime ingredient is data
We are given data which was either generated from this function unsupervised learn-
ing or well t this function supervised learning based on which we adjust our ap-
proximation to the function often iteratively to best t the data But I must note here
that it does not matter how well the approximated function ts the data it was tted to
but matters how well this approximation ts unseen data4
In language understanding this means that we collect a large data set of input and
output pairs or conversations together with the recording of the surrounding environ-
ment and t some arbitrary function to well predict the output given an input We
probably want to evaluate this approximation in a novel conversation If this function
makes a conversation just like a person voila we made a machine that passed the
Turing test Simple right
Problem Unfortunately as soon as we try to do this we run into a big problem This
problem is not from machine learning nor languages but the denition of this function
of language
Properly approximating this function requires us to either simulate or record the
whole world in fact the whole universe For this function takes as input and main-
tains as internal state the surrounding world context and the mental state of the in-
dividual speaker This is unavoidable if we wanted to very well approximate this
function as a whole
It is unclear however whether we want to approximate the full function For a
human to survive yes it is likely that the full function is needed But if our goal is
3 We assume here that a such thing exists however it is represented in our brain
4 This is a matter of generalization and we will talk about this more throughout the course
restricted to a certain task such as translation language modelling and so on we may
not want to approximate this function fully We probably want to approximate only a
subset of this whole function For instance if our goal is to understand the process
of translation from one language to another we can perhaps ignore all but the speech
input to the function and all but the speech output from the function because often a
trained person can translate a sentence in one language to another without knowing
the whole context
This latter approach to language understandingapproximating a partial function
of languages will be at the core of this course We will talk about various language
tasks that are a part of this whole function of language These tasks will include but
are not limited to language modelling machine translation imagevideo description
generation and question answering For these tasks and potentially more we will study
how to use machine learning or more specically deep learning to solve these tasks
by approximating sub-functions of language
Chapter 2
Function Approximation as
Supervised Learning
Throughout this course we will extensively use articial neural networks1 to approx-
imate a part of the function of natural language This makes it necessary for us to
study the basics of neural networks rst and this lecture and a couple of subsequent
ones are designed to serve this purpose
21 Function Approximation Parametric Approach
211 Expected Cost Function
Let us start by dening a data distribution pdata pdata is dened over a pair of input
and output vectors x  Id and y  Ok respectively I and O are respectively sets of
all possible input and output values such as R 01 and 01    L This data
distribution is not known to us
The goal is to nd a relationship between x and y More specically we are in-
terested in nding a function f  Rd  Ok that generates the output y given its corre-
sponding input x The very rst thing we should do is to put some constraints on the
function f to make our search for the correct f a bit less impossible In this lecture
and throughout the course I will consider only a parametric function f  in which case
the function is fully specied with a set of parameters 
Next we must dene a way to measure how well the function f approximates
the underlying mechanism of generation x  y Lets denote by y the output of the
function with a particular set  of parameters and a given input x
1 From here on I will simply drop articial and call them neural networks Whenever I say neural
network it refers to articial neural networks
y  f x
How well f approximates the true generating function is equivalent to how far y is from
the correct output y Lets use Dyy for now call this distance2 between y and y
It is clear that we want to nd  that minimizes Dyy for every pair in the space
RRd Ok But wait every pair equally likely Probably not for we do not care how
well f approximates the true function when a pair of input x and output y is unlikely
meaning we do not care how bad the approximation is if pdataxy is small However
this is a bit difcult to take into account as we must decided on the threshold below
which we consider any pair irrelevant
Hence we weight the distance between the approximated y and the correct y of
each pair xy in the space by its probability pxy Mathematically saying we want
to nd
where the integral cid82 should be replaced with the summation  if any of x and y is
pdataxyDyydxdy
cid90
cid90
discrete
We call this quantity being minimized with respect to the parameters  a cost func-
tion C  This is equivalent to computing the expected distance between the predicted
output y and the correct one y
C  
pdataxyDyydxdy
cid90
cid90
Exypdata Dyy
This is often called an expected loss or risk and minimizing this cost function is re-
ferred to as expected risk minimization 105
Unfortunately C  cannot be exactly computed for a number of reasons The
most important reason among them is simply that we dont know what the data distri-
bution pdata is Even if we have access to pdata we can exactly compute C  only with
heavy assumptions on both the data distribution and the distance function3
212 Empirical Cost Function
This does not mean that we are doomed from the beginning Instead of the full-blown
description of the data distribution pdata we will assume that someone miraculously
gave us a nite set of pairs drawn from the data distribution We will call this a training
cid8x1y1     xNyNcid9 
As we have access to the samples from the data distribution we can use Monte
Carlo method to approximate the expected cost function C  such that
C   C  
D ynyn
2 Note that we do not require this distance to satisfy the triangular inequality meaning that it does not
have to be a distance However I will just call it distance for now
We call this approximate C  of the expected cost function an empirical cost function
or empirical risk or empirical loss
Because empirical cost function is readily computable we will mainly work with
the empirical cost function not with the expected cost function However keep in mind
that at the end of the day the goal is to nd a set of parameters that minimizes the
expected cost
22 Learning as Optimization
We often call this process of nding a good set of parameters that minimizes the ex-
pected cost learning This term is used from the perspective of a machine which imple-
ments the function f  as it learns to approximate the true generating function f from
training data
From what I have described so far it may have become clear even without me men-
tioning that learning is optimization We have a clearly dened function the empirical
cost function C which needs to be minimized with respect to its input 
221 Gradient-based Local Iterative Optimization
There are many optimization algorithms one can use to nd a set of parameters that
minimizes C Sometimes you can even nd the optimal set of parameters in a closed
form equation4 In most cases because there is no known closed-form solution it is
typical to use an iterative optimization algorithm see 42 for in-depth discussion on
optimization
By an iterative optimization I mean an algorithm which renes its estimate of the
optimal set of parameters little by little until the values of the parameters converge to
the optimal expected cost function Also it is worthwhile to note that most iterative
optimization algorithms are local in the sense that they do not require us to evaluate
the whole parameter space but only a small subset along the path from the starting
point to the convergence point5
Here I will describe the simplest one among those local iterative optimization algo-
rithms called gradient descent GD algorithm As the name suggests this algorithm
depends entirely on the gradient of the cost function6
4 One such example is a linear regression where
 f Wx  Wx
 Dyy  1
2cid107y ycid1072
In this case the optimal W is
W  YXcid62XXcid621
X cid2x1   xNcid3 Y cid2y1   yNcid3 
Try it yourself
5 There are global optimization algorithms but they are out of scope for this course See for instance
18 for one such algorithm called Bayesian optimization
6 From here on I will use the cost function to refer to the empirical cost function
blue
Figure 21
sin10x  x
x  06
gradient at x  06
f x 
red a gradient at
magenta a negative
The gradient of a function  C is a vector whose direction points to the direction of
the greatest rate of increase in the functions value and whose magnitude measures this
rate At each point  t in the parameter space the gradient of the cost function  C t 
is the opposite direction toward which we want to move the parameters See Fig 21
for graphical illustration
One important point of GD that needs to be mentioned here is on how large a
step one takes each time As clear from the magenta line the direction opposite to
the direction given by the gradient in Fig 21 if too large a step is taken toward the
negative gradient direction the optimization process will overshoot and miss the local
minimum around x  08 This step size or sometimes called learning rate  is one
most important hyperparameter of the GD algorithm
Now we have all the ingredients for the GD algorithm  C and  The GD algo-
rithm iterates the following step
     C 
The iteration continues until a certain stopping criterion is met which we will discuss
shortly
222 Stochastic Gradient Descent
This simple GD algorithm works surprisingly quite well and it is a fundamental basis
upon which many advanced optimization algorithms have been built I will present a
list of few of those advanced algorithms later on and discuss them briey But before
going into those advanced algorithms lets solve one tiny but signicant issue of the
GD algorithm
This tiny but signicant issue arises especially often in machine learning That is
it is computationally very expensive to compute C and consequently its gradient  C
thanks to the ever increasing size of the training set D
Why is the growing size of the training set making it more and more computation-
ally demanding to compute C and  C This is because both of them are essentially
the sum of as many per-sample costs as there are examples in the training set In other
Cxnyn 
C  
 C  
 Cxnyn 
And N goes up to millions or billions very easily these days
This enormous computational cost involved in each GD step has motivated the
stochastic gradient descent SGD algorithm 88 15
First recall from Eq 23 that the cost function we minimize is the empirical
cost function C which is the sample-based approximation to the expected cost function
C This approximation was done by assuming that the training examples were drawn
randomly from the data distribution pdata
C   C  
D ynyn
In fact as long as this assumption on the training set holds we can always approximate
the expected cost function with a fewer number of training examples
C   CM   
M 
D ymym
where M cid28 N and M is the indices of the examples in this much smaller subset of the
training set We call this small subset a minibatch
Similarly this leads to a minibatch-based estimate of the gradient as well
 CM   
M 
D ymym
It must now be clear to you where I am headed toward At each GD step instead
of using the full training set we will use a small subset M which is randomly selected
to compute the gradient estimate In other words we use CM instead of C and  CM
instead of  C in Eq 25
Because computing CM and  CM is independent of the size of the training set we
can use SGD to make as many steps as we want without worrying about the growing
size of training examples This is highly benecial as regardless of how many train-
ing examples you used to compute the gradient we can only take a tiny step toward
that descending direction Furthermore the increased level of noisy in the gradient
estimate due to the small sample size has been suspected to help reaching a better so-
lution in high-dimensional non-convex problems such as those in training deep neural
networks 717
7 Why would this be the case It is worth thinking about this issue further
We can set M to be any constant and in an extreme we can set it to 1 as well In
this case we call it online SGD8 Surprisingly already in 1951 it was shown that using
a single example each time is enough for the SGD to converge to a minimum under
certain conditions obviously 88
This SGD algorithm will be at the core of this course and will be discussed further
in the future lectures
23 When do we stop learning
From here on I assume that we approximate the ground truth function by iteratively
rening its set of parameters in most cases using stochastic gradient descent In other
words learning of a machine that approximates the true generating function f happens
gradually as the machine goes over the training examples little by little over time
Let us go over again what kind of constraintsissue we have rst
1 Lack of access to the expected cost function C 
2 Computationally expensive empirical cost function C 
3 Potential non-convexity of the empirical cost function C 
The most severe issue is that we do not have access to the expected cost function
which is the one we want to minimize in order to work well with any pair of input x
and output y Instead we have access to the empirical cost function which is a nite
sample approximation to the expected cost function
Why is this a problem Because we do not have a guarantee that the local mini-
mum of the empirical cost function corresponds to the local minimum of the expected
cost function An example of this mismatch between the expected and empirical cost
functions is shown in Fig 22
As in the case shown in Fig 22 it is not desirable to minimize the empirical cost
function perfectly The parameters that perfectly minimize the empirical cost function
in the case of Fig 22 the slope a of a linear function f x  ax will likely be a
sub-optimal cost for the expected cost function about which we really care
231 Early Stopping
What should we do There are many ways to avoid this weird contradiction where
we want to optimize the cost function well but not too well Among those one most
important trick is early stopping which is only applicable when iterative optimization
is used
First we will split the training set D into two partitions Dtrain and Dval9 We call
them a training set and a validation set respectively In practice it is a good idea to
keep D much larger than Dcid48 because of the reasons that will become clear shortly
8 Okay this is not true in a strict sense SGD is an online algorithm with M  1 originally and using
M  1 is a variant of SGD often called minibatch SGD However as using minibatches M  1 is almost
always the case in practice I will refer to minibatch SGD as SGD and to the original SGD as online SGD
9 Later on we will split it further into three partitions
Figure 22 blue Expected cost
function C 
red Empirical
cost function C 
The un-
derlying true generating function
was f x  sin10x  x The
cost function uses the squared Eu-
clidean distance
The empiri-
cal cost function was computed
based on 10 noisy examples of
which xs were sampled from the
uniform distribution between 0
and 1 For each sample input x
noise from zero-mean Gaussian
distribution with standard devia-
tion 001 was added to f x to
emulate the noisy measurement
channel
Further let us dene the training cost as
C   Ctrain  
Dtrain 
xyDtrain
Dtrain yy
and the validation cost as
Cval  
Dval 
xyDval
D yy
With these two cost functions we are all ready to use early stopping now
After every few updates using SGD or GD the validation cost function is evalu-
ated with the current set of parameters The parameters are updated ie the training
cost function is optimized until the validation cost does not decrease or starts to in-
crease instead of decreasing
Thats it It is almost free as long as the size of the validation set is reasonable
since each evaluation is at most as expensive as computing the gradient of the empirical
cost function Because of the simplicity and effectiveness this early stopping strategy
has become de facto standard in deep learning and in general machine learning
The question that needs to be asked here is what the validation cost function does
here Clearly it approximates the expected cost function C similarly to the empirical
cost function C as well as the training cost function Ctrain In the innite limit of the
size of either training or validation set they should coincide but in the case of a nite
set those two cost functions differ by the noise in sampling sampling pairs from the
data distribution and observation noise in y  f x
The fact that we explicitly optimize the training cost function implies that there is
a possibility in fact almost surely in practice that the set of parameters found by this
optimization process may capture not only the underlying generating function but also
noise in the observation and sampling procedure This is an issue because we want our
machine to approximate the true generating function not the noise process involved
The validation cost function measures both the true generating structure as well as
noise injected during sampling and observation However assuming that noise is not
correlated with the underlying generating function noise introduced in the validation
cost function differs from that in the training cost function In other words the set
of parameters that perfectly minimizes the training cost function thereby capturing
even noise in the training set will be penalized when measured by the validation cost
function
232 Model Selection
In fact the use of the validation cost does not stop at the early stopping Rather it has a
more general role in model selection First we must talk about model selection itself
This whole procedure of optimization or learning can be cast as a process of
searching for the best hypothesis over the entire space H of hypotheses Here each
hypothesis corresponds to each possible function with a unique set of parameters and
a unique functional form that takes the input x and output y In the case of regression
x  Rd and y  R the hypothesis space includes an n-th order polynomial function
f x 
k1 iknik0
ai1i2ik
kcid481
kcid48
where ai1i2iks are the coefcients and any other functional form that you can imag-
ine as long as it can process x and return a real-valued scalar In the case of neural
networks this space includes all the possible model architectures which are dened by
the number of layers the type of nonlinearities the number of hidden units in each
layer and so on
Let us use M  H to denote one hypothesis10 One important thing to remember is
that the parameter space is only a subset of the hypothesis space because the parameter
space is dened by a family of hypotheses the parameter space of a linear function
cannot include a set of parameters for a second-order polynomial function
Given a denition of expected cost function we can score each hypothesis M by
the corresponding cost CM Then the whole goal of function approximation boils down
to the search for a hypothesis M with the minimal expected cost function C But of
course we do not have access to the expected cost function and resort to the empirical
cost function based on a given training set
The optimization-based approach we discussed so far searches for the best hypoth-
esis based on the empirical cost iteratively However because of the issue of overtting
which means that the optimization algorithm overshot and missed the local minimum
of the expected cost function because it was aimed at the local minimum of the empir-
ical cost function I introduced the concept of early stopping based on the validation
10 M because each hypothesis corresponds to one learning machine
This is unfortunately not satisfactory as we have only searched for the best hypoth-
esis inside a small subset of the whole hypothesis space H  What if another subset
of the hypothesis space includes a function that better suits the underlying generating
function f  Are we doomed
It is clearly better to try more than one subsets of the hypothesis space For in-
stance for a regression task we can try linear functions H1 quadratic second-order
polynomial functions H2 and sinusoidal functions H3 Lets say for each of these
subsets we found the best hypothesis using iterative optimization and early stopping
MH1 MH2 and MH3 Then the question is how we should choose one of those hy-
potheses
Similar to what weve done with early stopping we can use the validation cost to
compare these hypotheses Among those three we choose one that has the smallest
validation cost CvalM
This is one way to do model selection and we will talk about another way to do
this later
24 Evaluation
But wait if this is an argument for using the validation cost to early stop the optimiza-
tion or learning one needs to notice something weird What is it
Because we used the validation cost to stop the optimization there is a chance
that the set of parameters we found is optimal for the validation set whose structure
consists of both the true generating function and samplingobservation noise but not
to the general data distribution This means that we cannot tell whether the function
estimate f approximating the true generating function f is a good t by simply early
stopping based on the validation cost Once the optimization is done we need yet
another metric to see how well the learned function estimate f approximates f 
Therefore we need to split the training set not into two partitions but into three
partitions We call them a training set Dtrain a validation set Dval and a test set Dtest
Consequently we will have three cost functions a training cost function Ctrain a vali-
dation cost function Cval and a test cost function Ctest similarly to Eqs 2627
This test cost function is the one we use to compare different hypotheses or models
fairly Any hypothesis that worked best in terms of the test cost is the one that you
choose
Lets not Cheat One most important lesson here is that you must never look at a test
set As soon as you take a peak at the test set it will inuence your choice in the model
structure as well as any other hyperparameters biasing toward a better test cost The
best option is to never ever look at the test set until it is absolutely needed eg need
to present your result
25 Linear Regression for Non-Linear Functions
Let us start with a simple linear function to approximate a true generating function such
y  f x  Wcid62x
where W  Rdl is the weight matrix
parameter ie   W
The empirical cost function is then
C  
The gradient of the empirical cost function is
 C    1
In this case this weight matrix is the only
cid13cid13cid13yn  Wcid62xncid13cid13cid132
cid16
yn  Wcid62xncid17cid62
With these two well dened we can use the iterative optimization algorithm such
as GD or SGD to nd the best W that minimizes the empirical cost function11 Or
better is to use a validation set to stop the optimization algorithm at the point of the
minimal validation cost function remember early stopping
Now but we are not too satised with a linear network are we
251 Feature Extraction
Why are we not satised
First we are not sure whether the true generating function f was a linear function
If it is not can we expect linear regression to approximate the true function well Of
course not We will talk about this shortly
Second because we were given x meaning we did not have much control over what
we want to measure as x it is unclear how well x represents the input For instance
consider doing a sales forecast of air conditioner at one store which opened ve years
ago The input x is the number of days since the opening date of the store 1 Jan 2009
and the output y is the number of units sold on each day
Clearly in this example the relationship between x and y is not linear Furthermore
perhaps the most important feature for predicting the sales of air conditioners is missing
from the input x which is a month or a season if you prefer It is likely that the
sales bottoms out during the winter perhaps sometime around December January and
February and it hits the peak during summer months around May June and July
In other words if we look at how far the month is away from July we can predict the
sales quite well even with linear regression
11 In fact looking at Eq 28 its quite clear that you can compute the optimal W analytically See
Eq 24
Let us call this quantity  x or equivalent feature such that
 x  mx  
where mx  12    12 is the month of x and   55 With this feature we can t
linear regression to better approximate the sales gure of air conditioners Furthermore
we can add yet another feature to improve the predictive performance For instance
one such feature can be which day of week x is
This whole process of extracting a good set of features that will make our choice
of parametric function family such as linear regression in this case is called feature
extraction This feature extraction is an important step in machine learning and has
often been at the core of many applications such as computer vision the representative
example is SIFT 74
Feature extraction often requires heavy knowledge of the domain in which this
function approximation is applied To use linear regression for computer vision it is
a good idea to use computer vision knowledge to extract a good set of features If we
want to use it for environmental problems we must rst notice which features must be
important and how they should be represented for linear regression to work
This is okay for a machine learning practitioner in a particular eld because the
person has in-depth knowledge about the eld There are however many cases where
theres simply not enough domain knowledge to exploit To make the matter worse it
is likely that the domain knowledge is not correct making the whole business of using
manually extracted features futile
Chapter 3
Neural Networks and
Backpropagation Algorithm
31 Conditional Distribution Approximation
I have mainly described so far as if the function we approximate or the function we
use to approximate returns only a constant value as in one point y in the output space
This is however not true and in fact the function can return anything including a
distribution 17 35 12
Lets rst decompose the data distribution pdata into the product of two terms
pdataxy  pdataxpdatayx
It becomes clear that one way to sample from pdata is to sample an input xn from
pdatax and subsequently sample the corresponding output yn from the conditional
distribution pdatayxn
This implies that the function approximation of the generating function  f  x  y
is effectively equivalent to approximating the conditional distribution pdatayx This
may suddenly sound much more complicated but it should not alarm you at all As
long as we choose to use a distribution parametrized by a small number of param-
eters to approximate the conditional distribution pdatayx this is quite manageable
without almost any modication to the expected and empirical cost functions we have
discussed
approximating the true underlying probability distribution pdatayx As the notation
suggests the function now returns the parameters of the distribution  x given the
input x
For example lets say y  01k is a binary vector and we chose to use inde-
pendent Bernoulli distribution to approximate the conditional distribution pdatayx
In this case the parameters that dene the conditional distribution are the means of k
Let us use  x to denote a set of parameters for the probability distribution pyx x
dimensions
pyx 
kcid481
pykcid48x 
kcid481
kcid48 1 kcid481ykcid48 
ykcid48
Then the function  x should output a k-dimensional vector of which each element is
between 0 and 1
Another example lets say y  Rk is a real-valued vector It is quite natural to use a
Gaussian distribution with a diagonal covariance matrix to approximate the conditional
distribution pyx
cid32
cid33
pyx 
kcid481
2kcid48
ykcid48  kcid482
kcid48
The parameters for this conditional distribution are  x 1 2     k12    k
where k  R and k  R0
In this case of probability approximation it is natural to use Kullback-Leibler KL
divergence to measure the distance1 The KL divergence from one distribution P to the
other Q is dened2 by
KLPcid107Q 
Pxlog
In our case of functiondistribution approximation we want to minimize the KL di-
vergence from the data distribution pdatayx to the approximate distribution pyx
averaged over the data distribution pdatax
cid90
cid90
cid90
C  
pdataxKLpdatacid107 pdx 
pdatax
cid90
pdatayxlog
pdatayx
pyx
But again we do not have access to pdata and cannot compute this expected cost func-
Similarly to how we dened the empirical cost function earlier we must approxi-
mate this expected KL divergence using the training set
C  
log pynxn
As an example if we choose to return the binary vector y as in Eq 31 the empirical
cost function will be
C    1
ykcid48 log kcid48  1 ykcid48log1 kcid48
kcid481
1 Again we use a loose denition of the distance where triangular inequality is not enforced
2 Why dont I say the KL divergence between two distributions here Because the KL divergence is not
a symmetric measure ie KLPcid107Q cid54 KLQcid107P
which is often called a cross entropy cost In the case of Eq 32
C    1
kcid481
ykcid48  kcid482
kcid48
 logkcid48
Do you see something interesting in Eq 34 If we assume that the function
outputs 1 for all kcid48s we see that this cost function reduces to that using the Euclidean
distance between the true output y and the mean  What does this mean
There will be many occasions later on to discuss more about this perspective when
we discuss language modelling However one thing we must keep in our mind is that
there is nothing different between approximating a function and a distribution
311 Why do we want to do this
Before we move on to the main topic of todays lecture lets try to understand why
we want to output the distribution Unlike returning a single point in the space the
distribution returned by the function f incorporates both the most likely outcome y as
well as the uncertainty associated with this value
In the case of the Gaussian output in Eq 32 the standard deviation kcid48 or the
variance  2
kcid48 indicates how uncertain the function is about the output centered at kcid48
Similarly the mean kcid48 of the Bernoulli output in Eq 31 is directly proportional to
the functions condence in predicting that the kcid48-th dimension of the output is 1
Figure 31 Is this a duck or a rab-
bit 68 At the end of the day
we want our function f to return
a conditional distribution saying
that pduckx  prabbitx in-
stead of returning the answer out
of these two possible answers
This is useful in many aspects but one important aspect is that it reects the natural
uncertainty of the underlying generating function One input x may be interpreted in
more than one ways leading to two possible outputs which happens more often than
not in the real world For instance the famous picture in Fig 31 can be viewed as a
picture of a duck or a picture of a rabbit in which case the function needs to output the
probability distribution by which the same probability mass is assigned to both a duck
and a rabbit Furthermore there is observational noise that cannot easily be identied
and ignored by the function in which case the function should return the uncertainty
due to the observational noise along with the most likely or the average prediction
312 Other Distributions
I have described two distributions densities that are widely used
 Bernoulli distribution binary classication
 Gaussian distribution real value regression
Here let me present one more distribution which we will use almost everyday through
this course
Categorical Distribution Multi-Class Classication Multi-class classication is a
task in which each example belongs to one of K classes For each input x the problem
reduces to nd a probability pkx of the k-th class under the constraint that
pkx  1
It is clear that in this case the function f returns K values 1 2     K each
of which is between 0 and 1 Furthermore the sum of ks must sum to 1 This can be
achieved easily by letting f to compute afne transformation of x or  x to return K
unbounded real values followed by a so called softmax function 17
expwcid62
kcid481 expwcid62
k  x  bk
kcid48 x  bk
where wk  Rdim x and bk  R are the parameters of afne transformation
In this case the empirical cost function based on the KL divergence is
Ikyn 
C    1
Ikynk
cid26 1
if k  yn
0 otherwise
32 Feature Extraction is also a Function
We talked about the manual feature extraction in the previous lecture see Sec 251
But this is quite unsatisfactory because this whole process of manual feature extraction
is heavily dependent on the domain knowledge meaning that we cannot have a generic
principle on which we design features This raises a question instead of manually
designing features ourselves is it possible for this to happen automatically
One thing we notice is that the feature extraction process  x is nothing but a
function A function of a function is a function right In other words we will extend
our denition of the function to include the feature extraction function
y  f  x
We will assume that the feature extraction function  is also parametrized and its
parameters are included in the set of parameters which includes those of f  As an
example  in Eq 29 is a parameter of the feature extraction 
A natural next question is which family of parametric functions we should use for
 We run into the same issue we talked about earlier in Sec 23 the size of hypothesis
space is simply too large
Instead of choosing one great feature extraction function we can go for a stack of
simple transformations which are all learned3 Each transformation can be as simple
as afne transformation followed by a simple point-wise nonlinearity
where W0 is the weight matrix b0 is the bias and g is a point-wise nonlinearity such
as tanh4
0x  gW0x  b0
One interesting thing is that if the dimensionality of the transformed feature vector
0x is much larger than that of x the function f 0x can approximate any func-
tion from x to y under some assumptions even when the parameters W0 and b0 are
randomly selected 34
The problem solved right We just put a huge matrix W0 apply some nonlinear
function g to it and t linear regression as I described earlier We dont even need to
touch W0 and b0 All we need to do is replace the input xn of all the pairs in the training
set to 0xn
In fact there is a group of researchers claiming to have gured this out by them-
selves less than a decade ago as of 2015 who call this model an extreme learning
machine 54 There have been some debates about this so-called extreme learning
machine Here I will not make any comment myself but would be a good exercise for
you to gure out why there has been debates about this
But regardlessly this is not what we want5 What we want is to fully tune the
whole thing
33 Multilayer Perceptron
The basic idea of multilayer perceptron is to stack a large number of those feature
extraction layers in Eq 38 between the input and the output This idea is as old as
the whole eld of neural network research dating back to early 1960s 89 However
it took many more years for people to gure out a way to tune the whole network both
f and s together See 91 and 70 if you are interested in the history
3 A great article about
this was posted recently in httpcolahgithubioposts
2014-03-NN-Manifolds-Topology
4 Some of the widely used nonlinearities are
 Sigmoid  x 
 Hyperbolic function tanhx  1exp2x
1exp2x
 Rectied linear unit rectx  max0x
1expx
5 And more importantly I will not accept any nal project proposal whose main model is based on the
331 Example Binary classication with a single hidden unit
Let us start with the simplest example The input x  R is a real-valued scalar and
the output y  01 is a binary value corresponding to the inputs label The feature
extractor  is dened as
 x   ux  c
where u and c are the parameters The function f returns the mean of the Bernoulli
conditional distribution pyx
  f x   w x  b
In both of these equations  is a sigmoid function
 x 
1  expx
We use the KL divergence to measure the distance between the true conditional
distribution pyx and the predicted conditional distribution pyx
KLpcid107 p  
y01
y01
pyxlog
pyx
pyxlog pyx pyxlog pyx
Note that the rst term in the summation pyxlog pyx can be safely ignored in our
case Why Because this does not concern p which is one we change in order to
minimize this KL divergence
Lets approximate this KL divergence with a single sample from pyx and leave
only the relevant part We will call this a per-sample cost
Cx  log pyx
 log y1 1y
 ylog   1 ylog1 
where  is from Eq 310
It is okay to work with this per-sample cost function
instead of the full cost function because the full cost function is almost always the
unweighted sum of these per-sample cost functions See Eq 23
We now need to compute the gradient of this cost function Cx with respect to all the
parameters w b u and c First lets start with w
 w 
 w 
which is a simple application of chain rule of derivatives Compare this to
 b 
 b 
In both equations   w x  b which is the input to f 
Both of these derivatives share Cx
   where
y  y    y
  y
cid48 
1 
cid48 
1 
 cid124cid123cid122cid125
cid48
cid48 
  y
1 
cid48    y
because the derivative of the sigmoid function  
  is
cid48  1 
Note that this corresponds to computing the difference between the correct label y and
the predicted label probability 
Given this output derivative Cx
   all we need to compute are
 w   x
 b  1
From these computations we see that
 w    y x
 b    y
Let us continue on to u and c We can again rewrite the derivatives wrt these into
 u 
 c 
 c 
where  is the input to  similarly to  was to the input to 
There are two things to notice here First we already have Cx
derivatives wrt w and b meaning there is no need to re-compute it Second  
shared between the derivatives wrt u and c
  from computing the
 is
Therefore we rst compute  
 
cid124cid123cid122cid125
cid48
 wcid48  w x1  x
Next we compute
 u  x
 c  1
Now all the ingredients are there
 u   yw x1  xx
 c   yw x1  x
The most important lession to learn from here is that most of the computations
needed to get the derivatives in this seemingly complicated multilayered computational
graph multilayer perceptron are shared At the end of the day the amount of compu-
tation needed to compute the gradient of the cost function wrt all the parameters in
the network is only as expensive as computing the cost function itself
332 Example Binary classication with more than one hidden
Let us try to generalize this simple or rather simplest model into a slightly more
general setting We will still look at the binary classication but with multiple hidden
units and a multidimensional input such that
 x  Ux  c
where U  Rld and c  Rl Consequently w will be a l-dimensional vector
The output derivative Cx
  stays same as before See Eq 315 However we
note that the derivative of  with respect to w should now differ because its a vector6
Lets look at what this means
The  can be expressed as
  wcid62 x  b 
wiix  b
In this case we can start computing the derivative with respect to each element of wi
separately
 ix
6 The Matrix Cookbook 85 is a good reference for this section
and will put them into a vector
cid20  
 w 
    
cid21cid62
cid62
 1x2x    lx
  x
Then the derivative of the cost function Cy with respect to w can be written as
 w    y x
Now lets look at Cy
in which case nothing really changed from the case of a single hidden unit in Eq 316
  Again because  x is now a vector there has to be some
  In fact the
 w due to the symmetry
changes Because Cy
procedure for computing this is identical to that for computing  
in Eq 318 That is
  is already computed we only need to look at  
  w
Next what about 
  Because the nonlinear activation function  is applied
element-wise we can simply compute this derivative for each element in  x such
cid16cid2cid48
  diag
1xcid48
2x    cid48
l xcid3cid62cid17
where diag returns a diagonal matrix of the input vector In short we will denote this
as cid48
Overall so far we have got
    ywcid62cid48x    ywcid12 diagcid48x
where cid12 is an element-wise multiplication
Now it is time to compute 
Ucid62x
U  x
according to the Matrix Cookbook 85 Then lets look at the whole derivative wrt
U    ywcid12 diagcid48xxcid62
Note that all the vectors in this lecture note are column vectors
For c its straightforward since
 c  1
34 Automating Backpropagation
This procedure presented as two examples is called a backpropagation algorithm If
you read textbooks on neural networks you see a fancier way to explain this back-
propagation algorithm by introducing a lot of fancy terms such as local error  and
so on But personally I nd it much easier to understand backpropagation as a clever
application of the chain rule of derivatives to a directed acyclic graph DAG in which
each node computes a certain function  using the output of the previous nodes I will
refer to this DAG as a computational graph from here on
Figure 32 a A graphical representation of the computational graph of the example
network from Sec 332 b A graphical illustration of a function node  forward
pass  backward pass
A typical computational graph looks like the one in Fig 32 a This computational
graph has two types of nodes 1 function node cid13 and 2 variable node 2 There
are four different types of function nodes 1 MatMulAB  AB 2 MatSumAB 
AB 3  element-wise sigmoid function and 4 Cy cost node The variables nodes
correspond to either parameters or data x and y Each function node has a number
associated with it to distinguish between the nodes of the same function
Now in this computational graph let us start computing the gradient using the
 y and Cy
 1 
 MatSum1 and multiply it
backpropagation algorithm We start from the last code Cy by computing Cy
Then the function node  1 will compute its own derivative
with Cy
 1 passed back from the function node Cy So far weve computed
 1
 MatSum1 
 1
 1
 MatSum1
The function node MatSum1 has two inputs b and the output of MatMul1 Thus
 MatMul1  Each of these is multiplied
 MatSum1 from Eq 319 At this point we already
this node computes two derivatives  MatSum1
with the backpropagated derivative
have the derivative of the cost function Cy wrt one of the parameters b
and  MatSum1
 b 
 MatSum1
 MatSum1
111222This process continues mechanically until the very beginning of the graph a set
of root variable nodes is reached All we need in this process of backpropagating the
derivatives is that each function node implements both forward computation as well
as backward computation In the backward computation the function node received
the derivative from the next function node evaluates its own derivative with respect to
the inputs at the point of the forward activation and passes theses derivatives to the
corresponding previous nodes See Fig 32 b for the graphical illustration
Importantly the inner mechanism of a function node does not change depending on
its context or equivalently where the node is placed in a computational graph In other
words if each type of function nodes is implemented in advance it becomes trivial to
build a complicated neural network including multilayer perceptrons and compute
the gradient of the cost function which is one such function node in the graph with
respect to all the parameters as well as all the inputs
This is a special case called the reverse mode of automatic differentiation7 It
is probably the most valuable tool in deep learning and fortunately many widely used
toolkits such as Theano 10 4 have implemented this reverse mode of automatic differ-
entiation with an extensive number of function nodes used in deep learning everyday
Before nishing this discussion on automating backpropagation Id like you to
think of pushing this even further For instance you can think of each function node
returning not its numerical derivative on its backward pass but a computational sub-
graph computing its derivative This means that it will return a computational graph
of gradient where the output is the derivatives of all the variable nodes or a subset
of them Then we can use the same facility to compute the second-order derivatives
341 What if a Function is not Differentiable
From the description so far one thing we notice is that backpropagation works only
when each and every function node in a computational graph is differentiable In
other words the nonlinear activation function must be chosen such that almost every-
where it is differentiable All three activation functions I have presented so far have
this property
Logistic Functions A sigmoid function is dened as
and its derivative is
 x 
1  expx
cid48x   x1  x
A hyperbolic tangent function is
tanhx 
exp2x 1
exp2x  1
7 If anyones interested in digging more into the whole eld of automatic differentiation try to Google it
and youll nd tons of materials One such reference is 5
and its derivative is
cid18
tanhcid48x 
cid192
expx  expx
Piece-wise Linear Functions
81 46 earlier
I described a rectied linear unit rectier or ReLU
rectx  max0x
It is clear that this function is not strictly differentiable because of the discontinuity
at x  0 However the chance of the input to this rectier lands exactly at 0 has
zero probability meaning that we can forget about this extremely unlikely event The
derivative of the rectier in this case is
rectcid48x 
cid26 1
if x  0
if x  0
Although the rectier has become the most widely used nonlinearity especially
in deep learnings applications to computer vision8 there is a small issue with the
rectier That is for a half of the input space the derivative is zero meaning that the
error the output derivative from Eq 315 will be not well propagated through the
rectier function node
In 48 the rectier was extended to a maxout unit so as to avoid this issue of the
existence of zero-derivative region in the input to the rectier The maxout unit of rank
k is dened as
maxoutx1    xk  maxx1    xk
and its derivative as
 maxout
x1    xk 
cid26 1
if maxx1    xk  xi
0 otherwise
This means that the derivative is backpropagated only through one of the k inputs
Stochastic Variables These activation functions work well with the backpropagation
algorithm because they are differentiable almost everywhere in the input space How-
ever what happens if a function is non-differentiable at all One such example is a
binary stochastic node which is computed by
1 Compute p   x where x is the input to the function node
2 Consider p as a mean of a Bernoulli distribution ie Bp
3 Generate one sample s  01 from the Bernoulli distribution
4 Output s
8 Almost all the winning entries in ImageNet Large Scale Visual Recognition Challenges ILSVRC use a
convolutional neural network with rectiers See httpimage-netorgchallengesLSVRC
Clearly there is no derivative of this function node
Does it mean that were doomed in this case Fortunately no Although I will not
discuss about this any further in this course Bengio et al 7 provide an extensive list
of approaches we can take in order to compute the derivative of the stochastic function
Chapter 4
Recurrent Neural Networks and
Gated Recurrent Units
After the last lecture I hope that it has become clear how to build a multilayer percep-
tron Of course there are so many details that I did not mention but are extremely im-
portant in practice For instance how many layers of simple transformations Eq 38
should a multilayer perceptron have for a certain task How wide equiv dim0x
should each transformation be What other transformation layers are there What kind
of learning rate  see Eq 25 should we use How should we schedule this learning
rate over training Answers to many of these questions are unfortunately heavily task-
data- and model-dependent and I cannot provide any general answer to them
41 Recurrent Neural Networks
Instead I will move on to describing how we can build a neural network1 to handle
a variable length input Until now the input x was assumed to be either a scalar or
a vector of the xed number of dimensions From here on however we remove this
assumption of a xed size input and consider the case of having a variable length input
What do I mean by a variable length input A variable length input x is a sequence
where each input x has a different number of elements For instance the rst training
examples input x1 may consist of l1 elements such that
x1  x1
2    x1
Meanwhile another examples input xn may be a sequence of ln cid54 l1 elements
xn  xn
2    xn
Lets go back to very basic about dealing with these kinds of sequences Further-
more let us assume that each element xi is binary meaning that it is either 0 or 1 What
1 Now let me begin using a term neural network instead of a general function
would be the most natural way to write a function that returns the number of 1s in
an input sequence x  x1x2    xl My answer is to rst build a recursive function
called ADD1 shown in Alg 1 This function ADD1 will be called for each element of
the input x as in Alg 2
Algorithm 1 A function ADD1
s  0
function ADD1vs
if v  0 then return s
else return s  1
end function
Algorithm 2 A function ADD1
s  0
for i  12    l do s  ADD1xis
end for
There are two important components in this implementation First there is a mem-
ory s which counts the number of 1s in the input sequence x Second a single function
ADD1 is applied to each symbol in the sequence one at a time together with the mem-
ory s Thanks to these two properties our implementation of the function ADD1 can be
used with the input sequence of any length
Now let us generalize this idea of having a memory and a recursive function that
works over a variable length sequence One likely most general case of this idea is
a digital computer we use everyday A computer program is a sequence x of instruc-
tions xi A central processing unit CPU reads each instruction of this program and
manipulates its registers according to what the instruction says Manipulating registers
is often equivalent to manipulating any inputoutput IO device attached to the CPU
Once one instruction is executed the CPU moves on to the next instruction which will
be executed with the content of the registers from the previous step In other words
these registers work as a memory in this case s from Alg 2 and the execution of an
instruction by the CPU corresponds to a recursive function ADD1 from Alg 1
Both ADD1 and CPU are hard coded in the sense that they do what they have been
designed and manufactured to do Clearly this is not what we want because nobody
knows how to design a CPU or a recursive function for natural language understanding
which is our ultimate goal Instead what we want is to have a parametric recursive
function that is able to read a sequence of linguistic symbols and use a memory in
order to understand natural languages
To build this parametric recursive function2 that works on a variable-length input
sequence x  x1x2    xl we now know that there needs to be a memory We will
use one vector h  Rdh as this memory vector As is clear from Alg 1 this recursive
function takes as input both one input symbol xt and the memory vector h and it
2 In neural network research we call this function a recurrent neural network
returns the updated memory vector It often helps to time index the memory vector
as well such that the input to this function is ht1 the memory after processing the
previous symbol xt1 and we use ht to denote the memory vector returned by the
function This function is then
ht  f xt ht1
Now the big question is what kind of parametric form this recursive function f
takes We will follow the simple transformation layer from Eq 38 in which case we
f xt ht1  gW xt   Uht1
where  xt  is a function that transforms the input symbol often discrete into a d-
dimensional real-valued vector W  Rdhd and Udhdh are parameters of this function
A nonlinear activation function g can be any function but for now we will assume that
it is an element-wise nonlinear function such as tanh
411 Fixed-Size Output y
Because our goal is to approximate an underlying true function we now need to think
of how we use this recursive function to return an output y As with the case of variable-
length sequence input x y can only be either a xed-size output such as a category to
which the input x belongs or a variable-length sequence output Here let us discuss the
case of having a xed-size output y
The most natural approach is to use the last memory vector hl to produce the output
or more often output distribution Consider a task of binary classication where y is
either positive 1 or negative 0 in which case a Bernoulli distribution ts perfectly
A Bernoulli distribution is fully characterized by a single parameter  Hence
   vcid62hl
where v  Rdh is a weight vector and  is a sigmoid function
This now looks very much like the multilayer perceptron from Sec 33 The whole
function given an input sequence x computes
cid125
   vcid62 gW xl  UgW xl1  UgW xl2 gW x1  Uh0 
cid123cid122
cid124
a recurrence
where h0 is an initial memory state which can be simply set to an all-zero vector
The main difference is that the input is not given only to the rst simple trans-
formation layer but is given to all those transformation layers one at a time Also
each transformation layer shares the parameters W and U3 The rst two steps of the
3 Note that for brevity I have omitted bias vectors This should not matter much as having a bias vector
is equivalent to augmenting the input with a constant element whose value is xed at 1 Why Because
cid21
cid20 x
 Wx  b
Note that as I have declared before all vectors are column vectors
recurrence part a of Eq 42 are shown as a computational graph in Fig 41
Figure 41 Sample computational graph of the recurrence in Eq 42
As this is not any special computational graph the whole discussion on how to au-
tomate backpropagation computing the gradient of the cost function wrt the parame-
ters in Sec 34 applies to recurrent neural networks directly except for one potentially
confusing point
412 Multiple Child Nodes and Derivatives
It may be confusing how to handle those parameters that are shared across multiple
time steps W and U in Fig 41 In fact in the earlier section Sec 34 we did not
discuss about what to do when the output of one node is fed into multiple function
nodes Mathematically saying what do we do in the case of
c  g f1x f2x     fnx
g can be any function but let us look at two widely used cases
 Addition g f1x     fnx  n
 x 
 Multiplication g f1x     fnx  n
i12n
i1 fix
 g 
cid32
jcid54i
i1 fix
 x 
cid33
 x 
 g 
i12n
 x 
From these two cases we can see that in general
 x 
 g 
i12n
 x 
This means that when multiple derivatives are backpropagated into a single node the
node should rst sum them and multiply its summed derivative with its own derivative
What does this mean for the shared parameters of the recurrent neural network In
an equation
 MatSuml
 MatSuml
 MatMull
 MatMull
cid124
cid124
cid124
cid123cid122
cid123cid122
cid123cid122
cid125
cid125
cid125
cid124
cid124
 
cid123cid122
cid123cid122
cid125
cid125
cid124
 MatSuml
 MatSuml1
 MatSuml1
 MatMull1
 MatMull1
 MatSuml
 MatSuml
 MatSuml1
 MatSuml1
 MatSuml2
 MatSuml2
 MatMull2
 MatMull2
 MatSuml
cid123cid122
cid125
where the superscript l of each function node denotes the layer at which the function
node resides
Similarly to what weve observed in Sec 34 many derivatives are shared across
the terms inside the summation in Eq 43 This allows us to compute the derivative
of the cost function wrt the parameter W efciently by simply running the recurrent
neural network backward
413 Example Sentiment Analysis
There is a task in natural language processing called sentiment analysis As the name
suggests the goal of this task is to predict the sentiment of a given text This is de-
nitely one function that a human can do fairly well when you read a critiques review
of a movie you can easily tell whether the critique likes hates or is neutral to the
movie Also even without a star rating of a product on Amazon you can quite easily
tell whether a user like it by reading herhis review of the product
In this task an input sequence x is a given text and the xed-size output is its label
which is almost always one of positive negative or neutral Let us assume for now
that the input is a sequence of words where each word xi is represented as a so-called
one-hot vector4 In this case we can use
in Eq 41
 xt   xt
4 A one-hot vector is a way to represent a discrete symbol as a binary vector The one-hot vector vi of a
symbol i  V  12    V is
cid124 cid123cid122 cid125
vi  0    0
1i1
 1cid124cid123cid122cid125
cid124 cid123cid122 cid125
 0    0
i1V
cid62
Once the input sequence or paragraph in this specic example is read we get
the last memory state hl of the recurrent neural network We will afne-transform hl
followed by the softmax function to obtain the conditional distribution of the output
y  123 1 positive 2 neutral and 3 negative
cid62
  1 2 3
 softmaxVhl
where 1 2 and 3 are the probabilities of positive neural and negative See
Eq 35 for more details on the softmax function
Because this network returns a categorial distribution it is natural to use the cate-
gorical cross entropy as the cost function See Eq 36 A working example of this
sentiment analyzer based on recurrent neural networks will be introduced and discussed
during the lab session5
414 Variable-Length Output y x  y
Lets generalize what we have discussed so far to recurrent neural networks here In-
stead of a xed-size output y we will assume that the goal is to label each input symbol
resulting in the output sequence y  y1y2    yl of the same length as the input se-
quence x
What kind of applications can you think of that returns the output sequence as long
as the input sequence One of the most widely studied problems in natural language
processing is a problem of classifying each word in a sentence into one of part-of-
speech tags often called POS tagging see Sec 31 of 77 Unfortunately in my
personal opinion this is perhaps the least interesting problem of all time in natural
language understanding but perhaps the most well suited problem for this section
In its simplest form we can view this problem of POS tagging as classifying each
word in a sentence as one of noun verb adjective and others As an example given
the following input sentence x
the goal is to output
x  Childreneatsweetcandy
y  nounverbadjectivenoun
This task can be solved by a recurrent neural network from the preceding section
Sec 411 after a quite trivial modication Instead of waiting until the end of the
sentence to get the last memory state of the recurrent neural network we will use the
immediate memory state to predict the label at each time step t
At each time t we get the immediate memory state ht by
ht  f xt ht1
where f is from Eq 41 Instead of continuing on to processing the next word we
will rst predict the label of the t-th input word xt
5 For those eager to learn more see httpdeeplearningnettutoriallstmhtml in
advance of the lab session
This can be done by
cid62
t  t1 t2 t3 t4
 softmaxVht 
Four tis correspond to the probabilities of the four categories 1 noun 2 verb 3
adjective and 4 others
From this output distribution at time step t we can dene a per-step per-sample
cost function
Cxt     K
Ikytk
where K is the number of categories four in this case We discussed earlier in Eq 36
Naturally a per-sample cost function is dened as the sum of these per-step per-sample
cost functions
Cx    l
Ikytk
Incorporating the Output Structures This formulation of the cost function is equiv-
alent to maximizing the log-probability of the correct output sequence given an input
sequence where the conditional log-probability is dened as
cid124
log pyx 
cid124
cid123cid122
cid123cid122
Eq 47
Eq 48
log pytx1    xt 
cid125
cid125
This means that the network is predicting the label of the t-th input symbol using only
the input symbols read up to that point ie x1x2    xt
In other words this means that the recurrent neural network is not taking into ac-
count the structure of the output sequence For instance even without looking at the
input sequence in English it is well known that the probability of the next word being a
noun increases if the current word is an adjective6 This kind of structures in the output
are effectively ignored in this formulation
Why is this so in this formulation Because we have made an assumption that
the output symbols y1y2    yl are mutually independent conditioned on the input se-
quence This is clear from Eq 49 and the denition of the conditional independence
Y1 and Y2 are conditionally independent dependent on X
 pY1Y2X  pY1XpY2x
If the underlying true conditional distribution obeyed this assumption of condi-
tional independence there is no worry However this is a very strong assumption for
6 Okay this requires a more thorough analysis but for the sake of the argument which does not have to
do anything with actual POS tags lets believe that this is indeed the case
many of the tasks we run into apparently from the example of POS tagging Then
how can we exploit the structure in the output sequence
One simple way is to make a less strong assumption about the conditional proba-
bility of the output sequence y given x For instance we can assume that
log pyx 
log pyiyixi
where yi and xi denote all the output symbols before the i-th one and all the input
symbols up to the i-th one respectively
Now the question is how we can incorporate this into the existing formulation of
a recurrent neural network from Eq 45 It turned out that the answer is extremely
simple All we need to do is to compute the memory state of the recurrent neural
network based not only on the current input symbol xt and the previous memory state
ht1 but also on the previous output symbol yt1 such that
ht  f xt yt1ht1
Similarly to Eq 41 we can think of implementing f as
f xt yt1ht1  gWxxxt   Wyyyt1  Whht1
There are two questions naturally arising from this formulation First what do we
do when computing h1 This is equivalent to saying what yy0 is There are two
potential answers to this question
1 Fix yy0 to an all-zero vector
2 Consider yy0 as an additional parameter
In the latter case yy0 will be estimated together with all the other parameters such
as those weight matrices Wx Wy Wh and V
Inference The second question involves how to handle yt1 During training it is
quite straightforward as our cost function KL-divergence between the underlying
true distribution and the parametric conditional distribution pyx approximated by
Monte Carlo method says that we use the groundtruth value for yt1s
It is however not clear what we should do when we test the trained network because
then we are not given the groundtruth output sequence This process of nding an
output that maximizes the conditional log-probability is called inference7
y  argmax
log pyx
7 Okay I confess The term inference refers to a much larger class of problems even if we consider only
machine learning However let me simply use this term to refer to a task of nding the most likely output of
a function
The exact inference is quite straightforward One can simply evaluate log pyx for
every possible output sequence and choose the one with the highest conditional proba-
bility Unfortunately this is almost always intractable as the number of every possible
output sequence grows exponentially with respect to the length of the sequence
Y   Kl
where Y  K and l are the set of all possible output sequences the number of labels and
the length of the sequence respectively Thus this is necessary to resort to approximate
search over the set Y 
The most naive approach to approximate inference is a greedy one With the trained
model you predict the rst output symbol y1 based on the rst input symbol x1 by
selecting the category of the highest probability py1x1 Now given y1 x1 and x2
we compute py2x1x2y1 from which we select the next output symbol y2 with the
highest probability We continue this process iteratively until the last output symbol yl
is selected
This is greedy in the sense that any early choice with a high conditional probability
may turn out to be unlikely one due to extremely low conditional probabilities later on
It is highly related to the so-called garden path sentence problem To know more about
this read for instance Sec 324 of 77
It is possible to alleviate this issue by considering N  K best hypotheses of the
output sequence at each time step This procedure is called beam search and we will
discuss more about this in a later lecture on neural machine translation
42 Gated Recurrent Units
421 Making Simple Recurrent Neural Networks Realistic
Let us get back to the analogy we made in Sec 41 We compared a recurrent neural
network to how CPU works Executing a recurrent function f is equivalent to executing
one of the instructions on CPU and the memory state of the recurrent neural network is
equivalent to the registers of the CPU This analogy does sound plausible except that
it is not
In fact how a simple recurrent neural network works is far from being similar to
how CPU works I am now talking about how they are implemented in practice but
rather Im talking at the conceptual level What is it at the conceptual level that makes
the simple recurrent neural network unrealistic
An important observation we make about the simple recurrent neural network is
that it refreshes the whole memory state at each time step This is almost opposite to
how the registers on a CPU are maintained Each time an instruction is executed the
CPU does not clear up the whole registers and repopulate them Rather it works only
on a small number of registers All the other registers values are stored as they were
before the execution of the instruction
Lets try to write this procedure mathematically Each time based on the choice
of instruction to be executed a subset of the registers of a CPU or a subset of the
elements in the memory state of a recurrent neural network is selected This can be
written down as a binary vector u  01nh
cid26 0
if the registers value does not change
if the registers value will change
With this binary vector which I will call an update gate a new memory state or a
new register value at time t can be computed as a convex interpolation such that
ht  1 ucid12 ht1  ucid12 ht 
where cid12 is as usual an element-wise multiplication ht denotes a new memory state or
a new register value after executing the instruction at time t
Another unrealistic point about the simple recurrent neural network is that each
execution considers the whole registers It is almost impossible to imagine designing
an instruction on a CPU that requires to read the values of all the registers Instead
what almost always happens is that each instruction will consider only a small subset
of the registers which again we can use a binary vector to represent Let me call it a
reset gate r  01nh
cid26 0
if the registers value will not be used
if the registers value will be used
This reset gate can be multiplied to the register values before being used by the
instruction at time t8 If we use a recursive function f from Eq 41 it means that
ht  f xt rcid12 ht1  gW xt   Urcid12 ht1
Now let us put these two gates that are necessary to make the simple recurrent
neural network more realistic into one piece At each time step the candidate memory
state is computed based on a subset of the elements of the previous memory state
ht  gW xt   Urcid12 ht1
A new memory state is computed as a linear interpolation between the previous mem-
ory state and this candidate memory state using the update gate
ht  1 ucid12 ht1  ucid12 ht
See Fig 42 for the graphical illustration
422 Gated Recurrent Units
Now here goes a big question How are the update u and reset r gates computed
If we stick to our analogy to the CPU those gates must be pre-congured per
instruction Those binary gates are dependent on the instruction Again however this
8 It is important to note that this is not resetting the actual values of the registers but only the input to the
instructionrecursive function
Figure 42 A graphical illustration of a
gated recurrent unit 29
is not what we want to do in our case There is no set of predened instructions but the
execution of any instruction corresponds to computing a recurrent function based on the
input symbol and the memory state from the previous time step see eg Eq 41
Similarly to this what we want with the update and reset gates is that they are computed
by a function which depends on the input symbol and the previous memory state
This sounds like quite straightforward except that we dened the gates to be binary
This means that whatever the function we use to compute those gates the function will
be a discontinuous function with zero derivative almost everywhere except at the point
where a sharp transition from 0 to 1 happens We discussed the consequence of having
an activation function with zero derivative almost everywhere in Sec 341 and the
conclusion was that it becomes very difcult to compute the gradient of the cost func-
tion efciently and exactly with these discrete activation functions in a computational
One simple solution which turned out to be extremely efcient is to consider those
gates not as binary vectors but as real-valued coefcient vectors In other words we
redene the update and reset gates to be
u  01nh r  01nh 
This approach makes these gates leaky in the sense that they always allow some leak
of information through the gate
In the case of the reset gate rather than making a hard decision on which subset
of the registers or the elements of the memory state will be used it now decides how
much information from the previous memory state will be used The update gate on
the other hand now controls how much content in the memory state will be replaced
which is equivalent to saying that it controls how much information will be kept from
the previous memory state
Under this denition we can simply use a sigmoid function from Eq 311 to
compute these gates
r  Wr xt   Urht1
u  Wu xt   Uurcid12 ht1
where Wr Ur Wu and Uu are the additional parameters9 Since the sigmoid function
is differentiable everywhere we can use the backpropagation algorithm see Sec 34
9 Note that this is not the formulation available for computing the reset and update gates For instance
urhhxto compute the derivatives of the cost function with respect to these parameters and
estimate them together with all the other parameters
We call this recurrent activation function with the reset and update gates a gated
recurrent unit GRU and a recurrent neural network having this GRU as a gated re-
current network
423 Long Short-Term Memory
The gated recurrent unit GRU is highly motivated by a much earlier work on long
short-term memory LSTM units 5310 The LSTM was proposed in 1997 with
the goal of building a recurrent neural network that can learn long-term dependen-
cies across many number of timsteps which was deemed to be difcult to do so with a
simple recurrent neural network
Unlike the element-wise nonlinearity of the simple recurrent neural network and the
gated recurrent unit the LSTM explicitly separates the memory state ct and the output
ht The output is a small subset of the hidden memory state and only this subset of the
memory state is visibly exposed to any other part of the whole network
How does a recurrent neural network with LSTM units decide how much of the
memory state it will reveal As perhaps obvious at this point the LSTM uses a so-
called output gate o to achieve this goal Similarly to the reset and update gates of the
GRU the output gate is computed by
o   Wo xt   Uoht1
This output vector is multiplied to the memory state ct point-wise to result in the output
ht  ocid12 tanhct 
Updating the memory state ct closely resembles how it is updated in the GRU see
Eq 410 A major difference is that instead of using a single update gate the LSTM
uses two gates forget and input gates such that
ct  fcid12 ct1  icid12 ct 
where f  Rnh i  Rnh and ct are the forget gate input gate and the candidate memory
state respectively
The roles of those two gates are quite clear from their names The forget gate
decides how much information from the memory state will be forgotten while the
input gate controls how much informationa about the new input consisting of the input
one can use the following denitions of the reset and update gates
r  Wr xt   Urht1
u  Wu xt   Uuht1
which is more parallelizable than the original formulation from 29 This is because there is no more direct
dependency between r and u which makes it possible to compute them in parallel
10 Okay let me confess here I was not well aware of long short-term memory when I was designing the
gated recurrent unit together with Yoshua Bengio and Caglar Gulcehre in 2014
symbol and the previous output will be inputted to the memory They are computed
f  W f  xt   U f ht1
i  Wi xt   Uiht1
The candidate memory state is computed similarly to how it was done with the
GRU in Eq 411
ct  gWc xt   Ucht1
where g is often an element-wise tanh
All the additional parameters specic to the LSTMWoUoW f U f WiUiWc
and Uc are estimated together with all the other parameters Again every function
inside the LSTM is differentiable everywhere and we can use the backpropagation
algorithm to efcient compute the gradient of the cost function with respect to all the
parameters
Although I have described one formulation of the long short-term memory unit
here there are many other variants proposed over more than a decade since it was rst
proposed For instance the forget gate in Eq 412 was not present in the original
work 53 but was xed to 1 Gers et al 45 proposed the forget gate few years after
the LSTM was originally proposed and it turned out to be one of the most crucial
component in the LSTM For more variants of the LSTM I suggest you to read 49
43 Why not Rectiers
431 Rectiers Explode
Let us go back to the simple recurrent neural network which uses the simple transfor-
mation layer from Eq 41
f xt ht1  gW xt   Uht1
where g is an element-wise nonlinearity
One of the most widely used nonlinearities is a hyperbolic tangent function tanh
This is unlike the case in feedforward neural networks multilayer perceptrons where a
unbounded piecewise linear function such as a rectier and maxout has become stan-
dard In the case of feedforward neural networks you can safely assume that everyone
uses some kind of piecewise linear function as an activation function in the network
This has become pretty much standard since Krizhevsky et al 67 shocked the com-
puter vision research community by outperforming all the more traditional computer
vision teams in the ImageNet Large Scale Visual Recognition Challenge 201212
11 Interestingly based on the observation in 58 it seems like the plain LSTM with a forget gate and the
GRU seem to be close to the optimal gated unit we can nd
12 httpimage-netorgchallengesLSVRC2012resultshtml
The main difference between logistic functions tanh and sigmoid function and
piecewise linear functions rectiers and maxout is that the former is bounded from
both above and below while the latter is bounded only from below or in some cases
not bounded at all 5013
This unbounded nature of piece-wise linear functions makes it difcult for them to
be used in recurrent neural networks Why is this so
Let us consider the simplest case of unbounded element-wise nonlinearity a linear
function
ga  a
The hidden state after l symbols is
hl UUUU   W xl3  W xl2  W xl1  W xl
cid32 l2
cid33
lcid481
cid32 l1
cid33
cid32 lt
lcid481
cid124
lcid481
W x1 
W xt 
cid125
cid33
cid123cid122
W x2   UW xl1  W xl
where l is the length of the input sequence
Let us assume that
 U is a full rank matrix
 The input sequence is sparse l
 W xi  0 for all i
and consider Eq 414 a
t1 I xt cid540  c where c  O1
cid32ltcid48
lcid481
htcid48
cid33
W xtcid48
Now lets look at what happens to Eq 415 First the eigendecomposition of the
matrix U
U  QSQ1
where S is a diagonal matrix whose non-zero entries are eigenvalues Q is an orthogo-
nal matrix Then
13 A parametric rectier or PReLU is dened as
ltcid48
lcid481
U  QSltcid48Q1
cid26 x
if x  0
otherwise 
where a is a parameter to be estimated together with all the other parameters of a network
cid33
cid32ltcid48
lcid481
W xtcid48  diagSltcid48
cid124 cid123cid122 cid125
cid12 QQ1
W xtcid48 
where cid12 is an element-wise product
What happens if the largest eigenvalue emax  maxdiagS is larger than 1 the
norm of hl will explode ie cid107hlcid107   Furthermore due to the assumption that
W xtcid48  0 each element of hl will explode to innity as well The rate of growth is
exponentially with respect to the length of the input sequence meaning that even when
the input sequence is not too long the norm of the memory state grows quickly if emax
is reasonably larger than 1
This happens because the nonlinearity g is unbounded If g is bounded from both
above and below such as the case with tanh the norm of the memory state is also
bounded In the case of tanh  R  11
cid107hlcid107  dimhl
This is one reason why a logistic function such as tanh and  is most widely used
with recurrent neural networks compared to piecewise linear functions14 I will call
this recurrent neural network with tanh as an element-wise nonlinear function a simple
recurrent neural network
Is tanh a Blessing
Now the argument in the previous section may sound like tanh and  are the nonlinear
functions that one should use This seems quite convincing for recurrent neural net-
works and perhaps so for feedforward neural networks as well if the network is deep
enough
Here let me try to convince you otherwise by looking at how the norm of backprop-
agated derivative behaves Again this is much easier to see if we assume the following
 U is a full rank matrix
 The input sequence is sparse l
Similarly to Eq 414 let us consider a forward computational path until hl how-
t1 I xt cid540  c where c  O1
ever without assuming a linear activation function
hl  g Ug Ug Ug U    W xl3  W xl2  W xl1  W xl 
We will consider a subsequence of this process in which all the input symbols are 0
except for the rst symbol
hl1  gcid0Ugcid0Ucid0gcid0Uhl0  Wcid0xl01
cid1cid1cid1cid1cid1 
14 However it is not to say that piecewise linear functions are never used for recurrent neural networks
See for instance 69 6
It should be noted that as l approaches innity there will be at least one such sub-
sequence whose length also approaches innity due to the sparsity of the input we
assumed
From this equation lets look at
cid0xl01
cid1 
This measures the effect of the l0 1-th input symbol xl01 on the l1-th memory state
of the simple recurrent neural network This is also the crucial derivative that needs to
be computed in order to compute the gradient of the cost function using the automated
backpropagation procedure described in Sec 34
This derivative can be rewritten as
Among these three terms in the left hand side we will focus on the rst one a which
can be further expanded as
cid0xl01

cid124 cid123cid122 cid125
hl11
cid1 
hl01
hl01
hl01
cid124 cid123cid122 cid125
hl11
cid124 cid123cid122 cid125
hl11
hl11
hl12
cid124 cid123cid122 cid125
cid1 
hl01
cid0xl01
hl02

cid124 cid123cid122 cid125
hl02
hl1
cid124cid123cid122cid125
 
hl02
hl01
cid124 cid123cid122 cid125
hl01
Because this is a recurrent neural network we can see that the analytical forms for
the terms grouped by the parentheses in the above equation are identical except for the
subscripts indicating the time index In other words we can simply only on one of
those groups and the resulting analytical form will be generally applicable to all the
other groups
First we look at Eq 416 b which is nothing but a derivative of a nonlinear
activation function used in this simple recurrent neural network The derivatives of the
widely used logistic functions are
cid48x  x1  x
tanhcid48x 1 tanh2x
as described earlier in Sec 341 Both of these functions derivatives are bounded
In the simplest case in which g is a linear function ie x  gx we do not even
need to look at
from Eq 416
0  cid48x  025
0  tanhcid48x  1
cid13cid13cid13 ht
cid13cid13cid13 We simply ignore all the ht
Next consider Eq 416 c In this case of simple recurrent neural network we
notice that we have already learned how to compute this derivative earlier in Sec 332
From these two we get
hl01
ht1
cid19cid18hl11
cid18hl1
cid18ht
cid19
hl11
cid19
cid18hl02
hl02
cid19
Do you see how similar it looks like Eq 415 If the recurrent activation function
f is linear this whole term reduces to
hl01
 Ul1l01
which according to Sec 431 will explode as l   if
emax  1
where emax is the largest eigenvalue of U When emax  1 it will vanish ie cid107 hl1
hl01
0 exponentially fast
What if the recurrent activation function f is not linear at all Lets look at ht
cid107

cid124
f cid48
f cid48
cid18
f cid48
cid19
cid123cid122
cid17
cid16 ht
cid18ht
cid0QSQ1cid1 

cid125
cid19
cid12 S
where we used the eigendecomposition of U  QSQ1 This can be re-written into
This means that the eigenvalue of U will be scaled by the derivative of the recurrent ac-
tivation function at each timestep In this case we can bound the maximum eigenvalue
of ht
max   emax
where  is the upperbound on gcid48  ht
 See Eqs 417418 for the upperbounds of
the sigmoid and hyperbolic tangent functions
In other words if the largest eigenvalue of U is larger than 1
  it is likely that this
temporal derivative of hl1 with respect to hl01 will explode meaning that its norm will
grow exponentially large In the opposite case of emax  1
  the norm of the temporal
derivative likely shrinks toward 0 The former case is referred to as exploding gradient
and the latter vanishing gradient These cases were studied already at the very early
years of research in recurrent neural networks 9 52
Using tanh is a blessing in recurrent neural networks when running the network
forward as I described in the previous section This is however not necessarily true in
the case of backpropagating derivaties Especially because there is a higher chance of
vanishing gradient with tanh or even worse with  Why Because 1
  1 for almost
everywhere
433 Are We Doomed
Exploding Gradient Fortunately it turned out that the phenomenon of exploding
gradient is quite easy to address First it is straightforward to detect whether the ex-
ploding gradient happened by inspecting the norm of the gradient fo the cost with
respect to the parameterscid13cid13 Ccid13cid13 If the gradients norm is larger than some predened
threhold   0 we can simply renormalize the norm of the gradient to be  Otherwise
we leave it as it is
In mathematics
cid40
 
 cid107cid107 
if cid107cid107  
otherwise
 is a rescaled gradient update
where we used the shorthand notiation  for  C
direction which will be used by the stochastic gradient descent SGD algorithm from
Sec 222 This algorithm is referred to as gradient clipping 83
Vanishing Gradient What about vanishing gradient But rst what does vanishing
gradient mean We need to understand the meaning of this phenomenon in order to tell
whether this is a problem at all from the beginning
Let us consider a case the variable-length output where x  y from Sec 414
Lets assume that there exists a clear dependency between the output label yt and the
input symbol xtcid48 where tcid48 cid28 t This means that the empirical cost will decrease when
the weights are adjusted such that
log pyt  y
t      xtcid48   
is maximized where y
has great inuence on the t-th output yt and the inuence can be measured by
t is the ground truth output label at time t The value of  xtcid48
 log pyt  y
 xtcid48
Instead of exactly computing  log pyt y
t 
 xtcid48 
t    
 we can approximate it by the nite
difference method Let   Rdim xtcid48  be a vector of which each element is a very
small real value   0 Then
t    
 log pyt  y
 xtcid48
log pyt  y
 log pyt  y
t      xtcid48     
t      xtcid48     cid11 
where cid11 is an element-wise division This shows that  log pyt y
computes the
 xtcid48 
difference in the t-th output probability with respect to the change in the value of the
tcid48-th input
t 
In other words  log pyt y
 xtcid48 
directly reects the degree to which the t-th output
yt depends on the tcid48-th input xtcid48 according to the network To put it in another way
 log pyt y
reects how much dependency the recurrent neural network has cap-
 xtcid48 
tured the dependency between yt and xtcid48
t 
t 
Lets rewrite
 log pyt  y
 xtcid48
t    
 log pyt  y
t    
ht1
cid124
cid123cid122
cid125
 htcid481
htcid48
htcid48
 xt 
The terms marked with a looks exactly identical to Eq 416 We have already seen
that this term can easily vanish toward zero with a high probability see Sec 432
This means that the recurrent neural network is unlikely to capture this dependency
This is especially true when the temporal distance between the output and input ie
t tcid48 cid29 0
The biggest issue with this vanishing behaviour is that there is no straightforward
way to avoid it We cannot tell whether  log pyt y
 0 is due to the lack of this
 xtcid48 
dependency in the true underlying function or due to the wrong conguration param-
eter setting of the recurrent neural network If we are certain that there are indeed
these long-term dependencies we may simultaneously minimize the following auxil-
iary term together with the cost function
t 
1
cid13cid13cid13  C
cid13cid13cid13  C
ht1
ht1
cid13cid13cid13
ht1
cid13cid13cid13
2
This term which was introduced in 83 is minimized when the norm of the derivative
does not change as it is being backpropagated effectively forcing the gradient not to
vanish
This term however was found to help signicantly only when the target task or the
underlying function does indeed exhibit long-term dependencies How can we know
in advance Pascanu et al 83 showed this with the well-known toy tasks which were
specically designed to exhibit long-term dependencies 52
434 Gated Recurrent Units Address Vanishing Gradient
Will the same problems of vanishing gradient happen with the gated recurrent units
GRU or the long short-term memory units LSTM Let us write the memory state at
time t
ht ut cid12 ht  1 ut cid12cid0ut1 cid12 ht1  1 ut1cid12cid0ut2 cid12 ht2  1 ut2cid12  cid1cid1
ut cid12 ht  1 ut cid12 ut1 cid12 ht1  1 ut cid12 1 ut1cid12 ut2 cid12 ht2 
Lets be more specic and see what happens to this with respect to xtcid48
ht ut cid12 ht  1 ut cid12 ut1 cid12 ht1  1 ut cid12 1 ut1cid12 ut2 cid12 ht2 
cid32
cid124
kttcid481
cid33
1 uk
cid123cid122
cid12tanh W xtcid48  U rtcid48 cid12 htcid481 
cid12 utcid48
cid125
where  is for element-wise multiplication
What this implies is that the GRU effectively introduces a shortcut from time tcid48 to
t The change in xtcid48 will directly inuence the value of ht and subsequently the t-th
output symbol yt In other words all the issue with the simple recurrent neural network
we discussed earlier in Sec 433
The update gate controls the strength of these shortcuts Lets assume for now that
the update gate is xed to some predened value between 0 and 1 This effectively
makes the GRU a leaky integration unit 6 However as it is perhaps clear from
Eq 419 that we will inevitably run into an issue Why is this so
Lets say we are sure that there are many long-term dependencies in the data It is
natural to choose a large coefcient for the leaky integration unit meaning the update
gate is close to 1 This will denitely help carrying the dependency across many time
steps but this inevitably carries unnecessary information as well This means that
much of the representational power of the output function goutht  is wasted in ignoring
those unnecessary information
If the update gate is xed to something substantially smaller than 1 all the shortcuts
see Eq 419 a will exponentially vanish Why Because it is a repeated multipli-
cation of a scalar small than 1 In other words it does not really help to have a leaky
integration unit in the place of a simple tanh unit
This is however not the case with the actual GRU or LSTM because those update
gates are not xed but are adaptive with respect to the input If the network detects
that there is an important dependency being captured the update gate will be closed
u j  0 This will effectively strengthen the shortcut connection see Eq 419 a
When the network detects that there is no dependency anymore it will open the update
gate u j  1 which effectively cuts off the shortcut How does the network know or
detect the existence or lack of these dependencies Do we need to manually code this
up I will leave these questions for you to gure out
Chapter 5
Neural Language Models
51 Language Modeling First Step
What does it mean for a machine to understand natural language In other words how
can we tell that the machine understood natural language These are the two equivalent
questions that are at the core of this course
One of the most basic capability of a machine that can signal us that it indeed
understands natural language is for the machine to tell how likely a given sentence
is Of course this is extremely ill-dened as we probably cannot dene the likeliness
of a sentence because there are many different types of unlikeliness For instance
a sentence Colorless green ideas sleep furiously from Chomskys 32 is unlikely
according to our common sense because
1 An object idea cannot be both colorless and green
2 An object cannot sleep furiously
3 An idea does not sleep
On the other hand this sentence is a grammatically correct sentence
Lets take a look at another sentence Jane and me went to see a movie yesterday
Grammatically this is not the most correct sentence one can make It should be Jane
and I went to see a movie yesterday Even with a grammatical error in the original
sentence however the meaning of the sentence is clear to me and perhaps is much more
understandable than the sentence colorless green ideas sleep furiously Furthermore
many people likely say this saying me instead of I quite often This sentence is
thus likely according to our common sense but is not likely according to the grammar
This observation makes us wonder what is the criterion to use Is it correct for a
machine to tell whether the sentence is likely by analyzing its grammatical correctness
Or is it possible that the machine should deem a sentence likely only when its meaning
agrees well with common sense regardless of its grammatical correctness in the most
strict sense
As we discussed in the rst lecture of the course we are more interested in ap-
proaching natural language as a means for one to communicate ideas to a listener In
this sense language use is a function which takes as input the surrounding environ-
ment including the others speech and returns linguistic response and this function
is not given but learned via observing others use of language and the reinforcement
by the surrounding environment 97 Also throughout this course we are not con-
cerned too much about the existing syntactic or grammatical structures underlying
natural language which makes it difcult for us to say anything about the grammatical
correctness of a given sentence
In short we take the route here that the likeliness of a sentence be determined
based on its agreement with common sense The common sense here is captured by
everyday use of natural language which consequently implies that the statistics of
natural language use can be a strong indicator for determining the likely of a natural
language sentence
511 What if those linguistic structures do exist
Of course as we discussed earlier in Sec 11 and in this section not everyone agrees
This is due to the fact that a perfect grammatical sentence may be considered unlikely
just because it does not happen often In other words statistical approaches to language
modeling may conclude that a sentence with perfectly valid grammatical construction
is unlikely Is this a problem
This problem of telling how likely a given sentence is can be viewed very naturally
as building a probabilistic model of sentences In other words given a sentence S what
is the probability pS of S Let us briey talk about what this means for the case of
viewing the likeliness of a sentence as equivalent to its grammatical correctness1
We rst assume that there is an underlying linguistic structure G which has gener-
ated the observed sentence S Of course we do not know the correct G in advance and
unfortunately no one will tell us what the correct G is2 Thus G is a hidden variable
in this case This hidden structure G generates the observed sentence S according to an
unknown conditional distribution pSG Each and every grammatical structure G is
assigned a prior probability which is also unknown in advance3
With the conditional distribution SG and the prior distribution G we easily get the
joint distribution SG by
pSG  pSGpG
from the denition of conditional probability4 From this joint distribution we get the
1 Why briey and why here Because we will not pursue this line at all after this section
2 Here the correct G means the G that generated S not the whole structure of G which is assumed to
exist according to a certain set of rules
3 This is not necessarily true If we believe that each and every grammatical correct sentence is equally
likely and that each correct grammatical structure generates a single corresponding sentence the prior dis-
tribution over the hidden linguistic structure is such that any correct structure is given an equal probability
while any incorrect structure is given a zero probability But of course if we think about it there are clearly
certain structures that are more prevalent and others that are not
4 A conditional probability of A given B is dened as
pAB 
distribution over a given sentence S by marginalizing out G
pS  
pSG
This means that we should compute how likely a given sentence S is with respect to all
possible underlying linguistic structure This is very likely intractable because there
must be innite possible such structures
Instead of computing pS exactly we can simply look at its lowerbound For in-
stance one simplest and probably not the best way to do so is
pS  
pSG  pS G
where G  argmaxG pSG  argmaxG pGS5
This lowerbound is tight ie pS  pS G when there is only a single true un-
derlying linguistic structure G given S What this says is that there is no other possible
linguistic structure possible for a single observed sentence ie no ambiguity in infer-
ring the correct linguistic structure In other words we can compute the probability or
likeliness of a given sentence by inferring its correct underlying linguistic structure
However there are a few issues here First it is not clear which formalism G
follows and we have briey discussed about this at the very beginning of this course
Second it is quite well known that most of the formalisms do indeed have uncertainty
in inference Again we looked at one particular example in Sec 112 These two
issues make many people including myself quite uneasy about this type of model-
based approaches
In the remaining of this chapter I will thus talk about model-free approaches as
opposed to these model-based approaches
512 Quick Note on Linguistic Units
Before continuing there is one question that must be bugging you or at least has
bugged me a lot what is the minimal linguistic unit
If we think about written text the minimal unit does seem like a character With
spoken language the minimal unit seems to be a phoneme But is this the level at
which we want to model the process of understanding natural language In fact to
most of the existing natural language processing researchers as well as some or most
linguists the answer to this question is a hard no
The main reason is that these low-level units both characters and phonemes do not
convey any meaning themselves Does a Latin alphabet q have its own meaning The
answer by most of the people will be no Then starting from this alphabet q how
far should we climb up in the hierarchy of linguistic units to reach a level at which the
unit begins to convey its own meaning qu does not seem to have its own meaning
still qui in French means who but in English it does not really say much quit
in English is a valid word that has its own meaning and similarly quiet is a valid
word that has its own meaning quite apart from that of quit
5 This inequality holds due to the denition of probability which states that pX  0 and X pX  1
It looks like a word is the level at which meaning begins to form itself However
this raises a follow-up question on the denition of a word What is a word
It is tempting to say that a sequence of non-blank characters is a word This makes
everyones life so much easier because we can simply split each sentence by a blank
space to get a sequence of words Unfortunately this is a very bad strategy The sim-
plest counter example to this denition of words is a token which I will use to refer to
a sequence of non-blank characters consisting of a word followed by a punctuation
If we simply split a sentence into words by blank spaces we will get a bunch of re-
dundant words For instance llama llama llama llama llama llama
and llama will all be distinct words We will run into an issue of exploding vocab-
ulary with any morphologically rich language Furthermore in some languages such
as Chinese there is no blank space at all inside a sentence in which case this simple
strategy will completely fail to give us any meaningful small linguistic unit other than
sentences
Now at this point it almost seems like the best strategy is to use each character
as a linguistic unit This is not necessarily true due to the highly nonlinear nature of
orthography6 There are many examples in which this nonlinear nature shows its dif-
culty One such example is to consider the following three words quite quiet
and quit7 All three character sequences have near identical forms but their corre-
sponding meanings differ from each other substantially In other words any function
that maps from these character sequences to the corresponding meanings will have to
be extremely nonlinear and thus difcult to be learned from data Of course this is an
area with active research and I hope I am not giving you an impression that characters
are not the units to use see eg 61
Now then the question is whether there is some middle ground between characters
and words or blank-space-separated tokens that are more suitable to be used as ele-
mentary linguistic units see eg 93 Unfortunately this is again an area with active
research Hopefully we will have time later in the course to discuss this issue further
For now we will simply use blank-space-separated tokens as our linguistic units
52 Statistical Language Modeling
Regardless of which linguistic unit we use any natural language sentence S can be
represented as a sequence of T discrete symbols such that
S  w1w2    wT 
Each symbol is one element from a vocabulary V which contains all possible symbols
V cid8v1v2    vVcid9 
where V is used to mean the size of the vocabulary or the number of all symbols
6 Orthography is dened as the study of spelling and how letters combine to represent sounds and form
words
7 I would like to thank Bart van Merrienboer for this example
The problem of language modeling is equivalent to nding a model that assigns a
probability pS to a sentence
pS  pw1w2    wT 
Of course we are not given this distribution and need to learn this from data
Lets say we are given data D which contains N sentences such that
D cid8S1S2    SNcid9 
where each sentence Sn is
2    wn
meaning that each sentence has a different length
Sn  wn
Given this data D let us estimate the probability of a certain sentence S This is
quite straightforward
where I is the indicator function dened earlier in Eq 37 which is dened as
n1 ISSn
cid26 1
ISSn 
if S  Sn
0 otherwise
This is equivalent to counting how many times S occurs in the data8
521 Data SparsityScarcity
Has this solved the whole problem of language model No unfortunately not The
very major issue here is that however large your corpus is it is unlikely to contain all
reasonable sentences in the world Lets do simple counting here
There are V symbols in a vocabulary Each sentence can be as long as T symbols
Then there are VT possible sentences A reasonable range for the sentence length T
is roughly between 1 to 50 meaning that there are
possible sentences As its quite clear this is a huge space of sentences
Of course not all those sentences are plausible This is however conceivable that
even the fraction of that space will be gigantic especially considering that the size of
vocabulary often goes up to 100k to 1M words Many of the plausible sentences will
not appear in the corpus Is this true In fact yes it is
It is quite easy to nd such an example For instance Google Books Ngram
Viewer9 lets you search for a sentence or a sequence of up to ve English words from
8 A data set consisting of written text is often referred to as a corpus
9 httpsbooksgooglecomngrams
Figure 51 A picture of a llama lying down From httpsenwikipedia
orgwikiLlama
the gigantic corpus of Google Books Let me try to search for a very plausible sen-
tence I like llama and the Google Books Ngram10 Viewer returns an error saying
that Ngrams not found I like llama see Fig 51 in the case you are not familiar
with a llama See Fig 52 as an evidence
Figure 52 A resulting page of Google Books Ngram Viewer for the query I like
llama
What does this mean for the estimate in Eq 52 It means that this estimator will
be too harsh for many of the plausible sentences that do not occur in the data As soon
as a given sentence does not appear exactly as it is in the corpus this estimator will
say that there is a zero probability of the given sentence Although the sentence I like
llama is a likely sentence according to this estimator in Eq 52 it will be deemed
extremely unlikely
This problem is due to the issue of data sparsity Data sparsity here refers to the
10 We will discuss what Ngrams are in the later sections
phenomenon where a training set does not cover the whole space of input sufciently
In more concrete terms most of the points in the input space which have non-zero
probabilities according to the true underlying distribution do not appear in the training
set If the size of a training set is assumed to be xed the severity of data sparsity
increases as the average or maximum length of the sentences This follows from the
fact that the size of the input space the set of all possible sentences grows with respect
to the maximum possible length of a sentence
In the next section we will discuss the most straightforward approach to addressing
this issue of data sparsity
n-Gram Language Model
The fact that the issue of data sparsity worsens as the maximum length of sentences
grows hints us a straightforward approach to addressing this limit the maximum length
of phrasessentences we estimate a probability on This idea is a foundation on which
a so-called n-gram language model is based
In the n-gram language model we rst rewrite the probability of a given sentence
S from Eq 51 into
pS  pw1w2    wT   pw1pw2w1 pwkwk
 pwTwT 
cid124
cid123cid122
cid125
where wk denotes all the symbols before the k-th symbol wk From this the n-
gram language model makes an important assumption that each conditional probability
Eq 53 a is only conditioned on the n 1 preceding symbols only meaning
pwkwk  pwkwknwkn1    wk1
This results in
pS  T
pwtwtn    wt1
What does this mean Under this assumption we are saying that any symbol in a
sentence is predictable based on the n 1 preceding symbols This is in fact a quite
reasonable assumption in many languages For instance let us consider a phrase I am
from Even without any more context information surrounding this phrase such as
surrounding words and the identity of a speaker we know that the word following this
phrase will be likely a name of place or country In other words the probability of a
name of place or country given the three preceding words I am from is higher than
that of any other words
But of course this assumption does not always work For instance consider a
 Let us focus on
phrase In Korea more than half of all the residents speak Korean
cid124 cid123cid122 cid125
the last word Korean marked with a We immediately see that it will be useful
to condition its conditional probability on the second word Korea Why is this so
Because the conditional probability of Korean following speak should signicantly
increase over all the other words that correspond to other languages knowing the fact
that the sentence is talking about the residents of Korea This requires the conditional
distribution to be conditioned on at least 10 words  is considered a separate word
and this certainly will not be captured by n-gram language model with n  9
From these examples it is clear that theres a natural trade-off between the quality
of probability estimate and statistical efciency based on the choice of n in n-gram
language modeling The higher n the longer context the conditional distribution has
leading to a better modelestimate second example however resulting in a situation
of more sever data sparsity see Sec 521 On the other hand the lower n leads to
the worse language modeling second example but this will avoid the issue of data
sparsity
n-gram Probability Estimation We can estimate the n-gram conditional probability
pwkwkn    wk1 from the training corpus Since it is a conditional probability we
need to rewrite it according to the denition of the conditional probability
pwkwkn    wk1 
pwkn    wk1wk
pwkn    wk1
This rewrite implies that the n-gram probability is equivalent to counting the occur-
rences of the n-gram wkn    wk among all n-grams starting with wkn    wk1
Let us consider the denominator rst The denominator can be computed by the
marginalizing the k-th word wcid48 below
pwkn    wk1  
wcid48V
pwkn    wk1wcid48
From Eq 52 we know how to estimate pwkn    wk1wcid48
pwkn    wk1wcid48  cwkn    wk1wcid48
where c is the number of occurrences of the given n-gram in the training corpus and
Nn is the number of all n-grams in the training corpus
Now lets plug Eq 56 into Eqs 5455
pwkwkn    wk1 
cwkn    wk1wk
Nn wcid48V cwkn    wk1wcid48
531 Smoothing and Back-Off
Note that I am missing many references this section as I am writing this on my travel
I will ll in missing references once Im back from my travel
The biggest issue of having an n-gram that never occurs in the training corpus is
that any sentence containing the n-gram will be given a zero probability regardless
of how likely all the other n-grams are Let us continue with the example of I like
llama With an n-gram language model built using all the books in Google Books the
following totally valid sentence11 will be given a zero probability
 I like llama which is a domesticated South American camelid12
Why is this so Because the probability of this sentence is given as a product of all
possible trigrams
pI like llama which is a domesticated South American camelid
pIplikeI pllamaIlike
 pcamelidSouthAmerican
cid125
cid124
cid123cid122
One may mistakenly believe that we can simply increase the size of corpus col-
lecting even more data to avoid this issue However remember that data sparsity is
almost always an issue in statistical modeling 24 which means that more data call
for better statistical models with often more parameters leading to the issue of data
sparsity
One way to alleviate this problem is to assign a small probability to all unseen
n-grams At least in this case we will assign some small non-zero probability to
any sentence thereby avoiding a valid but zero-probability sentence under the n-gram
language model One simplest implementation of this approach is to assume that each
and every n-gram occurs at least  times and any occurrence in the training corpus is
in addition to this background occurrence
In this case the estimate of an n-gram becomes
pwkwkn    wk1 
  cwknwkn1    wk
wcid48V   cwknwkn1    wcid48
V  wcid48V cwknwkn1    wcid48
  cwknwkn1    wk
where cwknwkn1    wk is the number of occurrences of the given n-gram in the
training corpus cwknwkn1    wcid48 is the number of occurrences of the given n-
gram if the last word wk is substituted with a word wcid48 from the vocabulary V   is often
set to be a scalar such that 0    1 See the difference from the original estimate in
Eq 57
It is quite easy to see that this is a quite horrible estimator how does it make sense
to say that every unseen n-gram occurs with the same frequency Also knowing that
this is a horrible approach what can we do about this
One possibility is to smooth the n-gram probability by interpolating between the
estimate of the n-gram probability in Eq 57 and the estimate of the n 1-gram
probability This can written down as
pSwkwkn    wk1  wkn    wk1pwkwkn    wk1
 1  wkn    wk1pSwkwkn1    wk1
11 This is not strictly true as I should put a in front of the llama
12 The description of a llama taken from Wikipedia httpsenwikipediaorgwikiLlama
This implies that the n-gram smoothed probability is computed recursively by the
lower-order n-gram probabilities This is clearly an effective strategy considering that
falling off to the lower-order n-grams contains at least some information of the original
n-gram unlike the previous approach of adding a scalar  to every possible n-gram
Now a big question here is how the interpolation coefcient  is computed The
simplest approach we can think of is to t it to the data as well However the situ-
ation is not that easy as using the same training corpus which was used to estimate
pwkwkn    wk1 according to Eq 57 will lead to a degenerate case What is
this degenerate case If the same corpus is used to t both the non-smoothed n-gram
probability and  s the optimal solution is to simply set all  s to 1 as that will assign
the high probabilities to all the n-grams Therefore one needs to use a separate corpus
to t  s
More generally we may rewrite Eq 58 as
pSwkwkn    wk1 
cid26 wkwkn    wk1 if cwkn    wk1wk  0
wkn1    wkpSwkwkn1    wk1 otherwise
following the notation introduced in 63 Specic choices of  and  lead to a number
of different smoothing techniques For an extensive list of these smoothing techniques
see 24
Before ending this section on smoothing techniques for n-gram language modeling
let me briey describe one of the most widely used smoothing technique called the
modied Kneser-Ney smoothing KN smoothing described in 24 This modied
KN smoothing is efciently implemented in the open-source software package called
KenLM 51
First let us dene some quantities We will use nk to denote the total number of
n-grams that occur exactly k times in the training corpus With this we dene the
following so-called discounting factors
n1  2n2
D1 1 2Y
D2 2 3Y
D3 3 4Y
Also let us dene the following quantities describing the number of all possible words
following a given n-gram with a specied frequency l
Nlwkn    wk1  cwkn    wk1wk  l
The modied KN smoothing then denes  in Eq 59 to be
wkwkn    wk1 
cwkn    wk1wk Dcwkn    wk1wk
wcid48V cwkn    wk1wcid48
where D is
And  is dened as
wkn    wk1 

if c  0
if c  1
if c  2
if c  3
D1N1wkn    wk1  D2N2wkn    wk1  D3N3wkn    wk1
wcid48V cwkn    wk1wcid48
For details on how this modied KN smoothing has been designed see 24
532 Lack of Generalization
Although n-gram language modelling works like a charm in many cases This is still
not totally satisfactory because of the lack of generalization What do I mean by
generalization here
Consider an example where three trigrams13 were observed from a training corpus
chases a cat chases a dog and chases a rabbit There is a clear pattern here The
pattern is that it is highly likely that chases a will be followed by an animal
How do we know this This is a trivial example of humans generalization abil-
ity We have noticed a higher-level concept in this case an animal from observing
words such as cat dog and rabbit and based on this concept we generalize this
knowledge that chases a is followed by an animal to unseen trigrams in the form of
chases a animal
This however does not happen with n-gram language model As an example lets
consider a trigram chases a llama Unless this specic trigram occurred more than
once in the training corpus the conditional probability given by n-gram language mod-
eling will be zero14 This issue is closely related to data sparsity but the main differ-
ence is that it is not the lack of data or n-grams but the lack of world knowledge In
other words there exist relevant n-grams in the training corpus but n-gram language
modelling is not able to exploit these
At this point it almost seems trivial to address this issue by incorporating existing
knowledge into language modelling For instance one can think of using a dictionary
to nd the denition of a word in interest continuing on from the previous example
the denition of llama and letting the language model notice that llama is a a
13 Is trigram a proper term Certainly not but it is widely accepted by the whole community of natural
language processing researchers Heres an interesting discussion on how n-grams should be referred to
as from 77 these alternatives are usually referred to as a bigram a trigram and a four-gram model
respectively Revealing this will surely be enough to cause any Classicists who are reading this book to stop
and to leave the eld to uneducated engineering sorts  with the declining levels of education in recent
decades  some people do make an attempt at appearing educated by saying quadgram 
14 Here we assume that no smoothing or backoff is used However even when these techniques are used
we cannot be satised since the probability assigned to this trigram will be at best reasonable up to the point
that the n-gram language model is giving as high probability as the bigram chases a In other words we do
not get any generalization based on the fact that a llama is an animal similar to a cat dog or rabbit
domesticated pack animal of the camel family found in the Andes valued for its soft
woolly eece Based on this the language model should gure out that the probability
of chases a llama should be similar to chases a cat chases a dog or chases a
rabbit because all cat dog and rabbit are animals according to the dictionary
This is however not satisfactory for us First those denitions are yet another
natural language text and letting the model understand it becomes equivalent to nat-
ural language understanding which is the end-goal of this whole course Second
a dictionary or any human-curated knowledge base is an inherently limited resource
These are limited in the sense that they are often static not changing rapidly to reect
the changes in language use and are often too generic potentially not capturing any
domain-specic knowledge
In the next section I will describe an approach purely based on statistics of natural
language that is able to alleviate this lack of generalization
54 Neural Language Model
One thing we notice from n-gram language modelling is that this boils down to com-
puting the conditional distribution of a next word wk given n  1 preceding words
In other words the goal of n-gram language modeling is to nd a
wkn    wk1
function that takes as input n 1 words and returns a conditional probability of a next
pwkwkn    wk1  f wk
 wkn    wk1
This is almost exactly what we have learned in Chapter 2
First we should dene the input to this language modelling function Clearly the
input will be a sequence of n 1 words but the question is how each of these words
will be represented Since our goal is to put the least amount of prior knowledge we
want to represent each word such that each and every word in the vocabulary is equi-
distant away from the others One encoding scheme that achieves this goal is 1-of-K
coding
In this 1-of-K coding scheme each word i in the vocabulary V is represented as a
binary vector wi whose sum equals 1 To denote the i-th word with the vector wi we
set the i-th element of the vector wi to be 1 and consequently all the other elements
are set to zero Mathematically
wi  00    
    0cid62  01V
1cid124cid123cid122cid125
i-th element
cid26 1
This kind of vector is often called a one-hot vector
It is easy to see that this encoding scheme perfectly ts our goal of having minimal
prior because
wi  w j 
if i cid54 j
otherwise
Now the input to our function is a sequence of n  1 such vectors which I will
denote by w1w2    wn1 As we will use a neural network as a function approx-
imator here15 these vectors will be multiplied with a weight matrix E After this we
get a sequence of continuous vectors p1p2    pn1 where
p j  Ecid62w j
and E  RVd
Before continuing to build this function let us see what it means to multiply the
transpose of a matrix with an one-hot vector from left Since only one of the elements
of the one-hot vector is non-zero all the rows of the matrix will be ignored except for
the row corresponding to the index of the non-zero element of the one-hot vector This
row is multiplied by 1 which simply gives us the same row as the result of this whole
matrixvector multiplication In short the multiplication of the transpose of a matrix
with an one-hot vector is equivalent to slicing out a single row from the matrix
In other words let
 

where ei  Rd Then
Ecid62wi  ei
This view has two consequences First in practice it will be much more efcient
computationally to implement this multiplication as a simple table look-up For in-
stance in Python with NumPy do
p  Ei
instead of
p  numpydotET wi
Second from this perspective we can see each row of the matrix E as a continuous-
space representation of a corresponding word ei will be a vector representation of the
i-th word in the vocabulary V  This representation is often called a word embedding
and should reect the underlying meaning of the word We will discuss this further
shortly
Closely following 8 we will simply concatenate the continuous-space represen-
tations of the input words such that
p cid2p1p2   pn1cid3cid62
15 Obviously this does not have to be true but at the end of the day it is unclear if there is any parametric
function approximation other than neural networks
This vector p is a representation of n 1 input words in a continuous vector space and
often referred to as a context vector
This context vector is fed through a composition of nonlinear feature extraction
layers We can for instance apply the simple transformation layer from Eq 38 such
h  tanhWp  b
where W and b are the parameters
Once a set of nonlinear layers has been applied to the context vector its time to
compute the output probability distribution In this case of language modelling the
distribution outputted by the function is a categorical distribution We discussed how
we can build a function to return a categorical distribution already in Sec 312
As a recap a categorical distribution denes a probability of one event happening
among K discrete events The probability of the k-th event happening is often denoted
as k and
k  1
Therefore the function needs to return a K-dimensional vector 1 2     K In this
case of language modelling K  V and i corresponds to the probability of the i-th
word in the vocabulary for the next word
As discussed earlier in Sec 312 we can use softmax to compute each of those
output probabilities
pwn  kw1w2    wn1  k 
k h  ck
expucid62
kcid481 expucid62
kcid48h  ckcid48
where uk  Rdimh
This whole function is called a neural language model See Fig 53 a for the
graphical illustration of neural language model
541 How does Neural Language Model Generalize to Unseen n-
Grams  Distributional Hypothesis
Now that we have described neural language model let us take a look into what hap-
pens inside Especially we will focus on how the model generalizes to unseen n-grams
The previously described neural language model can be thought of as a composite
of two function g  f  The rst stage f projects a sequence of context words or
preceding n 1 words to a continuous vector space
f  01Vn1  Rd
We will call the resulting vector h a context vector The second stage g maps this
continuous vector h to the target word probability by applying afne transformation to
the vector h followed by softmax normalization
Figure 53 a Schematics of neural language model
language model generalizes to an unseen n-gram
b Example of how neural
Let us look more closely at what g does in Eq 514 If we ignore the effect of
the bias ck for now we can clearly see that the probability of the k-th word in the
vocabulary is large when the output vector uk or the k-th row of the output matrix U
is well aligned with the context vector h In other words the probability of the next
word being the k-th word in the vocabulary is roughly proportional to the inner product
between the context vector h and the corresponding target word vector uk
Now let us consider two context vectors h j and hk These contexts are followed by
a similar set of words meaning that the conditional distributions of the next word are
similar to each other Although these distributions are dened over all possibility target
words let us look at the probabilities of only one of the target words wl
cid16
cid16
cid17
cid17
j pwlh j 
k pwlhk 
cid16
k is then16
wcid62
wcid62
cid17
wcid62
l h j  hk
The ratio between pl
j and pl
From this we can clearly see that in order for the ratio pl
to be 1 ie pl
wcid62
l h j  hk  0
16 Note that both pl
j and pl
k are positive due to our use of softmax
1-of-K codingContinuous-spaceWord RepresentationSoftmaxNonlinear projectionthreefourteamsgroupsNow let us assume that wl is not an all-zero vector as otherwise it will be too dull
a case In this case the way to achieve the equality in Eq 515 is to drive the context
vectors h j and hk to each other In other words the context vectors must be similar
to each other in terms of Euclidean distance in order to result in similar conditional
distributions of the next word
What does this mean This means that the neural language model must project
n 1-grams that are followed by the same word to nearby points in the context vec-
tor space while keeping the other n-grams away from that neighbourhood This is
necessary in order to give a similar probability to the same word If two n 1-grams
which are followed by the same word in the training corpus are projected to far away
points in the context vector space it naturally follows from this argument that the prob-
ability over the next word will differ substantially resulting in a bad language model
Let us consider an extreme example where we do bigram modeling with the train-
ing corpus comprising only three sentences
 There are three teams left for the qualication
 four teams have passed the rst round
 four groups are playing in the eld
We will focus on the bold-faced phrases three teams four teams and four group
The rst word of each of these bigrams is a context word and neural language model
is asked to compute the probability of the word following the context word
It is important to notice that neural language model must project three and four
to nearby points in the context space see Eq 513 This is because the context
vectors from these two words need to give a similar probability to the word teams
This naturally follows from our discussion earlier on how dot product preserves the
ordering in the space And from these two context vectors which are close to each
other the model assigns similar probabilities to teams and groups because they
occur in the training corpus In other words the target word vector uteams and ugroups
will also be similar to each other because otherwise the probability of teams given
four pteamsfour and groups given four pgroupsfour will be very differ-
ent despite the fact that they occurred equally likely in the training corpus
Now lets assume the case where we use the neural language model trained on
this tiny training corpus to assign a probability to an unseen bigram three groups
The neural language model will project the context word three to a point in the con-
text space close to the point of four From this context vector the neural language
model will have to assign a high probability to the word groups because the context
vector hthree and the target word vector ugroups well align Thereby even without ever
seeing the bigram three groups the neural language model can assign a reasonable
probability See Fig 53 b for graphical illustration
What this example shows is that neural language model automatically learns the
similarity among different context words via context vectors h and also among dif-
ferent target words via target word vectors uk by exploiting co-occurrences of words
In this example the neural language model learned that four and three are similar
from the fact that both of them occur together with teams Similarly in the target
side the neural language model was able to capture the similarity between teams
and groups by noticing that they both follow a common word four
This is a clear real-world demonstration of the so-called distributional hypothe-
sis Distributional hypothesis states that words which are similar in meaning appear
in similar distributional contexts 41 By observing which words a given word co-
occurs together it is possible to peek into the words underlying meaning Of course
this is only a partial picture17 into the underlying meaning of each word or as a mat-
ter of fact a phrase but surely still a very interesting property that is being naturally
exploited by neural language model
In neural language model the most direct way to observe the effect of this dis-
tributional hypothesisstructure is to investigate the rst layers weight matrix E in
Eq 512 This weight matrix can be considered as a set of dense vectors of the
words in the input vocabularycid8e1e2    eVcid9 and any visualization technique such
as principal component analysis PCA or t-SNE 104 can be used to project each
high-dimensional word vector into a lower-dimensional space often 2-D or 3-D
542 Continuous Bag-of-Words Language Model
Maximum PseudoLikelihood Approach
This is about time someone asks a question why we are only considering the preceding
words when doing language modelling Is it a good assumption that the conditional
distribution over a word is only dependent on preceding words
In fact we do not have to do so We can certainly model a natural language sentence
such that each word in a sentence is conditioned on 2n surrounding words n words to
the left and n words to the right In this case we get a Markov random eld MRF
language model 56
Figure 54 An example Markov random eld language model MRF-LM with the
order n  1
In a Markov random eld MRF language model MRF-LM we say each word in
a given sentence is a random variable wi We connect each word with its 2n surrounding
words with undirected edges and these edges represent the conditional dependency
structure of the whole MRF-LM An example of an MRF-LM with n  1 is shown in
Fig 54
A probability over a Markov random eld is dened as a product of clique po-
tentials A potential is dened for each clique as a positive function whose input is
the values of the random variables in the clique
In the case of MRF-LM we will
assign 1 as a potential to every clique except for cliques of two random variables in
17 We will discuss why this is only a partial picture later on
other words we only use pairwise potentials only The pairwise potential between the
words i and j is dened as
cid16
Ecid62wicid62Ecid62w jcid17
cid16
cid17
ecid62
 wiw j  exp
where E is from Eq 512 and wi is the one-hot vector of the i-th word One must
note that this is one possible implementation of the pairwise potential and there may be
other possibilities such as to replace the dot product between the word vectors ecid62
wiew j
with a deeper network
With this pairwise potential the probability over the whole sentence is dened as
pw1w2    wT  
 wt w j 
cid32Tn
cid33
ecid62
wt ew j
where Z is the normalization constant This normalization constant makes the product
of the potentials to be a probability and often is at the core of computational intractabil-
ity in Markov random elds
Figure 55 Gray nodes indicate the Markov blank of the fourth word
Although compute the full sentence probability is intractable in this MRF-LM it is
quite straightforward to compute the conditional probability of each word wi given all
the other words When computing the conditional probability we must rst notice that
the conditional probability of wi only depends on the values of other words included
in its Markov blanket In the case of Markov random elds the Markov blanket of
a random variable is dened as a set of all immediate neighbours and it implies that
the conditional probability of wi is dependent only on n preceding words and the n
following words See Fig 55 for an example
Keeping this in mind we can easily see that
pwiwin    wi1wi1    win 
Zcid48 exp
where Zcid48 is a normalization constant computed by
cid32
cid32 n
Zcid48  
ecid62
ewik 
cid33cid33
ewik 
cid32 n
cid32
ecid62
cid33cid33
Do you see a stark similarity to neural language model we discussed earlier This
conditional probability is a shallow neural network with a single linear hidden layer
Figure 56 Continuous Bag-of-Words model approximates the conditional distribution
over the j-th word w j under the MRF-LM
whose input are the context words n preceding and n following words and the output
is the conditional distribution of the center word wi We will talk about this shortly in
more depth See Fig 56 for graphical illustration
Now we know that it is often difcult to compute the full sentence probability
pw1    wT  due to the intractable normalization constant Z We however know how
to compute the conditional probabilities for all words quite tractably The former
fact implies that it is perhaps not the best idea to maximize log-likelihood to train this
model18 The latter however sheds a bit of light because we can train a model to
maximize pseudolikelihood 11 instead19
Pseudolikelihood of the MRF-LM is dened as
logPL 
log pwiwin    wi1wi1    win
Maximizing this pseudolikelihood is equivalent to training a neural network in Fig 56
which approximates each conditional distribution pwiwin    wi1wi1    win
to give a higher probability to the ground-truth center word in the training corpus
Unfortunately even after training the model by maximizing the pseudolikelihood
in Eq 516 we do not have a good way to compute the full sentence probability under
this model Under certain conditions maximizing pseudolikelihood indeed converges
to the maximum likelihood solution but this does not mean that we can use the product
of all the conditionals as a replacement of the full sentence probability However this
does not mean that we cannot use this MRF-LM as a language model since given
a xed model the pseudoprobability the product of all the conditionals can score
different sentences
18 However this is not to say maximum likelihood in this case is impossible There are different ways to
approximate the full sentence probability under this model See 56 for one such approach
19 See the note by Amir Globerson later modied by David Sontag available at httpcsnyu
edudsontagcoursesinference14slidespseudolikelihoodnotespdf
1-of-K codingSoftmaxContinuous Bag-of-WordsThis is in contrast to the neural language model we discussed earlier in Sec 54 In
the case of neural language model we were able to compute the probability of a given
sentence by computing the conditional probability of each word reading from left until
the end of the sentence This is perhaps one of the reasons why the MRF-LM is not
often used in practice as a language model Then you must ask why I even bothered
to explain this MRF-LM in the rst place
This approach which was proposed in 79 as a continuous bag-of-words CBoW
model20 was found to exhibit an interesting property That is the word embedding
matrix E learned as a part of this CBoW model very well reects underlying structures
of words and this has become one of the darling models by natural language processing
researchers in recent years We will discuss further in the next section
Skip-Gram and Implicit Matrix Factorization In 79 another model called skip-
gram is proposed The skip-gram model is built by ipping the continuous bag-of-
words model Instead of trying to predict the middle word given 2n surrounding words
the skip-gram model tries to predict randomly chosen one of the 2n surrounding words
given the middle word From this description alone it is quite clear that this skip-gram
model is not going to be great as a language model However it turned out that the
word vectors obtained by training a skip-gram model were as good as those obtained by
either a continuous bag-of-words model or any other neural language model Of course
it is debatable which criterion be used to determine the goodness of word vectors but in
many of the existing so-called intrinsic evaluations those obtained from a skip-gram
model have been shown to excel
The authors of 72 recently showed that training a skip-gram model with negative
sampling see 79 is equivalent to factorizing a positive point-wise mutual informa-
tion matrix PPMI into two lower-dimensional matrices The left lower-dimensional
matrix corresponds to the input word embedding matrix E in a skip-gram model In
other words training a skip-gram model implicitly factorizes a PPMI matrix
Their work drew a nice connection between the existing works on distributional
word representations from natural language processing or even computational linguis-
tics and these more recent neural approaches I will not go into any further detail in
this course but I encourage readers to read 72
543 Semi-Supervised Learning with Pretrained Word Embeddings
One thing I want to emphasize in these language models including n-gram language
model neural language model and continuous bag-of-words model is that they are
purely unsupervised meaning that all we need is a large corpus of unannotated text
This is one thing that makes this statistical approach to language modelling much more
appealing than any other approach based on linguistic structures see Sec 511 for a
brief discussion
20 One difference between the model we derived in this section starting from the MRF-LM and the one
proposed in 79 is that in our derivation the neural network shares a single weight matrix E for both the
input and output layers
When it comes to neural language model and continuous bag-of-words model we
now know that these networks learn continuous vector representations of input words
target words and the context phrase h from Eq 513 We also discussed how these
vector representations encode similarities among different linguistic units be it a word
or a phrase
What this implies is that once we train this type of language model on a large or
effectively innite21 corpus of unlabelled text we get good vectors for those linguistic
units for free Among these word vectors the rows of the input weight matrix E in
Eq 512 have been extensively used in many natural language processing applica-
tions in recent years since 103 33 79
Let us consider an extreme example of classifying each English word as either
positive or negative For instance happy is positive while sad is negative A
training set of 2 examples1 positive and 1 negative words is given How would one
build a classier22
There are two issues here First it is unclear how we should represent the input in
this case a word A good reader who has read this note so far will be clearly ready to
use an one-hot vector and use a softmax layer in the output and I commend you for
that However this still does not solve a more serious issue which is that we have only
two training examples All the word vectors save for two vectors corresponding to the
words in the training set will not be updated at all
One way to overcome these two issues is to make somewhat strong but reasonable
assumption that similar input will have similar sentiments This assumption is at the
heart of semi-supervised learning 23 It says that high-dimensional data points in
effect lies on a lower-dimensional manifold and the target values of the points on this
manifold change smoothly Under this assumption if we can well model this lower-
dimensional data manifold using unlabelled training examples we can train a good
classier23
And guess what We have access to this lower-dimensional manifold which is
represented by the set of pretrained word vectors E Believing that similar words have
similar sentiment and that these pretrained word vectors indeed well reect similarities
among words let me build a simple nearest neighbour NN classier which uses the
pretrained word vectors
cid26 positive
NNw 
if cosewehappy  cosewebad
otherwise
where cos is a cosine similarity dened as
negative
coseie j 
ecid62
cid107eicid107cid107e jcid107 
21 Why Because of almost universal broadband access to the Internet
22 Although the setting of 2 training examples is extreme but the task itself turned out to be not-so-
extreme In fact there is multiple dictionaries of words sentiment maintained For instance check http
sentiwordnetisticnritsearchphpqllama
23 What do I mean by a good classier A good classier is a classier that classies unseen test examples
well See Sec 23
This use of a term similarity almost makes this set of pretrained word vectors
look like some kind of magical wand that can solve everything24 This is however not
true and using pretrained word vectors must be done with caution
Why should we be careful in using these pretrained word vectors We must remem-
ber that these word vectors were obtained by training a neural network to maximize a
certain objective or to minimize a certain cost function This means that these word
vectors capture certain aspects of words underlying structures that are necessary to
achieve the training objective and that there is no reason for these word vectors to
capture any other properties of the words that are not necessary for maximizing the
training objective In other words similarity among multiple words has many dif-
ferent aspects and these word vectors will capture only a few of these many aspects
Which few aspects will be determined by the choice of training objective
The hope is that language modelling is a good training objective that will encourage
the word vectors to capture as many aspects of similarity as possible25 But is this true
in general
Lets consider an example of words describing emotions such as happy sad
and angry in the context of a continuous bag-of-words model These emotion-
describing words often follow some forms of a verb feel such as feel feels
felt and feeling This means that those emotion-describing words will have to be
projected nearby in the context space in order to give a high probability to those forms
of feel as a middle word This is understandable and agrees quite well with our in-
tuition All those emotion-describing words are similar to each other in the sense that
they all describe emotion But wait this aspect of similarity is not going to help sen-
timent classication of words In fact this aspect of similarity will hurt the sentiment
classier because a positive word happy will be close to negative words sad and
angry in this word vector space
The lesson here is that when you are solving a language-related task with very little
data it is a good idea to consider using a set of pretrained word vectors from neural
language models However you must do so in caution and perhaps try to pretrain your
own word vectors by training a neural network to maximize a certain objective that
better suits your nal task
But then what other training objectives are there We will get to that later
55 Recurrent Language Model
Neural language model indeed avoids the lack of generalization in the conventional n-
gram language modeling It still assumes the n-th order Markov property meaning that
it looks only as far back into the past as n 1 words In Sec 53 I gave an example of
In Korea more than half of all the residents speak Korean In this example the con-
ditional distribution over the last word in the sentence clearly will be better estimated
24 For future reference I must say there were many papers claiming that the pretrained word vectors are
indeed magic wands at three top-tier natural language processing conferences ACL EMNLP NAACL in
2014 and 2015
25 Some may ask how a single vector which is a point in a space can capture multiple aspects of similarity
This is possible because these word vectors are high-dimensional
Figure 57
network language model
a A recurrent neural network from Sec 414 b A recurrent neural
if it is conditioned on the second word of the sentence which is more than 10 words
back in the past
Let us recall what we learned in Sec 414 There we learn how to build a recurrent
neural network to read a variable-length sequence and return a variable-length output
sequence An example we considered back then was a task of part-of-speech tagging
where the input is a sentence such as
x  Childreneatsweetcandy
and the target output is a sequence of part-of-speech tags such as
y  nounverbadjectivenoun
In order to make less of an assumption on the conditional independence of the
predicted tags we made a small adjustment such that the prediction Yt at each timestep
was fed back into the recurrent neural network in the next timestep together with the
input Xt1 See Fig 57 a for graphical illustration
Why am I talking about this again after saying that the task of part-of-speech tag-
ging is not even going to be considered as a valid topic for the nal project Because
the very same model for part-of-speech tagging will be turned into the very recurrent
neural network language model in this section
Let us start by considering a single conditional distribution marked a below from
the full sentence probability
pw1w2    wT  
cid124
pwtw1    wt1
cid123cid122
cid125
This conditional probability can be approximated by a neural network as weve been
doing over and over again throughout this course that takes as input w1    wt1 and
returns the probability over all possible words in the vocabulary V  This is not unlike
neural language model we discussed earlier in Sec 54 except that the input is now a
variable-length sequence
Figure 58 A recurrent neural network language model
htcid48
In this case we can use a recurrent neural network which is capable of summariz-
ingmemorizing a variable-length input sequence A recurrent neural network summa-
rizes a given input sequence w1    wt1 into a memory state ht1
cid26 0
f ewtcid48 htcid481
where tcid48 runs from 0 to t  1
f is a recurrent function which can be any of a naive
transition function from Eq 41 a gated recurrent unit or a long short-term memory
unit from Sec 422 ewtcid48 is a word vector corresponding to the word wtcid48
This summary ht1 is afne-transformed followed by a softmax nonlinear function
to compute the conditional probability of wt Hopefully everyone remembers how it is
done As in Eq 46
if tcid48  0
otherwise 
  softmaxVht1
where  is a vector of probabilities of all the words in the vocabulary
One thing to notice here is that the iteration procedure in Eq 517 computes a
sequence of every memory state vector ht by simply reading the input sentence once
In other words we can let the recurrent neural network read one word wt at a time
update the memory state ht and compute the conditional probability of the next word
pwt1wt 
This procedure is illustrated in Fig 57 b26 This language model is called a
recurrent neural network language model RNN-LM 80
But wait from looking at Figs 57 ab there is a clear difference between
the recurrent neural networks for part-of-speech tagging and language model That is
there is no feedback connection from the output of the previous time step back into the
recurrent neural network in the RNN-LM This is simply an illusion from the limitation
in the graphical illustration because the input wt1 in the next time step is in fact the
output wt1 at the current time step This becomes clearer by drawing the same gure
in a slightly different way as in Fig 58
26 In the gure you should notice the beginning-of-the-sentence symbol cid104scid105 This is necessary in order to
use the very same recurrent function f to compute the conditional probability of the rst word in the input
sentence
56 How do n-gram language model neural language
model and RNN-LM compare
Now the question is which one of these language models we should use in practice In
order to answer this we must rst discuss the metric most commonly used for evaluat-
ing language models
The most commonly used metric is a perplexity In the context of language mod-
elling the perplexity PPL of a model M is computed by
n1 logb pM wnwn
PPL  b 1
where N is the number of all the words in the validationtest corpus and b is some
constant that is often 2 or 10 in practice
What is this perplexed metric I totally agree with you on this one Of course there
is a quite well principled way to explain what this perplexity is based on information
theory This is however not necessary for us to understand this metric called perplexity
As the exponential function with base b in the case of perplexity in Eq 518
is a monotonically increasing function we see that the ordering of different language
models based on the perplexity will not change even if we only consider the exponent
logb pM wnwn
Furthermore assuming that b  1 we can simply replace logb with log natural loga-
rithm without changing the order of different language models
log pM wnwn
Now this looks awfully similar to the cost function or negative log-likelihood we
minimize in order to train a neural network see Chapter 2
Lets take a look at a single term inside the summation above
log pM wnwn
This is simply measuring how high a probability the language model M is assigning to
a correct next word given all the previous words Again because log is a monotonically
increasing function
In summary the inverse perplexity measures how high a probability the language
model M assigns to correct next words in the testvalidation corpus on average There-
fore a better language model is the one with a lower perplexity There is nothing so
perplexing about the perplexity once we start viewing it from this perspective
We are now ready to compare different language models or to be more precise
three different classes of language modelscount-based n-gram language model neural
n-gram language model and recurrent neural network language model The biggest
challenge in doing so is that this comparison will depend on many factors that are not
easy to control To list a few of them
 Language
 GenreTopic of training validation and test corpora
 Size of a training corpus
 Size of a language model
Figure 59 The perplexity word error rate WER and character error rate CER of
an automatic speech recognition system using different language models Note that all
the results by neural or recurrent language models are by interpolating these models
with the count-based n-gram language model Reprinted from 100
Because of this difculty this kind of comparison has often been done in the con-
text of a specic downstream application This choice of a downstream application
often puts rough constraints on the size of available or commonly used corpus target
language and reasonably accepted size of language models For instance the authors
of 3 compared the conventional n-gram language model and neural language model
with various approximation techniques with machine translation as a nal task In
100 the authors compared all the three classes of language model in the context of
automatic speech recognition
First let us look at one observation made in 100 From Fig 59 we can see that
it is benecial to use a recurrent neural network language model RNN-LM compared
to a usual neural language model Especially when long short-term memory units were
Figure 510 The trend of perplexity as the size of language model changes Reprinted
from 100
used the improvement over the neural language model was signicant Furthermore
we see that it is possible to improve these language models by simply increasing their
Similarly in Fig 510 from the same paper 100 it is observed that larger language
models tend to get betterlower perplexity and that RNN-LM in general outperforms
neural language models
These two observations do seem to suggest that neural and recurrent language mod-
els are better candidates as language model However this is not to be taken as an
evidence for choosing neural or recurrent language models It has been numerously
observed over years that the best performance both in terms of perplexity and in terms
of performance in the downstream applications such as machine translation and auto-
matic speech recognition is achieved by combining a count-based n-gram language
model and a neural or recurrent language model See for instance 92
This superiority of combined or hybrid language model suggests that the count-
based or conventional n-gram language model neural language model and recurrent
neural network language model are capturing underlying structures of natural language
sentences that are complement to each other However it is not crystal clear how these
captured structures differ from each other
Chapter 6
Neural Machine Translation
Finally we have come to the point in this course where we discuss an actual natural
language task In this chapter we will discuss how translation from one language to
another can be done with statistical methods more specically neural networks
61 Statistical Approach to Machine Translation
Lets rst think of what it means to translate one sentence X in a source language to an
equivalent sentence Y in a target language which is different from the source language
A process of translation is a function that takes as input the source sentence X and
returns a correct translation Y  and it is clear that there may be more than one correct
translations The latter fact implies that this function of translation should return not a
single correct translation but a probability distribution that assigns high probabilities
to more than one likely translations
Now let us write it in a more formal way First the input is a sequence of words
where Tx is the length of the source sentence A target sentence is
X  x1x2    xTx 
Y  y1y2    yTy
Similarly Ty is the length of the target sentence
The translation function f then reads the input sequence X and computes the prob-
ability over target sentences In other words
f  V 
x  CVy1
is a set of all possible source sentences of any
where Vx is a source vocabulary and V 
length Tx  0 Vy is a target vocabulary and Ck is a standard k-simplex
What is a standard k-simplex It is a set dened by
cid12cid12cid12cid12cid12 k
cid40
cid41
t0    tk  Rk1
tk  1 and ti  0 for all i
Figure 61 Graphical illustration of statistical machine translation
In short this set contains all possible settings for categorical distributions of k  1
possible outcomes This means that the translation function f returns a probability
distribution PYX over all possible translations of length Ty  1
Given a source sentence X this translation function f returns the conditional prob-
ability of a translation Y  PYX Let us rewrite this conditional probability according
to what we have discussed in Chapter 5
PYX 
cid124
Pyty1    yt1
Xcid124cid123cid122cid125
conditional
cid125
cid123cid122
language modelling
Looking at it in this way it is clear that this is nothing but conditional language mod-
elling This means that we can use any of the techniques we have used earlier in
Chapter 5 for statistical machine translation
Training can be trivially done by maximizing the log-likelihood or equivalently
minimizing the negative log-likelihood see Sec 31
given a training set
C    1
log pyn
t X n
D cid8X 1Y 1 X 2Y 2     X NY Ncid9
consisting of N training pairs
All these look extremely straightforward and do not deviate too much from what we
have learned so far in this course A big picture on this process translation is shown in
Fig 61 More specically building a statistical machine translation model is simple
because we have learned how to
1 Assign a probability to a sentence in Sec 52
2 Handle variable-length sequences with recurrent neural networks in Sec 41
3 Compute the gradient of an empirical cost function C with respect to the param-
eters  of a recurrent neural network in Sec 412 and Sec 34
Corporaf  La croissance conomique sest ralentie ces dernires annes e  Economic growth has slowed down in recent years 4 Use stochastic gradient descent to minimize the cost function in Sec 222
Of course simply knowing all these does not get you a working neural network that
translates from one language to another We will discuss in detail how we can build
such a neural network in the next section Before going to the next section we must
rst discuss two issues 1 where do we get training data 2 how do we evaluate
machine translation systems
611 Parallel Corpora Training Data for Machine Translation
First let us consider again what the problem were trying to solve here It is machine
translation and from the description in the previous section and from Eqs 6162
it is a sentence-to-sentence translation task We approach this problem by building a
model that takes as input a source sentence S and computes the probability PYX of
a target sentence Y  equivalently a translation In order for this model to translate we
must train it with a training set of pairs of a source sentence and its correct translation
The very rst problem we run into is where we can nd this training set which is
often called a parallel corpus It is not easy to think of documents which have been
translated into multiple languages Lets take for instance all the books that are being
translated each year According to 86 approximately 3 of titles published each year
in English are translations from another language1 A few international news agencies
publish some of their news articles in multiple languages For instance AFP publishes
1500 stories in French 700 stories in English 400 stories in Spanish 250 stories in
Arabic 200 stories in German and 150 stories in Portuguese each day and there are
some overlapping stories across these six languages2 Online commerce sites such as
eBay often list their products in international sites with their descriptions in multiple
languages3
Unfortunately these sources of multiple languages of the same content are not suit-
able for our purpose Why is this so Most importantly they are often copy-righted
and sold for personal use only We cannot buy more than 14400 books in order to
train a translation model We will likely go broke before completing the purchase
and even if so it is unclear whether it is acceptable under copyright to use these text
to build a translation model Because we are mixing multiple sources of which each
is protected under copyright is the translation model trained from a mix of all these
materials considered a derivative work4
This issue is nothing new and has been there since the very rst statistical machine
translation system was proposed in 19 Fortunately it turned out that there are a
number of legitimate sources where we can get documents translated in more than
one languages often very faithfully to their content These sources are parliamentary
proceedings of bilingual or multilingual countries
1 According to the information Bowker released in October of 2005 in 2004 there were 375000 new
books published in English  Of that total approx 14440 were new translations which is slightly more
than 3 of all books published 86
2 httpwwwafpcomenproductsservicestext
3 httpsellercentreebaycoukinternational-selling-tools
4 httpcopyrightgovcircscirc14pdf
Brown et al 19 used the proceedings from the Canadian parliament which are by
law kept in both French and English All of these proceedings are digitally available
and called Hansards You can check it yourself online at httpwwwparlgc
ca and heres an excerpt from the Prayers of the 2nd Session 41st Parliament Issue
 French ELIZABETH DEUX par la Grace de Dieu REINE du Royaume-
Uni du Canada et de ses autres royaumes et territoires Chef du Commonwealth
Defenseur de la Foi
 English ELIZABETH THE SECOND by the Grace of God of the United
Kingdom Canada and Her other Realms and Territories QUEEN Head of the
Commonwealth Defender of the Faith
Every single word spoken in the Canadian parliament is translated either into French
or into English A more recent version of Hansards preprocessed for research can be
found at httpwwwisiedunatural-languagedownloadhansard
Similarly the European parliament used to provided the parliamentary proceedings
in all 23 ofcial languages6 This is a unique data in the sense that each and every
sentence is translated into either 11 or 26 ofcial languages For instance here is one
example 65
 Danish det er nsten en personlig rekord for mig dette efterar
 German das ist fur mich fast personlicher rekord in diesem herbst 
 Greek omitted
 English that is almost a personal record for me this autumn 
 Spanish es la mejor marca que he alcanzado este otono 
 Finnish se on melkein minun ennatykseni tana syksyna 
 French c  est pratiquement un record personnel pour moi  cet automne 
 Italian e  quasi il mio record personale dell  autunno 
 Dutch dit is haast een persoonlijk record deze herfst 
 Portuguese e quase o meu recorde pessoal deste semestre 
 Swedish det ar nastan personligt rekord for mig denna host 
The European proceedings has been an invaluable resource for machine translation
research At least the existing multilingual proceedings up to 2011 can be still used
and it is known in the eld as the Europarl corpus 65 and can be downloaded from
httpwwwstatmtorgeuroparl
These proceedings-based parallel corpora have two distinct advantages First in
many cases the sentences in those corpora are well-formed and their translations are
5 This is one political lesson here Canada is still headed by the Queen of the United Kingdom
6 Unfortunately the European parliament decided to stop translating its proceedings into all 23 of-
cial languages on 21 Nov 2011 as an effort toward budget cut See httpwwweuractivcom
cultureparliament-cuts-translation-budg-news-516201
done by professionals meaning the quality of the corpora is guaranteed Second sur-
prisingly the topics discussed in those proceedings are quite diverse Clearly the mem-
bers of the parliament do not often chitchat too often but they do discuss a diverse set
of topics Heres one such example from the Europarl corpus
 English Although there are now two Finnish channels and one Portuguese one
there is still no Dutch channel which is what I had requested because Dutch
people here like to be able to follow the news too when we are sent to this place
of exile every month
 French Il y a bien deux chanes nnoises et une chane portugaise mais il
ny a toujours aucune chane neerlandaise Pourtant je vous avais demande une
chane neerlandaise car les Neerlandais aussi desirent pouvoir suivre les actu-
alites chaque mois lorsquils sont envoyes en cette terre dexil
One apparent limitation is that these proceedings cover only a handful of languages
in the world mostly west European languages This is not desirable Why According
to Ethnologue 20147 the top-ve most spoken languages in the world are
1 Chinese approx 12 billion
2 Spanish approx 414 million
3 English approx 335 million
4 Hindi approx 260 million
5 Arabic approx 237 million
There are only two European languages in this list
So then where can we get all data for all these non-European languages There
are a number of resources you can use and let me list a few of them here
You can nd the translated subtitle of the TED talks at the Web Inventory of
Transcribed and Translated Talks WIT3 httpswit3fbkeu 22
a quite small corpus but includes 104 languages For RussianEnglish data Yandex
released a parallel corpus of one million sentence pairs You can get it at https
translateyandexrucorpuslangen You can continue with other
languages by googling very hard but eventually you run into a hard wall
This hard wall is not only the lack of any resource but also lack of enough resource
For instance I quickly googled for KoreanEnglish parallel corpora and found the
following resources
 SWRC English-Korean multilingual corpus 60000 sentence pairs http
semanticwebkaistackrhomeindexphpCorpus10
 Jungyeuls English-Korean parallel corpus 94123 sentence pairs https
githubcomjungyeulkorean-parallel-corpora
This is just not large enough
One way to avoid this or mitigate this problem is to automatically mine parallel
corpora from the Internet There have been quite some work in this direction as a way
7 httpwwwethnologuecomworld
to increase the size of parallel corpora 87 112 The idea is to build an algorithm that
crawls the Internet and nd a pair of corresponding pages in two different languages
One of the largest preprocessed corpus of multiple languages from the Internet is the
Common Crawl Parallel Corpus created by Smith et al 98 available at http
wwwstatmtorgwmt13training-parallel-commoncrawltgz
612 Automatic Evaluation Metric
Lets say we have trained a machine translation model on a training corpus A big
question follows how do we evaluate this model
In the case of classication evaluation is quite straightforward All we need to do is
to classify held-out test examples with a trained classier and see how many examples
were correctly classied This is however not true in the case of translation
There are a number of issues but let us discuss two most important problems here
First there may be many correct translations given a single source sentence For in-
stance the following three sentences are the translations made by a human translator
given a single Chinese sentence 82
 It is a guide to action that ensures that the military will forever heed Party com-
 It is the guiding principle which guarantees the military forces always being
under the command of the Party
 It is the practical guide for the army always to heed the directions of the party
They all clearly differ from each other although they are the translations of a single
source sentence
Second the quality of translation cannot be measured as either success or failure
It is rather a smooth measure between success and failure Let us consider an English
translation of a French sentence Jaime un llama qui est un animal mignon qui vit en
Amerique du Sud8
One possible English translation of this French sentence is I like a llama which is a
cute animal living in South America Lets give this translation a score 100 success
According to Google translate the French sentence above is I like a llama a cute
animal that lives in South America I see that Google translate has omitted qui est
from the original sentence but the whole meaning has well been captured Let us give
this translation a slightly lower score of 90
Then how about I like a llama from South America This is certainly not a
correct translation but except for the part about a llama being cute this sentence does
communicate most of what the original French sentence tried to communicate Maybe
we can give this translation a score of 50
How about I do not like a llama which is an animal from South America This
translation correctly describes the characteristics of llama exactly as described in the
source sentence However this translation incorrectly states that I do not like a llama
when I like a llama according to the original French sentence What kind of score
would you give this translation
8 I would like to thank Laurent Dinh for the French translation
Even worse we want an automated evaluation algorithm We cannot look at thou-
sands of validation or test sentence pairs to tell how well a machine translation model
does Even if we somehow did it for a single model in order to compare this translation
model against others we must do it for every single machine translation model under
comparison We must have an automatic evaluation metric in order to efciently test
and compare different machine translation models
BLEU One of the most widely used automatic evaluation metric for assessing the
quality of translations is BLEU proposed in 82 BLEU computes the geometric mean
of the modied n-gram precision scores multiplied by brevity penalty Let me describe
this in detail here
First we dene the modied n-gram precision pn of a translation Y as
SC ngramS cngram
SC ngramS cngram
where C is a corpus of all the sentencestranslations and S is a set of all unique n-grams
in one sentence in C cngram is the count of the n-gram and cngram is
cngram  mincngramcrefngram
crefngram is the count of the n-gram in reference sentences
What does this modied n-gram precision measure It measures the ratio between
the number of n-grams in the translation and the number of those n-grams actually
occurred in a reference ground-truth translation If there is no n-gram from the trans-
lation in the reference this modied precision will be zero because cref will be zero
all the time
It is common to use the geometric average of modied 1- 2- 3- and 4-gram preci-
sions which is computed by
cid33
cid32
1  exp
If we use this geometric average P as it is there is a big loophole One can get
a high average modied precision by making as short a translation as possible For
instance a reference translation is
 I like a llama a cute animal that lives in South America 
and a translation we are trying to evaluate is
 cute animal that lives
This is clearly a very bad translation but the modied 1- 2- 3- and 4-gram precisions
will be high The modied precisions are
1  1  1  1
1  1  1  1
1  1  1
1  1  1
cid19
0  0  0  0
Their geometric average is then
1  exp
cid181
which is the maximum modied precision you can get
In order to avoid this behaviour BLEU penalizes the geometric average of the
modied n-gram precisions by the ratio of the lengths between the reference r and
translation l This is done by rst computing a brevity penalty
cid26 1
expcid01 r
cid1
 if l  r
 if l  r
If the translation is longer than the reference it uses the geometric average of the
modied n-gram precisions as it is Otherwise it will penalize it by multiplying the
average precision with a scalar less than 1 In the case of the example above the brevity
penalty is 0064 and the nal BLEU score is 0064
Figure 62 a BLEU vs bilingual and monolingual judgements of three machine
translation systems S1 S2 and S3 and two humans H1 and H2 Reprinted from
b BLEU vs human judgement adequacy and uency separately of three
machine translation systems two statistical and one rule-based systems Reprinted
from 20
The BLEU was shown to correlate well with human judgements in the original
article 82 Fig 62 a shows how BLEU correlates with the human judgements in
comparing different translation systems
This is however not to be taken as a message saying that the BLEU is the perfect
automatic evaluation metric
It has been shown that the BLEU is only adequate in
comparing two similar machine translation systems but not too much so in comparing
two very different systems For instance Callison-Burch et al 20 observed that the
BLEU underestimates the quality of the machine translation system that is not a phrase-
based statistical system See Fig 62 b for an example
BLEU is denitely not a perfect metric and many researchers strive to build a better
evaluation metric for machine translation systems Some of the alternatives available
at the moment are METEOR 36 and TER 99
62 Neural Machine Translation
Simple Encoder-Decoder Model
From the previous section and from Eq 62 it is clear that we need to model each
conditional distribution inside the product as a function This function will take as
input all the previous words in the target sentence Y  y1    yt1 and the whole
source sentence X  x1    xTx  Given these inputs the function will compute the
probabilities of all the words in the target vocabulary Vy In this section I will describe
an approach that was proposed multiple times independently over 17 years in 43 28
Let us start by tackling how to handle the source sentence X  x1    xTx  Since
this is a variable-length sequence we can readily use a recurrent neural network from
Chapter 4 However unlike the previous examples there is no explicit targetoutput in
this case All we need is a vector summary of the source sentence
We call this recurrent neural network an encoder as it encodes the source sentence
into a continuous vector code It is implemented as
ht1Ecid62
ht  enc
cid16
cid17
As usual enc can be any recurrent activation function but it is highly recommended to
use either gated recurrent units see Sec 422 or long short-term memory units see
Sec 423 Ex  RVxd is an input weight matrix containing word vectors as its rows
see Eq 512 in Sec 54 and xt is an one-hot vector representation of the word xt
see Eq 510 in Sec 54 h0 is initialized as an all-zero vector
After reading the whole sentence up to xTx the last memory state hTx of the encoder
summarizes the whole source sentence into a single vector as shown in Fig  a
Thanks to this encoder we can now work with a single vector instead of a whole
sequence of source words Let us denote this vector as c and call it a context vector
We now need to design a decoder again using a recurrent neural network As I
mentioned earlier the decoder is really nothing but a language model except that it is
conditioned on the source sentence X What this means is that we can build a recurrent
neural network language model from Sec 55 but feeding also the context vector at
each time step In other words
zt  dec
zt1
Ecid62
y yt1c
cid105cid17
cid16
cid104
Figure 63 a The encoder and b the decoder of a simple neural machine translation
Do you see the similarity and dissimilarity to Eq 517 from Sec 55 Its essentially
same except that the input at time t is a concatenated vector of the word vector of the
previous word yt1 and the context vector c
Once the decoders memory state is updated we can compute the probabilities of
all possible target words by
cid16
cid17
pyt  wcid48yt X  exp
ecid62
wcid48zt
where ewcid48 is the target word vector associated the word wcid48 This is equivalent to afne-
transforming zt followed by a softmax function from Eq 35 from Sec 31
Now should we again initialize z0 to be an all-zero vector Maybe or maybe not
One way to view what this decoder does is that the decoder models a trajectory in
a continuous vector space and each point in the trajectory is zt Then z0 acts as a
starting point of this trajectory and it is natural to initialize this starting point to be a
point relevant to the source sentence Because we have access to the source sentences
content via c we can again use it to initialize z0 as
z0  init c 
See Fig 63 b for the graphical illustration of the decoder
Although I have used c as if it is a separate variable this is not true c is simply
a shorthand notation of the last memory state of the encoder which is a function of
the whole source sentence What does this mean It means that we can compute the
gradient of the empirical cost function in Eq 63 with respect to all the parameters of
both the encoder and decoder and maximize the cost function using stochastic gradient
descent just like any other neural network we have learned so far in this course
621 Sampling vs Decoding
Sampling We are ready to compute the conditional distribution PYX over all pos-
sible translations given a source sentence When we have a distribution the rst thing
we can try is to sample from this distribution Often it is not straightforward to gen-
erate samples from a distribution but fortunately in this case we can readily generate
exact samples from the distribution PYX
We simply iterate over the following steps until a token indicating the end of a
sentence cid104eoscid105
1 Compute c Eq 65
2 Initialize z0 with c Eq 68
3 Compute zt given zt1 yt1 and c Eq 66
4 Compute pytyt X Eq 67
5 Sample yt from the compute distribution
6 Repeat 35 until yt  cid104eoscid105
After taking these steps we get a sample Y 
cid16
cid17
y1     y Y
given a source sentence
X Of course there is no guarantee that this will be a good translation of X In order to
nd a good translation meaning a translation with a high probability P YX we need
to repeatedly sample multiple translations from PYX and choose one with the high
probability
This is not too desirable as it is not clear how many translations we need to sample
from PYX and also it will likely be computationally expensive We must wonder
whether we can solve the following optimization problem directly
Y  argmax
logPYX
Unfortunately the exact solution to this requires evaluating PYX for every possible
Y  Even if we limit our search space of Y to consist of only sentences of length up
to a nite number it will likely become too large the cardinality of the set grows
exponentially with respect to the number of words in a translation Thus it only
makes sense to solving the optimization problem above approximately
Approximate Decoding Beamsearch Although it is quite clear that nding a trans-
lation Y that maximizes the log-probability logP YX is extremely expensive we will
regardlessly try it here
One very natural way to enumerate all possible target sentences and simultaneously
computing the log-probability of each and every one of them is to start from all possible
rst word compute the probabilities of them and from each potential rst word branch
into all possible second words and so on This procedure forms a tree and any path
from the root of this tree to any intermediate node is a valid but perhaps very unlikely
sentence See Fig 64 for the illustration The conditional probabilities of all these
paths or sentences can be computed as we expand this tree down by simply following
Eq 62
Of course we cannot compute the conditional probabilities of all possible sen-
tences Hence we must resort to some kind of approximate search Wait search Yes
this whole procedure of nding the most likely translation is equivalent to searching
through a space in this case a tree of all possible sentences for one sentence that has
the highest conditional probability
Figure 64 a Search space depicted as a tree b Greedy search
The most basic approach to approximately searching for the most likely translation
is to choose only a single branch at each time step t In other words
yt  argmax
wcid48V
log pyt  wcid48 yt X
where the conditional probability is dened in Eq 67 and yt   y1 y2     yt1 is
a sequence of greedily-selected target words up to the t  1-th step This procedure
is repeated until the selected yt is a symbol corresponding to the end of the translation
often denoted as cid104eoscid105 See Fig 64 b for illustration
There is a big problem of this greedy search That is as soon as it makes one
mistake at one time step there is no way for this search procedure to recover from this
mistake This happens because the conditional distributions at later steps depend on
the choices made earlier
Consider the following two sequences w1w2 and wcid48
1w2 These sequences
probabilities are
pw1w2  pw1pw2w1
pwcid48
1pw2wcid48
1w2  pwcid48
Lets assume that
where 0    1 meaning that pw1  pwcid48
choose w1 over wcid48
1 and ignore wcid48
 pw1  pwcid48
Now we can see that theres a problem with this Lets assume that
pw2wcid48
 pw2w1  pw2wcid48
1  pw2w1 
1 In this case the greedy search will
where  was dened earlier In this case
pw1w2 pw1pw2w1   pwcid48
1  pwcid48
 pwcid48
pw2wcid48
1pw2w1
1pw2wcid48
1  pwcid48
In short
It means that the sequence wcid48
algorithm is unable to notice this because simply pw1  pwcid48
pw1w2  pwcid48
1w2 is more likely than w1w2 but the greedy search
Unfortunately the only way to completely avoid this undesirable situation is to
consider all the possible paths starting from the very rst time step This is exactly the
reason why we introduced the greedy search in the rst place but the greedy search
is too greedy The question is then whether there is something in between the exact
search and the greedy search
Beam Search Let us start from the very rst position t  1 First we compute the
conditional probabilities of all the words in the vocabulary
py1  wX for all w  V
Among these we choose the K most likely words and initialize the K hypotheses
1 w1
2     w1
We use the subscript to denote the hypothesis and the subscript the time step As an
example w1
1 is the rst hypothesis at time step 1
For each hypothesis we compute the next conditional probabilities of all the words
in the vocabulary
py2  wy1  w1
i X for all w  V
where i  1    K We then have K V candidates with the corresponding probabil-

cid124
cid125
   
   
cid123cid122
   
Figure 65 Beam search with the beam width set to 3
Among these K V candidates we choose the K most likely candidates
1 w1
2     w1
Starting from these K new hypotheses we repeat the process of computing the proba-
bilities of all K V possible candidates and choosing among them the K most likely
new hypotheses
It should be clear that this procedure called beam search and shown in Fig 65
becomes equivalent to the exact search as K   Also when K  1 this procedure is
equivalent to the greedy search In other words this beam search interpolates between
the exact search which is computationally intractable but exact and the greedy search
which is computationally very cheap but probably quite inexact by changing the size
K of hypotheses maintained throughout the search procedure
How do we choose K One might mistakenly think that we can simply use as large
K as possible given the constraints on computation and memory Unfortunately this is
not necessarily true as this interpolation by K is not monotonic That is the quality of
the translation found by the beam search with a larger K is not necessarily better than
the translation found with a smaller K
Let us consider the case of vocabulary having three symbols abc and any valid
translation being of a length 3 In the rst step we have
pa  05 pb  015 pc  045
In the case of K  1 ie greedy search we choose a If K  2 we will keep a and
Given a as the rst symbol we have
paa  04 pba  03 pca  03
in which case we keep aa with K  1 With K  2 we should check also
pac  045 pbc  045 pcc  01
from which we maintain the hypotheses ca and cb 045 045 and 045 045
respectively Note that with K  2 we have discarded aa
Now the greedy search ends by computing the last conditional probabilities
paaa  09 pbaa  005 pcaa  005
The nal verdict from the greedy search is therefore aaa with its probability being
05 04 09  018
What happens with the beam search having K  2 We need to check the following
conditional probabilities
paca  07 pbca  02 pcca  01
pacb  04 pbcb  00 pccb  06
From here we consider caa and cbc with the corresponding probabilities 045
045 07  014175 and 045 045 06  01215 Among these two caa is
nally chosen due to its higher probability than that of cbc
In summary the greedy search found aaa whose probability is
paaa  018
and the beam search with K  2 found caa whose probability is
pcaa  014175
Even with a larger K the beam search found a worse translation
Now clearly what one can do is to set the maximum beam width K and try with
all possible 1  K  K Among the translations given by K beam search procedures
the best translation can be selected based on their corresponding probabilities From
the point of view of computational complexity this is perhaps the best approach to
upper-bound the worst-case memory consumption Doing the beam search once with
K or multiple beam searches with K  1     K are equivalent in terms of memory con-
sumption ie both are OKV Furthermore the worst-case computation is OKV
assuming a constant time computation for computing each conditional probability In
practice however the constant in front of KV does matter and we often choose K
based on the translation quality of the validation set after trying a number of values
124816
If youre interested in how to improve beam search by backtracking so that the
beam search becomes complete refer to eg 44 113 If youre interested in general
search strategies refer to 90 Also in the context of statistical machine translation it
is useful to read 64
63 Attention-based Neural Machine Translation
One important property of the simple encoder-decoder model for neural machine trans-
lation from Sec 62 is that a whole source sentence is compressed into a single real-
valued vector c This sounds okay since the space of all possible source sentences is
countable while the context vector space 11d is uncountable There exists a map-
ping from this sentence space to the context vector space and all we need to ensure is
that training the simple encoder-decoder model nds this mapping This is conditioned
on the assumption that the hypothesis space9 dened by the model architecturethe
number of hidden units and parameters includes this mapping from any source sen-
tence to a context vector
Unfortunately considering the complexity of any natural language sentence it is
quite easy to guess that this mapping must be highly nonlinear and will require a huge
encoder and consequently a huge decoder to map back from a context vector to a target
sentence In fact this fact was empirically validated last year 2014 when the almost
identical models from two groups 101 27 showed vastly different performances on
the same EnglishFrench translation task The only difference there was that the au-
thors of 101 used a much larger model than the authors of 27 did
At a more fundamental level theres a question of whether a natural language sen-
tence should be fully represented as a single vector For instance there is now a famous
quote by Prof Raymond Mooney10 of the University of Texas at Austin You cant
cram the meaning of a whole  sentence into a single  vector11 Though
our goal is not in answering this fundamental question from linguistics
Our goal is rather to investigate the possibility of avoiding this situation of having
to learn a highly nonlinear complex mapping from a source sentence to a single vector
The question we are more interested in is whether there exists a neural network that
can handle a variable-length sentence by building a variable-length representation of
it Especially we are interested in whether we can build a neural machine translation
system that can exploit a variable-length context representation
Variable-length Context Representation In the simple encoder-decoder model a
source sentence regardless of its length was mapped to a single context vector by a
recurrent neural network
cid16
ht  enc
cid17
ht1Ecid62
See Eq 65 and the surrounding text for more details
Instead here we will encode a source sentence X  x1x2    xTx  with a set C of
context vectors hts This is achieved by having two recurrent neural networks rather
than a single recurrent neural networks as in the simple encoder-decoder model The
rst recurrent neural network to which we will refer as a forward recurrent neural
network reads the source sentence as usual and results in a set of forward memory
9 See Sec 232
10 httpswwwcsutexasedumooney
11 httpnlpersblogspotcom201409amr-not-semantics-but-close-maybe
Figure 66 An encoder with a bidirectional recurrent neural network
h t for t  1    Tx The second recurrent neural network a reverse recurrent
neural network reads the source sentence in a reverse order starting from xTx to x1
h t for t 
This reverse network will output a sequence of reverse memory states
1    Tx
For each xt we will concatenate
h t and
cid35
cid34 
h t to form a context-dependent vector ht
h t
We will form a context set with these context-dependent vectors c  h1h2    hTx
See Fig 66 for the graphical illustration of this process
Now why is ht a context-dependent vector We should look at what the input was
to a function that computed ht The rst half of ht
cid16
h t  fenc
h t was computed by
cid17
cid17
x xt1
Ecid62
h t was
cid16
cid105cid62
where fenc is a forward recurrent activation function From this we see that
computed by all the source words up to t ie xt Similarly
Ecid62
h t depends on all the source
h t  renc
cid17
cid17
where renc is a reverse recurrent activation function and
words from t to the end ie xt
h cid62
In summary ht 
h cid62
cid104
is a vector representation of the t-th word xt with
respect to all the other words in the source sentence This is why ht is a context-
dependent representation But then what is the difference among all those context-
dependent representations h1    hTx We will discuss this shortly
cid105cid17cid104
Decoder with Attention Mechanism Before anything let us think of what the mem-
ory state zt of the decoder from Eq 66 does
cid105cid17
cid105cid17
cid16
cid16
cid104
cid104
Ecid62
y yt3c
Ecid62
y yt2c
Ecid62
y yt1c
zt  dec
cid16 
cid16 Ecid62
cid16 Ecid62
Figure 67 Illustration of how the relevance score e23 of the second context vector h2
at time step 3 dashed curves and box
It is computed based on all the generated target words so far  y1 y2     yt1 and
the context vector12 c which is the summary of the source sentence The very reason
why I designed the decoder in this way is so that the memory state zt is informative of
which target word should be generated at time t after generating the rst t  1 target
words given the source sentence In order to do so zt must encode what have been
translated so far among the words that are supposed to be translated which is encoded
in the context vector c Lets keep this in mind
In order to compute the new memory state zt with a context set C h1h2    hTx
we must rst get one vector out of Tx context vectors Why is this necessary Because
we cannot have an innitely large number of parameters to cope with any number of
context vectors Then how can we get a single vector from an unspecied number of
context vectors hts
First let us score each context vector h j  j  1    Tx based on how relevant it is
for translating a next target word This scoring needs to be based on 1 the previous
memory state zt1 which summarizes what has been translated up to the t  2-th
word13 2 the previously generated target word yt1 and 3 the j-th context vector
e jt  fscorezt1Ecid62
y yt1h j
Conceptually the score e jt will be computed by comparing zt1 yt1 with the con-
text vector c j See Fig 67 for graphical illustration
12 We will shortly switch to using a context set instead
13 Think of why this is only up to the t  2-th word not up to the t  1-th one
Figure 68 Computing the new memory state zt of the decoder based on the previous
memory state zt1 the previous target word yt1 and the weighted average of context
vectors according to the attention weights
Once the scores for all the context vectors h js  j  1    Tx are computed by
fscore we normalize them with a softmax function
expe jt 
jcid481 expe jcid48t 
 jt 
We call these normalized scores the attention weights as they correspond to how much
the decoder attends to each of the context vectors This whole process of computing
the attention weights is often referred to as an attention mechanism see eg 26
We take the weighted average of the context vectors with these attention weights
 jth j
This weighted average is used to compute the new memory state zt of the decoder
which is identical to the decoders update equation from the simple encoder-decoder
model see Eq 66 except that ct is used instead of c a in the equation below
zt1
Ecid62
y yt1 ctcid124cid123cid122cid125


zt  dec
See Fig 68 for the graphical illustration of how it works
Given the new memory state zt of the decoder the output probabilities of all the
target words in a vocabulary happen without any change from the simple encoder-
decoder model in Sec 62
We will call this model which has a bidirectional recurrent neural network as an en-
coder and a decoder with the attention mechanism an attention-based encoder-decoder
model This approach was proposed last year 2014 in the context of machine transla-
tion in 2 and has been studied extensively in 76
631 What does the Attention Mechanism do
One important thing to notice is that this attention-based encoder-decoder model can be
reduced to the simple encoder-decoder model easily This happens when the attention
mechanism fscore in Eq 610 returns a constant regardless of its input When this
happens the context vector ct at each time step t see Eq 612 is same for all the
time steps t  1    Ty
The encoder effectively maps the whole input sentence into a single vector which was
at the core of the simple encoder-decoder model from Sec 62
This is not the only situation in which this type of behaviour happens Another
h 1 of
possible scenario is for the encoder to make the last memory states
the forward and reverse recurrent neural networks to have a special mark telling that
these are the last states The attention mechanism then can exploit this to assign a large
score to these two memory states but still constant across time t This will become
even closer to the simple encoder-decoder model
h Tx and
The question is how we can avoid these degenerate cases Or is it necessary for us
to explicitly make these degenerate cases unlikely Of course there is no single answer
to this question Let me give you my answer which may differ from others answer
The goal of introducing a novel network architecture is to guide a model according
to our intuition or scientic observation so that it will do a better job at a target task In
our case the attention mechanism was introduced based on our observation and some
intuition that it is not desirable to ask the encoder to compress a whole source sentence
into a single vector
This incorporation of prior knowledge however should not put a hard constraint
We give a model a possibility of exploiting this prior knowledge but should not force
the model to use this prior knowledge exclusively As this prior knowledge based
on our observation of a small portion of data is not likely to be true in general the
model must be able to ignore this if the data does not exhibit the underlying structure
corresponding to this prior knowledge In this case of attention-based encoder-decoder
model the existence of those degenerate cases above is a direct evidence of what this
attention-based model can do if there is no such underlying structure present in the
Then a natural next question is whether there are such structures that can be well
exploited by this attention mechanism in real data
If we train this attention-based
encoder-decoder model on the parallel corpora we discussed earlier in Sec 611 what
kind of structure does this attention-based model learn
In order to answer this question we must rst realize that we can easily visualize
what is happening inside this attention-based model First note that given a pair of
source X and target Y sentences14 the attention-based model computes an alignment
matrix A  01
XY

X1 X2
1Y
2Y
 XY
 
where  jt is dened in Eq 611
Each column at of this alignment matrix A is how well each source word based
on its context-dependent vector representation from Eq 69 is aligned to the t-th
target word Each row b j similarly shows how well each target word is aligned to the
content-dependent vector of the j-th source word In other words we can simply draw
the alignment matrix A as if it were a gray scale 2-D image
In Fig 69 the visualization of four alignment matrices is presented It is quite
clear especially to a French-English bilingual speaker that the model indeed captured
the underlying structure of wordphrase mapping between two languages For instance
focus on European Economic Area in Fig 69 a The model correctly noticed
that Area corresponds to zone Economic to economique and European to
europeenne without any supervision about this type of alignment
This is nice to see that the model was able to notice these regularities from data
without any explicit supervision However the goal of introducing the attention mech-
anism was not to get these pretty gures After all our goal is not to build an inter-
pretable model but a model that is predictive of the correct output given an input see
Chapter 1 and 16 In this regard how much does the introduction of the attention
mechanism help
In 2 the attention-based encoder-decoder model was compared against the sim-
ple encoder-decoder model in the task of English-French translation They observed
the relative improvement of up to 60 in terms of BLEU see Sec 612 as shown in
Table 61 Furthermore by using some of the latest techniques such as handling large
vocabularies 55 building a vocabulary of subword units 93 and variants of the atten-
tion mechanism 76 it has been found possible to achieve a better translation quality
with neural machine translation than the existing state-of-the-art translation systems
14 Note that if youre given only a source sentence you can let the model translate and align simultane-
Simple EncDec
Attention-based EncDec
Attention-based EncDec LV
Attention-based EncDec LVcid63
State-of-the-art SMT
BLEU Rel Improvement
1060
Table 61 The translation performances and the relative improvements over the simple
encoder-decoder model on an English-to-French translation task WMT14 measured
by BLEU 2 55 cid63 an ensemble of multiple attention-based models  the state-of-
the-art phrase-based statistical machine translation system 39
64 Warren Weavers Memorandum
In 1949 Warren Weaver15 wrote a memorandum titled cid104Translationcid105 on machine trans-
lation 108 Although this text was written way before computers have become ubiq-
uitous16 there are many interesting ideas that are closely related to what we have dis-
cussed so far in this chapter Let us go over some parts of the Weavers memorandum
and see how the ideas there corresponds to modern-day machine translation
Necessity of Linguistic Knowledge Weaver talks about a distinguished mathemati-
cian P who was surprised by his colleague His colleague had an amateur interest in
cryptography and one day presented P his method to decipher an encrypted Turkish
text successfully The most important point according to Weaver from this instance
is that the decoding was done by someone who did not know Turkish Now this
sounds familiar doesnt it
As long as there was a parallel corpus we are able to use neural machine transla-
tion models described throughout this chapter without ever caring about which lan-
guages we are training a model to translate between Especially if we decide to consider
each sentence as a sequence of characters17 there is almost no need for any linguistic
knowledge when building these neural machine translation systems
This lack of necessity for linguistic knowledge is not new In fact the most widely
studied and used machine translation approach which is count-based statistical ma-
chine translation 19 66 does not require any prior knowledge about source and target
languages All it needs is a large corpus
Importance of Context Recall from Sec 63 that the encoder of an attention-based
neural machine translation uses a bidirectional recurrent neural network in order to ob-
tain a context set Each vector in the context set was considered a context-dependent
15 Yes this is the very same Weaver after which the building of the Courant Institute of Mathematical
Sciences has been named
16 Although Weaver talks about modern computers over and over in his memorandum what he refers to
is not exactly what we think of computers as these days
17 In fact only very recently people have started investigating the possibility of building a machine trans-
lation system based on character sequences 73 This has been made possible due to the recent success of
neural machine translation
vector as it represents what the center word means with respect to all the surround-
ing words This context dependency is a necessary component in making the whole
attention-based neural machine translation as it helps disambiguating the meaning of
each word and also distinguishing multiple occurrences of a single word by their con-
Weaver discusses this extensively in Sec 34 in his memorandum First to Weaver
it was amply clear that a translation procedure that does little more than handle a one-
to-one correspondence of words can not hope to be useful 
in which the problems
of  multiple meanings  are frequent In other words it is simply not possible to
look at each word separately from surrounding words or context and translate it to a
corresponding target word because there is uncertainty in the meaning of the source
word which can only be resolved by taking into account its context
So what does Weaver propose in order to address this issue He proposes in Sec 5
that if one can see not only the central word in question but also say N words on
either side then if sic N is large enough one can unambiguously decide the meaning
of the central word If we consider only a single sentence and take the innite limit of
N   we see that what Weaver refers to is exactly the bidirectional recurrent neural
network used by the encoder of the attention-based translation system Furthermore
we see that the continuous bag-of-words language model or Markov random eld
based language model from Sec 542 exactly does what Weaver proposed by setting
N to a nite number
In Sec 521 we talked about the issue of data sparsity and how it is desirable to
have a larger N but its often not a good idea statistically to do so Weaver was also
worried about this by saying that it would hardly be practical to do this by means of
a generalized dictionary which contains all possible phases sic 2N  1 words long
for the number of such phases sic is horrifying We learned that this issue of data
sparsity can be largely avoided by adopting a fully parametric approach instead of a
table-based approach in Sec 54
Common base of human communications Weaver suggested in the last section of
his memorandum that perhaps the way for translation is to descend from each lan-
guage down to the common base of human communication  the real but as yet undis-
covered universal language  and then re-emerge by whatever particular route is conve-
nient He specically talked about a universal language and this makes me wonder
if we can consider the memory state of the recurrent neural networks both of the en-
coder and decoder as this kind of intermediate language This intermediate language
radically departs from our common notion of natural languages Unlike conventional
languages it does not use discrete symbols but uses continuous vectors This use of
continuous vectors allows us to use simple arithmetics to manipulate the meaning as
well as its surface realization18
This view may sound radical considering that what weve discussed so far has been
conned to translating from one language to another After all this universal language
18 If you nd this view too radical or fascinating I suggest you to look at the presentation slides by
Geoff Hinton at httpsdrivegooglecomfiled0B16RwCMQqrtdMWFaeThBTC1mZkk
viewuspsharing
of ours is very specic to only a single source language with respect to a single target
language This is however not a constraint on the neural machine translation by design
but simply a consequence of our having focused on this specic case
Indeed in this year 2015 researchers have begun to report that it is possible to
build a neural machine translation model that considers multiple languages and even
further multiple tasks 38 75 More works in this line are expected and it will be
interesting to see if Weavers prediction again turns out to be true
Figure 69 Visualizations of the four sample alignment matrices The alignment
matrices were computed from an attention-based translation model trained to translate
a sentence in English to French Reprinted from 2
TheagreementontheEuropeanEconomicAreawassignedinAugust1992endLaccordsurlazoneconomiqueeuropenneatsignenaot1992endItshouldbenotedthatthemarineenvironmentistheleastknownofenvironmentsendIlconvientdenoterquelenvironnementmarinestlemoinsconnudelenvironnementendDestructionoftheequipmentmeansthatSyriacannolongerproducenewchemicalweaponsendLadestructiondelquipementsignifiequelaSyrienepeutplusproduiredenouvellesarmeschimiquesendThiswillchangemyfuturewithmyfamilythemansaidendCelavachangermonaveniravecmafamilleaditlhommeendChapter 7
Final Words
Let me wrap up this lecture note by describing some aspects of natural language under-
standing with distributed representations that I have not discussed in this course These
are the topics that I would have spent time on had the course been scheduled to last
twice the duration as it is now Afterward I will nalize this whole lecture note with a
short summary
71 Multimedia Description Generation as Translation
Those who have followed this course closely so far must have noticed that the neural
machine translation model described in the previous chapter is quite general in the
sense that the input to the model does not have to be a sentence In the case of the
simple encoder-decoder model from Sec 62 it is clear that any type of input X can be
used instead of a sentence as long as there is a feature extractor that returns the vector
representation c of the input
And fortunately we already learned how to build a feature extractor throughout
this course Almost every single model that is a neural network in our case converts
an input into a continuous vector Let us take a multilayer perceptron from Sec 33
as an example Any classier built as a multilayer perceptron can be considered as a
two-stage process see Sec 332 First the feature vector of the input is extracted see
Eq 39
 x   ux  c
The extracted feature vector  x is then afne-transformed followed by softmax func-
tion This results in a conditional distribution over all possible labels see Eq 44
This means that we can make the simple encoder-decoder model to work with non-
language input simply by replacing the recurrent neural network based encoder with
the feature extraction stage of the multilayer perceptron Furthermore it is possible to
pretrain this feature extractor by training the whole multilayer perceptron on a separate
classication dataset1
1 This way of using a feature extractor pretrained from another network has become a de facto standard
This approach of using the encoder-decoder model for describing non-language
input has become popular in recent years especially 2014 and 2015 and has been
applied to many applications including imagevideo description generation and speech
recognition For an extensive list of these applications I refer the readers to a recent
review article by Cho et al 26
Example Image Caption Generation Let me take as an example the task of im-
age caption generation The possibility of using the encoder-decoder model for image
caption generation was noticed by several research groups almost simultaneously last
year 2014 62 106 59 78 37 40 252 The success of neural machine translation
in 101 and earlier success of deep convolutional network on object recognition see
eg 67 96 102 inspired them the idea to use the deep convolutional networks fea-
ture extractor together with the recurrent neural network decoder for the task of image
caption generation
Right after these Xu et al 111
realized that it is possible to use the
attention-based encoder-decoder model
from Sec 63 for image caption gen-
eration Unlike the simple model the
attention-based model requires a context
set instead of a context vector The con-
text set should contain multiple context
vectors and each vector should repre-
sent a spatial location with respect to
the whole image meaning each context
vector is a spatially-localized context-
dependent image descriptor This was
achieved by using the last convolutional
layers activations of the pretrained deep
convolutional network instead of the last
fully-connected layers See Fig 71 for
graphical illustration of this approach
Figure 71 Image caption generation with
the attention-based encoder-decoder model
These approaches based on neural
networks or in other words based on dis-
tributed representations have been suc-
cessful at image caption generation Four out of ve top rankers in the recent Microsoft
CoCo Image Captioning Challenge 20153 were using variants of the neural encoder-
decoder model based on human evaluation of the captions
in many of the computer vision tasks 94 This is also closely related to semi-supervised learning with
pretrained word embeddings which we discussed in Sec 543 In that case it was only the rst input layer
that was pretrained and used later see Eqs 511512
2 I must however make a note that Kiros et al 62 proposed a fully neural network based image caption
generation earlier than all the others cited here did
3 httpmscocoorgdatasetcaptions-leaderboard
AnnotationVectorsWord SsampleuiRecurrentStatezif  a   man   is   jumping   into   a   lake   hjAttentionMechanismaAttention        weightjaj1Convolutional Neural Network72 Language Understanding with World Knowledge
In Sec 12 we talked about how we view natural languages as a function This function
of natural language maps from a tuple of a speakers speech a listeners mental state
and the surrounding world to the listeners reaction often as a form of natural language
response Unfortunately in order to make it manageable we decided to build a model
that approximates only a part of this true function
Immediate state of the surrounding world In this course of action one thing we
have dropped out is the surrounding world The surrounding world may mean many
different things One of them is the current state of the surrounding world As an
example when I say look at this cute llama it is quite likely that the surrounding
world at the current state contains either an actual llama or at least a picture of a llama
A listener then understands easily what a llama is even without having known what a
llama is in advance By looking at the picture of llama the listener makes a mental note
that the llama looks similar to a camel and therefore must be a four-legged animal
If the surrounding world is not taken into account as weve been doing so far
the listener can only generalize based on the context words Just like how the neural
language model from Sec 54 generalized to unseen or rarely seen words the listener
can infer that llama must be a type of animal by remembering that the phrase look
at this cute has mainly been followed by an animal such as cat or dog However
it is quite clear that look at this cute is also followed by many other nouns including
baby book and so on
The question is then how to exploit this How can we incorporate for instance
vision information from the surrounding world into natural language understanding
The simplest approach is to simply concatenate a word embedding vector see
Eq 512 and a corresponding image vector obtained from an existing feature ex-
tractor see above 60 This can be applied to any existing language models such as
neural language model see Sec 54 and neural machine translation model see Chap-
ter 6 This approach gives a strong signal to the model the similarities among different
words based on the corresponding objects appearances This approach of concatenat-
ing vectors of two different modalities eg language and vision was earlier proposed
in 109
A more sophisticated approach is to design and train a model to solve a task that
requires tight interaction between language and other modalities As our original goal
is to build a natural language function all we need to do is to build a function approxi-
mator that takes as input both language and other modalities Recently Antol et al 1
built a large-scale dataset of question-answer-image triplets called visual question an-
swering VQA for this specic purpose They have carefully built the dataset such
that many if not most questions can only be answered when the accompanying image
is taken into consideration Any model thats able to solve the questions in this dataset
well will have to consider both language and vision
Knowledge base Lost in a library So far we have talked about incorporating an
immediate state of the surrounding world However our use of languages is more
sophisticated This is especially apparent in written languages Let us take an example
of me writing this lecture note It is not the case where I simply sit and start writing
the whole text based purely on my mental state with memory of my past research and
the immediate surrounding world state which has almost nothing to do with Rather
a large part of this writing process is spent on going through various research articles
and books written by others in order to nd relevant details of the topic
In this case the surrounding world is a database in which human knowledge is
stored You can think of a library or the Internet As the amount of knowledge is
simply too large to be memorized in the entirety it is necessary for a person to be able
to search through the vast knowledge base But wait what does it have to do with
natural language understanding
Consider the case where the context phrase is Llama is a domesticated camelid
from Without access to the knowledge base or in this specic instance access to
Wikipedia any language model can only say as much as that this context phrase is
likely followed by a name of some place This is especially true if we assume that the
training corpus did not mention llama at all However if the language model is able
to search Wikipedia and condition on its search result it suddenly becomes so obvious
that this context phrase is followed by South America or the name of any region on
Andean mountain rages
Although this may sound too complicated a task to incorporate into a neural net-
work the concept of how to incorporate this is not necessarily complicated In fact we
can use the attention mechanism discussed in Sec 63 almost as it is Let us describe
here a conceptual picture of how this can be done
Let D  d1d2    dM be a set of knowledge vectors Each knowledge vector
di is a vector representation of a piece of knowledge For instance di can be a vector
representation of one Wikipedia article It is certainly unclear what is the best way to
obtain this vector representation of an entire article but let us assume that an oracle
gave us a means to do so
Let us focus on recurrent language modelling from Sec 554 At each time step
we have access to the following vectors
1 Context vector ht1 the summary all the preceding words
2 Current word wt the current input word
Similarly to what we have done in Sec 63 we will dene a scoring function fscore
which scores each knowledge vector di with respect to the context vector and the cur-
rent word
it  exp  fscorediht1ewt  
where ewt is a vector representation of the current word wt
This score reects the relevance of the knowledge in predicting the next word and
once it is computed for every knowledge vector we compute the weighted sum of all
4 This approach of using attention mechanism for external knowledge pieces has been proposed recently
in 14 in the context of question-answering Here we stick to language modelling as the course has not
dealt with question-answering tasks
the knowledge
itdi
This vector dt is a vector summary of the knowledge relevant to the next word taking
into account the context phrase In the case of an earlier example the scoring function
gives a high score to the Wikipedia article on llama based on the history of preceding
words Llama is a domesticated camelid from
This knowledge vector is used when updating the memory state of the recurrent
neural network
cid0ht1ewt  dt
cid1 
ht  frec
From this updated memory state which also contains the knowledge extracted from
the selected knowledge vector the next words distribution is computed according to
Eq 46
One important issue with this approach is that the size of knowledge set D is often
extremely large For instance English Wikipedia contains more than 5M articles as of
23 Nov 20155 It easily becomes impossible to score each and every knowledge vector
not to mention to extract knowledge vectors of all the articles6 It is an open question
how this unreasonable amount of computation needed for search can be avoided
Why is this any signicant One may naively think that if we train a large enough
network with a large enough data which contains all those world knowledge a trained
network will be able to contain all those world knowledge likely in a compressed
form in its parameters together with its network architecture This is true up to a
certain level but there are many issues here
First the world knowledge were talking about here contains all the knowledge
accumulated so far Even a human brain arguably the best working neural network
to date cannot store all the world knowledge and must resort to searching over the
external database of knowledge It is no wonder we have libraries where people can go
and look for relevant knowledge
Second the world knowledge is dynamic Every day some parts of the world
knowledge become obsolete and at the same time previously unknown facts are added
to the world knowledge If anyone looked up Facebook before 2004 they wouldve
ended up with yearly facebooks from American universities Nowadays it is almost
certain that when a person looks up Facebook they will nd information on Face-
book the social network site Having all the current world knowledge encoded in the
models parameters is not ideal in this sense
5 httpsenwikipediaorgwikiWikipediaStatistics
6 This is true especially when those knowledge vectors are also updated during training
73 Larger-Context Language Understanding
Beyond Sentences and Beyond Words
If we view natural language as a function it becomes clear that what weve discussed
so far throughout the course is heavily restrictive There are two reasons behind this
restriction
First what we have discussed so far has narrowly focused on handling a sentence
In Sec 52 I have described language model as a way to model a sentence probability
pS This is a bit weird in the sense that weve been using a term language modelling
not sentence modelling Keeping it in mind we can start looking at a probability of a
document or discourse D as a whole rather than as a product of sentence probabilities
pSkSk
where the document D consists of N sentences This approach is readily integrated into
the language modelling approaches we discussed earlier in Chapter 5 by
pw jw jSk
This is applicable to any language-related models we have discussed so far includ-
ing neural language model from Sec 54 recurrent language model from Sec 55
Markov random eld language model from Sec 542 and neural machine translation
from Chapter 6
In the context of language modelling two recent articles proposed to explore this
direction I refer the readers to 107 and 57
Second we have stuck to representing a sentence as a sequence of words so far
despite a short discussion in Sec 512 where I strongly claim that this does not have
to be This is indeed true and in fact even if we replace most occurrence of word
in this course with for instance character all the arguments stand Of course by
using smaller units than words we run into many practical and theoretical issues One
most severe practical issue is that each sentence suddenly becomes much longer One
most sever theoretical issue is that it is a highly nonlinear mapping from a sequence
of characters to its meaning as we discussed earlier in Sec 512 Nevertheless the
advance in computing and deep neural networks which are capable of learning such
a highly nonlinear mapping have begun to let researchers directly work on this prob-
lem of using subword units see eg 61 73 Note that I am not trying to say that
characters are the only possible sub-word units and recently an effective statistical ap-
proach to deriving sub-word units off-line was proposed and applied to neural machine
translation in 93
74 Warning and Summary
Before I nish this lecture note with the summary of what we have discussed through-
out this course let me warn you by quoting Claude Shannon 957
It will be all too easy for our somewhat articial prosperity to collapse
overnight when it is realized that the use of a few exciting words like in-
formation entropy redundancy do not solve all our problems
Natural language understanding with distributed representation is a fascinating topic
that has recently gathered large interest from both machine learning and natural lan-
guage processing communities This may give a wrong sign that this approach with
neural networks is an ultimate winner in natural language understandingprocessing
though without any ill intention As Shannon pointed out this prosperity of distributed
representation based natural language understanding may collapse overnight as can
any other approaches out there8 Therefore I warn the readers especially students to
keep this quote in their mind and remember that it is not a few recent successes of this
approach to natural language understanding but the fundamental ideas underlying this
approach that matter and should be remembered after this course
Summary Finally here goes the summary of what we have learned throughout this
semester We began our journey by a brief discussion on how we view human language
as and we decided to stick to the idea that a language is a function not an entity existing
independent of the surrounding world including speakers and listeners Is this a correct
way to view a human language Maybe maybe not I will leave it up to you to decide
In order to build a machine that can approximate this language function in Chap-
ter 2 we studied basic ideas behind supervised learning in machine learning We de-
ned what a cost function is how we can minimize it using an iterative optimization
algorithm specically stochastic gradient descent and learned the importance of hav-
ing a validation set for both early-stopping and model selection These are all basic
topics that are dealt in almost any basic machine learning course and the only thing
that I would like to emphasize is the importance of not looking at a held-out test set
One must always select anything related to learning eg hyperparameters networks
architectures and so on based solely on a validation set As soon as one tunes any
of those based on the test set performance any result from this tuning easily becomes
invalid or at least highly disputable
In Chapter 3 we nally talked about deep neural networks or more traditionally
called multilayer perceptron9 I tried to go over basic but important details as slowly as
possible including how to build a deep neural network based classier how to dene
a cost function and how to compute the gradient wrt the parameters of the network
However I must confess that there are better materials for this topic than this lecture
7 I would like to thank Adam Lopez for pointing me to this quote
8 Though it is interesting to note that information theory never really collapsed overnight Rather its
prosperity has been continuing for more than half a century since Shannon warned us about its potential
overnight collapse in 1956
9 I personally prefer multilayer perceptron but it seems like it has gone out of fashion
We then moved on to recurrent neural networks in Chapter 4 This was a necessary
step in order to build a neural network based model that can handle both variable-length
input and output Again my goal here was to take as much time as it is needed to moti-
vate the need of recurrent networks and to give you basic ideas underlying them Also
I spent quite some time on why it has been considered difcult to train recurrent neural
networks by stochastic gradient descent like algorithms and as a remedy introduced
gated recurrent units and long short-term memory units
Only after these long four to ve weeks have I started talking about how to handle
language data in Chapter 5 I motivated neural language models by the lack of general-
ization and the curse of data sparsity It is my regret that I have not spent much time on
discussing the existing techniques for count-based n-gram language models but again
there are much better materials and better lecturers for these techniques already Af-
ter the introduction of neural language model I spent some time on describing how
this neural language model is capable of generalizing to unseen phrases Continuing
from this neural language model in Sec 55 language modelling using recurrent neu-
ral networks was introduced as a way to avoid Markov assumption of n-gram language
This discussion on neural language model naturally continued on to neural machine
translation in Chapter 6 Rather than going directly into describing neural machine
translation models I have spent a full week on two issues that are often overlooked
data preparation in Sec 611 and evaluation in Sec 612 I wish the discussion of these
two topics has reminded students that machine learning is not only about algorithms
and models but is about a full pipeline starting from data collection to evaluation often
with loops here and there This chapter nished with where we are in 2015 compared
to what Weaver predicted in 1949
Of course there are so many interesting topics in this area of natural language
understanding I am not qualied nor knowledgeable to teach many if not most of
those topics unfortunately and have focused on those few topics that I have worked on
myself I hope this lecture note will serve at least as a useful starting point into more
advanced topics in natural language understanding with distributed representations
Bibliography
1 S Antol A Agrawal J Lu M Mitchell D Batra C L Zitnick and D Parikh
Vqa Visual question answering In International Conference on Computer Vi-
sion ICCV 2015
2 D Bahdanau K Cho and Y Bengio Neural machine translation by jointly
learning to align and translate arXiv preprint arXiv14090473 2014
3 P Baltescu and P Blunsom Pragmatic neural language modelling in machine
translation arXiv preprint arXiv14127119 2014
4 F Bastien P Lamblin R Pascanu J Bergstra I Goodfellow A Bergeron
N Bouchard D Warde-Farley and Y Bengio Theano new features and speed
improvements arXiv preprint arXiv12115590 2012
5 A G Baydin B A Pearlmutter and A A Radul Automatic differentiation in
machine learning a survey arXiv preprint arXiv150205767 2015
6 Y Bengio N Boulanger-Lewandowski and R Pascanu Advances in optimiz-
ing recurrent networks In Acoustics Speech and Signal Processing ICASSP
2013 IEEE International Conference on pages 86248628 IEEE 2013
7 Y Bengio N Leonard and A Courville Estimating or propagating gradi-
ents through stochastic neurons for conditional computation arXiv preprint
arXiv13083432 2013
8 Y Bengio H Schwenk J-S Senecal F Morin and J-L Gauvain Neural
probabilistic language models In Innovations in Machine Learning pages 137
186 Springer Berlin Heidelberg 2006
9 Y Bengio P Simard and P Frasconi Learning long-term dependencies with
gradient descent is difcult Neural Networks IEEE Transactions on 52157
166 1994
10 J Bergstra O Breuleux F Bastien P Lamblin R Pascanu G Desjardins
J Turian D Warde-Farley and Y Bengio Theano a cpu and gpu math expres-
sion compiler In Proceedings of the Python for scientic computing conference
SciPy volume 4 page 3 Austin TX 2010
11 J Besag Statistical analysis of non-lattice data The statistician pages 179195
12 C M Bishop Mixture density networks 1994
13 C M Bishop Pattern recognition and machine learning springer 2006
14 A Bordes N Usunier S Chopra and J Weston Large-scale simple question
answering with memory networks arXiv preprint arXiv150602075 2015
15 L Bottou Online algorithms and stochastic approximations In D Saad edi-
tor Online Learning and Neural Networks Cambridge University Press Cam-
bridge UK 1998
16 L Breiman et al Statistical modeling The two cultures with comments and a
rejoinder by the author Statistical Science 163199231 2001
17 J S Bridle Training stochastic model recognition algorithms as networks can
lead to maximum mutual information estimation of parameters In D Touretzky
editor Advances in Neural Information Processing Systems 2 pages 211217
Morgan-Kaufmann 1990
18 E Brochu V M Cora and N de Freitas A tutorial on Bayesian optimization
of expensive cost functions with application to active user modeling and hierar-
chical reinforcement learning arXiv10122599 csLG Dec 2010
19 P F Brown J Cocke S A D Pietra V J D Pietra F Jelinek J D Lafferty
R L Mercer and P S Roossin A statistical approach to machine translation
Computational linguistics 1627985 1990
20 C Callison-Burch M Osborne and P Koehn Re-evaluation the role of bleu in
machine translation research In EACL volume 6 pages 249256 2006
21 A Carnie Syntax A generative introduction John Wiley  Sons 2013
22 M Cettolo C Girardi and M Federico Wit3 Web inventory of transcribed
and translated talks In Proceedings of the 16th Conference of the European As-
sociation for Machine Translation EAMT pages 261268 Trento Italy May
23 O Chapelle B Scholkopf and A Zien editors Semi-Supervised Learning
MIT Press Cambridge MA 2006
24 S F Chen and J Goodman An empirical study of smoothing techniques for
language modeling In Proceedings of the 34th annual meeting on Association
for Computational Linguistics pages 310318 Association for Computational
Linguistics 1996
25 X Chen and C L Zitnick Learning a recurrent visual representation for image
caption generation arXiv14115654 2014
26 K Cho A Courville and Y Bengio Describing multimedia content using
attention-based encoderdecoder networks 2015
27 K Cho B van Merrienboer D Bahdanau and Y Bengio On the properties
of neural machine translation Encoder-decoder approaches arXiv preprint
arXiv14091259 2014
28 K Cho B Van Merrienboer C Gulcehre D Bahdanau F Bougares
H Schwenk and Y Bengio Learning phrase representations using rnn encoder-
decoder for statistical machine translation arXiv preprint arXiv14061078
29 K Cho B van Merrienboer C Gulcehre F Bougares H Schwenk and Y Ben-
gio Learning phrase representations using RNN encoder-decoder for statistical
machine translation In Proceedings of the Empiricial Methods in Natural Lan-
guage Processing EMNLP 2014 Oct 2014
30 N Chomsky A review of B F skinners verbal behavior Language 35126
58 1959
31 N Chomsky Linguistic contributions to the study of mind future Language
and thinking pages 323364 1968
32 N Chomsky Syntactic structures Walter de Gruyter 2002
33 R Collobert J Weston L Bottou M Karlen K Kavukcuoglu and P Kuksa
Natural language processing almost from scratch The Journal of Machine
Learning Research 1224932537 2011
34 T M Cover Geometrical and statistical properties of systems of linear inequal-
ities with applications in pattern recognition IEEE Transactions on Electronic
Computers EC-143326334 1965
35 J Denker and Y Lecun Transforming neural-net output levels to probability
distributions In Advances in Neural Information Processing Systems 3 Citeseer
36 M Denkowski and A Lavie Meteor universal Language specic translation
evaluation for any target language In Proceedings of the EACL 2014 Workshop
on Statistical Machine Translation 2014
37 J Donahue L A Hendricks S Guadarrama M Rohrbach S Venugopalan
K Saenko and T Darrell Long-term recurrent convolutional networks for vi-
sual recognition and description arXiv14114389 2014
38 D Dong H Wu W He D Yu and H Wang Multi-task learning for multiple
language translation ACL 2015
39 N Durrani B Haddow P Koehn and K Heaeld Edinburghs phrase-based
machine translation systems for WMT-14 In Proceedings of the Ninth Work-
shop on Statistical Machine Translation pages 97104 Association for Com-
putational Linguistics Baltimore MD USA 2014
40 H Fang S Gupta F Iandola R Srivastava L Deng P Dollar J Gao X He
M Mitchell J C Platt C L Zitnick and G Zweig From captions to visual
concepts and back arXiv14114952 2014
41 J R Firth A synopsis of linguistic theory 1930-1955 Oxford Philological
Society 1957
42 R Fletcher Practical Methods of Optimization Wiley-Interscience New York
NY USA 2nd edition 1987
43 M L Forcada and R P Neco Recursive hetero-associative memories for trans-
lation In Biological and Articial Computation From Neuroscience to Tech-
nology pages 453462 Springer 1997
44 D Furcy and S Koenig Limited discrepancy beam search
125131 2005
In IJCAI pages
45 F A Gers J Schmidhuber and F Cummins Learning to forget Continual
prediction with lstm Neural computation 121024512471 2000
46 X Glorot A Bordes and Y Bengio Deep sparse rectier neural networks
In International Conference on Articial Intelligence and Statistics pages 315
323 2011
47 Y Goldberg A primer on neural network models for natural language process-
ing arXiv preprint arXiv151000726 2015
48 I Goodfellow D Warde-farley M Mirza A Courville and Y Bengio Maxout
In Proceedings of the 30th International Conference on Machine
networks
Learning ICML-13 pages 13191327 2013
49 K Greff R K Srivastava J Koutnk B R Steunebrink and J Schmidhuber
Lstm A search space odyssey arXiv preprint arXiv150304069 2015
50 K He X Zhang S Ren and J Sun Delving deep into rectiers Sur-
passing human-level performance on imagenet classication arXiv preprint
arXiv150201852 2015
51 K Heaeld I Pouzyrevsky J H Clark and P Koehn Scalable modied
In Proceedings of the 51st Annual
Kneser-Ney language model estimation
Meeting of the Association for Computational Linguistics pages 690696 Soa
Bulgaria August 2013
52 S Hochreiter Y Bengio P Frasconi and J Schmidhuber Gradient ow in
recurrent nets the difculty of learning long-term dependencies volume 1 A
eld guide to dynamical recurrent neural networks IEEE Press 2001
53 S Hochreiter and J Schmidhuber Long short-term memory Neural computa-
tion 9817351780 1997
54 G-B Huang Q-Y Zhu and C-K Siew Extreme learning machine Theory
and applications Neurocomputing 7013489501 2006
55 S Jean K Cho R Memisevic and Y Bengio On using very large target
vocabulary for neural machine translation In ACL 2015 2014
56 Y Jernite A M Rush and D Sontag A fast variational approach for learn-
ing markov random eld language models 32nd International Conference on
Machine Learning ICML 2015
57 Y Ji T Cohn L Kong C Dyer and J Eisenstein Document context language
models arXiv preprint arXiv151103962 2015
58 R Jozefowicz W Zaremba and I Sutskever An empirical exploration of recur-
rent network architectures In Proceedings of the 32nd International Conference
on Machine Learning ICML-15 pages 23422350 2015
59 A Karpathy and F-F Li Deep visual-semantic alignments for generating image
descriptions arXiv14122306 2014
60 D Kiela and L Bottou Learning image embeddings using convolutional neural
networks for improved multi-modal semantics Proceedings of EMNLP 2014
61 Y Kim Y Jernite D Sontag and A M Rush Character-aware neural language
models arXiv preprint arXiv150806615 2015
62 R Kiros R Salakhutdinov and R Zemel Multimodal neural language models
In ICML2014 2014
63 R Kneser and H Ney Improved backing-off for m-gram language modeling
In Acoustics Speech and Signal Processing 1995 ICASSP-95 1995 Interna-
tional Conference on volume 1 pages 181184 IEEE 1995
64 P Koehn Pharaoh a beam search decoder for phrase-based statistical machine
translation models In Machine translation From real users to research pages
115124 Springer 2004
65 P Koehn Europarl A parallel corpus for statistical machine translation In MT
summit volume 5 pages 7986 Citeseer 2005
66 P Koehn F J Och and D Marcu Statistical phrase-based translation In Pro-
ceedings of the 2003 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Language Technology-Volume 1
pages 4854 Association for Computational Linguistics 2003
67 A Krizhevsky I Sutskever and G E Hinton Imagenet classication with deep
convolutional neural networks In Advances in neural information processing
systems pages 10971105 2012
68 T S Kuhn The structure of scientic revolutions University of Chicago press
69 Q V Le N Jaitly and G E Hinton A simple way to initialize recurrent net-
works of rectied linear units arXiv preprint arXiv150400941 2015
70 Y LeCun Y Bengio and G Hinton Deep learning Nature 5217553436
444 2015
71 Y LeCun L Bottou G Orr and K R Muller Efcient BackProp In G Orr
and K Muller editors Neural Networks Tricks of the Trade volume 1524 of
Lecture Notes in Computer Science pages 550 Springer Verlag 1998
72 O Levy and Y Goldberg Neural word embedding as implicit matrix factor-
ization In Z Ghahramani M Welling C Cortes N Lawrence and K Wein-
berger editors Advances in Neural Information Processing Systems 27 pages
21772185 Curran Associates Inc 2014
73 W Ling I Trancoso C Dyer and A W Black Character-based neural machine
translation arXiv preprint arXiv151104586 2015
74 D G Lowe Object recognition from local scale-invariant features In Computer
vision 1999 The proceedings of the seventh IEEE international conference on
volume 2 pages 11501157 Ieee 1999
75 M-T Luong Q V Le I Sutskever O Vinyals and L Kaiser Multi-task
sequence to sequence learning arXiv preprint arXiv151106114 2015
76 M-T Luong H Pham and C D Manning Effective approaches to attention-
based neural machine translation arXiv preprint arXiv150804025 2015
77 C D Manning and H Schutze Foundations of statistical natural language
processing MIT press 1999
78 J Mao W Xu Y Yang J Wang and A L Yuille Explain images with multi-
modal recurrent neural networks arXiv14101090 2014
79 T Mikolov K Chen G Corrado and J Dean Efcient estimation of word
representations in vector space arXiv preprint arXiv13013781 2013
80 T Mikolov M Karaat L Burget J Cernocky and S Khudanpur Recurrent
neural network based language model In INTERSPEECH 2010 pages 1045
1048 2010
81 V Nair and G E Hinton Rectied linear units improve restricted boltzmann
In Proceedings of the 27th International Conference on Machine
machines
Learning ICML-10 pages 807814 2010
82 K Papineni S Roukos T Ward and W-J Zhu Bleu a method for automatic
evaluation of machine translation In Proceedings of the 40th annual meeting
on association for computational linguistics pages 311318 Association for
Computational Linguistics 2002
83 R Pascanu T Mikolov and Y Bengio On the difculty of training recurrent
neural networks In Proceedings of The 30th International Conference on Ma-
chine Learning pages 13101318 2013
84 A Perfors J Tenenbaum and T Regier Poverty of the stimulus a rational
approach In Annual Conference 2006
85 K B Petersen M S Pedersen et al The matrix cookbook Technical University
of Denmark 715 2008
86 C W Post The Three Percent Problem Rants and Responses on Publishing
Translation and the Future of Reading Open Letter 2011
87 P Resnik and N A Smith The web as a parallel corpus Computational Lin-
guistics 293349380 2003
88 H Robbins and S Monro A stochastic approximation method The Annals of
Mathematical Statistics 223400407 1951
89 F Rosenblatt Principles of neurodynamics perceptrons and the theory of brain
mechanisms Report Cornell Aeronautical Laboratory Spartan Books 1962
90 S Russell and P Norvig Articial intelligence a modern approach 1995
91 J Schmidhuber Deep learning in neural networks An overview Neural Net-
works 6185117 2015
92 H Schwenk Continuous space language models Computer Speech  Lan-
guage 213492518 2007
93 R Sennrich B Haddow and A Birch Neural machine translation of rare words
with subword units arXiv preprint arXiv150807909 2015
94 P Sermanet D Eigen X Zhang M Mathieu R Fergus and Y LeCun Over-
feat Integrated recognition localization and detection using convolutional net-
works arXiv preprint arXiv13126229 2013
95 C Shannon The bandwagon edtl IRE Transactions on Information Theory
123 1956
96 K Simonyan and A Zisserman Very deep convolutional networks for large-
scale image recognition arXiv preprint arXiv14091556 2014
97 B F Skinner Verbal behavior BF Skinner Foundation 2014
98 J R Smith H Saint-Amand M Plamada P Koehn C Callison-Burch and
A Lopez Dirt cheap web-scale parallel text from the common crawl In ACL
1 pages 13741383 2013
99 M Snover B Dorr R Schwartz L Micciulla and J Makhoul A study of trans-
lation edit rate with targeted human annotation In Proceedings of association
for machine translation in the Americas pages 223231 2006
100 M Sundermeyer H Ney and R Schluter From feedforward to recurrent lstm
neural networks for language modeling Audio Speech and Language Process-
ing IEEEACM Transactions on 233517529 2015
101 I Sutskever O Vinyals and Q V Le Sequence to sequence learning with
neural networks In Advances in neural information processing systems pages
31043112 2014
102 C Szegedy W Liu Y Jia P Sermanet S Reed D Anguelov D Erhan V Van-
houcke and A Rabinovich Going deeper with convolutions arXiv preprint
arXiv14094842 2014
103 J Turian L Ratinov and Y Bengio Word representations a simple and general
method for semi-supervised learning In Proceedings of the 48th annual meeting
of the association for computational linguistics pages 384394 Association for
Computational Linguistics 2010
104 L van der Maaten and G E Hinton Visualizing data using t-SNE Journal of
Machine Learning Research 925792605 November 2008
105 V Vapnik The Nature of Statistical Learning Theory Springer-Verlag New
York Inc New York NY USA 1995
106 O Vinyals A Toshev S Bengio and D Erhan Show and tell A neural image
caption generator arXiv preprint arXiv14114555 2014
107 T Wang and K Cho Larger-context language modelling arXiv preprint
arXiv151103729 2015
108 W Weaver Translation Machine translation of languages 141523 1955
109 J Weston S Bengio and N Usunier Large scale image annotation learning to
rank with joint word-image embeddings Machine learning 8112135 2010
110 T Winograd Understanding natural language Cognitive psychology 311
191 1972
111 K Xu J Ba R Kiros K Cho A Courville R Salakhutdinov R Zemel and
Y Bengio Show attend and tell Neural image caption generation with visual
attention In International Conference on Machine Learning 2015
112 Y Zhang K Wu J Gao and P Vines Automatic acquisition of chineseenglish
parallel corpus from the web In Advances in Information Retrieval pages 420
431 Springer 2006
113 R Zhou and E A Hansen Beam-stack search Integrating backtracking with
beam search In ICAPS pages 9098 2005
