A Primer on Neural Network Models
for Natural Language Processing
Yoav Goldberg
Draft as of October 6 2015
The most up-to-date version of this manuscript is available at httpwwwcsbiu
acilyogonnlppdf Major updates will be published on arxiv periodically
I welcome any comments you may have regarding the content and presentation If you
spot a missing reference or have relevant work youd like to see mentioned do let me know
firstlastgmail
Abstract
Over the past few years neural networks have re-emerged as powerful machine-learning
models yielding state-of-the-art results in elds such as image recognition and speech
processing More recently neural network models started to be applied also to textual
natural language signals again with very promising results This tutorial surveys neural
network models from the perspective of natural language processing research in an attempt
to bring natural-language researchers up to speed with the neural techniques The tutorial
covers input encoding for natural language tasks feed-forward networks convolutional
networks recurrent networks and recursive networks as well as the computation graph
abstraction for automatic gradient computation
1 Introduction
For a long time core NLP techniques were dominated by machine-learning approaches that
used linear models such as support vector machines or logistic regression trained over very
high dimensional yet very sparse feature vectors
Recently the eld has seen some success in switching from such linear models over
sparse inputs to non-linear neural-network models over dense inputs While most of the
neural network techniques are easy to apply sometimes as almost drop-in replacements of
the old linear classiers there is in many cases a strong barrier of entry In this tutorial I
attempt to provide NLP practitioners as well as newcomers with the basic background
jargon tools and methodology that will allow them to understand the principles behind
the neural network models and apply them to their own work This tutorial is expected
to be self-contained while presenting the dierent approaches under a unied notation and
framework
It also points to
external sources for more advanced topics when appropriate
It repeats a lot of material which is available elsewhere
This primer is not intended as a comprehensive resource for those that will go on and
develop the next advances in neural-network machinery though it may serve as a good entry
point Rather it is aimed at those readers who are interested in taking the existing useful
technology and applying it in useful and creative ways to their favourite NLP problems For
more in-depth general discussion of neural networks the theory behind them advanced
optimization methods and other advanced topics the reader is referred to other existing
resources In particular the book by Bengio et al 2015 is highly recommended
Scope The focus is on applications of neural networks to language processing tasks How-
ever some subareas of language processing with neural networks were decidedly left out of
scope of this tutorial These include the vast literature of language modeling and acoustic
modeling the use of neural networks for machine translation and multi-modal applications
combining language and other signals such as images and videos eg caption generation
Caching methods for ecient runtime performance methods for ecient training with large
output vocabularies and attention models are also not discussed Word embeddings are dis-
cussed only to the extent that is needed to understand in order to use them as inputs for
other models Other unsupervised approaches including autoencoders and recursive au-
toencoders also fall out of scope While some applications of neural networks for language
modeling and machine translation are mentioned in the text their treatment is by no means
comprehensive
A Note on Terminology The word feature is used to refer to a concrete linguistic
input such as a word a sux or a part-of-speech tag For example in a rst-order part-
of-speech tagger the features might be current word previous word next word previous
part of speech The term input vector is used to refer to the actual input that is fed
to the neural-network classier Similarly input vector entry refers to a specic value
of the input This is in contrast to a lot of the neural networks literature in which the
word feature is overloaded between the two uses and is used primarily to refer to an
input-vector entry
Mathematical Notation I use bold upper case letters to represent matrices X Y
Z and bold lower-case letters to represent vectors b When there are series of related
matrices and vectors for example where each matrix corresponds to a dierent layer in
the network superscript indices are used W1 W2 For the rare cases in which we want
indicate the power of a matrix or a vector a pair of brackets is added around the item to
be exponentiated W2 W32 Unless otherwise stated vectors are assumed to be row
vectors We use v1 v2 to denote vector concatenation
2 Neural Network Architectures
Neural networks are powerful learning models We will discuss two kinds of neural network
architectures that can be mixed and matched  feed-forward networks and Recurrent 
Recursive networks Feed-forward networks include networks with fully connected layers
such as the multi-layer perceptron as well as networks with convolutional and pooling
layers All of the networks act as classiers but each with dierent strengths
Fully connected feed-forward neural networks Section 4 are non-linear learners that
can for the most part be used as a drop-in replacement wherever a linear learner is used
This includes binary and multiclass classication problems as well as more complex struc-
tured prediction problems Section 8 The non-linearity of the network as well as the
ability to easily integrate pre-trained word embeddings often lead to superior classication
accuracy A series of works Chen  Manning 2014 Weiss Alberti Collins  Petrov
2015 Pei Ge  Chang 2015 Durrett  Klein 2015 managed to obtain improved syntac-
tic parsing results by simply replacing the linear model of a parser with a fully connected
feed-forward network Straight-forward applications of a feed-forward network as a classi-
er replacement usually coupled with the use of pre-trained word vectors provide benets
also for CCG supertagging Lewis  Steedman 2014 dialog state tracking Henderson
Thomson  Young 2013 pre-ordering for statistical machine translation de Gispert
Iglesias  Byrne 2015 and language modeling Bengio Ducharme Vincent  Janvin
2003 Vaswani Zhao Fossum  Chiang 2013 Iyyer et al 2015 demonstrate that multi-
layer feed-forward networks can provide competitive results on sentiment classication and
factoid question answering
Networks with convolutional and pooling layers Section 9 are useful for classication
tasks in which we expect to nd strong local clues regarding class membership but these
clues can appear in dierent places in the input For example in a document classication
task a single key phrase or an ngram can help in determining the topic of the document
Johnson  Zhang 2015 We would like to learn that certain sequences of words are good
indicators of the topic and do not necessarily care where they appear in the document
Convolutional and pooling layers allow the model to learn to nd such local indicators
regardless of their position Convolutional and pooling architecture show promising results
on many tasks including document classication Johnson  Zhang 2015 short-text cat-
egorization Wang Xu Xu Liu Zhang Wang  Hao 2015a sentiment classication
Kalchbrenner Grefenstette  Blunsom 2014 Kim 2014 relation type classication be-
tween entities Zeng Liu Lai Zhou  Zhao 2014 dos Santos Xiang  Zhou 2015 event
detection Chen Xu Liu Zeng  Zhao 2015 Nguyen  Grishman 2015 paraphrase iden-
tication Yin  Schutze 2015 semantic role labeling Collobert Weston Bottou Karlen
Kavukcuoglu  Kuksa 2011 question answering Dong Wei Zhou  Xu 2015 predict-
ing box-oce revenues of movies based on critic reviews Bitvai  Cohn 2015 modeling
text interestingness Gao Pantel Gamon He  Deng 2014 and modeling the relation
between character-sequences and part-of-speech tags Santos  Zadrozny 2014
In natural language we often work with structured data of arbitrary sizes such as
sequences and trees We would like to be able to capture regularities in such structures
or to model similarities between such structures
In many cases this means encoding
the structure as a xed width vector which we can then pass on to another statistical
learner for further processing While convolutional and pooling architectures allow us to
encode arbitrary large items as xed size vectors capturing their most salient features
they do so by sacricing most of the structural information Recurrent Section 10 and
recursive Section 12 architectures on the other hand allow us to work with sequences
and trees while preserving a lot of the structural information Recurrent networks Elman
1990 are designed to model sequences while recursive networks Goller  Kuchler 1996
are generalizations of recurrent networks that can handle trees We will also discuss an
extension of recurrent networks that allow them to model stacks Dyer Ballesteros Ling
Matthews  Smith 2015 Watanabe  Sumita 2015
Recurrent models have been shown to produce very strong results for language model-
ing including Mikolov Karaat Burget Cernocky  Khudanpur 2010 Mikolov Kom-
brink Lukas Burget Cernocky  Khudanpur 2011 Mikolov 2012 Duh Neubig Sudoh
 Tsukada 2013 Adel Vu  Schultz 2013 Auli Galley Quirk  Zweig 2013 Auli 
Gao 2014 as well as for sequence tagging Irsoy  Cardie 2014 Xu Auli  Clark 2015
Ling Dyer Black Trancoso Fermandez Amir Marujo  Luis 2015b machine transla-
tion Sundermeyer Alkhouli Wuebker  Ney 2014 Tamura Watanabe  Sumita 2014
Sutskever Vinyals  Le 2014 Cho van Merrienboer Gulcehre Bahdanau Bougares
Schwenk  Bengio 2014b dependency parsing Dyer et al 2015 Watanabe  Sumita
2015 sentiment analysis Wang Liu SUN Wang  Wang 2015b noisy text normal-
ization Chrupala 2014 dialog state tracking Mrksic O Seaghdha Thomson Gasic Su
Vandyke Wen  Young 2015 response generation Sordoni Galley Auli Brockett Ji
Mitchell Nie Gao  Dolan 2015 and modeling the relation between character sequences
and part-of-speech tags Ling et al 2015b
Recursive models were shown to produce state-of-the-art or near state-of-the-art re-
sults for constituency Socher Bauer Manning  Andrew Y 2013 and dependency Le
 Zuidema 2014 Zhu Qiu Chen  Huang 2015a parse re-ranking discourse parsing
Li Li  Hovy 2014 semantic relation classication Hashimoto Miwa Tsuruoka 
Chikayama 2013 Liu Wei Li Ji Zhou  WANG 2015 political ideology detection
based on parse trees Iyyer Enns Boyd-Graber  Resnik 2014b sentiment classication
Socher Perelygin Wu Chuang Manning Ng  Potts 2013 Hermann  Blunsom 2013
target-dependent sentiment classication Dong Wei Tan Tang Zhou  Xu 2014 and
question answering Iyyer Boyd-Graber Claudino Socher  Daume III 2014a
3 Feature Representation
Before discussing the network structure in more depth it is important to pay attention
to how features are represented For now we can think of a feed-forward neural network
as a function N N x that takes as input a din dimensional vector x and produces a dout
dimensional output vector The function is often used as a classier assigning the input
x a degree of membership in one or more of dout classes The function can be complex
and is almost always non-linear Common structures of this function will be discussed
in Section 4 Here we focus on the input x When dealing with natural language the
input x encodes features such as words part-of-speech tags or other linguistic information
Perhaps the biggest jump when moving from sparse-input linear models to neural-network
based models is to stop representing each feature as a unique dimension the so called
one-hot representation and representing them instead as dense vectors That is each core
feature is embedded into a d dimensional space and represented as a vector in that space1
The embeddings the vector representation of each core feature can then be trained like
the other parameter of the function N N  Figure 1 shows the two approaches to feature
representation
The feature embeddings the values of the vector entries for each feature are treated
as model parameters that need to be trained together with the other components of the
network Methods of training or obtaining the feature embeddings will be discussed later
For now consider the feature embeddings as given
The general structure for an NLP classication system based on a feed-forward neural
network is thus
1 Extract a set of core linguistic features f1     fk that are relevant for predicting the
output class
2 For each feature fi of interest retrieve the corresponding vector vfi
3 Combine the vectors either by concatenation summation or a combination of both
into an input vector x
4 Feed x into a non-linear classier feed-forward neural network
The biggest change in the input then is the move from sparse representations in which
each feature is its own dimension to a dense representation in which each feature is mapped
to a vector Another dierence is that we extract only core features and not feature com-
binations We will elaborate on both these changes briey
Dense Vectors vs One-hot Representations What are the benets of representing
our features as vectors instead of as unique IDs Should we always represent features as
dense vectors Lets consider the two kinds of representations
One Hot Each feature is its own dimension
 Dimensionality of one-hot vector is same as number of distinct features
1 Dierent feature types may be embedded into dierent spaces For example one may represent word
features using 100 dimensions and part-of-speech features using 20 dimensions
Figure 1 Sparse vs dense feature representations Two encodings of the informa-
tion current word is dog previous word is the previous pos-tag is DET
a Sparse feature vector Each dimension represents a feature Feature combi-
nations receive their own dimensions Feature values are binary Dimensionality
is very high b Dense embeddings-based feature vector Each core feature is
represented as a vector Each feature corresponds to several input vector en-
tries No explicit encoding of feature combinations Dimensionality is low The
feature-to-vector mappings come from an embedding table
 Features are completely independent from one another The feature word is
dog  is as dis-similar to word is thinking  than it is to word is cat 
Dense Each feature is a d-dimensional vector
 Dimensionality of vector is d
 Similar features will have similar vectors  information is shared between similar
features
One benet of using dense and low-dimensional vectors is computational the majority
of neural network toolkits do not play well with very high-dimensional sparse vectors
However this is just a technical obstacle which can be resolved with some engineering
eort
The main benet of the dense representations is in generalization power if we believe
some features may provide similar clues it is worthwhile to provide a representation that
is able to capture these similarities For example assume we have observed the word dog
many times during training but only observed the word cat a handful of times or not at
all If each of the words is associated with its own dimension occurrences of dog will not
tell us anything about the occurrences of cat However in the dense vectors representation
the learned vector for dog may be similar to the learned vector from cat allowing the
model to share statistical strength between the two events This argument assumes that
good vectors are somehow given to us Section 5 describes ways of obtaining such vector
representations
In cases where we have relatively few distinct features in the category and we believe
there are no correlations between the dierent features we may use the one-hot representa-
tion However if we believe there are going to be correlations between the dierent features
in the group for example for part-of-speech tags we may believe that the dierent verb
inections VB and VBZ may behave similarly as far as our task is concerned it may be
worthwhile to let the network gure out the correlations and gain some statistical strength
by sharing the parameters It may be the case that under some circumstances when the
feature space is relatively small and the training data is plentiful or when we do not wish to
share statistical information between distinct words there are gains to be made from using
the one-hot representations However this is still an open research question and there are
no strong evidence to either side The majority of work pioneered by Collobert  Weston
2008 Collobert et al 2011 Chen  Manning 2014 advocate the use of dense trainable
embedding vectors for all features For work using neural network architecture with sparse
vector encodings see Johnson  Zhang 2015
Finally it is important to note that representing features as dense vectors is an integral
part of the neural network framework and that consequentially the dierences between
using sparse and dense feature representations are subtler than they may appear at rst
In fact using sparse one-hot vectors as input when training a neural network amounts
to dedicating the rst layer of the network to learning a dense embedding vector for each
feature based on the training data We touch on this in Section 44
Variable Number of Features Continuous Bag of Words Feed-forward networks
assume a xed dimensional input This can easily accommodate the case of a feature-
extraction function that extracts a xed number of features each feature is represented
as a vector and the vectors are concatenated This way each region of the resulting
input vector corresponds to a dierent feature However in some cases the number of
features is not known in advance for example in document classication it is common
that each word in the sentence is a feature We thus need to represent an unbounded
number of features using a xed size vector One way of achieving this is through a so-
called continuous bag of words CBOW representation Mikolov Chen Corrado  Dean
2013 The CBOW is very similar to the traditional bag-of-words representation in which
we discard order information and works by either summing or averaging the embedding
vectors of the corresponding features2
2 Note that if the vfis were one-hot vectors rather than dense feature representations the CBOW and
W CBOW equations above would reduce to the traditional weighted bag-of-words representations
which is in turn equivalent to a sparse feature-vector representation in which each binary indicator
feature corresponds to a unique word
CBOW f1  fk 
kcid88
A simple variation on the CBOW representation is weighted CBOW in which dierent
vectors receive dierent weights
W CBOW f1  fk 
1cid80k
kcid88
aivfi
Here each feature fi has an associated weight ai indicating the relative importance of
the feature For example in a document classication task a feature fi may correspond to
a word in the document and the associated weight ai could be the words TF-IDF score
Distance and Position Features The linear distance in between two words in a sentence
may serve as an informative feature For example in an event extraction task3 we may be
given a trigger word and a candidate argument word and asked to predict if the argument
word is indeed an argument of the trigger The distance or relative position between the
trigger and the argument is a strong signal for this prediction task In the traditional NLP
setup distances are usually encoded by binning the distances into several groups ie 1 2
3 4 510 10 and associating each bin with a one-hot vector In a neural architecture
where the input vector is not composed of binary indicator features it may seem natural to
allocate a single input vector entry to the distance feature where the numeric value of that
entry is the distance However this approach is not taken in practice Instead distance
features are encoded similarly to the other feature types each bin is associated with a
d-dimensional vector and these distance-embedding vectors are then trained as regular
parameters in the network Zeng et al 2014 dos Santos et al 2015 Zhu et al 2015a
Nguyen  Grishman 2015
Feature Combinations Note that the feature extraction stage in the neural-network
settings deals only with extraction of core features This is in contrast to the traditional
linear-model-based NLP systems in which the feature designer had to manually specify not
only the core features of interests but also interactions between them eg introducing not
only a feature stating word is X and a feature stating tag is Y but also combined feature
stating word is X and tag is Y or sometimes even word is X tag is Y and previous word
is Z The combination features are crucial in linear models because they introduce more
dimensions to the input transforming it into a space where the data-points are closer to
being linearly separable On the other hand the space of possible combinations is very
large and the feature designer has to spend a lot of time coming up with an eective
set of feature combinations One of the promises of the non-linear neural network models
is that one needs to dene only the core features The non-linearity of the classier as
dened by the network structure is expected to take care of nding the indicative feature
combinations alleviating the need for feature combination engineering
3 The event extraction task involves identication of events from a predened set of event types For
example identication of purchase events or terror-attack events Each event type can be triggered
by various triggering words commonly verbs and has several slots arguments that needs to be lled
ie who purchased what was purchased at what amount
Kernel methods Shawe-Taylor  Cristianini 2004 and in particular polynomial kernels
Kudo  Matsumoto 2003 also allow the feature designer to specify only core features
leaving the feature combination aspect to the learning algorithm In contrast to neural-
network models kernels methods are convex admitting exact solutions to the optimization
problem However the classication eciency in kernel methods scales linearly with the
size of the training data making them too slow for most practical purposes and not suitable
for training with large datasets On the other hand neural network classication eciency
scales linearly with the size of the network regardless of the training data size
Dimensionality How many dimensions should we allocate for each feature Unfortu-
nately there are no theoretical bounds or even established best-practices in this space
Clearly the dimensionality should grow with the number of the members in the class you
probably want to assign more dimensions to word embeddings than to part-of-speech embed-
dings but how much is enough In current research the dimensionality of word-embedding
vectors range between about 50 to a few hundreds and in some extreme cases thousands
Since the dimensionality of the vectors has a direct eect on memory requirements and
processing time a good rule of thumb would be to experiment with a few dierent sizes
and choose a good trade-o between speed and task accuracy
Vector Sharing Consider a case where you have a few features that share the same
vocabulary For example when assigning a part-of-speech to a given word we may have a
set of features considering the previous word and a set of features considering the next word
When building the input to the classier we will concatenate the vector representation of
the previous word to the vector representation of the next word The classier will then
be able to distinguish the two dierent indicators and treat them dierently But should
the two features share the same vectors Should the vector for dogprevious-word be the
same as the vector of dognext-word Or should we assign them two distinct vectors
This again is mostly an empirical question If you believe words behave dierently when
they appear in dierent positions eg word X behaves like word Y when in the previous
position but X behaves like Z when in the next position then it may be a good idea to
use two dierent vocabularies and assign a dierent set of vectors for each feature type
However if you believe the words behave similarly in both locations then something may
be gained by using a shared vocabulary for both feature types
Networks Output For multi-class classication problems with k classes the networks
output is a k-dimensional vector in which every dimension represents the strength of a
particular output class That is the output remains as in the traditional linear models 
scalar scores to items in a discrete set However as we will see in Section 4 there is a d k
matrix associated with the output layer The columns of this matrix can be thought of as
d dimensional embeddings of the output classes The vector similarities between the vector
representations of the k classes indicate the models learned similarities between the output
classes
Historical Note Representing words as dense vectors for input to a neural network was
introduced by Bengio et al Bengio et al 2003 in the context of neural language modeling
It was introduced to NLP tasks in the pioneering work of Collobert Weston and colleagues
2008 2011 Using embeddings for representing not only words but arbitrary features was
popularized following Chen and Manning 2014
4 Feed-forward Neural Networks
A Brain-inspired metaphor As the name suggest neural-networks are inspired by the
brains computation mechanism which consists of computation units called neurons In the
metaphor a neuron is a computational unit that has scalar inputs and outputs Each input
has an associated weight The neuron multiplies each input by its weight and then sums4
them applies a non-linear function to the result and passes it to its output The neurons
are connected to each other forming a network the output of a neuron may feed into the
inputs of one or more neurons Such networks were shown to be very capable computational
devices If the weights are set correctly a neural network with enough neurons and a non-
linear activation function can approximate a very wide range of mathematical functions we
will be more precise about this later
cid82
cid82
cid82
cid82
cid82
cid82
cid82
cid82
cid82
cid82
cid82
Input layer
Figure 2 Feed-forward neural network with two hidden layers
A typical feed-forward neural network may be drawn as in Figure 2 Each circle is a
neuron with incoming arrows being the neurons inputs and outgoing arrows being the neu-
rons outputs Each arrow carries a weight reecting its importance not shown Neurons
are arranged in layers reecting the ow of information The bottom layer has no incom-
ing arrows and is the input to the network The top-most layer has no outgoing arrows
and is the output of the network The other layers are considered hidden The sigmoid
shape inside the neurons in the middle layers represent a non-linear function typically a
11  ex that is applied to the neurons value before passing it to the output In the
gure each neuron is connected to all of the neurons in the next layer  this is called a
fully-connected layer or an ane layer
4 While summing is the most common operation other functions such as a max are also possible
While the brain metaphor is sexy and intriguing it is also distracting and cumbersome
to manipulate mathematically We therefore switch to using more concise mathematic no-
tation The values of each row of neurons in the network can be thought of as a vector In
Figure 2 the input layer is a 4 dimensional vector x and the layer above it is a 6 dimen-
sional vector h1 The fully connected layer can be thought of as a linear transformation
from 4 dimensions to 6 dimensions A fully-connected layer implements a vector-matrix
multiplication h  xW where the weight of the connection from the ith neuron in the
input row to the jth neuron in the output row is Wij5 The values of h are then trans-
formed by a non-linear function g that is applied to each value before being passed on to the
next input The whole computation from input to output can be written as gxW1W2
where W1 are the weights of the rst layer and W2 are the weights of the second one
In Mathematical Notation From this point on we will abandon the brain metaphor
and describe networks exclusively in terms of vector-matrix operations
The simplest neural network is the perceptron which is a linear function of its inputs
N NP erceptronx  xW  b
x  Rdin W  Rdindout b  Rdout
W is the weight matrix and b is a bias term6 In order to go beyond linear functions we
introduce a non-linear hidden layer the network in Figure 2 has two such layers resulting
in the 1-layer Multi Layer Perceptron MLP1 A one-layer feed-forward neural network
has the form
N NM LP 1x  gxW1  b1W2  b2
x  Rdin W1  Rdind1 b1  Rd1 W2  Rd1d2 b2  Rd2
Here W1 and b1 are a matrix and a bias term for the rst linear transformation of the
input g is a non-linear function that is applied element-wise also called a non-linearity or
an activation function and W2 and b2 are the matrix and bias term for a second linear
transform
Breaking it down xW1b1 is a linear transformation of the input x from din dimensions
to d1 dimensions g is then applied to each of the d1 dimensions and the matrix W2 together
with bias vector b2 are then used to transform the result into the d2 dimensional output
vector The non-linear activation function g has a crucial role in the networks ability to
represent complex functions Without the non-linearity in g the neural network can only
represent linear transformations of the input7
We can add additional linear-transformations and non-linearities resulting in a 2-layer
MLP the network in Figure 2 is of this form
N NM LP 2x  g2g1xW1  b1W2  b2W3
of hj is then hj cid804
i1 xi  wij
5 To see why this is the case denote the weight of the ith input of the jth neuron in h as wij The value
6 The network in gure 2 does not include bias terms A bias term can be added to a layer by adding to
it an additional neuron that does not have any incoming connections whose value is always 1
7 To see why consider that a sequence of linear transformations is still a linear transformation
It is perhaps clearer to write deeper networks like this using intermediary variables
N NM LP 2x y
h1 g1xW1  b1
h2 g2h1W2  b2
y h2W3
The vector resulting from each linear transform is referred to as a layer The outer-most
linear transform results in the output layer and the other linear transforms result in hidden
layers Each hidden layer is followed by a non-linear activation In some cases such as in
the last layer of our example the bias vectors are forced to 0 dropped
Layers resulting from linear transformations are often referred to as fully connected or
ane Other types of architectures exist In particular image recognition problems benet
from convolutional and pooling layers Such layers have uses also in language processing
and will be discussed in Section 9 Networks with more than one hidden layer are said to
be deep networks hence the name deep learning
When describing a neural network one should specify the dimensions of the layers and
the input A layer will expect a din dimensional vector as its input and transform it into a
dout dimensional vector The dimensionality of the layer is taken to be the dimensionality
of its output For a fully connected layer lx  xW  b with input dimensionality din and
output dimensionality dout the dimensions of x is 1  din of W is din  dout and of b is
1  dout
The output of the network is a dout dimensional vector In case dout  1 the networks
output is a scalar Such networks can be used for regression or scoring by considering
the value of the output or for binary classication by consulting the sign of the output
Networks with dout  k  1 can be used for k-class classication by associating each
dimension with a class and looking for the dimension with maximal value Similarly if
the output vector entries are positive and sum to one the output can be interpreted as
a distribution over class assignments such output normalization is typically achieved by
applying a softmax transformation on the output layer see Section 43
The matrices and the bias terms that dene the linear transformations are the parame-
ters of the network It is common to refer to the collection of all parameters as  Together
with the input the parameters determine the networks output The training algorithm is
responsible for setting their values such that the networks predictions are correct Training
is discussed in Section 6
41 Representation Power
In terms of representation power it was shown by Hornik Stinchcombe  White 1989
Cybenko 1989 that MLP1 is a universal approximator  it can approximate with any
desired non-zero amount of error a family of functions8 that include all continuous functions
8 Specically a feed-forward network with linear output layer and at least one hidden layer with a squash-
ing activation function can approximate any Borel measurable function from one nite dimensional space
to another
on a closed and bounded subset of Rn and any function mapping from any nite dimensional
discrete space to another This may suggest there is no reason to go beyond MLP1 to more
complex architectures However the theoretical result does not state how large the hidden
layer should be nor does it say anything about the learnability of the neural network it
states that a representation exists but does not say how easy or hard it is to set the
parameters based on training data and a specic learning algorithm
It also does not
guarantee that a training algorithm will nd the correct function generating our training
data Since in practice we train neural networks on relatively small amounts of data using
a combination of the backpropagation algorithm and variants of stochastic gradient descent
and use hidden layers of relatively modest sizes up to several thousands there is benet
to be had in trying out more complex architectures than MLP1 In many cases however
MLP1 does indeed provide very strong results For further discussion on the representation
power of feed-forward neural networks see Bengio et al 2015 Section 65
42 Common Non-linearities
The non-linearity g can take many forms There is currently no good theory as to which
non-linearity to apply in which conditions and choosing the correct non-linearity for a
given task is for the most part an empirical question I will now go over the common non-
linearities from the literature the sigmoid tanh hard tanh and the rectied linear unit
ReLU Some NLP researchers also experimented with other forms of non-linearities such
as cube and tanh-cube
Sigmoid The sigmoid activation function x  11  ex is an S-shaped function
transforming each value x into the range 0 1
Hyperbolic tangent tanh The hyperbolic tangent tanhx  e2x1
tion is an S-shaped function transforming the values x into the range 1 1
Hard tanh The hard-tanh activation function is an approximation of the tanh function
which is faster to compute and take derivatives of
e2x1 activation func-
1 x  1
otherwise
hardtanhx 
Rectier ReLU The Rectier activation function Glorot Bordes  Bengio 2011
also known as the rectied linear unit is a very simple activation function that is easy to
work with and was shown many times to produce excellent results9 The ReLU unit clips
each value x  0 at 0 Despite its simplicity it performs well for many tasks especially
when combined with the dropout regularization technique see Section 64
9 The technical advantages of the ReLU over the sigmoid and tanh activation functions is that it does not
involve expensive-to-compute functions and more importantly that it does not saturate The sigmoid
and tanh activation are capped at 1 and the gradients at this region of the functions are near zero
driving the entire gradient near zero The ReLU activation does not have this problem making it
especially suitable for networks with multiple layers which are susceptible to the vanishing gradients
problem when trained with the saturating units
ReLU x  max0 x 
cid40
0 x  0
x otherwise
As a rule of thumb ReLU units work better than tanh and tanh works better than
sigmoid10
43 Output Transformations
In many cases the output layer vector is also transformed A common transformation is
the softmax 
x x1     xk
sof tmaxxi 
exicid80k
j1 exj
The result is a vector of non-negative real numbers that sum to one making it a discrete
probability distribution over k possible outcomes
The sof tmax output transformation is used when we are interested in modeling a prob-
ability distribution over the possible output classes To be eective it should be used in
conjunction with a probabilistic training objective such as cross-entropy see Section 45
below
When the softmax transformation is applied to the output of a network without a hidden
layer the result is the well known multinomial logistic regression model also known as a
maximum-entropy classier
44 Embedding Layers
Up until now the discussion ignored the source of x treating it as an arbitrary vector
In an NLP application x is usually composed of various embeddings vectors We can be
explicit about the source of x and include it in the networks denition We introduce c
a function from core features to an input vector
It is common for c to extract the embedding vector associated with each feature and
concatenate them
10 In addition to these activation functions recent works from the NLP community experiment with and
reported success with other forms of non-linearities The Cube activation function gx  x3 was
suggested by Chen  Manning 2014 who found it to be more eective than other non-linearities in
a feed-forward network that was used to predict the actions in a greedy transition-based dependency
parser The tanh cube activation function gx  tanhx3  x was proposed by Pei et al 2015
who found it to be more eective than other non-linearities in a feed-forward network that was used as
a component in a structured-prediction graph-based dependency parser
The cube and tanh-cube activation functions are motivated by the desire to better capture interac-
tions between dierent features While these activation functions are reported to improve performance
in certain situations their general applicability is still to be determined
x  cf1 f2 f3 vf1 vf2 vf3
N NM LP 1x N NM LP 1cf1 f2 f3
N NM LP 1vf1 vf2 vf3
gvf1 vf2 vf3W1  b1W2  b2
Another common choice is for c to sum the embedding vectors this assumes the em-
bedding vectors all share the same dimensionality
x  cf1 f2 f3 vf1  vf2  vf3
N NM LP 1x N NM LP 1cf1 f2 f3
N NM LP 1vf1  vf2  vf3
gvf1  vf2  vf3W1  b1W2  b2
The form of c is an essential part of the networks design In many papers it is common
to refer to c as part of the network and likewise treat the word embeddings vfi as resulting
from an embedding layer or lookup layer Consider a vocabulary of V  words each
embedded as a d dimensional vector The collection of vectors can then be thought of as a
V   d embedding matrix E in which each row corresponds to an embedded feature Let
fi be a V -dimensional vector which is all zeros except from one index corresponding to
the value of the ith feature in which the value is 1 this is called a one-hot vector The
multiplication fiE will then select the corresponding row of E Thus vfi can be dened
in terms of E and fi
And similarly
vfi  fiE
CBOW f1  fk 
kcid88
kcid88
fiE  
The input to the network is then considered to be a collection of one-hot vectors While
this is elegant and well dened mathematically an ecient implementation typically involves
a hash-based data structure mapping features to their corresponding embedding vectors
without going through the one-hot representation
In this tutorial we take c to be separate from the network architecture the networks
inputs are always dense real-valued input vectors and c is applied before the input is passed
the network similar to a feature function in the familiar linear-models terminology How-
ever when training a network the input vector x does remember how it was constructed
and can propagate error gradients back to its component embedding vectors as appropriate
A note on notation When describing network layers that get concatenated vectors x
y and z as input some authors use explicit concatenation x y zW  b while others use
an ane transformation xU  yV  zW  b If the weight matrices U V W in the
ane transformation are dierent than one another the two notations are equivalent
A note on sparse vs dense features Consider a network which uses a traditional
sparse representation for its input vectors and no embedding layer Assuming the set of all
available features is V and we have k on features f1     fk fi  V  the networks input
kcid88
and so the rst layer ignoring the non-linear activation is
x  NV 
kcid88
xW  b  
W  RV d b  Rd
This layer selects rows of W corresponding to the input features in x and sums them
then adding a bias term This is very similar to an embedding layer that produces a CBOW
representation over the features where the matrix W acts as the embedding matrix The
main dierence is the introduction of the bias vector b and the fact that the embedding
layer typically does not undergo a non-linear activation but rather passed on directly to the
rst layer Another dierence is that this scenario forces each feature to receive a separate
vector row in W while the embedding layer provides more exibility allowing for example
for the features next word is dog and previous word is dog to share the same vector
However these dierences are small and subtle When it comes to multi-layer feed-forward
networks the dierence between dense and sparse inputs is smaller than it may seem at
rst sight
45 Loss Functions
When training a neural network more on training in Section 6 below much like when
training a linear classier one denes a loss function Ly y stating the loss of predicting
y when the true output is y The training objective is then to minimize the loss across the
dierent training examples The loss Ly y assigns a numerical score a scalar for the
networks output y given the true expected output y11 The loss is always positive and
should be zero only for cases where the networks output is correct
The parameters of the network the matrices Wi the biases bi and commonly the em-
beddings E are then set in order to minimize the loss L over the training examples usually
it is the sum of the losses over the dierent training examples that is being minimized
The loss can be an arbitrary function mapping two vectors to a scalar For practical
purposes of optimization we restrict ourselves to functions for which we can easily compute
gradients or sub-gradients In most cases it is sucient and advisable to rely on a common
loss function rather than dening your own For a detailed discussion on loss functions for
neural networks see LeCun Chopra Hadsell Ranzato  Huang 2006 LeCun  Huang
2005 Bengio et al 2015 We now discuss some loss functions that are commonly used in
neural networks for NLP
11 In our notation both the models output and the expected output are vectors while in many cases it is
more natural to think of the expected output as a scalar class assignment In such cases y is simply
the corresponding one-hot vector
Hinge binary For binary classication problems the networks output is a single scalar
y and the intended output y is in 11 The classication rule is signy and a
classication is considered correct if y  y  0 meaning that y and y share the same sign
The hinge loss also known as margin loss or SVM loss is dened as
Lhingebinaryy y  max0 1  y  y
The loss is 0 when y and y share the same sign and y  1 Otherwise the loss is linear
In other words the binary hinge loss attempts to achieve a correct classication with a
margin of at least 1
Hinge multiclass The hinge loss was extended to the multiclass setting by Crammer
and Singer 2002 Let y  y1     yn be the networks output vector and y be the one-hot
vector for the correct output class
The classication rule is dened as selecting the class with the highest score
prediction  arg max
Denote by t  arg maxi yi the correct class and by k  arg maxicid54t yi the highest scoring
class such that k cid54 t The multiclass hinge loss is dened as
Lhingemulticlassy y  max0 1  yt  yk
The multiclass hinge loss attempts to score the correct class above all other classes with a
margin of at least 1
Both the binary and multiclass hinge losses are intended to be used with a linear output
layer The hinge losses are useful whenever we require a hard decision rule and do not
attempt to model class membership probability
Log loss The log loss is a common variation of the hinge loss which can be seen as a
soft version of the hinge loss with an innite margin LeCun et al 2006
Llogy y  log1  expyt  yk
Categorical cross-entropy loss The categorical cross-entropy loss also referred to as
negative log likelihood  is used when a probabilistic interpretation of the scores is desired
Let y  y1     yn be a vector representing the true multinomial distribution over the
labels 1     n and let y  y1     yn be the networks output which was transformed by the
sof tmax activation function and represent the class membership conditional distribution
yi  P y  ix The categorical cross entropy loss measures the dissimilarity between
the true label distribution y and the predicted label distribution y and is dened as cross
entropy
Lcrossentropyy y  
cid88
yi logyi
For hard classication problems in which each training example has a single correct
class assignment y is a one-hot vector representing the true class In such cases the cross
entropy can be simplied to
Lcrossentropyhard classicationy y   logyt
where t is the correct class assignment This attempts to set the probability mass assigned
to the correct class t to 1 Because the scores y have been transformed using the sof tmax
function and represent a conditional distribution increasing the mass assigned to the correct
class means decreasing the mass assigned to all the other classes
The cross-entropy loss is very common in the neural networks literature and produces
a multi-class classier which does not only predict the one-best class label but but also
predicts a distribution over the possible labels When using the cross-entropy loss it is
assumed that the networks output is transformed using the sof tmax transformation
Ranking losses
In some settings we are not given supervision in term of labels but
rather as pairs of correct and incorrect items x and xcid48 and our goal is to score correct
items above incorrect ones Such training situations arise when we have only positive
examples and generate negative examples by corrupting a positive example A useful loss
in such scenarios is the margin-based ranking loss dened for a pair of correct and incorrect
examples
Lrankingmarginx xcid48  max0 1  N N x  N N xcid48
where N N x is the score assigned by the network for input vector x The objective is to
score rank correct inputs over incorrect ones with a margin of at least 1
A common variation is to use the log version of the ranking loss
Lrankinglogx xcid48  log1  expN N x  N N xcid48
Examples using the ranking hinge loss in language tasks include training with the aux-
iliary tasks used for deriving pre-trained word embeddings see section 5 in which we are
given a correct word sequence and a corrupted word sequence and our goal is to score
the correct sequence above the corrupt one Collobert  Weston 2008 Similarly Van
de Cruys 2014 used the ranking loss in a selectional-preferences task in which the net-
work was trained to rank correct verb-object pairs above incorrect automatically derived
ones and Weston Bordes Yakhnenko  Usunier 2013 trained a model to score correct
headrelationtrail triplets above corrupted ones in an information-extraction setting An
example of using the ranking log loss can be found in Gao et al 2014 A variation of the
ranking log loss allowing for a dierent margin for the negative and positive class is given
in dos Santos et al 2015
5 Word Embeddings
A main component of the neural-network approach is the use of embeddings  representing
each feature as a vector in a low dimensional space But where do the vectors come from
This section will survey the common approaches
51 Random Initialization
When enough supervised training data is available one can just treat the feature embeddings
the same as the other model parameters initialize the embedding vectors to random values
and let the network-training procedure tune them into good vectors
Some care has to be taken in the way the random initialization is performed The method
used by the eective word2vec implementation Mikolov et al 2013 Mikolov Sutskever
Chen Corrado  Dean 2013 is to initialize the word vectors to uniformly sampled random
cid104
numbers in the range  1
2d  where d is the number of dimensions Another option is to
use xavier initialization see Section 63 and initialize with uniformly sampled values from
6d
6d
cid105
In practice one will often use the random initialization approach to initialize the em-
bedding vectors of commonly occurring features such as part-of-speech tags or individual
letters while using some form of supervised or unsupervised pre-training to initialize the
potentially rare features such as features for individual words The pre-trained vectors can
then either be treated as xed during the network training process or more commonly
treated like the randomly-initialized vectors and further tuned to the task at hand
52 Supervised Task-specic Pre-training
If we are interested in task A for which we only have a limited amount of labeled data for
example syntactic parsing but there is an auxiliary task B say part-of-speech tagging
for which we have much more labeled data we may want to pre-train our word vectors so
that they perform well as predictors for task B and then use the trained vectors for training
task A In this way we can utilize the larger amounts of labeled data we have for task B
When training task A we can either treat the pre-trained vectors as xed or tune them
further for task A Another option is to train jointly for both objectives see Section 7 for
more details
53 Unsupervised Pre-training
The common case is that we do not have an auxiliary task with large enough amounts of
annotated data or maybe we want to help bootstrap the auxiliary task training with better
vectors In such cases we resort to unsupervised methods which can be trained on huge
amounts of unannotated text
The techniques for training the word vectors are essentially those of supervised learning
but instead of supervision for the task that we care about we instead create practically
unlimited number of supervised training instances from raw text hoping that the tasks
that we created will match or be close enough to the nal task we care about12
The key idea behind the unsupervised approaches is that one would like the embedding
vectors of similar words to have similar vectors While word similarity is hard to dene
and is usually very task-dependent the current approaches derive from the distributional
hypothesis Harris 1954 stating that words are similar if they appear in similar contexts
The dierent methods all create supervised training instances in which the goal is to either
predict the word from its context or predict the context from the word
An important benet of training word embeddings on large amounts of unannotated
data is that it provides vector representations for words that do not appear in the super-
vised training set Ideally the representations for these words will be similar to those of
related words that do appear in the training set allowing the model to generalize better on
unseen events It is thus desired that the similarity between word vectors learned by the un-
supervised algorithm captures the same aspects of similarity that are useful for performing
the intended task of the network
Common unsupervised word-embedding algorithms include word2vec 13 Mikolov et al
2013 2013 GloVe Pennington Socher  Manning 2014 and the Collobert and Weston
2008 2011 embeddings algorithm These models are inspired by neural networks and
are based on stochastic gradient training However they are deeply connected to another
family of algorithms which evolved in the NLP and IR communities and that are based on
matrix factorization see Levy  Goldberg 2014b Levy et al 2015 for a discussion
Arguably the choice of auxiliary problem what is being predicted based on what kind
of context aects the resulting vectors much more than the learning method that is being
used to train them We thus focus on the dierent choices of auxiliary problems that are
available and only skim over the details of the training methods Several software packages
for deriving word vectors are available including word2vec14 and Gensim15 implementing
the word2vec models with word-windows based contexts word2vecf16 which is a modied
version of word2vec allowing the use of arbitrary contexts and GloVe17 implementing the
GloVe model Many pre-trained word vectors are also available for download on the web
While beyond the scope of this tutorial it is worth noting that the word embeddings
derived by unsupervised training algorithms have a wide range of applications in NLP
beyond using them for initializing the word-embeddings layer of a neural-network model
54 Training Objectives
Given a word w and its context c dierent algorithms formulate dierent auxiliary tasks
In all cases each word is represented as a d-dimensional vector which is initialized to a
random value Training the model to perform the auxiliary tasks well will result in good
12 The interpretation of creating auxiliary problems from raw text is inspired by Ando and Zhang Ando
 Zhang 2005a 2005b
13 While often treated as a single algorithm word2vec is actually a software package including various
training objectives optimization methods and other hyperparameters See Rong 2014 Levy Goldberg
 Dagan 2015 for a discussion
14 httpscodegooglecompword2vec
15 httpsradimrehurekcomgensim
16 httpsbitbucketorgyoavgoword2vecf
17 httpnlpstanfordeduprojectsglove
word embeddings for relating the words to the contexts which in turn will result in the
embedding vectors for similar words to be similar to each other
Language-modeling inspired approaches such as those taken by Mikolov et al 2013
Mnih  Kavukcuoglu 2013 as well as GloVe Pennington et al 2014 use auxiliary tasks
in which the goal is to predict the word given its context This is posed in a probabilistic
setup trying to model the conditional probability P wc
Other approaches reduce the problem to that of binary classication In addition to
the set D of observed word-context pairs a set D is created from random words and
context pairings The binary classication problem is then does the given w c pair
come from D or not The approaches dier in how the set D is constructed what is
the structure of the classier and what is the objective being optimized Collobert and
Weston 2008 2011 take a margin-based binary ranking approach training a feed-forward
neural network to score correct w c pairs over incorrect ones Mikolov et al 2013 2014
take instead a probabilistic version training a log-bilinear model to predict the probability
P w c  Dw c that the pair come from the corpus rather than the random sample
55 The Choice of Contexts
In most cases the contexts of a word are taken to be other words that appear in its
surrounding either in a short window around it or within the same sentence paragraph
or document In some cases the text is automatically parsed by a syntactic parser and
the contexts are derived from the syntactic neighbourhood induced by the automatic parse
trees Sometimes the denitions of words and context change to include also parts of words
such as prexes or suxes
Neural word embeddings originated from the world of language modeling in which a
network is trained to predict the next word based on a sequence of preceding words Bengio
et al 2003 There the text is used to create auxiliary tasks in which the aim is to predict
a word based on a context the k previous words While training for the language modeling
auxiliary prediction problems indeed produce useful embeddings this approach is needlessly
restricted by the constraints of the language modeling task in which one is allowed to look
only at the previous words If we do not care about language modeling but only about the
resulting embeddings we may do better by ignoring this constraint and taking the context
to be a symmetric window around the focus word
551 Window Approach
The most common approach is a sliding window approach in which auxiliary tasks are
created by looking at a sequence of 2k  1 words The middle word is callled the focus word
and the k words to each side are the contexts Then either a single task is created in which
the goal is to predict the focus word based on all of the context words represented either
using CBOW Mikolov et al 2013 or vector concatenation Collobert  Weston 2008
or 2k distinct tasks are created each pairing the focus word with a dierent context word
The 2k tasks approach popularized by Mikolov et al 2013 is referred to as a skip-gram
model Skip-gram based approaches are shown to be robust and ecient to train Mikolov
et al 2013 Pennington et al 2014 and often produce state of the art results
Eect of Window Size The size of the sliding window has a strong eect on the re-
sulting vector similarities Larger windows tend to produce more topical similarities ie
dog bark and leash will be grouped together as well as walked run and walk-
ing while smaller windows tend to produce more functional and syntactic similarities ie
Poodle Pitbull Rottweiler or walkingrunningapproaching
Positional Windows When using the CBOW or skip-gram context representations all
the dierent context words within the window are treated equally There is no distinction
between context words that are close to the focus words and those that are farther from
it and likewise there is no distinction between context words that appear before the focus
words to context words that appear after it Such information can easily be factored in by
using positional contexts indicating for each context word also its relative position to the
focus words ie instead of the context word being the it becomes the2 indicating
the word appears two positions to the right of the focus word The use of positional context
together with smaller windows tend to produce similarities that are more syntactic with
a strong tendency of grouping together words that share a part of speech as well as being
functionally similar in terms of their semantics Positional vectors were shown by Ling
Dyer Black  Trancoso 2015a to be more eective than window-based vectors when used
to initialize networks for part-of-speech tagging and syntactic dependency parsing
Variants Many variants on the window approach are possible One may lemmatize words
before learning apply text normalization lter too short or too long sentences or remove
capitalization see eg the pre-processing steps described in dos Santos  Gatti 2014
One may sub-sample part of the corpus skipping with some probability the creation of tasks
from windows that have too common or too rare focus words The window size may be
dynamic using a dierent window size at each turn One may weigh the dierent positions
in the window dierently focusing more on trying to predict correctly close word-context
pairs than further away ones Each of these choices will eect the resulting vectors Some
of these hyperparameters and others are discussed in Levy et al 2015
552 Sentences Paragraphs or Documents
Using a skip-grams or CBOW approach one can consider the contexts of a word to be all
the other words that appear with it in the same sentence paragraph or document This is
equivalent to using very large window sizes and is expected to result in word vectors that
capture topical similarity words from the same topic ie words that one would expect to
appear in the same document are likely to receive similar vectors
553 Syntactic Window
Some work replace the linear context within a sentence with a syntactic one Levy 
Goldberg 2014a Bansal Gimpel  Livescu 2014 The text is automatically parsed
using a dependency parser and the context of a word is taken to be the words that are
in its proximity in the parse tree together with the syntactic relation by which they are
connected Such approaches produce highly functional similarities grouping together words
than can ll the same role in a sentence eg colors names of schools verbs of movement
The grouping is also syntactic grouping together words that share an inection Levy 
Goldberg 2014a
554 Multilingual
Another option is using multilingual translation based contexts Hermann  Blunsom
2014 Faruqui  Dyer 2014 For example given a large amount of sentence-aligned parallel
text one can run a bilingual alignment model such as the IBM model 1 or model 2 ie
using the GIZA software and then use the produced alignments to derive word contexts
Here the context of a word instance are the foreign language words that are aligned to it
Such alignments tend to result in synonym words receiving similar vectors Some authors
work instead on the sentence alignment level without relying on word alignments An
appealing method is to mix a monolingual window-based approach with a multilingual
approach creating both kinds of auxiliary tasks This is likely to produce vectors that are
similar to the window-based approach but reducing the somewhat undesired eect of the
window-based approach in which antonyms eg hot and cold high and low tend to receive
similar vectors Faruqui  Dyer 2014
555 Character-based and Sub-word Representations
An interesting line of work attempts to derive the vector representation of a word from the
characters that compose it Such approaches are likely to be particularly useful for tasks
which are syntactic in nature as the character patterns within words are strongly related
to their syntactic function These approaches also have the benet of producing very small
model sizes only one vector for each character in the alphabet together with a handful of
small matrices needs to be stored and being able to provide an embedding vector for every
word that may be encountered dos Santos and Gatti 2014 and dos Santos and Zadrozny
2014 model the embedding of a word using a convolutional network see Section 9 over
the characters Ling et al 2015b model the embedding of a word using the concatenation
of the nal states of two RNN LSTM encoders Section 10 one reading the characters
from left to right and the other from right to left Both produce very strong results for
part-of-speech tagging The work of Ballesteros et al 2015 show that the two-LSTMs
encoding of Ling et al 2015b is benecial also for representing words in dependency
parsing of morphologically rich languages
Deriving representations of words from the representations of their characters is moti-
vated by the unknown words problem  what do you do when you encounter a word for
which you do not have an embedding vector Working on the level of characters alleviates
this problem to a large extent as the vocabulary of possible characters is much smaller
than the vocabulary of possible words However working on the character level is very
challenging as the relationship between form characters and function syntax semantics
in language is quite loose Restricting oneself to stay on the character level may be an
unnecessarily hard constraint Some researchers propose a middle-ground in which a word
is represented as a combination of a vector for the word itself with vectors of sub-word
units that comprise it The sub-word embeddings then help in sharing information between
dierent words with similar forms as well as allowing back-o to the subword level when
the word is not observed At the same time the models are not forced to rely solely on
form when enough observations of the word are available Botha and Blunsom 2014 sug-
gest to model the embedding vector of a word as a sum of the word-specic vector if such
vector is available with vectors for the dierent morphological components that comprise
it the components are derived using Morfessor Creutz  Lagus 2007 an unsupervised
morphological segmentation method Gao et al Gao et al 2014 suggest using as core
features not only the word form itself but also a unique feature hence a unique embedding
vector for each of the letter-trigrams in the word
6 Neural Network Training
Neural network training is done by trying to minimize a loss function over a training set
using a gradient-based method Roughly speaking all training methods work by repeatedly
computing an estimate of the error over the dataset computing the gradient with respect
to the error and then moving the parameters in the direction of the gradient Models dier
in how the error estimate is computed and how moving in the direction of the gradient
is dened We describe the basic algorithm stochastic gradient descent SGD and then
briey mention the other approaches with pointers for further reading Gradient calculation
is central to the approach Gradients can be eciently and automatically computed using
reverse mode dierentiation on a computation graph  a general algorithmic framework for
automatically computing the gradient of any network and loss function
61 Stochastic Gradient Training
The common approach for training neural networks is using the stochastic gradient descent
SGD algorithm Bottou 2012 LeCun Bottou Orr  Muller 1998a or a variant of it
SGD is a general optimization algorithm It receives a function f parameterized by  a
loss function and desired input and output pairs It then attempts to set the parameters 
such that the loss of f with respect to the training examples is small The algorithm works
as follows
Algorithm 1 Online Stochastic Gradient Descent Training
1 Input Function f x  parameterized with parameters 
2 Input Training set of inputs x1     xn and outputs y1     yn
3 Input Loss function L
4 while stopping criteria not met do
Sample a training example xi yi
Compute the loss Lf xi  yi
g  gradients of Lf xi  yi wrt 
    kg
9 return 
cid80n
The goal of the algorithm is to set the parameters  so as to minimize the total loss
i1 Lf xi  yi over the training set It works by repeatedly sampling a training exam-
ple and computing the gradient of the error on the example with respect to the parameters
 line 7  the input and expected output are assumed to be xed and the loss is treated
as a function of the parameters  The parameters  are then updated in the direction of
the gradient scaled by a learning rate k line 8 For further discussion on setting the
learning rate see Section 63
Note that the error calculated in line 6 is based on a single training example and is thus
just a rough estimate of the corpus-wide loss that we are aiming to minimize The noise in
the loss computation may result in inaccurate gradients A common way of reducing this
noise is to estimate the error and the gradients based on a sample of m examples This
gives rise to the minibatch SGD algorithm
Algorithm 2 Minibatch Stochastic Gradient Descent Training
1 Input Function f x  parameterized with parameters 
2 Input Training set of inputs x1     xn and outputs y1     yn
3 Input Loss function L
4 while stopping criteria not met do
Sample a minibatch of m examples x1 y1     xm ym
g  0
for i  1 to m do
Compute the loss Lf xi  yi
g  g  gradients of 1
m Lf xi  yi wrt 
11 return 
    kg
In lines 6  9 the algorithm estimates the gradient of the corpus loss based on the
minibatch After the loop g contains the gradient estimate and the parameters  are
updated toward g The minibatch size can vary in size from m  1 to m  n Higher
values provide better estimates of the corpus-wide gradients while smaller values allow
more updates and in turn faster convergence Besides the improved accuracy of the gradients
estimation the minibatch algorithm provides opportunities for improved training eciency
For modest sizes of m some computing architectures ie GPUs allow an ecient parallel
implementation of the computation in lines 69 With a small enough learning rate SGD is
guaranteed to converge to a global optimum if the function is convex However it can also
be used to optimize non-convex functions such as neural-network While there are no longer
guarantees of nding a global optimum the algorithm proved to be robust and performs
well in practice
When training a neural network the parameterized function f is the neural network
and the parameters  are the layer-transfer matrices bias terms embedding matrices and
so on The gradient computation is a key step in the SGD algorithm as well as in all
other neural network training algorithms The question is then how to compute the
gradients of the networks error with respect to the parameters Fortunately there is an easy
solution in the form of the backpropagation algorithm Rumelhart Hinton  Williams 1986
Lecun Bottou Bengio  Haner 1998b The backpropagation algorithm is a fancy name
for methodologically computing the derivatives of a complex expression using the chain-
rule while caching intermediary results More generally the backpropagation algorithm
is a special case of the reverse-mode automatic dierentiation algorithm Neidinger 2010
Section 7 Baydin Pearlmutter Radul  Siskind 2015 Bengio 2012The following
section describes reverse mode automatic dierentiation in the context of the computation
graph abstraction
Beyond SGD While the SGD algorithm can and often does produce good results more
advanced algorithms are also available The SGDMomentum Polyak 1964 and Nesterov
Momentum Sutskever Martens Dahl  Hinton 2013 algorithms are variants of SGD in
which previous gradients are accumulated and aect the current update Adaptive learning
rate algorithms including AdaGrad Duchi Hazan  Singer 2011 AdaDelta Zeiler 2012
RMSProp Tieleman  Hinton 2012 and Adam Kingma  Ba 2014 are designed to
select the learning rate for each minibatch sometimes on a per-coordinate basis potentially
alleviating the need of ddling with learning rate scheduling For details of these algorithms
see the original papers or Bengio et al 2015 Sections 83 84 As many neural-network
software frameworks provide implementations of these algorithms it is easy and sometimes
worthwhile to try out dierent variants
62 The Computation Graph Abstraction
While one can compute the gradients of the various parameters of a network by hand and
implement them in code this procedure is cumbersome and error prone For most pur-
poses it is preferable to use automatic tools for gradient computation Bengio 2012 The
computation-graph abstraction allows us to easily construct arbitrary networks evaluate
their predictions for given inputs forward pass and compute gradients for their parameters
with respect to arbitrary scalar losses backward pass
A computation graph is a representation of an arbitrary mathematical computation as
a graph It is a directed acyclic graph DAG in which nodes correspond to mathematical
operations or bound variables and edges correspond to the ow of intermediary values
between the nodes The graph structure denes the order of the computation in terms of
the dependencies between the dierent components The graph is a DAG and not a tree as
the result of one operation can be the input of several continuations Consider for example
a graph for the computation of a  b  1  a  b  2
The computation of a b is shared We restrict ourselves to the case where the computation
graph is connected
Since a neural network is essentially a mathematical expression it can be represented
as a computation graph
For example Figure 3a presents the computation graph for a 1-layer MLP with a soft-
max output transformation In our notation oval nodes represent mathematical operations
or functions and shaded rectangle nodes represent parameters bound variables Network
inputs are treated as constants and drawn without a surrounding node Input and param-
eter nodes have no incoming arcs and output nodes have no outgoing arcs The output of
each node is a matrix the dimensionality of which is indicated above the node
This graph is incomplete without specifying the inputs we cannot compute an output
Figure 3b shows a complete graph for an MLP that takes three words as inputs and predicts
the distribution over part-of-speech tags for the third word This graph can be used for
prediction but not for training as the output is a vector not a scalar and the graph does
not take into account the correct answer or the loss term Finally the graph in 3c shows the
computation graph for a specic training example in which the inputs are the embeddings
ab12Figure 3 Computation Graph for MLP1 a Graph with unbound input b Graph
with concrete input c Graph with concrete input expected output and loss
of the words the black dog and the expected output is NOUN whose index is
Once the graph is built it is straightforward to run either a forward computation com-
pute the result of the computation or a backward computation computing the gradients
as we show below Constructing the graphs may look daunting but is actually very easy
using dedicated software libraries and APIs
Forward Computation The forward pass computes the outputs of the nodes in the
graph Since each nodes output depends only on itself and on its incoming edges it is
trivial to compute the outputs of all nodes by traversing the nodes in a topological order and
computing the output of each node given the already computed outputs of its predecessors
More formally in a graph of N nodes we associate each node with an index i according
to their topological ordering Let fi be the function computed by node i eg multiplication
addition     Let i be the parent nodes of node i and 1i  j  i  j the
children nodes of node i these are the arguments of fi Denote by vi the output of node
x1150W115020MUL120ADD120b1120tanh120W22017b2117MUL117ADD117softmax117aconcat1150lookup150lookup150lookup150theblackdogEV50W115020MUL120ADD120b1120tanh120W22017b2117MUL117ADD117softmax117bconcat1150lookup150lookup150lookup150theblackdogEV50W115020MUL120ADD120b1120tanh120W22017b2117MUL117ADD117softmax117pick115log11neg11ci that is the application of fi to the output values of its arguments 1i For variable
and input nodes fi is a constant function and 1i is empty The Forward algorithm
computes the values vi for all i  1 N 
Algorithm 3 Computation Graph Forward Pass
1 for i  1 to N do
Let a1     am  1i
vi  fiva1     vam
Backward Computation Derivatives Backprop The backward pass begins by des-
ignating a node N with scalar 1 1 output as a loss-node and running forward computa-
tion up to that node The backward computation will computes the gradients with respect
 The backpropagation algorithm is
to that nodes value Denote by di the quantity
used to compute the values di for all nodes i
The backward pass lls a table di as follows
Algorithm 4 Computation Graph Backward Pass Backpropagation
1 dN   1
2 for i  N-1 to 1 do
cid80
di 
ji dj 
is the partial derivative of fj1j wrt the argument i  1j
The quantity
This value depends on the function fj and the values va1     vam where a1     am 
1j of its arguments which were computed in the forward pass
Thus in order to dene a new kind of node one need to dene two methods one for
calculating the forward value vi based on the nodes inputs and the another for calculating
for each x  1i
For further information on automatic dierentiation see Neidinger 2010 Section 7
Baydin et al 2015 For more in depth discussion of the backpropagation algorithm and
computation graphs also called ow graphs see Bengio et al 2015 Section 64 Lecun
et al 1998b Bengio 2012 For a popular yet technical presentation see Chris Olahs
description at httpcolahgithubioposts2015-08-Backprop
Software Several software packages implement the computation-graph model including
Theano18 Chainer19 penne20 and CNNpyCNN21 All these packages support all the es-
sential components node types for dening a wide range of neural network architectures
covering the structures described in this tutorial and more Graph creation is made almost
transparent by use of operator overloading The framework denes a type for representing
graph nodes commonly called expressions methods for constructing nodes for inputs and
18 httpdeeplearningnetsoftwaretheano
19 httpchainerorg
20 httpsbitbucketorgndnlppenne
21 httpsgithubcomclabcnn
parameters and a set of functions and mathematical operations that take expressions as
input and result in more complex expressions For example the python code for creating
the computation graph from Figure 3c using the pyCNN framework is
from pycnn import 
 model initialization
model  Model
modeladdparametersW1 20150
modeladdparametersb1 20
modeladdparametersW2 1720
modeladdparametersb2 17
modeladdlookupparameterswords 100 50
 Building the computation graph
renewcg  create a new graph
 Wrap the model parameters as graph-nodes
W1  parametermodelW1
b1  parametermodelb1
W2  parametermodelW2
b2  parametermodelb2
def getindexx return 1
 Generate the embeddings layer
vblack  lookupmodelwords getindexblack
 lookupmodelwords getindexdog
 lookupmodelwords getindexthe
 Connect the leaf nodes into a complete graph
x  concatenatevthe vblack vdog
output  softmaxW2tanhW1xb1b2
loss  -logpickoutput 5
lossvalue  lossforward
lossbackward  the gradient is computed
 and stored in the corresponding
 parameters
Most of the code involves various initializations the rst block denes model parameters
that are be shared between dierent computation graphs recall that each graph corresponds
to a specic training example The second block turns the model parameters into the graph-
node Expression types The third block retrieves the Expressions for the embeddings of the
input words Finally the fourth block is where the graph is created Note how transparent
the graph creation is  there is an almost a one-to-one correspondence between creating
the graph and describing it mathematically The last block shows a forward and backward
pass The other software frameworks follow similar patterns
Theano involves an optimizing compiler for computation graphs which is both a blessing
and a curse On the one hand once compiled large graphs can be run eciently on either
the CPU or a GPU making it ideal for large graphs with a xed structure where only the
inputs change between instances However the compilation step itself can be costly and it
makes the interface a bit cumbersome to work with In contrast the other packages focus on
building large and dynamic computation graphs and executing them on the y without a
compilation step While the execution speed may suer with respect to Theanos optimized
version these packages are especially convenient when working with the recurrent and
recursive networks described in Sections 10 12 as well as in structured prediction settings
as described in Section 8
Implementation Recipe Using the computation graph abstraction the pseudo-code for
a network training algorithm is given in Algorithm 5
Algorithm 5 Neural Network Training with Computation Graph Abstraction using mini-
batches of size 1
1 Dene network parameters
2 for iteration  1 to N do
for Training example xi yi in dataset do
loss node  build computation graphxi yi parameters
loss nodeforward
gradients  loss nodebackward
parameters  update parametersparameters gradients
8 return parameters
Here build computation graph is a user-dened function that builds the computation
graph for the given input output and network structure returning a single loss node
update parameters is an optimizer specic update rule The recipe species that a new
graph is created for each training example This accommodates cases in which the network
structure varies between training example such as recurrent and recursive neural networks
to be discussed in Sections 10  12 For networks with xed structures such as an MLPs it
may be more ecient to create one base computation graph and vary only the inputs and
expected outputs between examples
Network Composition As long as the networks output is a vector 1  k matrix it
is trivial to compose networks by making the output of one network the input of another
creating arbitrary networks The computation graph abstractions makes this ability explicit
a node in the computation graph can itself be a computation graph with a designated output
node One can then design arbitrarily deep and complex networks and be able to easily
evaluate and train them thanks to automatic forward and gradient computation This makes
it easy to dene and train networks for structured outputs and multi-objective training as
we discuss in Section 7 as well as complex recurrent and recursive networks as discussed
in Sections 1012
63 Optimization Issues
Once the gradient computation is taken care of the network is trained using SGD or another
gradient-based optimization algorithm The function being optimized is not convex and for
a long time training of neural networks was considered a black art which can only be done
by selected few Indeed many parameters aect the optimization process and care has to
be taken to tune these parameters While this tutorial is not intended as a comprehensive
guide to successfully training neural networks we do list here a few of the prominent issues
For further discussion on optimization techniques and algorithms for neural networks refer
to Bengio et al 2015 Chapter 8 For some theoretical discussion and analysis refer
to Glorot  Bengio 2010 For various practical tips and recommendations see LeCun
et al 1998a Bottou 2012
Initialization The non-convexity of the loss function means the optimization procedure
may get stuck in a local minimum or a saddle point and that starting from dierent initial
points eg dierent random values for the parameters may result in dierent results Thus
it is advised to run several restarts of the training starting at dierent random initializations
and choosing the best one based on a development set22 The amount of variance in the
results is dierent for dierent network formulations and datasets and cannot be predicted
in advance
The magnitude of the random values has an important eect on the success of training
An eective scheme due to Glorot and Bengio 2010 called xavier initialization after
Glorots rst name suggests initializing a weight matrix W  Rdindout as
cid34
W  U
din  dout
din  dout
cid35
where U a b is a uniformly sampled random value in the range a b This advice works
well on many occasions and is the preferred default initialization method by many
Analysis by He et al 2015 suggests that when using ReLU non-linearities the weights
should be initialized by sampling from a zero-mean Gaussian distribution whose standard
deviation is
 This initialization was found by He et al to work better than xavier
initialization in an image classication task especially when deep networks were involved
cid113 2
Vanishing and Exploding Gradients
In deep networks it is common for the error
gradients to either vanish become exceedingly close to 0 or explode become exceedingly
high as they propagate back through the computation graph The problem becomes more
severe in deeper networks and especially so in recursive and recurrent networks Pascanu
Mikolov  Bengio 2012 Dealing with the vanishing gradients problem is still an open
research question Solutions include making the networks shallower step-wise training rst
train the rst layers based on some auxiliary output signal then x them and train the upper
layers of the complete network based on the real task signal or specialized architectures
that are designed to assist in gradient ow eg the LSTM and GRU architectures for
recurrent networks discussed in Section 11 Dealing with the exploding gradients has
a simple but very eective solution clipping the gradients if their norm exceeds a given
threshold Let g be the gradients of all parameters in the network and cid107gcid107 be their L2
norm Pascanu et al 2012 suggest to set g  threshold
Saturation and Dead Neurons Layers with tanh and sigmoid activations can become
saturated  resulting in output values for that layer that are all close to one the upper-
limit of the activation function Saturated neurons have very small gradients and should
be avoided Layers with the ReLU activation cannot be saturated but can die  most
or all values are negative and thus clipped at zero for all inputs resulting in a gradient
of zero for that layer If your network does not train well it is advisable to monitor the
network for saturated or dead layers Saturated neurons are caused by too large values
g if cid107gcid107  threshold
cid107gcid107
22 When debugging and for reproducibility of results it is advised to used a xed random seed
entering the layer This may be controlled for by changing the initialization scaling the
range of the input values or changing the learning rate Dead neurons are caused by all
weights entering the layer being negative for example this can happen after a large gradient
update Reducing the learning rate will help in this situation For saturated layers another
option is to normalize the values in the saturated layer after the activation ie instead of
gh  tanhh using gh  tanhh
 Layer normalization is an eective measure for
cid107tanhhcid107
countering saturation but is also expensive in terms of gradient computation
Shuing The order in which the training examples are presented to the network is im-
portant The SGD formulation above species selecting a random example in each turn
In practice most implementations go over the training example in order It is advised to
shue the training examples before each pass through the data
Learning Rate Selection of the learning rate is important Too large learning rates
will prevent the network from converging on an eective solution Too small learning
rates will take very long time to converge As a rule of thumb one should experiment
with a range of initial learning rates in range 0 1 eg 0001 001 01 1 Monitor
the networks loss over time and decrease the learning rate once the network seem to be
stuck in a xed region Learning rate scheduling decrease the rate as a function of the
number of observed minibatches A common schedule is dividing the initial learning rate
by the iteration number Leon Bottou 2012 recommends using a learning rate of the form
t  01  0t1 where 0 is the initial learning rate t is the learning rate to use on
the tth training example and  is an additional hyperparameter He further recommends
determining a good value of 0 based on a small sample of the data prior to running on the
entire dataset
Minibatches Parameter updates occur either every training example minibatches of size
1 or every k training examples Some problems benet from training with larger minibatch
sizes In terms of the computation graph abstraction one can create a computation graph
for each of the k training examples and then connecting the k loss nodes under an averaging
node whose output will be the loss of the minibatch Large minibatched training can also
be benecial in terms of computation eciency on specialized computing architectures such
as GPUs This is beyond the scope of this tutorial
64 Regularization
Neural network models have many parameters and overtting can easily occur Overtting
can be alleviated to some extent by regularization A common regularization method is
L2 regularization placing a squared penalty on parameters with large values by adding
an additive 
2cid107cid1072 term to the objective function to be minimized where  is the set of
model parameters cid107  cid1072 is the squared L2 norm sum of squares of the values and  is a
hyperparameter controlling the amount of regularization
A recently proposed alternative regularization method is dropout Hinton Srivastava
Krizhevsky Sutskever  Salakhutdinov 2012 The dropout method is designed to prevent
the network from learning to rely on specic weights It works by randomly dropping set-
ting to 0 half of the neurons in the network or in a specic layer in each training example
Work by Wager et al 2013 establishes a strong connection between the dropout method
and L2 regularization Gal and Gharamani 2015 show that a multi-layer perceptron with
dropout applied at every layer can be interpreted as Bayesian model averaging
The dropout technique is one of the key factors contributing to very strong results of
neural-network methods on image classication tasks Krizhevsky Sutskever  Hinton
2012 especially when combined with ReLU activation units Dahl Sainath  Hinton
2013 The dropout technique is eective also in NLP applications of neural networks
7 Cascading and Multi-task Learning
The combination of online training methods with automatic gradient computations using
the computation graph abstraction allows for an easy implementation of model cascading
parameter sharing and multi-task learning
Model cascading is a powerful technique in which large networks are built by composing
them out of smaller component networks For example we may have a feed-forward network
for predicting the part of speech of a word based on its neighbouring words andor the
characters that compose it In a pipeline approach we would use this network for predicting
parts of speech and then feed the predictions as input features to neural network that does
syntactic chunking or parsing Instead we could think of the hidden layers of this network
as an encoding that captures the relevant information for predicting the part of speech In
a cascading approach we take the hidden layers of this network and connect them and not
the part of speech prediction themselves as the inputs for the syntactic network We now
have a larger network that takes as input sequences of words and characters and outputs a
syntactic structure The computation graph abstraction allows us to easily propagate the
error gradients from the syntactic task loss all the way back to the characters
To combat the vanishing gradient problem of deep networks as well as to make better
use of available training material the individual component networks parameters can be
bootstrapped by training them separately on a relevant task before plugging them in to
the larger network for further tuning For example the part-of-speech predicting network
can be trained to accurately predict parts-of-speech on a relatively large annotated corpus
before plugging its hidden layer into the syntactic parsing network for which less training
data is available In case the training data provide direct supervision for both tasks we can
make use of it during training by creating a network with two outputs one for each task
computing a separate loss for each output and then summing the losses into a single node
from which we backpropagate the error gradients
Model cascading is very common when using convolutional recursive and recurrent
neural networks where for example a recurrent network is used to encode a sentence into
a xed sized vector which is then used as the input of another network The supervision
signal of the recurrent network comes primarily from the upper network that consumes the
recurrent networks output as it inputs
Multi-task learning is used when we have related prediction tasks that do not neces-
sarily feed into one another but we do believe that information that is useful for one type
of prediction can be useful also to some of the other tasks For example chunking named
entity recognition NER and language modeling are examples of synergistic tasks Infor-
mation for predicting chunk boundaries named-entity boundaries and the next word in the
sentence all rely on some shared underlying syntactic-semantic representation Instead of
training a separate network for each task we can create a single network with several out-
puts A common approach is to have a multi-layer feed-forward network whose nal hidden
layer or a concatenation of all hidden layers is then passed to dierent output layers This
way most of the parameters of the network are shared between the dierent tasks Useful
information learned from one task can then help to disambiguate other tasks Again the
computation graph abstraction makes it very easy to construct such networks and compute
the gradients for them by computing a separate loss for each available supervision signal
and then summing the losses into a single loss that is used for computing the gradients In
case we have several corpora each with dierent kind of supervision signal eg we have
one corpus for NER and another for chunking the training procedure will shue all of the
available training example performing gradient computation and updates with respect to
a dierent loss in every turn Multi-task learning in the context of language-processing is
introduced and discussed in Collobert et al 2011
8 Structured Output Prediction
Many problems in NLP involve structured outputs cases where the desired output is not
a class label or distribution over class labels but a structured object such as a sequence
a tree or a graph Canonical examples are sequence tagging eg part-of-speech tagging
sequence segmentation chunking NER and syntactic parsing In this section we discuss
how feed-forward neural network models can be used for structured tasks In later sections
we discuss specialized neural network models for dealing with sequences Section 10 and
trees Section 12
81 Greedy Structured Prediction
The greedy approach to structured prediction is to decompose the structure prediction
problem into a sequence of local prediction problems and training a classier to perform
each local decision At test time the trained classier is used in a greedy manner Examples
of this approach are left-to-right tagging models Gimenez  Marquez 2004 and greedy
transition-based parsing Nivre 2008 Such approaches are easily adapted to use neural
networks by simply replacing the local classier from a linear classier such as an SVM or a
logistic regression model to a neural network as demonstrated in Chen  Manning 2014
Lewis  Steedman 2014
The greedy approaches suer from error propagation where mistakes in early decisions
carry over and inuence later decisions The overall higher accuracy achievable with non-
linear neural network classiers helps in osetting this problem to some extent In addition
training techniques were proposed for mitigating the error propagation problem by either
attempting to take easier predictions before harder ones the easy-rst approach Goldberg
 Elhadad 2010 or making training conditions more similar to testing conditions by
exposing the training procedure to inputs that result from likely mistakes Hal Daume III
Langford  Marcu 2009 Goldberg  Nivre 2013 These are eective also for training
greedy neural network models as demonstrated by Ma et al Ma Zhang  Zhu 2014
easy-rst tagger and  dynamic oracle training for greedy dependency parsing
82 Search Based Structured Prediction
The common approach to predicting natural language structures is search based For in-
depth discussion of search-based structure prediction in NLP see the book by Smith Smith
2011 The techniques can easily be adapted to use a neural-network In the neural-networks
literature such models were discussed under the framework of energy based learning LeCun
et al 2006 Section 7 They are presented here using setup and terminology familiar to
the NLP community
Search-based structured prediction is formulated as a search problem over possible struc-
predictx  arg max
yYx
scorex y
where x is an input structure y is an output over x in a typical example x is a sentence
and y is a tag-assignment or a parse-tree over the sentence Yx is the set of all valid
structures over x and we are looking for an output y that will maximize the score of the
x y pair
The scoring function is dened as a linear model
where  is a feature extraction function and w is a weight vector
scorex y  x y  w
In order to make the search for the optimal y tractable the structure y is decomposed
into parts and the feature function is dened in terms of the parts where p is a part-local
feature extraction function
cid88
x y 
ppartsxy
Each part is scored separately and the structure score is the sum of the component
parts scores
scorex y w  x y  w 
cid88
p 
cid88
w  p 
cid88
scorep
where p  y is a shorthand for p  partsx y The decomposition of y into parts is such
that there exists an inference algorithm that allows for ecient search for the best scoring
structure given the scores of the individual parts
One can now trivially replace the linear scoring function over parts with a neural-
network
scorex y 
cid88
where cp maps the part p into a din dimensional vector
In case of a one hidden-layer feed-forward network
scorep 
N N cp
cid88
cid88
cid88
scorex y 
N NM LP 1cp 
gcpW1  b1w
cp  Rdin W1  Rdind1 b1  Rd1 w  Rd1 A common objective in structured
prediction is making the gold structure y score higher than any other structure ycid48 leading
to the following generalized perceptron loss
ycid48 scorex ycid48  scorex y
In terms of implementation this means create a computation graph CGp for each of
the possible parts and calculate its score Then run inference over the scored parts to
nd the best scoring structure ycid48 Connect the output nodes of the computation graphs
corresponding to parts in the gold predicted structure y ycid48 into a summing node CGy
CGcid48y Connect CGy and CGcid48y using a minus node CGl and compute the gradients
As argued in LeCun et al 2006 Section 5 the generalized perceptron loss may not
be a good loss function when training structured prediction neural networks as it does not
have a margin and a margin-based hinge loss is preferred
max0 m  scorex y  max
cid54y
ycid48
scorex ycid48
It is trivial to modify the implementation above to work with the hinge loss
Note that in both cases we lose the nice properties of the linear model In particular the
model is no longer convex This is to be expected as even the simplest non-linear neural
network is already non-convex Nonetheless we could still use standard neural-network
optimization techniques to train the structured model
Training and inference is slower as we have to evaluate the neural network and take
gradients partsx y times
Structured prediction is a vast eld and is beyond the scope of this tutorial but loss func-
tions regularizers and methods described in eg Smith 2011 such as cost-augmented
decoding can be easily applied or adapted to the neural-network framework23
Probabilistic objective CRF
In a probabilistic framework CRF we treat each
of the parts scores as a clique potential see Smith 2011 and dene the score of each
structure y to be
cid80
cid80
py eN N cp
rameters of the network such that corpus conditional log likelihoodcid80
cid80
cid80
py escorep
The scoring function denes a conditional distribution P yx and we wish to set the pa-
scoreCRF x y  P yx 
ycid48
Yx
pycid48 eN N cp
ycid48
Yx
pycid48 escorep
xiyitraining log P yixi
cid80
cid80
is maximized
The loss for a given training example x y is then  log scoreCRF x y Taking the
gradient with respect to the loss is as involved as building the associated computation
graph The tricky part is the denominator the partition function which requires summing
over the potentially exponentially many structures in Y However for some problems a
dynamic programming algorithm exists for eciently solving the summation in polynomial
time When such an algorithm exists it can be adapted to also create a polynomial-size
computation graph
When an ecient enough algorithm for computing the partition function is not available
approximate methods can be used For example one may use beam search for inference
and for the partition function sum over the structures remaining in the beam instead of
over the exponentially large Yx
A hinge based approached was used by Pei et al 2015 for arc-factored dependency
parsing and the probabilistic approach by Durrett and Klein Durrett  Klein 2015 for a
CRF constituency parser The approximate beam-based partition function was eectively
used by Zhou et al 2015 in a transition based parser
Reranking When searching over all possible structures is intractable inecient or hard
to integrate into a model reranking methods are often used In the reranking framework
Charniak  Johnson 2005 Collins  Koo 2005 a base model is used to produce a
23 One should keep in mind that the resulting objectives are no longer convex and so lack the formal guar-
antees and bounds associated with convex optimization problems Similarly the theory learning bounds
and guarantees associated with the algorithms do not automatically transfer to the neural versions
list of the k-best scoring structures A more complex model is then trained to score the
candidates in the k-best list such that the best structure with respect to the gold one is
scored highest As the search is now performed over k items rather than over an exponential
space the complex model can condition on extract features from arbitrary aspects of the
scored structure Reranking methods are natural candidates for structured prediction using
neural-network models as they allow the modeler to focus on the feature extraction and
network structure while removing the need to integrate the neural network scoring into a
decoder Indeed reranking methods are often used for experimenting with neural models
that are not straightforward to integrate into a decoder such as convolutional recurrent
and recursive networks which will be discussed in later sections Works using the reranking
approach include Socher et al 2013 Auli et al 2013 Le  Zuidema 2014 Zhu et al
MEMM and hybrid approaches Other formulations are of course also possible For
example an MEMM McCallum Freitag  Pereira 2000 can be trivially adapted to the
neural network world by replacing the logistic regression Maximum Entropy component
with an MLP
Hybrid approaches between neural networks and linear models are also explored
particular Weiss et al Weiss et al 2015 report strong results for transition-based depen-
dency parsing in a two-stage model In the rst stage a static feed-forward neural network
MLP2 is trained to perform well on each of the individual decisions of the structured
problem in isolation In the second stage the neural network model is held xed and the
dierent layers output as well as hidden layer vectors for each input are then concatenated
and used as the input features of a linear structured perceptron model Collins 2002 that
is trained to perform beam-search for the best resulting structure While it is not clear
that such training regime is more eective than training a single structured-prediction neu-
ral network the use of two simpler isolated models allowed the researchers to perform a
much more extensive hyper-parameter search eg tuning layer sizes activation functions
learning rates and so on for each model than is feasible with more complicated networks
9 Convolutional Layers
Sometimes we are interested in making predictions based on ordered sets of items eg
the sequence of words in a sentence the sequence of sentences in a document and so on
Consider for example predicting the sentiment positive negative or neutral of a sentence
Some of the sentence words are very informative of the sentiment other words are less
informative and to a good approximation an informative clue is informative regardless
of its position in the sentence We would like to feed all of the sentence words into a
learner and let the training process gure out the important clues One possible solution is
feeding a CBOW representation into a fully connected network such as an MLP However
a downside of the CBOW approach is that it ignores the ordering information completely
assigning the sentences it was not good it was actually quite bad and it was not bad
it was actually quite good the exact same representation While the global position of the
indicators not good and not bad does not matter for the classication task the local
ordering of the words that the word not appears right before the word bad is very
important A naive approach would suggest embedding word-pairs bi-grams rather than
words and building a CBOW over the embedded bigrams While such architecture could be
eective it will result in huge embedding matrices will not scale for longer n-grams and will
suer from data sparsity problems as it does not share statistical strength between dierent
n-grams the embedding of quite good and very good are completely independent of
one another so if the learner saw only one of them during training it will not be able to
deduce anything about the other based on its component words The convolution-and-
pooling also called convolutional neural networks or CNNs architecture is an elegant and
robust solution to the this modeling problem A convolutional neural network is designed
to identify indicative local predictors in a large structure and combine them to produce a
xed size vector representation of the structure capturing these local aspects that are most
informative for the prediction task at hand
Convolution-and-pooling architectures LeCun  Bengio 1995 evolved in the neural
networks vision community where they showed great success as object detectors  recog-
nizing an object from a predened category cat bicycles regardless of its position in
the image Krizhevsky et al 2012 When applied to images the architecture is using 2-
dimensional grid convolutions When applied to text NLP we are mainly concerned with
1-d sequence convolutions Convolutional networks were introduced to the NLP commu-
nity in the pioneering work of Collobert Weston and Colleagues 2011 who used them for
semantic-role labeling and later by Kalchbrenner et al 2014 and Kim Kim 2014 who
used them for sentiment and question-type classication
91 Basic Convolution  Pooling
The main idea behind a convolution and pooling architecture for language tasks is to apply
a non-linear learned function over each instantiation of a k-word sliding window over
the sentence This function also called lter transforms a window of k words into a d
dimensional vector that captures important properties of the words in the window each
dimension is sometimes referred to in the literature as a channel Then a pooling
operation is used combine the vectors resulting from the dierent windows into a single
d-dimensional vector by taking the max or the average value observed in each of the d
channels over the dierent windows The intention is to focus on the most important
features in the sentence regardless of their location The d-dimensional vector is then
fed further into a network that is used for prediction The gradients that are propagated
back from the networks loss during the training process are used to tune the parameters
of the lter function to highlight the aspects of the data that are important for the task
the network is trained for Intuitively when the sliding window is run over a sequence the
lter function learns to identify informative k-grams
More formally consider a sequence of words x  x1     xn each with their correspond-
ing demb dimensional word embedding vxi A 1d convolution layer24 of width k works by
moving a sliding window of size k over the sentence and applying the same lter to each
window in the sequence vxi vxi1     vxik1 The lter function is usually a linear
transformation followed by a non-linear activation function
Let the concatenated vector of the ith window be wi  vxi vxi1 vxik1 wi 
Rkdemb Depending on whether we pad the sentence with k  1 words to each side we may
get either m  n k  1 narrow convolution or m  n  k  1 windows wide convolution
Kalchbrenner et al 2014 The result of the convolution layer is m vectors p1     pm
pi  Rdconv where
pi  gwiW  b
g is a non-linear activation function that is applied element-wise W  Rkdembdconv and
b  Rdconv are parameters of the network Each pi is a dconv dimensional vector encoding
the information in wi Ideally each dimension captures a dierent kind of indicative infor-
mation The m vectors are then combined using a max pooling layer resulting in a single
dconv dimensional vector c
cj  max
1im
pij denotes the jth component of pi The eect of the max-pooling operation is to get the
most salient information across window positions Ideally each dimension will specialize
in a particular sort of predictors and max operation will pick on the most important
predictor of each type
Figure 4 provides an illustration of the process
The resulting vector c is a representation of the sentence in which each dimension
reects the most salient information with respect to some prediction task c is then fed
into a downstream network layers perhaps in parallel to other vectors culminating in an
output layer which is used for prediction The training procedure of the network calculates
the loss with respect to the prediction task and the error gradients are propagated all the
way back through the pooling and convolution layers as well as the embedding layers 25
24 1d here refers to a convolution operating over 1-dimensional inputs such as sequences as opposed to 2d
convolutions which are applied to images
25 Besides being useful for prediction a by-product of the training procedure is a set of parameters W B
and embeddings v that can be used in a convolution and pooling architecture to encode arbitrary length
sentences into xed-size vectors such that sentences that share the same kind of predictive information
will be close to each other
Figure 4 1d convolutionpooling over the sentence the quick brown fox jumped over the
lazy dog This is a narrow convolution no padding is added to the sentence
with a window size of 3 Each word is translated to a 2-dim embedding vector
not shown The embedding vectors are then concatenated resulting in 6-dim
window representations Each of the seven windows is transfered through a 6 3
lter linear transformation followed by element-wise tanh resulting in seven
3-dimensional ltered representations Then a max-pooling operation is applied
taking the max over each dimension resulting in a nal 3-dimensional pooled
vector
While max-pooling is the most common pooling operation in text applications other
pooling operations are also possible the second most common operation being average
pooling taking the average value of each index instead of the max
92 Dynamic Hierarchical and k-max Pooling
Rather than performing a single pooling operation over the entire sequence we may want
to retain some positional information based on our domain understanding of the prediction
problem at hand To this end we can split the vectors pi into cid96 distinct groups apply
the pooling separately on each group and then concatenate the cid96 resulting dconv vectors
c1     ccid96 The division of the pis into groups is performed based on domain knowledge For
example we may conjecture that words appearing early in the sentence are more indicative
than words appearing late We can then split the sequence into cid96 equally sized regions
applying a separate max-pooling to each region For example Johnson and Zhang Johnson
 Zhang 2014 found that when classifying documents into topics it is useful to have 20
average-pooling regions clearly separating the initial sentences where the topic is usually
introduced from later ones while for a sentiment classication task a single max-pooling
operation over the entire sentence was optimal suggesting that one or two very strong
signals are enough to determine the sentiment regardless of the position in the sentence
thequickbrownquickbrownfoxbrownfoxjumpedfoxjumpedoverjumpedovertheoverthelazythelazydogMULtanhMULtanhMULtanhMULtanhMULtanhMULtanhMULtanhW63thequickbrownfoxjumpedoverthelazydogmaxconvolutionpoolingSimilarly in a relation extraction kind of task we may be given two words and asked to
determine the relation between them We could argue that the words before the rst word
the words after the second word and the words between them provide three dierent kinds
of information Chen et al 2015 We can thus split the pi vectors accordingly pooling
separately the windows resulting from each group
Another variation is performing hierarchical pooling in which we have a succession
of convolution and pooling layers where each stage applies a convolution to a sequence
pools every k neighboring vectors performs a convolution on the resulting pooled sequence
applies another convolution and so on This architecture allows sensitivity to increasingly
larger structures
Finally Kalchbrenner et al 2014 introduced a k-max pooling operation in which the
top k values in each dimension are retained instead of only the best one while preserving
the order in which they appeared in the text For example a consider the following matrix


cid21
cid209 6 3
A 1-max pooling over the column vectors will result in cid29 8 5cid3 while a 2-max pooling
cid29 6 3 7 8 5cid3
whose rows will then be concatenated to
will result in the following matrix
The k-max pooling operation makes it possible to pool the k most active indicators that
may be a number of positions apart it preserves the order of the features but is insensitive
to their specic positions It can also discern more nely the number of times the feature
is highly activated Kalchbrenner et al 2014
93 Variations
Rather than a single convolutional layer several convolutional layers may be applied in
parallel For example we may have four dierent convolutional layers each with a dierent
window size in the range 25 capturing n-gram sequences of varying lengths The result
of each convolutional layer will then be pooled and the resulting vectors concatenated and
fed to further processing Kim 2014
The convolutional architecture need not be restricted into the linear ordering of a sen-
tence For example Ma et al 2015 generalize the convolution operation to work over
syntactic dependency trees There each window is around a node in the syntactic tree
and the pooling is performed over the dierent nodes Similarly Liu et al 2015 apply a
convolutional architecture on top of dependency paths extracted from dependency trees Le
and Zuidema 2015 propose to perform max pooling over vectors representing the dierent
derivations leading to the same chart item in a chart parser
10 Recurrent Neural Networks  Modeling Sequences and Stacks
When dealing with language data it is very common to work with sequences such as words
sequences of letters sentences sequences of words and documents We saw how feed-
forward networks can accommodate arbitrary feature functions over sequences through the
use of vector concatenation and vector addition CBOW In particular the CBOW rep-
resentations allows to encode arbitrary length sequences as xed sized vectors However
the CBOW representation is quite limited and forces one to disregard the order of fea-
tures The convolutional networks also allow encoding a sequence into a xed size vector
While representations derived from convolutional networks are an improvement above the
CBOW representation as they oer some sensitivity to word order their order sensitivity is
restricted to mostly local patterns and disregards the order of patterns that are far apart
in the sequence
Recurrent neural networks RNNs Elman 1990 allow representing arbitrarily sized
structured inputs in a xed-size vector while paying attention to the structured properties
of the input
101 The RNN Abstraction
We use xij to denote the sequence of vectors xi     xj The RNN abstraction takes as
input an ordered list of input vectors x1  xn together with an initial state vector s0
and returns an ordered list of state vectors s1  sn as well as an ordered list of output
vectors y1  yn An output vector yi is a function of the corresponding state vector
si The input vectors xi are presented to the RNN in a sequential fashion and the state
vector si and output vector yi represent the state of the RNN after observing the inputs
x1i The output vector yi is then used for further prediction For example a model for
predicting the conditional probability of an event e given the sequence m1i can be dened
as pe  jx1i  sof tmaxyiW  bj The RNN model provides a framework for
conditioning on the entire history x1     xi without resorting to the Markov assumption
which is traditionally used for modeling sequences Indeed RNN-based language models
result in very good perplexity scores when compared to n-gram based models
Mathematically we have a recursively dened function R that takes as input a state
vector si and an input vector xi1 and results in a new state vector si1 An additional
function O is used to map a state vector si to an output vector yi When constructing an
RNN much like when constructing a feed-forward network one has to specify the dimension
of the inputs xi as well as the dimensions of the outputs yi The dimensions of the states
si are a function of the output dimension26
26 While RNN architectures in which the state dimension is independent of the output dimension are
possible the current popular architectures including the Simple RNN the LSTM and the GRU do not
follow this exibility
RN N s0 x1n s1n y1n
si  Rsi1 xi
yi  Osi
xi  Rdin yi  Rdout si  Rf dout
The functions R and O are the same across the sequence positions but the RNN keeps
track of the states of computation through the state vector that is kept and being passed
between invocations of R
Graphically the RNN has been traditionally presented as in Figure 5
Figure 5 Graphical representation of an RNN recursive
This presentation follows the recursive denition and is correct for arbitrary long sequences
However for a nite sized input sequence and all input sequences we deal with are nite
one can unroll the recursion resulting in the structure in Figure 6
Figure 6 Graphical representation of an RNN unrolled
While not usually shown in the visualization we include here the parameters  in order
to highlight the fact that the same parameters are shared across all time steps Dierent
ROxiyisisi1s0ROx1y1ROx2y2s1ROx3y3s2ROx4y4s3ROx5y5s4s5instantiations of R and O will result in dierent network structures and will exhibit dierent
properties in terms of their running times and their ability to be trained eectively using
gradient-based methods However they all adhere to the same abstract interface We will
provide details of concrete instantiations of R and O  the Simple RNN the LSTM and the
GRU  in Section 11 Before that lets consider modeling with the RNN abstraction
First we note that the value of si is based on the entire input x1  xi For example
by expanding the recursion for i  4 we get
s4 Rs3 x4
cid122
Rs2 x3 x4
cid125cid124
cid123
cid122
cid125cid124
cid123
cid125cid124
cid122
cid123
Rs1 x2 x3 x4
RRR
Rs0 x1 x2 x3 x4
Thus sn as well as yn could be thought of as encoding the entire input sequence27 Is
the encoding useful This depends on our denition of usefulness The job of the network
training is to set the parameters of R and O such that the state conveys useful information
for the task we are tying to solve
102 RNN Training
Viewed as in Figure 6 it is easy to see that an unrolled RNN is just a very deep neural
network or rather a very large computation graph with somewhat complex nodes in
which the same parameters are shared across many parts of the computation To train an
RNN network then all we need to do is to create the unrolled computation graph for a
given input sequence add a loss node to the unrolled graph and then use the backward
backpropagation algorithm to compute the gradients with respect to that loss This
procedure is referred to in the RNN literature as backpropagation through time or BPTT
Werbos 199028 There are various ways in which the supervision signal can be applied
Acceptor One option is to base the supervision signal only on the nal output vector
yn Viewed this way the RNN is an acceptor We observe the nal state and then decide
27 Note that unless R is specically designed against this it is likely that the later elements of the input
sequence have stronger eect on sn than earlier ones
28 Variants of the BPTT algorithm include unrolling the RNN only for a xed number of input symbols at
each time rst unroll the RNN for inputs x1k resulting in s1k Compute a loss and backpropagate
the error through the network k steps back Then unroll the inputs xk12k this time using sk as the
initial state and again backpropagate the error for k steps and so on This strategy is based on the
observations that for the Simple-RNN variant the gradients after k steps tend to vanish for large enough
k and so omitting them is negligible This procedure allows training of arbitrarily long sequences For
RNN variants such as the LSTM or the GRU that are designed specically to mitigate the vanishing
gradients problem this xed size unrolling is less motivated yet it is still being used for example when
doing language modeling over a book without breaking it into sentences
on an outcome29 For example consider training an RNN to read the characters of a word
one by one and then use the nal state to predict the part-of-speech of that word this is
inspired by Ling et al 2015b an RNN that reads in a sentence and based on the nal
state decides if it conveys positive or negative sentiment this is inspired by Wang et al
2015b or an RNN that reads in a sequence of words and decides whether it is a valid
noun-phrase The loss in such cases is dened in terms of a function of yn  Osn and
the error gradients will backpropagate through the rest of the sequence see Figure 730
The loss can take any familiar form  cross entropy hinge margin etc
Figure 7 Acceptor RNN Training Graph
Encoder Similar to the acceptor case an encoder supervision uses only the nal output
vector yn However unlike the acceptor where a prediction is made solely on the basis
of the nal vector here the nal vector is treated as an encoding of the information in the
sequence and is used as additional information together with other signals For example
an extractive document summarization system may rst run over the document with an
RNN resulting in a vector yn summarizing the entire document Then yn will be used
together with with other features in order to select the sentences to be included in the
summarization
then be L y1n y1n  cid80n
Transducer Another option is to treat the RNN as a transducer producing an output for
each input it reads in Modeled this way we can compute a local loss signal Llocal yi yi
for each of the outputs yi based on a true label yi The loss for unrolled sequence will
i1 Llocal yi yi or using another combination rather than a
sum such as an average or a weighted average see Figure 8 One example for such a
transducer is a sequence tagger in which we take xin to be feature representations for the
n words of a sentence and yi as an input for predicting the tag assignment of word i based
on words 1i A CCG super-tagger based on such an architecture provides state-of-the art
CCG super-tagging results Xu et al 2015
A very natural use-case of the transduction setup is for language modeling in which the
sequence of words x1i is used to predict a distribution over the i  1th word RNN based
29 The terminology is borrowed from Finite-State Acceptors However the RNN has a potentially innite
number of states making it necessary to rely on a function other than a lookup table for mapping states
to decisions
30 This kind of supervision signal may be hard to train for long sequences especially so with the Simple-
RNN because of the vanishing gradients problem It is also a generally hard learning task as we do not
tell the process on which parts of the input to focus
ROx1s0ROx2s1ROx3s2ROx4s3ROx5s4predictcalclossy5lossFigure 8 Transducer RNN Training Graph
language models are shown to provide much better perplexities than traditional language
models Mikolov et al 2010 Sundermeyer Schluter  Ney 2012 Mikolov 2012
Using RNNs as transducers allows us to relax the Markov assumption that is tradition-
ally taken in language models and HMM taggers and condition on the entire prediction
history The power of the ability to condition on arbitrarily long histories is demonstrated
in generative character-level RNN models in which a text is generated character by charac-
ter each character conditioning on the previous ones Sutskever Martens  Hinton 2011
The generated texts show sensitivity to properties that are not captured by n-gram language
models including line lengths and nested parenthesis balancing For a good demonstration
and analysis of the properties of RNN-based character level language models see Karpathy
Johnson  Li 2015
Encoder - Decoder Finally an important special case of the encoder scenario is the
Encoder-Decoder framework Cho van Merrienboer Bahdanau  Bengio 2014a Sutskever
et al 2014 The RNN is used to encode the sequence into a vector representation yn and
this vector representation is then used as auxiliary input to another RNN that is used as
a decoder For example in a machine-translation setup the rst RNN encodes the source
sentence into a vector representation yn and then this state vector is fed into a separate
decoder RNN that is trained to predict using a transducer-like language modeling ob-
jective the words of the target language sentence based on the previously predicted words
as well as yn The supervision happens only for the decoder RNN but the gradients are
propagated all the way back to the encoder RNN see Figure 9
Such an approach was shown to be surprisingly eective for Machine Translation Sutskever
et al 2014 using LSTM RNNs In order for this technique to work Sutskever et al found it
eective to input the source sentence in reverse such that xn corresponds to the rst word
of the sentence In this way it is easier for the second RNN to establish the relation be-
tween the rst word of the source sentence to the rst word of the target sentence Another
use-case of the encoder-decoder framework is for sequence transduction Here in order to
generate tags t1     tn an encoder RNN is rst used to encode the sentence x1n into xed
sized vector This vector is then fed as the initial state vector of another transducer RNN
which is used together with x1n to predict the label ti at each position i This approach
ROx1s0predictcalclossy1ROx2s1predictcalclossy2ROx3s2predictcalclossy3ROx4s3predictcalclossy4ROx5s4predictcalclossy5sumlossFigure 9 Encoder-Decoder RNN Training Graph
was used in Filippova Alfonseca Colmenares Kaiser  Vinyals 2015 to model sentence
compression by deletion
103 Multi-layer stacked RNNs
RNNs can be stacked in layers forming a grid Hihi  Bengio 1996 Consider k RNNs
RN N1     RN Nk where the jth RNN has states sj
1n The input for the
rst RNN are x1n while the input of the jth RNN j  2 are the outputs of the RNN
below it yj1
Such layered architectures are often called deep RNNs A visual representation of a 3-layer
RNN is given in Figure 10
1n  The output of the entire formation is the output of the last RNN yk
1n and outputs yj
While it is not theoretically clear what is the additional power gained by the deeper
architecture it was observed empirically that deep RNNs work better than shallower ones
on some tasks In particular Sutskever et al 2014 report that a 4-layers deep architec-
ture was crucial in achieving good machine-translation performance in an encoder-decoder
framework Irsoy and Cardie 2014 also report improved results from moving from a one-
layer BI-RNN to an architecture with several layers Many other works report result using
layered RNN architectures but do not explicitly compare to 1-layer RNNs
REOEx1se0REOEx2se1REOEx3se2REOEx4se3REOEx5se4se5RDODx1sd0predictcalclossy1RDODx2sd1predictcalclossy2RDODx3sd2predictcalclossy3RDODx4sd3predictcalclossy4RDODx5sd4predictcalclossy5sumlossFigure 10 A 3-layer deep RNN architecture
104 BI-RNN
A useful elaboration of an RNN is a bidirectional-RNN BI-RNN Schuster  Paliwal 1997
Graves 200831 Consider the task of sequence tagging over a sentence x1     xn An RNN
allows us to compute a function of the ith word xi based on the past  the words x1i up
to and including it However the following words xin may also be useful for prediction as
is evident by the common sliding-window approach in which the focus word is categorized
based on a window of k words surrounding it Much like the RNN relaxes the Markov
assumption and allows looking arbitrarily back into the past the BI-RNN relaxes the xed
window size assumption allowing to look arbitrarily far at both the past and the future
i and sb
for each input position i The forward state sf
Consider an input sequence x1n The BI-RNN works by maintaining two separate
states sf
i is based on x1 x2     xi
while the backward state sb
is based on xn xn1     xi The forward and backward states
are generated by two dierent RNNs The rst RNN Rf  Of  is fed the input sequence
x1n as is while the second RNN Rb Ob is fed the input sequence in reverse The state
representation si is then composed of both the forward and backward states
i  Obsb
i   Of sf
The output at position i is based on the concatenation of the two output vectors
yi  yf
i  taking into account both the past and the future The
vector yi can then be used directly for prediction or fed as part of the input to a more
complex network While the two RNNs are run independently of each other the error gra-
dients at position i will ow both forward and backward through the two RNNs A visual
representation of the BI-RNN architecture is given in Figure 11
The use of BI-RNNs for sequence tagging was introduced to the NLP community by
Irsoy and Cardie 2014
105 RNNs for Representing Stacks
Some algorithms in language processing including those for transition-based parsing Nivre
2008 require performing feature extraction over a stack
Instead of being conned to
31 When used with a specic RNN architecture such as an LSTM the model is called BI-LSTM
R1O1R2O2y11s10R3O3y21s20s30x1y1y31R1O1R2O2y12s11R3O3y22s21s31x2y2y32R1O1R2O2y13s12R3O3y23s22s32x3y3y33R1O1R2O2y14s13R3O3y24s23s33x4y4y34R1O1R2O2y15s14R3O3y25s24s34x5y5y35s15s25s35Figure 11 BI-RNN over the sentence the brown fox jumped 
looking at the k top-most elements of the stack the RNN framework can be used to provide
a xed-sized vector encoding of the entire stack
The main intuition is that a stack is essentially a sequence and so the stack state can be
represented by taking the stack elements and feeding them in order into an RNN resulting
in a nal encoding of the entire stack In order to do this computation eciently without
performing an On stack encoding operation each time the stack changes the RNN state
is maintained together with the stack state
If the stack was push-only this would be
trivial whenever a new element x is pushed into the stack the corresponding vector x
will be used together with the RNN state si in order to obtain a new state si1 Dealing
with pop operation is more challenging but can be solved by using the persistent-stack
data-structure Okasaki 1999 Goldberg Zhao  Huang 2013 Persistent or immutable
data-structures keep old versions of themselves intact when modied The persistent stack
construction represents a stack as a pointer to the head of a linked list An empty stack is
the empty list The push operation appends an element to the list returning the new head
The pop operation then returns the parent of the head but keeping the original list intact
From the point of view of someone who held a pointer to the previous head the stack did
not change A subsequent push operation will add a new child to the same node Applying
this procedure throughout the lifetime of the stack results in a tree where the root is an
empty stack and each path from a node to the root represents an intermediary stack state
Figure 12 provides an example of such a tree The same process can be applied in the
computation graph construction creating an RNN with a tree structure instead of a chain
structure Backpropagating the error from a given node will then aect all the elements
that participated in the stack when the node was created in order Figure 13 shows the
computation graph for the stack-RNN corresponding to the last state in Figure 12 This
modeling approach was proposed independently by Dyer et al and Watanabe et al Dyer
et al 2015 Watanabe  Sumita 2015 for transition-based dependency parsing
RfOfxtheconcatyf1sf0RfOfxbrownconcatyf2sf1RfOfxfoxconcatyf3sf2RfOfxjumpedconcatyf4sf3RfOfxconcatyf5sf4sf5RbObs0sb0yb1RbObs1sb1yb2RbObs2sb2yb3RbObs3sb3yb4RbObs4sb4yb5sb5ytheybrownyfoxyjumpedyFigure 12 An immutable stack construction for the sequence of operations push a push b
push c pop push d pop pop push e push f
Figure 13 The stack-RNN corresponding to the nal state in Figure 12
ahead1pushaabhead2pushbabchead3pushcabheadc4popabcdhead5pushdabheadcd6popaheadbcd7popabcdehead8pusheabcdefhead9pushfsoROyaxaROsayabxbROsabyacxcsacROsabyabdxdsabdROsayaexeROsaeyaefxfsaef11 Concrete RNN Architectures
We now turn to present three dierent instantiations of the abstract RN N architecture
discussed in the previous section providing concrete denitions of the functions R and O
These are the Simple RNN SRNN the Long Short-Term Memory LSTM and the Gated
Recurrent Unit GRU
111 Simple RNN
The simplest RNN formulation known as an Elman Network or Simple-RNN S-RNN was
proposed by Elman 1990 and explored for use in language modeling by Mikolov 2012
The S-RNN takes the following form
si RSRN N si1 xi  gxiWx  si1Ws  b
yi OSRN N si  si
si yi  Rds xi  Rdx Wx  Rdxds Ws  Rdsds b  Rds
That is the state at position i is a linear combination of the input at position i and
the previous state passed through a non-linear activation commonly tanh or ReLU The
output at position i is the same as the hidden state in that position32
In spite of its simplicity the Simple RNN provides strong results for sequence tagging
Xu et al 2015 as well as language modeling For comprehensive discussion on using
Simple RNNs for language modeling see the PhD thesis by Mikolov 2012
112 LSTM
The S-RNN is hard to train eectively because of the vanishing gradients problem Error
signals gradients in later steps in the sequence diminish in quickly in the back-propagation
process and do not reach earlier input signals making it hard for the S-RNN to capture
long-range dependencies The Long Short-Term Memory LSTM architecture Hochreiter
 Schmidhuber 1997 was designed to solve the vanishing gradients problem The main
idea behind the LSTM is to introduce as part of the state representation also memory
cells a vector that can preserve gradients across time Access to the memory cells is
controlled by gating components  smooth mathematical functions that simulate logical
gates At each input state a gate is used to decide how much of the new input should be
written to the memory cell and how much of the current content of the memory cell should
be forgotten Concretely a gate g  0 1n is a vector of values in the range 0 1 that is
multiplied component-wise with another vector v  Rn and the result is then added to
another vector The values of g are designed to be close to either 0 or 1 ie by using a
sigmoid function Indices in v corresponding to near-one values in g are allowed to pass
while those corresponding to near-zero values are blocked
32 Some authors treat the output at position i as a more complicated function of the state In our presen-
tation such further transformation of the output are not considered part of the RNN but as separate
computations that are applied to the RNNs output The distinction between the state and the output
are needed for the LSTM architecture in which not all of the state is observed outside of the RNN
Mathematically the LSTM architecture is dened as33
sj  RLST M sj1 xj cj hj
cj cj1 cid12 f  g cid12 i
hj  tanhcj cid12 o
i xjWxi  hj1Whi
f xjWxf  hj1Whf 
o xjWxo  hj1Who
g  tanhxjWxg  hj1Whg
yj  OLST M sj hj
sj  R2dh xi  Rdx cj hj i f  o g  Rdh Wx  Rdxdh Wh  Rdhdh
The symbol cid12 is used to denote component-wise product The state at time j is com-
posed of two vectors cj and hj where cj is the memory component and hj is the output
or state component There are three gates i f and o controlling for input f orget and
output The gate values are computed based on linear combinations of the current input
xj and the previous state hj1 passed through a sigmoid activation function An update
candidate g is computed as a linear combination of xj and hj1 passed through a tanh
activation function The memory cj is then updated the forget gate controls how much
of the previous memory to keep cj1 cid12 f  and the input gate controls how much of the
proposed update to keep g cid12 i Finally the value of hj which is also the output yj is
determined based on the content of the memory cj passed through a tanh non-linearity
and controlled by the output gate The gating mechanisms allow for gradients related to
the memory part cj to stay high across very long time ranges
For further discussion on the LSTM architecture see the PhD thesis by Alex Graves
2008 as well as Chris Olahs description34 For an analysis of the behavior of an LSTM
when used as a character-level language model see Karpathy et al 2015
LSTMs are currently the most successful type of RNN architecture and they are re-
sponsible for many state-of-the-art sequence modeling results The main competitor of the
LSTM-RNN is the GRU to be discussed next
Practical Considerations When training LSTM networks Jozefowicz et al 2015 strongly
recommend to always initialize the bias term of the forget gate to be close to one When
applying dropout to an RNN with an LSTM Zaremba et al 2014 found out that it is
33 There are many variants on the LSTM architecture presented here For example forget gates were not
part of the original proposal in Hochreiter  Schmidhuber 1997 but are shown to be an important
part of the architecture Other variants include peephole connections and gate-tying For an overview
and comprehensive empirical comparison of various LSTM architectures see Gre Srivastava Koutnk
Steunebrink  Schmidhuber 2015
34 httpcolahgithubioposts2015-08-Understanding-LSTMs
crucial to apply dropout only on the non-recurrent connection ie only to apply it between
layers and not between sequence positions
113 GRU
The LSTM architecture is very eective but also quite complicated The complexity of the
system makes it hard to analyze and also computationally expensive to work with The
gated recurrent unit GRU was recently introduced by Cho et al 2014b as an alternative
to the LSTM It was subsequently shown by Chung et al 2014 to perform comparably to
the LSTM on several non textual datasets
Like the LSTM the GRU is also based on a gating mechanism but with substantially
fewer gates and without a separate memory component
sj  RGRU sj1 xj 1  z cid12 sj1  z cid12 h
z xjWxz  hj1Whz
r xjWxr  hj1Whr
h  tanhxjWxh  hj1 cid12 rWhg
yj  OLST M sj sj
sj  Rdh xi  Rdx z r h  Rdh Wx  Rdxdh Wh  Rdhdh
One gate r is used to control access to the previous state sj1 and compute a proposed
update h The updated state sj which also serves as the output yj is then determined based
on an interpolation of the previous state sj1 and the proposal h where the proportions of
the interpolation are controlled using the gate z
The GRU was shown to be eective in language modeling and machine translation
However the jury between the GRU the LSTM and possible alternative RNN architectures
is still out and the subject is actively researched For an empirical exploration of the GRU
and the LSTM architectures see Jozefowicz et al 2015
114 Other Variants
The gated architectures of the LSTM and the GRU help in alleviating the vanishing gradi-
ents problem of the Simple RNN and allow these RNNs to capture dependencies that span
long time ranges Some researchers explore simpler architectures than the LSTM and the
GRU for achieving similar benets
Mikolov et al 2014 observed that the matrix multiplication si1Ws coupled with the
nonlinearity g in the update rule R of the Simple RNN causes the state vector si to undergo
large changes at each time step prohibiting it from remembering information over long
time periods They propose to split the state vector si into a slow changing component ci
context units and a fast changing component hi35 The slow changing component ci is
35 We depart from the notation in Mikolov et al 2014 and reuse the symbols used in the LSTM descrip-
updated according to a linear interpolation of the input and the previous component ci 
1  xiWx1  ci1 where   0 1 This update allows ci to accumulate the previous
inputs The fast changing component hi is updated similarly to the Simple RNN update
rule but changed to take ci into account as well36 hi  xiWx2  hi1Wh  ciWc
Finally the output yi is the concatenation of the slow and the fast changing parts of the
state yi  ci hi Mikolov et al demonstrate that this architecture provides competitive
perplexities to the much more complex LSTM on language modeling tasks
The approach of Mikolov et al can be interpreted as constraining the block of the matrix
Ws in the S-RNN corresponding to ci to be a multiply of the identity matrix see Mikolov
et al 2014 for the details Le et al Le Jaitly  Hinton 2015 propose an even simpler
approach set the activation function of the S-RNN to a ReLU and initialize the biases b
as zeroes and the matrix Ws as the identify matrix This causes an untrained RNN to copy
the previous state to the current state add the eect of the current input xi and set the
negative values to zero After setting this initial bias towards state copying the training
procedure allows Ws to change freely Le et al demonstrate that this simple modication
makes the S-RNN comparable to an LSTM with the same number of parameters on several
tasks including language modeling
36 The update rule diverges from the S-RNN update rule also by xing the non-linearity to be a sigmoid
function and by not using a bias term However these changes are not discussed as central to the
proposal
12 Modeling Trees  Recursive Neural Networks
The RNN is very useful for modeling sequences In language processing it is often natural
and desirable to work with tree structures The trees can be syntactic trees discourse trees
or even trees representing the sentiment expressed by various parts of a sentence Socher
et al 2013 We may want to predict values based on specic tree nodes predict values
based on the root nodes or assign a quality score to a complete tree or part of a tree In
other cases we may not care about the tree structure directly but rather reason about spans
in the sentence In such cases the tree is merely used as a backbone structure which help
guide the encoding process of the sequence into a xed size vector
The recursive neural network RecNN abstraction Pollack 1990 popularized in NLP
by Richard Socher and colleagues Socher Manning  Ng 2010 Socher Lin Ng  Man-
ning 2011 Socher et al 2013 Socher 2014 is a generalization of the RNN from sequences
to binary trees37
Much like the RNN encodes each sentence prex as a state vector the RecNN encodes
each tree-node as a state vector in Rd We can then use these state vectors either to predict
values of the corresponding nodes assign quality values to each node or as a semantic
representation of the spans rooted at the nodes
The main intuition behind the recursive neural networks is that each subtree is repre-
sented as a d dimensional vector and the representation of a node p with children c1 and c2
is a function of the representation of the nodes vecp  f vecc1 vecc2 where f is a
composition function taking two d-dimensional vectors and returning a single d-dimensional
vector Much like the RNN state si is used to encode the entire sequence x1  i the RecNN
state associated with a tree node p encodes the entire subtree rooted at p See Figure 14
for an illustration
121 Formal Denition
Consider a binary parse tree T over an n-word sentence As a reminder an ordered
unlabeled tree over a string x1     xn can be represented as a unique set of triplets i k j
st i  k  j Each such triplet indicates that a node spanning words xij is parent of the
nodes spanning xik and xk1j Triplets of the form i i i correspond to terminal symbols
at the tree leaves the words xi Moving from the unlabeled case to the labeled one we can
represent a tree as a set of 6-tuples A  B C i k j whereas i k and j indicate the spans
as before and A B and C are the node labels of of the nodes spanning xij xik and xk1j
respectively Here leaf nodes have the form A  A A i i i where A is a pre-terminal
symbol We refer to such tuples as production rules For an example consider the syntactic
tree for the sentence the boy saw her duck
37 While presented in terms of binary parse trees the concepts easily transfer to general recursively-dened
data structures with the major technical challenge is the denition of an eective form for R the
combination function
Figure 14 Illustration of a recursive neural network The representations of V and NP1
are combined to form the representation of VP The representations of VP and
NP2 are then combined to form the representation of S
Its corresponding unlabeled and labeled representations are 
Unlabeled
111
222
333
444
555
445
335
112
125
Labeled
Det Det Det 1 1 1
Corresponding Span
x11 the
Noun Noun Noun 2 2 2 x22 boy
Verb Verb Verb 3 3 3
Det Det Det 4 4 4
Noun Noun Noun 5 5 5 duck
NP Det Noun 4 4 5
VP Verb NP 3 3 5
NP Det Noun 1 1 2
S NP VP 1 2 5
her duck
saw her duck
the boy
the boy saw her duck
The set of production rules above can be uniquely converted to a set tree nodes qA
indicating a node with symbol A over the span xij by simply ignoring the elements
VNP1combineVPNP2combineSB C k in each production rule We are now in position to dene the Recursive Neural
Network
A Recursive Neural Network RecNN is a function that takes as input a parse tree
over an n-word sentence x1     xn Each of the sentences words is represented as a d-
dimensional vector xi and the tree is represented as a set T of production rules A 
B C i j k Denote the nodes of T by qA
ij The RecNN returns as output a correspond-
ij  Rd represents the
ing set of inside state vectors sA
corresponding tree node qA
ij and encodes the entire structure rooted at that node Like
the sequence RNN the tree shaped RecNN is dened recursively using a function R where
the inside vector of a given node is dened as a function of the inside vectors of its direct
children38 Formally
ij where each inside state vector sA
RecN N x1     xnT  sA
ii vxi
ij RA B C sB
ij  Rd  qA
ij  T 
ik sC
ik  T  qC
k1j  T
The function R usually takes the form of a simple linear transformation which may or
may not be followed by a non-linear activation function g
RA B C sB
ik sC
k1j  gsB
ik sC
k1jW
This formulation of R ignores the tree labels using the same matrix W  R2dd for all
combinations This may be a useful formulation in case the node labels do not exist eg
when the tree does not represent a syntactic structure with clearly dened labels or when
they are unreliable However if the labels are available it is generally useful to include them
in the composition function One approach would be to introduce label embeddings vA
mapping each non-terminal symbol to a dnt dimensional vector and change R to include
the embedded symbols in the combination function
RA B C sB
ik sC
k1j  gsB
ik sC
k1j vA vBW
here W  R2d2dntd Such approach is taken by Qian Tian Huang Liu Zhu 
Zhu 2015 An alternative approach due to Socher et al 2013 is to untie the weights
according to the non-terminals using a dierent composition matrix for each B C pair of
symbols39
RA B C sB
ik sC
k1j  gsB
ik sC
k1jWBC
38 Le and Zuidema 2014 extend the RecNN denition such that each node has in addition to its inside
state vector also an outside state vector representing the entire structure around the subtree rooted
at that node Their formulation is based on the recursive computation of the classic inside-outside
algorithm and can be thought of as the BI-RNN counterpart of the tree RecNN For details see Le 
Zuidema 2014
39 While not explored in the literature a trivial extension would condition the transformation matrix also
This formulation is useful when the number of non-terminal symbols or the number of
possible symbol combinations is relatively small as is usually the case with phrase-structure
parse trees A similar model was also used by Hashimoto et al 2013 to encode subtrees
in semantic-relation classication task
122 Extensions and Variations
As all of the denitions of R above suer from the vanishing gradients problem of the
Simple RNN several authors sought to replace it with functions inspired by the Long Short-
Term Memory LSTM gated architecture resulting in Tree-shaped LSTMs Tai Socher 
Manning 2015 Zhu Sobhani  Guo 2015b The question of optimal tree representation
is still very much an open research question and the vast space of possible combination
functions R is yet to be explored Other proposed variants on tree-structured RNNs includes
a recursive matrix-vector model Socher Huval Manning  Ng 2012 and recursive neural
tensor network Socher et al 2013
In the rst variant each word is represented as a
combination of a vector and a matrix where the vector denes the words static semantic
content as before while the matrix acts as a learned operator for the word allowing
more subtle semantic compositions than the addition and weighted averaging implied by
the concatenation followed by linear transformation function In the second variant words
are associated with vectors as usual but the composition function becomes more expressive
by basing it on tensor instead of matrix operations
123 Training Recursive Neural Networks
The training procedure for a recursive neural network follows the same recipe as training
other forms of networks dene a loss spell out the computation graph compute gradients
using backpropagation40 and train the parameters using SGD
With regard to the loss function similar to the sequence RNN one can associate a loss
either with the root of the tree with any given node or with a set of nodes in which case
the individual nodes losses are combined usually by summation The loss function is based
on the labeled training data which associates a label or other quantity with dierent tree
Additionally one can treat the RecNN as an Encoder whereas the inside-vector associ-
ated with a node is taken to be an encoding of the tree rooted at that node The encoding
can potentially be sensitive to arbitrary properties of the structure The vector is then
passed as input to another network
For further discussion on recursive neural networks and their use in natural language
tasks refer to the PhD thesis of Richard Socher 2014
40 Before the introduction of the computation graph abstraction the specic backpropagation procedure
for computing the gradients in a RecNN as dened above was referred to as the Back-propagation trough
Structure BPTS algorithm Goller  Kuchler 1996
13 Conclusions
Neural networks are powerful learners providing opportunities ranging from non-linear
classication to non-Markovian modeling of sequences and trees We hope that this expo-
sition help NLP researchers to incorporate neural network models in their work and take
advantage of their power
References
Adel H Vu N T  Schultz T 2013 Combination of Recurrent Neural Networks and
Factored Language Models for Code-Switching Language Modeling In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics Vol-
ume 2 Short Papers pp 206211 Soa Bulgaria Association for Computational
Linguistics
Ando R  Zhang T 2005a A High-Performance Semi-Supervised Learning Method
for Text Chunking In Proceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics ACL05 pp 19 Ann Arbor Michigan Association for
Computational Linguistics
Ando R K  Zhang T 2005b A framework for learning predictive structures from
multiple tasks and unlabeled data The Journal of Machine Learning Research 6
18171853
Auli M Galley M Quirk C  Zweig G 2013 Joint Language and Translation Mod-
eling with Recurrent Neural Networks
In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing pp 10441054 Seattle Washing-
ton USA Association for Computational Linguistics
Auli M  Gao J 2014 Decoder Integration and Expected BLEU Training for Recurrent
Neural Network Language Models In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics Volume 2 Short Papers pp 136142
Baltimore Maryland Association for Computational Linguistics
Ballesteros M Dyer C  Smith N A 2015 Improved Transition-based Parsing by
Modeling Characters instead of Words with LSTMs In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language Processing pp 349359 Lisbon
Portugal Association for Computational Linguistics
Bansal M Gimpel K  Livescu K 2014 Tailoring Continuous Word Representations
for Dependency Parsing In Proceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics Volume 2 Short Papers pp 809815 Baltimore
Maryland Association for Computational Linguistics
Baydin A G Pearlmutter B A Radul A A  Siskind J M 2015 Automatic
dierentiation in machine learning a survey arXiv150205767 cs
Bengio Y 2012 Practical recommendations for gradient-based training of deep architec-
tures arXiv12065533 cs
Bengio Y Ducharme R Vincent P  Janvin C 2003 A Neural Probabilistic Lan-
guage Model J Mach Learn Res 3 11371155
Bengio Y Goodfellow I J  Courville A 2015 Deep Learning Book in preparation
for MIT Press
Bitvai Z  Cohn T 2015 Non-Linear Text Regression with a Deep Convolutional
Neural Network In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Lan-
guage Processing Volume 2 Short Papers pp 180185 Beijing China Association
for Computational Linguistics
Botha J A  Blunsom P 2014 Compositional Morphology for Word Representations
In Proceedings of the 31st International Conference on
and Language Modelling
Machine Learning ICML Beijing China Award for best application paper
Bottou L 2012 Stochastic gradient descent tricks In Neural Networks Tricks of the
Trade pp 421436 Springer
Charniak E  Johnson M 2005 Coarse-to-Fine n-Best Parsing and MaxEnt Discrim-
inative Reranking In Proceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics ACL05 pp 173180 Ann Arbor Michigan Association
for Computational Linguistics
Chen D  Manning C 2014 A Fast and Accurate Dependency Parser using Neural
Networks In Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing EMNLP pp 740750 Doha Qatar Association for Compu-
tational Linguistics
Chen Y Xu L Liu K Zeng D  Zhao J 2015 Event Extraction via Dynamic
Multi-Pooling Convolutional Neural Networks
In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing Volume 1 Long Papers pp 167
176 Beijing China Association for Computational Linguistics
Cho K van Merrienboer B Bahdanau D  Bengio Y 2014a On the Properties of
Neural Machine Translation EncoderDecoder Approaches In Proceedings of SSST-
8 Eighth Workshop on Syntax Semantics and Structure in Statistical Translation
pp 103111 Doha Qatar Association for Computational Linguistics
Cho K van Merrienboer B Gulcehre C Bahdanau D Bougares F Schwenk H 
Bengio Y 2014b Learning Phrase Representations using RNN EncoderDecoder for
Statistical Machine Translation In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing EMNLP pp 17241734 Doha Qatar
Association for Computational Linguistics
Chrupala G 2014 Normalizing tweets with edit scripts and recurrent neural embeddings
In Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-
guistics Volume 2 Short Papers pp 680686 Baltimore Maryland Association for
Computational Linguistics
Chung J Gulcehre C Cho K  Bengio Y 2014 Empirical Evaluation of Gated
Recurrent Neural Networks on Sequence Modeling arXiv14123555 cs
Collins M 2002 Discriminative Training Methods for Hidden Markov Models Theory
In Proceedings of the 2002 Confer-
and Experiments with Perceptron Algorithms
ence on Empirical Methods in Natural Language Processing pp 18 Association for
Computational Linguistics
Collins M  Koo T 2005 Discriminative Reranking for Natural Language Parsing
Computational Linguistics 31 1 2570
Collobert R  Weston J 2008 A unied architecture for natural language processing
Deep neural networks with multitask learning In Proceedings of the 25th international
conference on Machine learning pp 160167 ACM
Collobert R Weston J Bottou L Karlen M Kavukcuoglu K  Kuksa P 2011
Natural language processing almost from scratch The Journal of Machine Learning
Research 12 24932537
Crammer K  Singer Y 2002 On the algorithmic implementation of multiclass kernel-
based vector machines The Journal of Machine Learning Research 2 265292
Creutz M  Lagus K 2007 Unsupervised Models for Morpheme Segmentation and
Morphology Learning ACM Trans Speech Lang Process 4 1 31334
Cybenko G 1989 Approximation by superpositions of a sigmoidal function Mathematics
of Control Signals and Systems 2 4 303314
Dahl G Sainath T  Hinton G 2013 Improving deep neural networks for LVCSR
using rectied linear units and dropout In 2013 IEEE International Conference on
Acoustics Speech and Signal Processing ICASSP pp 86098613
de Gispert A Iglesias G  Byrne B 2015 Fast and Accurate Preordering for SMT
using Neural Networks In Proceedings of the 2015 Conference of the North American
Chapter of the Association for Computational Linguistics Human Language Technolo-
gies pp 10121017 Denver Colorado Association for Computational Linguistics
Dong L Wei F Tan C Tang D Zhou M  Xu K 2014 Adaptive Recursive Neural
Network for Target-dependent Twitter Sentiment Classication
In Proceedings of
the 52nd Annual Meeting of the Association for Computational Linguistics Volume
2 Short Papers pp 4954 Baltimore Maryland Association for Computational
Linguistics
Dong L Wei F Zhou M  Xu K 2015 Question Answering over Freebase with
In Proceedings of the 53rd Annual
Multi-Column Convolutional Neural Networks
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing Volume 1 Long Papers pp 260
269 Beijing China Association for Computational Linguistics
dos Santos C  Gatti M 2014 Deep Convolutional Neural Networks for Sentiment
Analysis of Short Texts In Proceedings of COLING 2014 the 25th International Con-
ference on Computational Linguistics Technical Papers pp 6978 Dublin Ireland
Dublin City University and Association for Computational Linguistics
dos Santos C Xiang B  Zhou B 2015 Classifying Relations by Ranking with
Convolutional Neural Networks
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing Volume 1 Long Papers pp 626634 Beijing
China Association for Computational Linguistics
Duchi J Hazan E  Singer Y 2011 Adaptive subgradient methods for online learning
and stochastic optimization The Journal of Machine Learning Research 12 2121
Duh K Neubig G Sudoh K  Tsukada H 2013 Adaptation Data Selection us-
ing Neural Language Models Experiments in Machine Translation In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics Vol-
ume 2 Short Papers pp 678683 Soa Bulgaria Association for Computational
Linguistics
Durrett G  Klein D 2015 Neural CRF Parsing In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing Volume 1 Long Papers pp 302
312 Beijing China Association for Computational Linguistics
Dyer C Ballesteros M Ling W Matthews A  Smith N A 2015 Transition-
Based Dependency Parsing with Stack Long Short-Term Memory In Proceedings of
the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing Volume 1 Long
Papers pp 334343 Beijing China Association for Computational Linguistics
Elman J L 1990 Finding Structure in Time Cognitive Science 14 2 179211
Faruqui M  Dyer C 2014 Improving Vector Space Word Representations Using Mul-
tilingual Correlation In Proceedings of the 14th Conference of the European Chapter
of the Association for Computational Linguistics pp 462471 Gothenburg Sweden
Association for Computational Linguistics
Filippova K Alfonseca E Colmenares C A Kaiser L  Vinyals O 2015 Sentence
Compression by Deletion with LSTMs
In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing pp 360368 Lisbon Portugal
Association for Computational Linguistics
Gal Y  Ghahramani Z 2015 Dropout as a Bayesian Approximation Representing
Model Uncertainty in Deep Learning arXiv150602142 cs stat
Gao J Pantel P Gamon M He X  Deng L 2014 Modeling Interestingness with
Deep Neural Networks In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing EMNLP pp 213 Doha Qatar Association for
Computational Linguistics
Gimenez J  Marquez L 2004 SVMTool A general POS tagger generator based on
Support Vector Machines In Proceedings of the 4th LREC Lisbon Portugal
Glorot X  Bengio Y 2010 Understanding the diculty of training deep feedforward
neural networks In International conference on articial intelligence and statistics
pp 249256
Glorot X Bordes A  Bengio Y 2011 Deep sparse rectier neural networks
International Conference on Articial Intelligence and Statistics pp 315323
Goldberg Y  Elhadad M 2010 An Ecient Algorithm for Easy-First Non-Directional
Dependency Parsing In Human Language Technologies The 2010 Annual Conference
of the North American Chapter of the Association for Computational Linguistics pp
742750 Los Angeles California Association for Computational Linguistics
Goldberg Y  Levy O 2014 word2vec Explained deriving Mikolov et als negative-
sampling word-embedding method arXiv14023722 cs stat
Goldberg Y  Nivre J 2013 Training Deterministic Parsers with Non-Deterministic
Oracles Transactions of the Association for Computational Linguistics 1 0 403
Goldberg Y Zhao K  Huang L 2013 Ecient Implementation of Beam-Search
Incremental Parsers In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics Volume 2 Short Papers pp 628633 Soa Bulgaria
Association for Computational Linguistics
Goller C  Kuchler A 1996 Learning Task-Dependent Distributed Representations
by Backpropagation Through Structure In In Proc of the ICNN-96 pp 347352
Graves A 2008 Supervised sequence labelling with recurrent neural networks PhD
thesis Technische Universitat Munchen
Gre K Srivastava R K Koutnk J Steunebrink B R  Schmidhuber J 2015
LSTM A Search Space Odyssey arXiv150304069 cs
Hal Daume III Langford J  Marcu D 2009 Search-based Structured Prediction
Machine Learning Journal MLJ
Harris Z 1954 Distributional Structure Word 10 23 146162
Hashimoto K Miwa M Tsuruoka Y  Chikayama T 2013 Simple Customization
of Recursive Neural Networks for Semantic Relation Classication
In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing pp
13721376 Seattle Washington USA Association for Computational Linguistics
He K Zhang X Ren S  Sun J 2015 Delving Deep into Rectiers Surpassing
Human-Level Performance on ImageNet Classication arXiv150201852 cs
Henderson M Thomson B  Young S 2013 Deep Neural Network Approach for the
Dialog State Tracking Challenge In Proceedings of the SIGDIAL 2013 Conference
pp 467471 Metz France Association for Computational Linguistics
Hermann K M  Blunsom P 2013 The Role of Syntax in Vector Space Models of
Compositional Semantics
In Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics Volume 1 Long Papers pp 894904 Soa
Bulgaria Association for Computational Linguistics
Hermann K M  Blunsom P 2014 Multilingual Models for Compositional Distributed
Semantics In Proceedings of the 52nd Annual Meeting of the Association for Com-
putational Linguistics Volume 1 Long Papers pp 5868 Baltimore Maryland
Association for Computational Linguistics
Hihi S E  Bengio Y 1996 Hierarchical Recurrent Neural Networks for Long-Term
Dependencies In Touretzky D S Mozer M C  Hasselmo M E Eds Advances
in Neural Information Processing Systems 8 pp 493499 MIT Press
Hinton G E Srivastava N Krizhevsky A Sutskever I  Salakhutdinov R R
2012 Improving neural networks by preventing co-adaptation of feature detectors
arXiv12070580 cs
Hochreiter S  Schmidhuber J 1997 Long short-term memory Neural computation
9 8 17351780
Hornik K Stinchcombe M  White H 1989 Multilayer feedforward networks are
universal approximators Neural Networks 2 5 359366
Irsoy O  Cardie C 2014 Opinion Mining with Deep Recurrent Neural Networks
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing EMNLP pp 720728 Doha Qatar Association for Computational Lin-
guistics
Iyyer M Boyd-Graber J Claudino L Socher R  Daume III H 2014a A Neural
Network for Factoid Question Answering over Paragraphs In Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing EMNLP pp
633644 Doha Qatar Association for Computational Linguistics
Iyyer M Enns P Boyd-Graber J  Resnik P 2014b Political Ideology Detection
Using Recursive Neural Networks In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics Volume 1 Long Papers pp 11131122
Baltimore Maryland Association for Computational Linguistics
Iyyer M Manjunatha V Boyd-Graber J  Daume III H 2015 Deep Unordered
Composition Rivals Syntactic Methods for Text Classication In Proceedings of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing Volume 1 Long Pa-
pers pp 16811691 Beijing China Association for Computational Linguistics
Johnson R  Zhang T 2014 Eective Use of Word Order for Text Categorization with
Convolutional Neural Networks arXiv14121058 cs stat
Johnson R  Zhang T 2015 Eective Use of Word Order for Text Categorization with
Convolutional Neural Networks In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computational Linguistics Human Lan-
guage Technologies pp 103112 Denver Colorado Association for Computational
Linguistics
Jozefowicz R Zaremba W  Sutskever I 2015 An Empirical Exploration of Recur-
rent Network Architectures In Proceedings of the 32nd International Conference on
Machine Learning ICML-15 pp 23422350
Kalchbrenner N Grefenstette E  Blunsom P 2014 A Convolutional Neural Network
for Modelling Sentences In Proceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics Volume 1 Long Papers pp 655665 Baltimore
Maryland Association for Computational Linguistics
Karpathy A Johnson J  Li F-F 2015 Visualizing and Understanding Recurrent
Networks arXiv150602078 cs
Kim Y 2014 Convolutional Neural Networks for Sentence Classication In Proceed-
ings of the 2014 Conference on Empirical Methods in Natural Language Processing
EMNLP pp 17461751 Doha Qatar Association for Computational Linguistics
Kingma D  Ba J
2014
Adam A Method for Stochastic Optimization
arXiv14126980 cs
Krizhevsky A Sutskever I  Hinton G E 2012 ImageNet Classication with Deep
Convolutional Neural Networks In Pereira F Burges C J C Bottou L  Wein-
berger K Q Eds Advances in Neural Information Processing Systems 25 pp
10971105 Curran Associates Inc
Kudo T  Matsumoto Y 2003 Fast Methods for Kernel-based Text Analysis
Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -
Volume 1 ACL 03 pp 2431 Stroudsburg PA USA Association for Computational
Linguistics
Le P  Zuidema W 2014 The Inside-Outside Recursive Neural Network model for
Dependency Parsing In Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing EMNLP pp 729739 Doha Qatar Association for
Computational Linguistics
Le P  Zuidema W 2015 The Forest Convolutional Network Compositional Distri-
butional Semantics with a Neural Chart and without Binarization
In Proceedings
of the 2015 Conference on Empirical Methods in Natural Language Processing pp
11551164 Lisbon Portugal Association for Computational Linguistics
Le Q V Jaitly N  Hinton G E 2015 A Simple Way to Initialize Recurrent Networks
of Rectied Linear Units arXiv150400941 cs
LeCun Y  Bengio Y 1995 Convolutional Networks for Images Speech and Time-
Series In Arbib M A Ed The Handbook of Brain Theory and Neural Networks
MIT Press
LeCun Y Bottou L Orr G  Muller K 1998a Ecient BackProp In Orr G 
K M Eds Neural Networks Tricks of the trade Springer
Lecun Y Bottou L Bengio Y  Haner P 1998b Gradient Based Learning Applied
to Pattern Recognition
LeCun Y Chopra S Hadsell R Ranzato M  Huang F 2006 A tutorial on energy-
based learning Predicting structured data 1 0
LeCun Y  Huang F 2005 Loss functions for discriminative training of energybased
models AIStats
Levy O  Goldberg Y 2014a Dependency-Based Word Embeddings In Proceedings of
the 52nd Annual Meeting of the Association for Computational Linguistics Volume
2 Short Papers pp 302308 Baltimore Maryland Association for Computational
Linguistics
Levy O  Goldberg Y 2014b Neural Word Embedding as Implicit Matrix Factoriza-
tion In Ghahramani Z Welling M Cortes C Lawrence N D  Weinberger
K Q Eds Advances in Neural Information Processing Systems 27 pp 21772185
Curran Associates Inc
Levy O Goldberg Y  Dagan I 2015
Improving Distributional Similarity with
Lessons Learned from Word Embeddings Transactions of the Association for Com-
putational Linguistics 3 0 211225
Lewis M  Steedman M 2014 Improved CCG Parsing with Semi-supervised Supertag-
ging Transactions of the Association for Computational Linguistics 2 0 327338
Li J Li R  Hovy E 2014 Recursive Deep Models for Discourse Parsing In Proceed-
ings of the 2014 Conference on Empirical Methods in Natural Language Processing
EMNLP pp 20612069 Doha Qatar Association for Computational Linguistics
Ling W Dyer C Black A W  Trancoso I 2015a TwoToo Simple Adaptations of
Word2Vec for Syntax Problems In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computational Linguistics Human Lan-
guage Technologies pp 12991304 Denver Colorado Association for Computational
Linguistics
Ling W Dyer C Black A W Trancoso I Fermandez R Amir S Marujo L 
Luis T 2015b Finding Function in Form Compositional Character Models for
Open Vocabulary Word Representation In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing pp 15201530 Lisbon Portugal
Association for Computational Linguistics
Liu Y Wei F Li S Ji H Zhou M  WANG H 2015 A Dependency-Based Neural
Network for Relation Classication In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing Volume 2 Short Papers pp 285290 Beijing
China Association for Computational Linguistics
Ma J Zhang Y  Zhu J 2014 Tagging The Web Building A Robust Web Tagger
with Neural Network
In Proceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics Volume 1 Long Papers pp 144154 Baltimore
Maryland Association for Computational Linguistics
Ma M Huang L Zhou B  Xiang B 2015 Dependency-based Convolutional Neural
Networks for Sentence Embedding In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing Volume 2 Short Papers pp 174179 Beijing
China Association for Computational Linguistics
McCallum A Freitag D  Pereira F C 2000 Maximum Entropy Markov Models for
Information Extraction and Segmentation In ICML Vol 17 pp 591598
Mikolov T Chen K Corrado G  Dean J 2013 Ecient Estimation of Word
Representations in Vector Space arXiv13013781 cs
Mikolov T Joulin A Chopra S Mathieu M  Ranzato M 2014 Learning Longer
Memory in Recurrent Neural Networks arXiv14127753 cs
Mikolov T Karaat M Burget L Cernocky J  Khudanpur S 2010 Recurrent
neural network based language model In INTERSPEECH 2010 11th Annual Con-
ference of the International Speech Communication Association Makuhari Chiba
Japan September 26-30 2010 pp 10451048
Mikolov T Kombrink S Lukas Burget Cernocky J H  Khudanpur S 2011 Ex-
tensions of recurrent neural network language model In Acoustics Speech and Signal
Processing ICASSP 2011 IEEE International Conference on pp 55285531 IEEE
Mikolov T Sutskever I Chen K Corrado G S  Dean J 2013 Distributed Rep-
resentations of Words and Phrases and their Compositionality In Burges C J C
Bottou L Welling M Ghahramani Z  Weinberger K Q Eds Advances in
Neural Information Processing Systems 26 pp 31113119 Curran Associates Inc
Mikolov T 2012 Statistical language models based on neural networks PhD thesis Ph
D thesis Brno University of Technology
Mnih A  Kavukcuoglu K 2013 Learning word embeddings eciently with noise-
contrastive estimation In Burges C J C Bottou L Welling M Ghahramani Z
 Weinberger K Q Eds Advances in Neural Information Processing Systems 26
pp 22652273 Curran Associates Inc
Mrksic N O Seaghdha D Thomson B Gasic M Su P-H Vandyke D Wen T-H
 Young S 2015 Multi-domain Dialog State Tracking using Recurrent Neural
Networks In Proceedings of the 53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint Conference on Natural Language
Processing Volume 2 Short Papers pp 794799 Beijing China Association for
Computational Linguistics
Neidinger R 2010
Introduction to Automatic Dierentiation and MATLAB Object-
Oriented Programming SIAM Review 52 3 545563
Nguyen T H  Grishman R 2015 Event Detection and Domain Adaptation with
Convolutional Neural Networks
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing Volume 2 Short Papers pp 365371 Beijing
China Association for Computational Linguistics
Nivre J 2008 Algorithms for Deterministic Incremental Dependency Parsing Compu-
tational Linguistics 34 4 513553
Okasaki C 1999 Purely Functional Data Structures Cambridge University Press Cam-
bridge UK New York
Pascanu R Mikolov T  Bengio Y 2012 On the diculty of training Recurrent
Neural Networks arXiv12115063 cs
Pei W Ge T  Chang B 2015 An Eective Neural Network Model for Graph-based
Dependency Parsing In Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing Volume 1 Long Papers pp 313322 Beijing China Associa-
tion for Computational Linguistics
Pennington J Socher R  Manning C 2014 Glove Global Vectors for Word Rep-
resentation In Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing EMNLP pp 15321543 Doha Qatar Association for Com-
putational Linguistics
Pollack J B 1990 Recursive Distributed Representations Articial Intelligence 46
77105
Polyak B T 1964 Some methods of speeding up the convergence of iteration methods
USSR Computational Mathematics and Mathematical Physics 4 5 1  17
Qian Q Tian B Huang M Liu Y Zhu X  Zhu X 2015 Learning Tag Embeddings
and Tag-specic Composition Functions in Recursive Neural Network In Proceedings
of the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing Volume 1 Long
Papers pp 13651374 Beijing China Association for Computational Linguistics
Rong X 2014 word2vec Parameter Learning Explained arXiv14112738 cs
Rumelhart D E Hinton G E  Williams R J 1986 Learning representations by
back-propagating errors Nature 323 6088 533536
Santos C D  Zadrozny B 2014 Learning Character-level Representations for Part-
of-Speech Tagging pp 18181826
Schuster M  Paliwal K K 1997 Bidirectional recurrent neural networks
Transactions on Signal Processing 45 11 26732681
Shawe-Taylor J  Cristianini N 2004 Kernel Methods for Pattern Analysis Cambridge
University Press
Smith N A 2011 Linguistic Structure Prediction Synthesis Lectures on Human Lan-
guage Technologies Morgan and Claypool
Socher R 2014 Recursive Deep Learning For Natural Language Processing and Computer
Vision PhD thesis Stanford University
Socher R Bauer J Manning C D  Andrew Y N 2013 Parsing with Compositional
Vector Grammars In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics Volume 1 Long Papers pp 455465 Soa Bulgaria
Association for Computational Linguistics
Socher R Huval B Manning C D  Ng A Y 2012 Semantic Compositionality
through Recursive Matrix-Vector Spaces In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning pp 12011211 Jeju Island Korea Association for Computational
Linguistics
Socher R Lin C C-Y Ng A Y  Manning C D 2011 Parsing Natural Scenes
and Natural Language with Recursive Neural Networks In Getoor L  Scheer T
Eds Proceedings of the 28th International Conference on Machine Learning ICML
2011 Bellevue Washington USA June 28 - July 2 2011 pp 129136 Omnipress
Socher R Manning C  Ng A 2010 Learning Continuous Phrase Representations
and Syntactic Parsing with Recursive Neural Networks In Proceedings of the Deep
Learning and Unsupervised Feature Learning Workshop of NIPS 2010 pp 19
Socher R Perelygin A Wu J Chuang J Manning C D Ng A  Potts C 2013
Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing pp 16311642 Seattle Washington USA Association for Computational
Linguistics
Sordoni A Galley M Auli M Brockett C Ji Y Mitchell M Nie J-Y Gao J
 Dolan B 2015 A Neural Network Approach to Context-Sensitive Generation
of Conversational Responses
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computational Linguistics Human Lan-
guage Technologies pp 196205 Denver Colorado Association for Computational
Linguistics
Sundermeyer M Alkhouli T Wuebker J  Ney H 2014 Translation Modeling
with Bidirectional Recurrent Neural Networks In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing EMNLP pp 1425 Doha
Qatar Association for Computational Linguistics
Sundermeyer M Schluter R  Ney H 2012 LSTM Neural Networks for Language
Modeling In INTERSPEECH
Sutskever I Martens J Dahl G  Hinton G 2013 On the importance of initialization
and momentum in deep learning In Proceedings of the 30th international conference
on machine learning ICML-13 pp 11391147
Sutskever I Martens J  Hinton G E 2011 Generating text with recurrent neural
networks In Proceedings of the 28th International Conference on Machine Learning
ICML-11 pp 10171024
Sutskever I Vinyals O  Le Q V V 2014 Sequence to Sequence Learning with
Neural Networks In Ghahramani Z Welling M Cortes C Lawrence N D 
Weinberger K Q Eds Advances in Neural Information Processing Systems 27 pp
31043112 Curran Associates Inc
Tai K S Socher R  Manning C D 2015 Improved Semantic Representations From
Tree-Structured Long Short-Term Memory Networks In Proceedings of the 53rd An-
nual Meeting of the Association for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Processing Volume 1 Long Papers
pp 15561566 Beijing China Association for Computational Linguistics
Tamura A Watanabe T  Sumita E 2014 Recurrent Neural Networks for Word
Alignment Model
In Proceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics Volume 1 Long Papers pp 14701480 Baltimore
Maryland Association for Computational Linguistics
Tieleman T  Hinton G 2012 Lecture 65RmsProp Divide the gradient by a running
average of its recent magnitude COURSERA Neural Networks for Machine Learning
Van de Cruys T 2014 A Neural Network Approach to Selectional Preference Acquisi-
tion In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan-
guage Processing EMNLP pp 2635 Doha Qatar Association for Computational
Linguistics
Vaswani A Zhao Y Fossum V  Chiang D 2013 Decoding with Large-Scale Neu-
ral Language Models Improves Translation In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing pp 13871392 Seattle Washing-
ton USA Association for Computational Linguistics
Wager S Wang S  Liang P S 2013 Dropout Training as Adaptive Regularization
In Burges C J C Bottou L Welling M Ghahramani Z  Weinberger K Q
Eds Advances in Neural Information Processing Systems 26 pp 351359 Curran
Associates Inc
Wang P Xu J Xu B Liu C Zhang H Wang F  Hao H 2015a Semantic Cluster-
ing and Convolutional Neural Network for Short Text Categorization In Proceedings
of the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing Volume 2 Short
Papers pp 352357 Beijing China Association for Computational Linguistics
Wang X Liu Y SUN C Wang B  Wang X 2015b Predicting Polarities of Tweets
by Composing Word Embeddings with Long Short-Term Memory In Proceedings of
the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing Volume 1 Long
Papers pp 13431353 Beijing China Association for Computational Linguistics
Watanabe T  Sumita E 2015 Transition-based Neural Constituent Parsing In Pro-
ceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing Volume
1 Long Papers pp 11691179 Beijing China Association for Computational Lin-
guistics
Weiss D Alberti C Collins M  Petrov S 2015 Structured Training for Neural
Network Transition-Based Parsing In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing Volume 1 Long Papers pp 323333 Beijing
China Association for Computational Linguistics
Werbos P J 1990 Backpropagation through time What it does and how to do it
Proceedings of the IEEE 78 10 1550  1560
Weston J Bordes A Yakhnenko O  Usunier N 2013 Connecting Language and
Knowledge Bases with Embedding Models for Relation Extraction
In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing pp
13661371 Seattle Washington USA Association for Computational Linguistics
Xu W Auli M  Clark S 2015 CCG Supertagging with a Recurrent Neural Network
In Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference on Natural Language Processing
Volume 2 Short Papers pp 250255 Beijing China Association for Computational
Linguistics
Yin W  Schutze H 2015 Convolutional Neural Network for Paraphrase Identication
In Proceedings of the 2015 Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics Human Language Technologies pp 901911
Denver Colorado Association for Computational Linguistics
Zaremba W Sutskever I  Vinyals O 2014 Recurrent Neural Network Regularization
arXiv14092329 cs
Zeiler M D 2012 ADADELTA An Adaptive Learning Rate Method arXiv12125701
Zeng D Liu K Lai S Zhou G  Zhao J 2014 Relation Classication via Convolu-
tional Deep Neural Network In Proceedings of COLING 2014 the 25th International
Conference on Computational Linguistics Technical Papers pp 23352344 Dublin
Ireland Dublin City University and Association for Computational Linguistics
Zhou H Zhang Y Huang S  Chen J 2015 A Neural Probabilistic Structured-
Prediction Model for Transition-Based Dependency Parsing
In Proceedings of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing Volume 1 Long Pa-
pers pp 12131222 Beijing China Association for Computational Linguistics
Zhu C Qiu X Chen X  Huang X 2015a A Re-ranking Model for Dependency
Parser with Recursive Convolutional Neural Network
In Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics and the 7th Inter-
national Joint Conference on Natural Language Processing Volume 1 Long Papers
pp 11591168 Beijing China Association for Computational Linguistics
Zhu X Sobhani P  Guo H 2015b Long Short-Term Memory Over Tree Structures
arXiv150304881 cs
A Primer on Neural Network Models
for Natural Language Processing
Yoav Goldberg
Draft as of October 6 2015
The most up-to-date version of this manuscript is available at httpwwwcsbiu
acilyogonnlppdf Major updates will be published on arxiv periodically
I welcome any comments you may have regarding the content and presentation If you
spot a missing reference or have relevant work youd like to see mentioned do let me know
firstlastgmail
Abstract
Over the past few years neural networks have re-emerged as powerful machine-learning
models yielding state-of-the-art results in elds such as image recognition and speech
processing More recently neural network models started to be applied also to textual
natural language signals again with very promising results This tutorial surveys neural
network models from the perspective of natural language processing research in an attempt
to bring natural-language researchers up to speed with the neural techniques The tutorial
covers input encoding for natural language tasks feed-forward networks convolutional
networks recurrent networks and recursive networks as well as the computation graph
abstraction for automatic gradient computation
1 Introduction
For a long time core NLP techniques were dominated by machine-learning approaches that
used linear models such as support vector machines or logistic regression trained over very
high dimensional yet very sparse feature vectors
Recently the eld has seen some success in switching from such linear models over
sparse inputs to non-linear neural-network models over dense inputs While most of the
neural network techniques are easy to apply sometimes as almost drop-in replacements of
the old linear classiers there is in many cases a strong barrier of entry In this tutorial I
attempt to provide NLP practitioners as well as newcomers with the basic background
jargon tools and methodology that will allow them to understand the principles behind
the neural network models and apply them to their own work This tutorial is expected
to be self-contained while presenting the dierent approaches under a unied notation and
framework
It also points to
external sources for more advanced topics when appropriate
It repeats a lot of material which is available elsewhere
This primer is not intended as a comprehensive resource for those that will go on and
develop the next advances in neural-network machinery though it may serve as a good entry
point Rather it is aimed at those readers who are interested in taking the existing useful
technology and applying it in useful and creative ways to their favourite NLP problems For
more in-depth general discussion of neural networks the theory behind them advanced
optimization methods and other advanced topics the reader is referred to other existing
resources In particular the book by Bengio et al 2015 is highly recommended
Scope The focus is on applications of neural networks to language processing tasks How-
ever some subareas of language processing with neural networks were decidedly left out of
scope of this tutorial These include the vast literature of language modeling and acoustic
modeling the use of neural networks for machine translation and multi-modal applications
combining language and other signals such as images and videos eg caption generation
Caching methods for ecient runtime performance methods for ecient training with large
output vocabularies and attention models are also not discussed Word embeddings are dis-
cussed only to the extent that is needed to understand in order to use them as inputs for
other models Other unsupervised approaches including autoencoders and recursive au-
toencoders also fall out of scope While some applications of neural networks for language
modeling and machine translation are mentioned in the text their treatment is by no means
comprehensive
A Note on Terminology The word feature is used to refer to a concrete linguistic
input such as a word a sux or a part-of-speech tag For example in a rst-order part-
of-speech tagger the features might be current word previous word next word previous
part of speech The term input vector is used to refer to the actual input that is fed
to the neural-network classier Similarly input vector entry refers to a specic value
of the input This is in contrast to a lot of the neural networks literature in which the
word feature is overloaded between the two uses and is used primarily to refer to an
input-vector entry
Mathematical Notation I use bold upper case letters to represent matrices X Y
Z and bold lower-case letters to represent vectors b When there are series of related
matrices and vectors for example where each matrix corresponds to a dierent layer in
the network superscript indices are used W1 W2 For the rare cases in which we want
indicate the power of a matrix or a vector a pair of brackets is added around the item to
be exponentiated W2 W32 Unless otherwise stated vectors are assumed to be row
vectors We use v1 v2 to denote vector concatenation
2 Neural Network Architectures
Neural networks are powerful learning models We will discuss two kinds of neural network
architectures that can be mixed and matched  feed-forward networks and Recurrent 
Recursive networks Feed-forward networks include networks with fully connected layers
such as the multi-layer perceptron as well as networks with convolutional and pooling
layers All of the networks act as classiers but each with dierent strengths
Fully connected feed-forward neural networks Section 4 are non-linear learners that
can for the most part be used as a drop-in replacement wherever a linear learner is used
This includes binary and multiclass classication problems as well as more complex struc-
tured prediction problems Section 8 The non-linearity of the network as well as the
ability to easily integrate pre-trained word embeddings often lead to superior classication
accuracy A series of works Chen  Manning 2014 Weiss Alberti Collins  Petrov
2015 Pei Ge  Chang 2015 Durrett  Klein 2015 managed to obtain improved syntac-
tic parsing results by simply replacing the linear model of a parser with a fully connected
feed-forward network Straight-forward applications of a feed-forward network as a classi-
er replacement usually coupled with the use of pre-trained word vectors provide benets
also for CCG supertagging Lewis  Steedman 2014 dialog state tracking Henderson
Thomson  Young 2013 pre-ordering for statistical machine translation de Gispert
Iglesias  Byrne 2015 and language modeling Bengio Ducharme Vincent  Janvin
2003 Vaswani Zhao Fossum  Chiang 2013 Iyyer et al 2015 demonstrate that multi-
layer feed-forward networks can provide competitive results on sentiment classication and
factoid question answering
Networks with convolutional and pooling layers Section 9 are useful for classication
tasks in which we expect to nd strong local clues regarding class membership but these
clues can appear in dierent places in the input For example in a document classication
task a single key phrase or an ngram can help in determining the topic of the document
Johnson  Zhang 2015 We would like to learn that certain sequences of words are good
indicators of the topic and do not necessarily care where they appear in the document
Convolutional and pooling layers allow the model to learn to nd such local indicators
regardless of their position Convolutional and pooling architecture show promising results
on many tasks including document classication Johnson  Zhang 2015 short-text cat-
egorization Wang Xu Xu Liu Zhang Wang  Hao 2015a sentiment classication
Kalchbrenner Grefenstette  Blunsom 2014 Kim 2014 relation type classication be-
tween entities Zeng Liu Lai Zhou  Zhao 2014 dos Santos Xiang  Zhou 2015 event
detection Chen Xu Liu Zeng  Zhao 2015 Nguyen  Grishman 2015 paraphrase iden-
tication Yin  Schutze 2015 semantic role labeling Collobert Weston Bottou Karlen
Kavukcuoglu  Kuksa 2011 question answering Dong Wei Zhou  Xu 2015 predict-
ing box-oce revenues of movies based on critic reviews Bitvai  Cohn 2015 modeling
text interestingness Gao Pantel Gamon He  Deng 2014 and modeling the relation
between character-sequences and part-of-speech tags Santos  Zadrozny 2014
In natural language we often work with structured data of arbitrary sizes such as
sequences and trees We would like to be able to capture regularities in such structures
or to model similarities between such structures
In many cases this means encoding
the structure as a xed width vector which we can then pass on to another statistical
learner for further processing While convolutional and pooling architectures allow us to
encode arbitrary large items as xed size vectors capturing their most salient features
they do so by sacricing most of the structural information Recurrent Section 10 and
recursive Section 12 architectures on the other hand allow us to work with sequences
and trees while preserving a lot of the structural information Recurrent networks Elman
1990 are designed to model sequences while recursive networks Goller  Kuchler 1996
are generalizations of recurrent networks that can handle trees We will also discuss an
extension of recurrent networks that allow them to model stacks Dyer Ballesteros Ling
Matthews  Smith 2015 Watanabe  Sumita 2015
Recurrent models have been shown to produce very strong results for language model-
ing including Mikolov Karaat Burget Cernocky  Khudanpur 2010 Mikolov Kom-
brink Lukas Burget Cernocky  Khudanpur 2011 Mikolov 2012 Duh Neubig Sudoh
 Tsukada 2013 Adel Vu  Schultz 2013 Auli Galley Quirk  Zweig 2013 Auli 
Gao 2014 as well as for sequence tagging Irsoy  Cardie 2014 Xu Auli  Clark 2015
Ling Dyer Black Trancoso Fermandez Amir Marujo  Luis 2015b machine transla-
tion Sundermeyer Alkhouli Wuebker  Ney 2014 Tamura Watanabe  Sumita 2014
Sutskever Vinyals  Le 2014 Cho van Merrienboer Gulcehre Bahdanau Bougares
Schwenk  Bengio 2014b dependency parsing Dyer et al 2015 Watanabe  Sumita
2015 sentiment analysis Wang Liu SUN Wang  Wang 2015b noisy text normal-
ization Chrupala 2014 dialog state tracking Mrksic O Seaghdha Thomson Gasic Su
Vandyke Wen  Young 2015 response generation Sordoni Galley Auli Brockett Ji
Mitchell Nie Gao  Dolan 2015 and modeling the relation between character sequences
and part-of-speech tags Ling et al 2015b
Recursive models were shown to produce state-of-the-art or near state-of-the-art re-
sults for constituency Socher Bauer Manning  Andrew Y 2013 and dependency Le
 Zuidema 2014 Zhu Qiu Chen  Huang 2015a parse re-ranking discourse parsing
Li Li  Hovy 2014 semantic relation classication Hashimoto Miwa Tsuruoka 
Chikayama 2013 Liu Wei Li Ji Zhou  WANG 2015 political ideology detection
based on parse trees Iyyer Enns Boyd-Graber  Resnik 2014b sentiment classication
Socher Perelygin Wu Chuang Manning Ng  Potts 2013 Hermann  Blunsom 2013
target-dependent sentiment classication Dong Wei Tan Tang Zhou  Xu 2014 and
question answering Iyyer Boyd-Graber Claudino Socher  Daume III 2014a
3 Feature Representation
Before discussing the network structure in more depth it is important to pay attention
to how features are represented For now we can think of a feed-forward neural network
as a function N N x that takes as input a din dimensional vector x and produces a dout
dimensional output vector The function is often used as a classier assigning the input
x a degree of membership in one or more of dout classes The function can be complex
and is almost always non-linear Common structures of this function will be discussed
in Section 4 Here we focus on the input x When dealing with natural language the
input x encodes features such as words part-of-speech tags or other linguistic information
Perhaps the biggest jump when moving from sparse-input linear models to neural-network
based models is to stop representing each feature as a unique dimension the so called
one-hot representation and representing them instead as dense vectors That is each core
feature is embedded into a d dimensional space and represented as a vector in that space1
The embeddings the vector representation of each core feature can then be trained like
the other parameter of the function N N  Figure 1 shows the two approaches to feature
representation
The feature embeddings the values of the vector entries for each feature are treated
as model parameters that need to be trained together with the other components of the
network Methods of training or obtaining the feature embeddings will be discussed later
For now consider the feature embeddings as given
The general structure for an NLP classication system based on a feed-forward neural
network is thus
1 Extract a set of core linguistic features f1     fk that are relevant for predicting the
output class
2 For each feature fi of interest retrieve the corresponding vector vfi
3 Combine the vectors either by concatenation summation or a combination of both
into an input vector x
4 Feed x into a non-linear classier feed-forward neural network
The biggest change in the input then is the move from sparse representations in which
each feature is its own dimension to a dense representation in which each feature is mapped
to a vector Another dierence is that we extract only core features and not feature com-
binations We will elaborate on both these changes briey
Dense Vectors vs One-hot Representations What are the benets of representing
our features as vectors instead of as unique IDs Should we always represent features as
dense vectors Lets consider the two kinds of representations
One Hot Each feature is its own dimension
 Dimensionality of one-hot vector is same as number of distinct features
1 Dierent feature types may be embedded into dierent spaces For example one may represent word
features using 100 dimensions and part-of-speech features using 20 dimensions
Figure 1 Sparse vs dense feature representations Two encodings of the informa-
tion current word is dog previous word is the previous pos-tag is DET
a Sparse feature vector Each dimension represents a feature Feature combi-
nations receive their own dimensions Feature values are binary Dimensionality
is very high b Dense embeddings-based feature vector Each core feature is
represented as a vector Each feature corresponds to several input vector en-
tries No explicit encoding of feature combinations Dimensionality is low The
feature-to-vector mappings come from an embedding table
 Features are completely independent from one another The feature word is
dog  is as dis-similar to word is thinking  than it is to word is cat 
Dense Each feature is a d-dimensional vector
 Dimensionality of vector is d
 Similar features will have similar vectors  information is shared between similar
features
One benet of using dense and low-dimensional vectors is computational the majority
of neural network toolkits do not play well with very high-dimensional sparse vectors
However this is just a technical obstacle which can be resolved with some engineering
eort
The main benet of the dense representations is in generalization power if we believe
some features may provide similar clues it is worthwhile to provide a representation that
is able to capture these similarities For example assume we have observed the word dog
many times during training but only observed the word cat a handful of times or not at
all If each of the words is associated with its own dimension occurrences of dog will not
tell us anything about the occurrences of cat However in the dense vectors representation
the learned vector for dog may be similar to the learned vector from cat allowing the
model to share statistical strength between the two events This argument assumes that
good vectors are somehow given to us Section 5 describes ways of obtaining such vector
representations
In cases where we have relatively few distinct features in the category and we believe
there are no correlations between the dierent features we may use the one-hot representa-
tion However if we believe there are going to be correlations between the dierent features
in the group for example for part-of-speech tags we may believe that the dierent verb
inections VB and VBZ may behave similarly as far as our task is concerned it may be
worthwhile to let the network gure out the correlations and gain some statistical strength
by sharing the parameters It may be the case that under some circumstances when the
feature space is relatively small and the training data is plentiful or when we do not wish to
share statistical information between distinct words there are gains to be made from using
the one-hot representations However this is still an open research question and there are
no strong evidence to either side The majority of work pioneered by Collobert  Weston
2008 Collobert et al 2011 Chen  Manning 2014 advocate the use of dense trainable
embedding vectors for all features For work using neural network architecture with sparse
vector encodings see Johnson  Zhang 2015
Finally it is important to note that representing features as dense vectors is an integral
part of the neural network framework and that consequentially the dierences between
using sparse and dense feature representations are subtler than they may appear at rst
In fact using sparse one-hot vectors as input when training a neural network amounts
to dedicating the rst layer of the network to learning a dense embedding vector for each
feature based on the training data We touch on this in Section 44
Variable Number of Features Continuous Bag of Words Feed-forward networks
assume a xed dimensional input This can easily accommodate the case of a feature-
extraction function that extracts a xed number of features each feature is represented
as a vector and the vectors are concatenated This way each region of the resulting
input vector corresponds to a dierent feature However in some cases the number of
features is not known in advance for example in document classication it is common
that each word in the sentence is a feature We thus need to represent an unbounded
number of features using a xed size vector One way of achieving this is through a so-
called continuous bag of words CBOW representation Mikolov Chen Corrado  Dean
2013 The CBOW is very similar to the traditional bag-of-words representation in which
we discard order information and works by either summing or averaging the embedding
vectors of the corresponding features2
2 Note that if the vfis were one-hot vectors rather than dense feature representations the CBOW and
W CBOW equations above would reduce to the traditional weighted bag-of-words representations
which is in turn equivalent to a sparse feature-vector representation in which each binary indicator
feature corresponds to a unique word
CBOW f1  fk 
kcid88
A simple variation on the CBOW representation is weighted CBOW in which dierent
vectors receive dierent weights
W CBOW f1  fk 
1cid80k
kcid88
aivfi
Here each feature fi has an associated weight ai indicating the relative importance of
the feature For example in a document classication task a feature fi may correspond to
a word in the document and the associated weight ai could be the words TF-IDF score
Distance and Position Features The linear distance in between two words in a sentence
may serve as an informative feature For example in an event extraction task3 we may be
given a trigger word and a candidate argument word and asked to predict if the argument
word is indeed an argument of the trigger The distance or relative position between the
trigger and the argument is a strong signal for this prediction task In the traditional NLP
setup distances are usually encoded by binning the distances into several groups ie 1 2
3 4 510 10 and associating each bin with a one-hot vector In a neural architecture
where the input vector is not composed of binary indicator features it may seem natural to
allocate a single input vector entry to the distance feature where the numeric value of that
entry is the distance However this approach is not taken in practice Instead distance
features are encoded similarly to the other feature types each bin is associated with a
d-dimensional vector and these distance-embedding vectors are then trained as regular
parameters in the network Zeng et al 2014 dos Santos et al 2015 Zhu et al 2015a
Nguyen  Grishman 2015
Feature Combinations Note that the feature extraction stage in the neural-network
settings deals only with extraction of core features This is in contrast to the traditional
linear-model-based NLP systems in which the feature designer had to manually specify not
only the core features of interests but also interactions between them eg introducing not
only a feature stating word is X and a feature stating tag is Y but also combined feature
stating word is X and tag is Y or sometimes even word is X tag is Y and previous word
is Z The combination features are crucial in linear models because they introduce more
dimensions to the input transforming it into a space where the data-points are closer to
being linearly separable On the other hand the space of possible combinations is very
large and the feature designer has to spend a lot of time coming up with an eective
set of feature combinations One of the promises of the non-linear neural network models
is that one needs to dene only the core features The non-linearity of the classier as
dened by the network structure is expected to take care of nding the indicative feature
combinations alleviating the need for feature combination engineering
3 The event extraction task involves identication of events from a predened set of event types For
example identication of purchase events or terror-attack events Each event type can be triggered
by various triggering words commonly verbs and has several slots arguments that needs to be lled
ie who purchased what was purchased at what amount
Kernel methods Shawe-Taylor  Cristianini 2004 and in particular polynomial kernels
Kudo  Matsumoto 2003 also allow the feature designer to specify only core features
leaving the feature combination aspect to the learning algorithm In contrast to neural-
network models kernels methods are convex admitting exact solutions to the optimization
problem However the classication eciency in kernel methods scales linearly with the
size of the training data making them too slow for most practical purposes and not suitable
for training with large datasets On the other hand neural network classication eciency
scales linearly with the size of the network regardless of the training data size
Dimensionality How many dimensions should we allocate for each feature Unfortu-
nately there are no theoretical bounds or even established best-practices in this space
Clearly the dimensionality should grow with the number of the members in the class you
probably want to assign more dimensions to word embeddings than to part-of-speech embed-
dings but how much is enough In current research the dimensionality of word-embedding
vectors range between about 50 to a few hundreds and in some extreme cases thousands
Since the dimensionality of the vectors has a direct eect on memory requirements and
processing time a good rule of thumb would be to experiment with a few dierent sizes
and choose a good trade-o between speed and task accuracy
Vector Sharing Consider a case where you have a few features that share the same
vocabulary For example when assigning a part-of-speech to a given word we may have a
set of features considering the previous word and a set of features considering the next word
When building the input to the classier we will concatenate the vector representation of
the previous word to the vector representation of the next word The classier will then
be able to distinguish the two dierent indicators and treat them dierently But should
the two features share the same vectors Should the vector for dogprevious-word be the
same as the vector of dognext-word Or should we assign them two distinct vectors
This again is mostly an empirical question If you believe words behave dierently when
they appear in dierent positions eg word X behaves like word Y when in the previous
position but X behaves like Z when in the next position then it may be a good idea to
use two dierent vocabularies and assign a dierent set of vectors for each feature type
However if you believe the words behave similarly in both locations then something may
be gained by using a shared vocabulary for both feature types
Networks Output For multi-class classication problems with k classes the networks
output is a k-dimensional vector in which every dimension represents the strength of a
particular output class That is the output remains as in the traditional linear models 
scalar scores to items in a discrete set However as we will see in Section 4 there is a d k
matrix associated with the output layer The columns of this matrix can be thought of as
d dimensional embeddings of the output classes The vector similarities between the vector
representations of the k classes indicate the models learned similarities between the output
classes
Historical Note Representing words as dense vectors for input to a neural network was
introduced by Bengio et al Bengio et al 2003 in the context of neural language modeling
It was introduced to NLP tasks in the pioneering work of Collobert Weston and colleagues
2008 2011 Using embeddings for representing not only words but arbitrary features was
popularized following Chen and Manning 2014
4 Feed-forward Neural Networks
A Brain-inspired metaphor As the name suggest neural-networks are inspired by the
brains computation mechanism which consists of computation units called neurons In the
metaphor a neuron is a computational unit that has scalar inputs and outputs Each input
has an associated weight The neuron multiplies each input by its weight and then sums4
them applies a non-linear function to the result and passes it to its output The neurons
are connected to each other forming a network the output of a neuron may feed into the
inputs of one or more neurons Such networks were shown to be very capable computational
devices If the weights are set correctly a neural network with enough neurons and a non-
linear activation function can approximate a very wide range of mathematical functions we
will be more precise about this later
cid82
cid82
cid82
cid82
cid82
cid82
cid82
cid82
cid82
cid82
cid82
Input layer
Figure 2 Feed-forward neural network with two hidden layers
A typical feed-forward neural network may be drawn as in Figure 2 Each circle is a
neuron with incoming arrows being the neurons inputs and outgoing arrows being the neu-
rons outputs Each arrow carries a weight reecting its importance not shown Neurons
are arranged in layers reecting the ow of information The bottom layer has no incom-
ing arrows and is the input to the network The top-most layer has no outgoing arrows
and is the output of the network The other layers are considered hidden The sigmoid
shape inside the neurons in the middle layers represent a non-linear function typically a
11  ex that is applied to the neurons value before passing it to the output In the
gure each neuron is connected to all of the neurons in the next layer  this is called a
fully-connected layer or an ane layer
4 While summing is the most common operation other functions such as a max are also possible
While the brain metaphor is sexy and intriguing it is also distracting and cumbersome
to manipulate mathematically We therefore switch to using more concise mathematic no-
tation The values of each row of neurons in the network can be thought of as a vector In
Figure 2 the input layer is a 4 dimensional vector x and the layer above it is a 6 dimen-
sional vector h1 The fully connected layer can be thought of as a linear transformation
from 4 dimensions to 6 dimensions A fully-connected layer implements a vector-matrix
multiplication h  xW where the weight of the connection from the ith neuron in the
input row to the jth neuron in the output row is Wij5 The values of h are then trans-
formed by a non-linear function g that is applied to each value before being passed on to the
next input The whole computation from input to output can be written as gxW1W2
where W1 are the weights of the rst layer and W2 are the weights of the second one
In Mathematical Notation From this point on we will abandon the brain metaphor
and describe networks exclusively in terms of vector-matrix operations
The simplest neural network is the perceptron which is a linear function of its inputs
N NP erceptronx  xW  b
x  Rdin W  Rdindout b  Rdout
W is the weight matrix and b is a bias term6 In order to go beyond linear functions we
introduce a non-linear hidden layer the network in Figure 2 has two such layers resulting
in the 1-layer Multi Layer Perceptron MLP1 A one-layer feed-forward neural network
has the form
N NM LP 1x  gxW1  b1W2  b2
x  Rdin W1  Rdind1 b1  Rd1 W2  Rd1d2 b2  Rd2
Here W1 and b1 are a matrix and a bias term for the rst linear transformation of the
input g is a non-linear function that is applied element-wise also called a non-linearity or
an activation function and W2 and b2 are the matrix and bias term for a second linear
transform
Breaking it down xW1b1 is a linear transformation of the input x from din dimensions
to d1 dimensions g is then applied to each of the d1 dimensions and the matrix W2 together
with bias vector b2 are then used to transform the result into the d2 dimensional output
vector The non-linear activation function g has a crucial role in the networks ability to
represent complex functions Without the non-linearity in g the neural network can only
represent linear transformations of the input7
We can add additional linear-transformations and non-linearities resulting in a 2-layer
MLP the network in Figure 2 is of this form
N NM LP 2x  g2g1xW1  b1W2  b2W3
of hj is then hj cid804
i1 xi  wij
5 To see why this is the case denote the weight of the ith input of the jth neuron in h as wij The value
6 The network in gure 2 does not include bias terms A bias term can be added to a layer by adding to
it an additional neuron that does not have any incoming connections whose value is always 1
7 To see why consider that a sequence of linear transformations is still a linear transformation
It is perhaps clearer to write deeper networks like this using intermediary variables
N NM LP 2x y
h1 g1xW1  b1
h2 g2h1W2  b2
y h2W3
The vector resulting from each linear transform is referred to as a layer The outer-most
linear transform results in the output layer and the other linear transforms result in hidden
layers Each hidden layer is followed by a non-linear activation In some cases such as in
the last layer of our example the bias vectors are forced to 0 dropped
Layers resulting from linear transformations are often referred to as fully connected or
ane Other types of architectures exist In particular image recognition problems benet
from convolutional and pooling layers Such layers have uses also in language processing
and will be discussed in Section 9 Networks with more than one hidden layer are said to
be deep networks hence the name deep learning
When describing a neural network one should specify the dimensions of the layers and
the input A layer will expect a din dimensional vector as its input and transform it into a
dout dimensional vector The dimensionality of the layer is taken to be the dimensionality
of its output For a fully connected layer lx  xW  b with input dimensionality din and
output dimensionality dout the dimensions of x is 1  din of W is din  dout and of b is
1  dout
The output of the network is a dout dimensional vector In case dout  1 the networks
output is a scalar Such networks can be used for regression or scoring by considering
the value of the output or for binary classication by consulting the sign of the output
Networks with dout  k  1 can be used for k-class classication by associating each
dimension with a class and looking for the dimension with maximal value Similarly if
the output vector entries are positive and sum to one the output can be interpreted as
a distribution over class assignments such output normalization is typically achieved by
applying a softmax transformation on the output layer see Section 43
The matrices and the bias terms that dene the linear transformations are the parame-
ters of the network It is common to refer to the collection of all parameters as  Together
with the input the parameters determine the networks output The training algorithm is
responsible for setting their values such that the networks predictions are correct Training
is discussed in Section 6
41 Representation Power
In terms of representation power it was shown by Hornik Stinchcombe  White 1989
Cybenko 1989 that MLP1 is a universal approximator  it can approximate with any
desired non-zero amount of error a family of functions8 that include all continuous functions
8 Specically a feed-forward network with linear output layer and at least one hidden layer with a squash-
ing activation function can approximate any Borel measurable function from one nite dimensional space
to another
on a closed and bounded subset of Rn and any function mapping from any nite dimensional
discrete space to another This may suggest there is no reason to go beyond MLP1 to more
complex architectures However the theoretical result does not state how large the hidden
layer should be nor does it say anything about the learnability of the neural network it
states that a representation exists but does not say how easy or hard it is to set the
parameters based on training data and a specic learning algorithm
It also does not
guarantee that a training algorithm will nd the correct function generating our training
data Since in practice we train neural networks on relatively small amounts of data using
a combination of the backpropagation algorithm and variants of stochastic gradient descent
and use hidden layers of relatively modest sizes up to several thousands there is benet
to be had in trying out more complex architectures than MLP1 In many cases however
MLP1 does indeed provide very strong results For further discussion on the representation
power of feed-forward neural networks see Bengio et al 2015 Section 65
42 Common Non-linearities
The non-linearity g can take many forms There is currently no good theory as to which
non-linearity to apply in which conditions and choosing the correct non-linearity for a
given task is for the most part an empirical question I will now go over the common non-
linearities from the literature the sigmoid tanh hard tanh and the rectied linear unit
ReLU Some NLP researchers also experimented with other forms of non-linearities such
as cube and tanh-cube
Sigmoid The sigmoid activation function x  11  ex is an S-shaped function
transforming each value x into the range 0 1
Hyperbolic tangent tanh The hyperbolic tangent tanhx  e2x1
tion is an S-shaped function transforming the values x into the range 1 1
Hard tanh The hard-tanh activation function is an approximation of the tanh function
which is faster to compute and take derivatives of
e2x1 activation func-
1 x  1
otherwise
hardtanhx 
Rectier ReLU The Rectier activation function Glorot Bordes  Bengio 2011
also known as the rectied linear unit is a very simple activation function that is easy to
work with and was shown many times to produce excellent results9 The ReLU unit clips
each value x  0 at 0 Despite its simplicity it performs well for many tasks especially
when combined with the dropout regularization technique see Section 64
9 The technical advantages of the ReLU over the sigmoid and tanh activation functions is that it does not
involve expensive-to-compute functions and more importantly that it does not saturate The sigmoid
and tanh activation are capped at 1 and the gradients at this region of the functions are near zero
driving the entire gradient near zero The ReLU activation does not have this problem making it
especially suitable for networks with multiple layers which are susceptible to the vanishing gradients
problem when trained with the saturating units
ReLU x  max0 x 
cid40
0 x  0
x otherwise
As a rule of thumb ReLU units work better than tanh and tanh works better than
sigmoid10
43 Output Transformations
In many cases the output layer vector is also transformed A common transformation is
the softmax 
x x1     xk
sof tmaxxi 
exicid80k
j1 exj
The result is a vector of non-negative real numbers that sum to one making it a discrete
probability distribution over k possible outcomes
The sof tmax output transformation is used when we are interested in modeling a prob-
ability distribution over the possible output classes To be eective it should be used in
conjunction with a probabilistic training objective such as cross-entropy see Section 45
below
When the softmax transformation is applied to the output of a network without a hidden
layer the result is the well known multinomial logistic regression model also known as a
maximum-entropy classier
44 Embedding Layers
Up until now the discussion ignored the source of x treating it as an arbitrary vector
In an NLP application x is usually composed of various embeddings vectors We can be
explicit about the source of x and include it in the networks denition We introduce c
a function from core features to an input vector
It is common for c to extract the embedding vector associated with each feature and
concatenate them
10 In addition to these activation functions recent works from the NLP community experiment with and
reported success with other forms of non-linearities The Cube activation function gx  x3 was
suggested by Chen  Manning 2014 who found it to be more eective than other non-linearities in
a feed-forward network that was used to predict the actions in a greedy transition-based dependency
parser The tanh cube activation function gx  tanhx3  x was proposed by Pei et al 2015
who found it to be more eective than other non-linearities in a feed-forward network that was used as
a component in a structured-prediction graph-based dependency parser
The cube and tanh-cube activation functions are motivated by the desire to better capture interac-
tions between dierent features While these activation functions are reported to improve performance
in certain situations their general applicability is still to be determined
x  cf1 f2 f3 vf1 vf2 vf3
N NM LP 1x N NM LP 1cf1 f2 f3
N NM LP 1vf1 vf2 vf3
gvf1 vf2 vf3W1  b1W2  b2
Another common choice is for c to sum the embedding vectors this assumes the em-
bedding vectors all share the same dimensionality
x  cf1 f2 f3 vf1  vf2  vf3
N NM LP 1x N NM LP 1cf1 f2 f3
N NM LP 1vf1  vf2  vf3
gvf1  vf2  vf3W1  b1W2  b2
The form of c is an essential part of the networks design In many papers it is common
to refer to c as part of the network and likewise treat the word embeddings vfi as resulting
from an embedding layer or lookup layer Consider a vocabulary of V  words each
embedded as a d dimensional vector The collection of vectors can then be thought of as a
V   d embedding matrix E in which each row corresponds to an embedded feature Let
fi be a V -dimensional vector which is all zeros except from one index corresponding to
the value of the ith feature in which the value is 1 this is called a one-hot vector The
multiplication fiE will then select the corresponding row of E Thus vfi can be dened
in terms of E and fi
And similarly
vfi  fiE
CBOW f1  fk 
kcid88
kcid88
fiE  
The input to the network is then considered to be a collection of one-hot vectors While
this is elegant and well dened mathematically an ecient implementation typically involves
a hash-based data structure mapping features to their corresponding embedding vectors
without going through the one-hot representation
In this tutorial we take c to be separate from the network architecture the networks
inputs are always dense real-valued input vectors and c is applied before the input is passed
the network similar to a feature function in the familiar linear-models terminology How-
ever when training a network the input vector x does remember how it was constructed
and can propagate error gradients back to its component embedding vectors as appropriate
A note on notation When describing network layers that get concatenated vectors x
y and z as input some authors use explicit concatenation x y zW  b while others use
an ane transformation xU  yV  zW  b If the weight matrices U V W in the
ane transformation are dierent than one another the two notations are equivalent
A note on sparse vs dense features Consider a network which uses a traditional
sparse representation for its input vectors and no embedding layer Assuming the set of all
available features is V and we have k on features f1     fk fi  V  the networks input
kcid88
and so the rst layer ignoring the non-linear activation is
x  NV 
kcid88
xW  b  
W  RV d b  Rd
This layer selects rows of W corresponding to the input features in x and sums them
then adding a bias term This is very similar to an embedding layer that produces a CBOW
representation over the features where the matrix W acts as the embedding matrix The
main dierence is the introduction of the bias vector b and the fact that the embedding
layer typically does not undergo a non-linear activation but rather passed on directly to the
rst layer Another dierence is that this scenario forces each feature to receive a separate
vector row in W while the embedding layer provides more exibility allowing for example
for the features next word is dog and previous word is dog to share the same vector
However these dierences are small and subtle When it comes to multi-layer feed-forward
networks the dierence between dense and sparse inputs is smaller than it may seem at
rst sight
45 Loss Functions
When training a neural network more on training in Section 6 below much like when
training a linear classier one denes a loss function Ly y stating the loss of predicting
y when the true output is y The training objective is then to minimize the loss across the
dierent training examples The loss Ly y assigns a numerical score a scalar for the
networks output y given the true expected output y11 The loss is always positive and
should be zero only for cases where the networks output is correct
The parameters of the network the matrices Wi the biases bi and commonly the em-
beddings E are then set in order to minimize the loss L over the training examples usually
it is the sum of the losses over the dierent training examples that is being minimized
The loss can be an arbitrary function mapping two vectors to a scalar For practical
purposes of optimization we restrict ourselves to functions for which we can easily compute
gradients or sub-gradients In most cases it is sucient and advisable to rely on a common
loss function rather than dening your own For a detailed discussion on loss functions for
neural networks see LeCun Chopra Hadsell Ranzato  Huang 2006 LeCun  Huang
2005 Bengio et al 2015 We now discuss some loss functions that are commonly used in
neural networks for NLP
11 In our notation both the models output and the expected output are vectors while in many cases it is
more natural to think of the expected output as a scalar class assignment In such cases y is simply
the corresponding one-hot vector
Hinge binary For binary classication problems the networks output is a single scalar
y and the intended output y is in 11 The classication rule is signy and a
classication is considered correct if y  y  0 meaning that y and y share the same sign
The hinge loss also known as margin loss or SVM loss is dened as
Lhingebinaryy y  max0 1  y  y
The loss is 0 when y and y share the same sign and y  1 Otherwise the loss is linear
In other words the binary hinge loss attempts to achieve a correct classication with a
margin of at least 1
Hinge multiclass The hinge loss was extended to the multiclass setting by Crammer
and Singer 2002 Let y  y1     yn be the networks output vector and y be the one-hot
vector for the correct output class
The classication rule is dened as selecting the class with the highest score
prediction  arg max
Denote by t  arg maxi yi the correct class and by k  arg maxicid54t yi the highest scoring
class such that k cid54 t The multiclass hinge loss is dened as
Lhingemulticlassy y  max0 1  yt  yk
The multiclass hinge loss attempts to score the correct class above all other classes with a
margin of at least 1
Both the binary and multiclass hinge losses are intended to be used with a linear output
layer The hinge losses are useful whenever we require a hard decision rule and do not
attempt to model class membership probability
Log loss The log loss is a common variation of the hinge loss which can be seen as a
soft version of the hinge loss with an innite margin LeCun et al 2006
Llogy y  log1  expyt  yk
Categorical cross-entropy loss The categorical cross-entropy loss also referred to as
negative log likelihood  is used when a probabilistic interpretation of the scores is desired
Let y  y1     yn be a vector representing the true multinomial distribution over the
labels 1     n and let y  y1     yn be the networks output which was transformed by the
sof tmax activation function and represent the class membership conditional distribution
yi  P y  ix The categorical cross entropy loss measures the dissimilarity between
the true label distribution y and the predicted label distribution y and is dened as cross
entropy
Lcrossentropyy y  
cid88
yi logyi
For hard classication problems in which each training example has a single correct
class assignment y is a one-hot vector representing the true class In such cases the cross
entropy can be simplied to
Lcrossentropyhard classicationy y   logyt
where t is the correct class assignment This attempts to set the probability mass assigned
to the correct class t to 1 Because the scores y have been transformed using the sof tmax
function and represent a conditional distribution increasing the mass assigned to the correct
class means decreasing the mass assigned to all the other classes
The cross-entropy loss is very common in the neural networks literature and produces
a multi-class classier which does not only predict the one-best class label but but also
predicts a distribution over the possible labels When using the cross-entropy loss it is
assumed that the networks output is transformed using the sof tmax transformation
Ranking losses
In some settings we are not given supervision in term of labels but
rather as pairs of correct and incorrect items x and xcid48 and our goal is to score correct
items above incorrect ones Such training situations arise when we have only positive
examples and generate negative examples by corrupting a positive example A useful loss
in such scenarios is the margin-based ranking loss dened for a pair of correct and incorrect
examples
Lrankingmarginx xcid48  max0 1  N N x  N N xcid48
where N N x is the score assigned by the network for input vector x The objective is to
score rank correct inputs over incorrect ones with a margin of at least 1
A common variation is to use the log version of the ranking loss
Lrankinglogx xcid48  log1  expN N x  N N xcid48
Examples using the ranking hinge loss in language tasks include training with the aux-
iliary tasks used for deriving pre-trained word embeddings see section 5 in which we are
given a correct word sequence and a corrupted word sequence and our goal is to score
the correct sequence above the corrupt one Collobert  Weston 2008 Similarly Van
de Cruys 2014 used the ranking loss in a selectional-preferences task in which the net-
work was trained to rank correct verb-object pairs above incorrect automatically derived
ones and Weston Bordes Yakhnenko  Usunier 2013 trained a model to score correct
headrelationtrail triplets above corrupted ones in an information-extraction setting An
example of using the ranking log loss can be found in Gao et al 2014 A variation of the
ranking log loss allowing for a dierent margin for the negative and positive class is given
in dos Santos et al 2015
5 Word Embeddings
A main component of the neural-network approach is the use of embeddings  representing
each feature as a vector in a low dimensional space But where do the vectors come from
This section will survey the common approaches
51 Random Initialization
When enough supervised training data is available one can just treat the feature embeddings
the same as the other model parameters initialize the embedding vectors to random values
and let the network-training procedure tune them into good vectors
Some care has to be taken in the way the random initialization is performed The method
used by the eective word2vec implementation Mikolov et al 2013 Mikolov Sutskever
Chen Corrado  Dean 2013 is to initialize the word vectors to uniformly sampled random
cid104
numbers in the range  1
2d  where d is the number of dimensions Another option is to
use xavier initialization see Section 63 and initialize with uniformly sampled values from
6d
6d
cid105
In practice one will often use the random initialization approach to initialize the em-
bedding vectors of commonly occurring features such as part-of-speech tags or individual
letters while using some form of supervised or unsupervised pre-training to initialize the
potentially rare features such as features for individual words The pre-trained vectors can
then either be treated as xed during the network training process or more commonly
treated like the randomly-initialized vectors and further tuned to the task at hand
52 Supervised Task-specic Pre-training
If we are interested in task A for which we only have a limited amount of labeled data for
example syntactic parsing but there is an auxiliary task B say part-of-speech tagging
for which we have much more labeled data we may want to pre-train our word vectors so
that they perform well as predictors for task B and then use the trained vectors for training
task A In this way we can utilize the larger amounts of labeled data we have for task B
When training task A we can either treat the pre-trained vectors as xed or tune them
further for task A Another option is to train jointly for both objectives see Section 7 for
more details
53 Unsupervised Pre-training
The common case is that we do not have an auxiliary task with large enough amounts of
annotated data or maybe we want to help bootstrap the auxiliary task training with better
vectors In such cases we resort to unsupervised methods which can be trained on huge
amounts of unannotated text
The techniques for training the word vectors are essentially those of supervised learning
but instead of supervision for the task that we care about we instead create practically
unlimited number of supervised training instances from raw text hoping that the tasks
that we created will match or be close enough to the nal task we care about12
The key idea behind the unsupervised approaches is that one would like the embedding
vectors of similar words to have similar vectors While word similarity is hard to dene
and is usually very task-dependent the current approaches derive from the distributional
hypothesis Harris 1954 stating that words are similar if they appear in similar contexts
The dierent methods all create supervised training instances in which the goal is to either
predict the word from its context or predict the context from the word
An important benet of training word embeddings on large amounts of unannotated
data is that it provides vector representations for words that do not appear in the super-
vised training set Ideally the representations for these words will be similar to those of
related words that do appear in the training set allowing the model to generalize better on
unseen events It is thus desired that the similarity between word vectors learned by the un-
supervised algorithm captures the same aspects of similarity that are useful for performing
the intended task of the network
Common unsupervised word-embedding algorithms include word2vec 13 Mikolov et al
2013 2013 GloVe Pennington Socher  Manning 2014 and the Collobert and Weston
2008 2011 embeddings algorithm These models are inspired by neural networks and
are based on stochastic gradient training However they are deeply connected to another
family of algorithms which evolved in the NLP and IR communities and that are based on
matrix factorization see Levy  Goldberg 2014b Levy et al 2015 for a discussion
Arguably the choice of auxiliary problem what is being predicted based on what kind
of context aects the resulting vectors much more than the learning method that is being
used to train them We thus focus on the dierent choices of auxiliary problems that are
available and only skim over the details of the training methods Several software packages
for deriving word vectors are available including word2vec14 and Gensim15 implementing
the word2vec models with word-windows based contexts word2vecf16 which is a modied
version of word2vec allowing the use of arbitrary contexts and GloVe17 implementing the
GloVe model Many pre-trained word vectors are also available for download on the web
While beyond the scope of this tutorial it is worth noting that the word embeddings
derived by unsupervised training algorithms have a wide range of applications in NLP
beyond using them for initializing the word-embeddings layer of a neural-network model
54 Training Objectives
Given a word w and its context c dierent algorithms formulate dierent auxiliary tasks
In all cases each word is represented as a d-dimensional vector which is initialized to a
random value Training the model to perform the auxiliary tasks well will result in good
12 The interpretation of creating auxiliary problems from raw text is inspired by Ando and Zhang Ando
 Zhang 2005a 2005b
13 While often treated as a single algorithm word2vec is actually a software package including various
training objectives optimization methods and other hyperparameters See Rong 2014 Levy Goldberg
 Dagan 2015 for a discussion
14 httpscodegooglecompword2vec
15 httpsradimrehurekcomgensim
16 httpsbitbucketorgyoavgoword2vecf
17 httpnlpstanfordeduprojectsglove
word embeddings for relating the words to the contexts which in turn will result in the
embedding vectors for similar words to be similar to each other
Language-modeling inspired approaches such as those taken by Mikolov et al 2013
Mnih  Kavukcuoglu 2013 as well as GloVe Pennington et al 2014 use auxiliary tasks
in which the goal is to predict the word given its context This is posed in a probabilistic
setup trying to model the conditional probability P wc
Other approaches reduce the problem to that of binary classication In addition to
the set D of observed word-context pairs a set D is created from random words and
context pairings The binary classication problem is then does the given w c pair
come from D or not The approaches dier in how the set D is constructed what is
the structure of the classier and what is the objective being optimized Collobert and
Weston 2008 2011 take a margin-based binary ranking approach training a feed-forward
neural network to score correct w c pairs over incorrect ones Mikolov et al 2013 2014
take instead a probabilistic version training a log-bilinear model to predict the probability
P w c  Dw c that the pair come from the corpus rather than the random sample
55 The Choice of Contexts
In most cases the contexts of a word are taken to be other words that appear in its
surrounding either in a short window around it or within the same sentence paragraph
or document In some cases the text is automatically parsed by a syntactic parser and
the contexts are derived from the syntactic neighbourhood induced by the automatic parse
trees Sometimes the denitions of words and context change to include also parts of words
such as prexes or suxes
Neural word embeddings originated from the world of language modeling in which a
network is trained to predict the next word based on a sequence of preceding words Bengio
et al 2003 There the text is used to create auxiliary tasks in which the aim is to predict
a word based on a context the k previous words While training for the language modeling
auxiliary prediction problems indeed produce useful embeddings this approach is needlessly
restricted by the constraints of the language modeling task in which one is allowed to look
only at the previous words If we do not care about language modeling but only about the
resulting embeddings we may do better by ignoring this constraint and taking the context
to be a symmetric window around the focus word
551 Window Approach
The most common approach is a sliding window approach in which auxiliary tasks are
created by looking at a sequence of 2k  1 words The middle word is callled the focus word
and the k words to each side are the contexts Then either a single task is created in which
the goal is to predict the focus word based on all of the context words represented either
using CBOW Mikolov et al 2013 or vector concatenation Collobert  Weston 2008
or 2k distinct tasks are created each pairing the focus word with a dierent context word
The 2k tasks approach popularized by Mikolov et al 2013 is referred to as a skip-gram
model Skip-gram based approaches are shown to be robust and ecient to train Mikolov
et al 2013 Pennington et al 2014 and often produce state of the art results
Eect of Window Size The size of the sliding window has a strong eect on the re-
sulting vector similarities Larger windows tend to produce more topical similarities ie
dog bark and leash will be grouped together as well as walked run and walk-
ing while smaller windows tend to produce more functional and syntactic similarities ie
Poodle Pitbull Rottweiler or walkingrunningapproaching
Positional Windows When using the CBOW or skip-gram context representations all
the dierent context words within the window are treated equally There is no distinction
between context words that are close to the focus words and those that are farther from
it and likewise there is no distinction between context words that appear before the focus
words to context words that appear after it Such information can easily be factored in by
using positional contexts indicating for each context word also its relative position to the
focus words ie instead of the context word being the it becomes the2 indicating
the word appears two positions to the right of the focus word The use of positional context
together with smaller windows tend to produce similarities that are more syntactic with
a strong tendency of grouping together words that share a part of speech as well as being
functionally similar in terms of their semantics Positional vectors were shown by Ling
Dyer Black  Trancoso 2015a to be more eective than window-based vectors when used
to initialize networks for part-of-speech tagging and syntactic dependency parsing
Variants Many variants on the window approach are possible One may lemmatize words
before learning apply text normalization lter too short or too long sentences or remove
capitalization see eg the pre-processing steps described in dos Santos  Gatti 2014
One may sub-sample part of the corpus skipping with some probability the creation of tasks
from windows that have too common or too rare focus words The window size may be
dynamic using a dierent window size at each turn One may weigh the dierent positions
in the window dierently focusing more on trying to predict correctly close word-context
pairs than further away ones Each of these choices will eect the resulting vectors Some
of these hyperparameters and others are discussed in Levy et al 2015
552 Sentences Paragraphs or Documents
Using a skip-grams or CBOW approach one can consider the contexts of a word to be all
the other words that appear with it in the same sentence paragraph or document This is
equivalent to using very large window sizes and is expected to result in word vectors that
capture topical similarity words from the same topic ie words that one would expect to
appear in the same document are likely to receive similar vectors
553 Syntactic Window
Some work replace the linear context within a sentence with a syntactic one Levy 
Goldberg 2014a Bansal Gimpel  Livescu 2014 The text is automatically parsed
using a dependency parser and the context of a word is taken to be the words that are
in its proximity in the parse tree together with the syntactic relation by which they are
connected Such approaches produce highly functional similarities grouping together words
than can ll the same role in a sentence eg colors names of schools verbs of movement
The grouping is also syntactic grouping together words that share an inection Levy 
Goldberg 2014a
554 Multilingual
Another option is using multilingual translation based contexts Hermann  Blunsom
2014 Faruqui  Dyer 2014 For example given a large amount of sentence-aligned parallel
text one can run a bilingual alignment model such as the IBM model 1 or model 2 ie
using the GIZA software and then use the produced alignments to derive word contexts
Here the context of a word instance are the foreign language words that are aligned to it
Such alignments tend to result in synonym words receiving similar vectors Some authors
work instead on the sentence alignment level without relying on word alignments An
appealing method is to mix a monolingual window-based approach with a multilingual
approach creating both kinds of auxiliary tasks This is likely to produce vectors that are
similar to the window-based approach but reducing the somewhat undesired eect of the
window-based approach in which antonyms eg hot and cold high and low tend to receive
similar vectors Faruqui  Dyer 2014
555 Character-based and Sub-word Representations
An interesting line of work attempts to derive the vector representation of a word from the
characters that compose it Such approaches are likely to be particularly useful for tasks
which are syntactic in nature as the character patterns within words are strongly related
to their syntactic function These approaches also have the benet of producing very small
model sizes only one vector for each character in the alphabet together with a handful of
small matrices needs to be stored and being able to provide an embedding vector for every
word that may be encountered dos Santos and Gatti 2014 and dos Santos and Zadrozny
2014 model the embedding of a word using a convolutional network see Section 9 over
the characters Ling et al 2015b model the embedding of a word using the concatenation
of the nal states of two RNN LSTM encoders Section 10 one reading the characters
from left to right and the other from right to left Both produce very strong results for
part-of-speech tagging The work of Ballesteros et al 2015 show that the two-LSTMs
encoding of Ling et al 2015b is benecial also for representing words in dependency
parsing of morphologically rich languages
Deriving representations of words from the representations of their characters is moti-
vated by the unknown words problem  what do you do when you encounter a word for
which you do not have an embedding vector Working on the level of characters alleviates
this problem to a large extent as the vocabulary of possible characters is much smaller
than the vocabulary of possible words However working on the character level is very
challenging as the relationship between form characters and function syntax semantics
in language is quite loose Restricting oneself to stay on the character level may be an
unnecessarily hard constraint Some researchers propose a middle-ground in which a word
is represented as a combination of a vector for the word itself with vectors of sub-word
units that comprise it The sub-word embeddings then help in sharing information between
dierent words with similar forms as well as allowing back-o to the subword level when
the word is not observed At the same time the models are not forced to rely solely on
form when enough observations of the word are available Botha and Blunsom 2014 sug-
gest to model the embedding vector of a word as a sum of the word-specic vector if such
vector is available with vectors for the dierent morphological components that comprise
it the components are derived using Morfessor Creutz  Lagus 2007 an unsupervised
morphological segmentation method Gao et al Gao et al 2014 suggest using as core
features not only the word form itself but also a unique feature hence a unique embedding
vector for each of the letter-trigrams in the word
6 Neural Network Training
Neural network training is done by trying to minimize a loss function over a training set
using a gradient-based method Roughly speaking all training methods work by repeatedly
computing an estimate of the error over the dataset computing the gradient with respect
to the error and then moving the parameters in the direction of the gradient Models dier
in how the error estimate is computed and how moving in the direction of the gradient
is dened We describe the basic algorithm stochastic gradient descent SGD and then
briey mention the other approaches with pointers for further reading Gradient calculation
is central to the approach Gradients can be eciently and automatically computed using
reverse mode dierentiation on a computation graph  a general algorithmic framework for
automatically computing the gradient of any network and loss function
61 Stochastic Gradient Training
The common approach for training neural networks is using the stochastic gradient descent
SGD algorithm Bottou 2012 LeCun Bottou Orr  Muller 1998a or a variant of it
SGD is a general optimization algorithm It receives a function f parameterized by  a
loss function and desired input and output pairs It then attempts to set the parameters 
such that the loss of f with respect to the training examples is small The algorithm works
as follows
Algorithm 1 Online Stochastic Gradient Descent Training
1 Input Function f x  parameterized with parameters 
2 Input Training set of inputs x1     xn and outputs y1     yn
3 Input Loss function L
4 while stopping criteria not met do
Sample a training example xi yi
Compute the loss Lf xi  yi
g  gradients of Lf xi  yi wrt 
    kg
9 return 
cid80n
The goal of the algorithm is to set the parameters  so as to minimize the total loss
i1 Lf xi  yi over the training set It works by repeatedly sampling a training exam-
ple and computing the gradient of the error on the example with respect to the parameters
 line 7  the input and expected output are assumed to be xed and the loss is treated
as a function of the parameters  The parameters  are then updated in the direction of
the gradient scaled by a learning rate k line 8 For further discussion on setting the
learning rate see Section 63
Note that the error calculated in line 6 is based on a single training example and is thus
just a rough estimate of the corpus-wide loss that we are aiming to minimize The noise in
the loss computation may result in inaccurate gradients A common way of reducing this
noise is to estimate the error and the gradients based on a sample of m examples This
gives rise to the minibatch SGD algorithm
Algorithm 2 Minibatch Stochastic Gradient Descent Training
1 Input Function f x  parameterized with parameters 
2 Input Training set of inputs x1     xn and outputs y1     yn
3 Input Loss function L
4 while stopping criteria not met do
Sample a minibatch of m examples x1 y1     xm ym
g  0
for i  1 to m do
Compute the loss Lf xi  yi
g  g  gradients of 1
m Lf xi  yi wrt 
11 return 
    kg
In lines 6  9 the algorithm estimates the gradient of the corpus loss based on the
minibatch After the loop g contains the gradient estimate and the parameters  are
updated toward g The minibatch size can vary in size from m  1 to m  n Higher
values provide better estimates of the corpus-wide gradients while smaller values allow
more updates and in turn faster convergence Besides the improved accuracy of the gradients
estimation the minibatch algorithm provides opportunities for improved training eciency
For modest sizes of m some computing architectures ie GPUs allow an ecient parallel
implementation of the computation in lines 69 With a small enough learning rate SGD is
guaranteed to converge to a global optimum if the function is convex However it can also
be used to optimize non-convex functions such as neural-network While there are no longer
guarantees of nding a global optimum the algorithm proved to be robust and performs
well in practice
When training a neural network the parameterized function f is the neural network
and the parameters  are the layer-transfer matrices bias terms embedding matrices and
so on The gradient computation is a key step in the SGD algorithm as well as in all
other neural network training algorithms The question is then how to compute the
gradients of the networks error with respect to the parameters Fortunately there is an easy
solution in the form of the backpropagation algorithm Rumelhart Hinton  Williams 1986
Lecun Bottou Bengio  Haner 1998b The backpropagation algorithm is a fancy name
for methodologically computing the derivatives of a complex expression using the chain-
rule while caching intermediary results More generally the backpropagation algorithm
is a special case of the reverse-mode automatic dierentiation algorithm Neidinger 2010
Section 7 Baydin Pearlmutter Radul  Siskind 2015 Bengio 2012The following
section describes reverse mode automatic dierentiation in the context of the computation
graph abstraction
Beyond SGD While the SGD algorithm can and often does produce good results more
advanced algorithms are also available The SGDMomentum Polyak 1964 and Nesterov
Momentum Sutskever Martens Dahl  Hinton 2013 algorithms are variants of SGD in
which previous gradients are accumulated and aect the current update Adaptive learning
rate algorithms including AdaGrad Duchi Hazan  Singer 2011 AdaDelta Zeiler 2012
RMSProp Tieleman  Hinton 2012 and Adam Kingma  Ba 2014 are designed to
select the learning rate for each minibatch sometimes on a per-coordinate basis potentially
alleviating the need of ddling with learning rate scheduling For details of these algorithms
see the original papers or Bengio et al 2015 Sections 83 84 As many neural-network
software frameworks provide implementations of these algorithms it is easy and sometimes
worthwhile to try out dierent variants
62 The Computation Graph Abstraction
While one can compute the gradients of the various parameters of a network by hand and
implement them in code this procedure is cumbersome and error prone For most pur-
poses it is preferable to use automatic tools for gradient computation Bengio 2012 The
computation-graph abstraction allows us to easily construct arbitrary networks evaluate
their predictions for given inputs forward pass and compute gradients for their parameters
with respect to arbitrary scalar losses backward pass
A computation graph is a representation of an arbitrary mathematical computation as
a graph It is a directed acyclic graph DAG in which nodes correspond to mathematical
operations or bound variables and edges correspond to the ow of intermediary values
between the nodes The graph structure denes the order of the computation in terms of
the dependencies between the dierent components The graph is a DAG and not a tree as
the result of one operation can be the input of several continuations Consider for example
a graph for the computation of a  b  1  a  b  2
The computation of a b is shared We restrict ourselves to the case where the computation
graph is connected
Since a neural network is essentially a mathematical expression it can be represented
as a computation graph
For example Figure 3a presents the computation graph for a 1-layer MLP with a soft-
max output transformation In our notation oval nodes represent mathematical operations
or functions and shaded rectangle nodes represent parameters bound variables Network
inputs are treated as constants and drawn without a surrounding node Input and param-
eter nodes have no incoming arcs and output nodes have no outgoing arcs The output of
each node is a matrix the dimensionality of which is indicated above the node
This graph is incomplete without specifying the inputs we cannot compute an output
Figure 3b shows a complete graph for an MLP that takes three words as inputs and predicts
the distribution over part-of-speech tags for the third word This graph can be used for
prediction but not for training as the output is a vector not a scalar and the graph does
not take into account the correct answer or the loss term Finally the graph in 3c shows the
computation graph for a specic training example in which the inputs are the embeddings
ab12Figure 3 Computation Graph for MLP1 a Graph with unbound input b Graph
with concrete input c Graph with concrete input expected output and loss
of the words the black dog and the expected output is NOUN whose index is
Once the graph is built it is straightforward to run either a forward computation com-
pute the result of the computation or a backward computation computing the gradients
as we show below Constructing the graphs may look daunting but is actually very easy
using dedicated software libraries and APIs
Forward Computation The forward pass computes the outputs of the nodes in the
graph Since each nodes output depends only on itself and on its incoming edges it is
trivial to compute the outputs of all nodes by traversing the nodes in a topological order and
computing the output of each node given the already computed outputs of its predecessors
More formally in a graph of N nodes we associate each node with an index i according
to their topological ordering Let fi be the function computed by node i eg multiplication
addition     Let i be the parent nodes of node i and 1i  j  i  j the
children nodes of node i these are the arguments of fi Denote by vi the output of node
x1150W115020MUL120ADD120b1120tanh120W22017b2117MUL117ADD117softmax117aconcat1150lookup150lookup150lookup150theblackdogEV50W115020MUL120ADD120b1120tanh120W22017b2117MUL117ADD117softmax117bconcat1150lookup150lookup150lookup150theblackdogEV50W115020MUL120ADD120b1120tanh120W22017b2117MUL117ADD117softmax117pick115log11neg11ci that is the application of fi to the output values of its arguments 1i For variable
and input nodes fi is a constant function and 1i is empty The Forward algorithm
computes the values vi for all i  1 N 
Algorithm 3 Computation Graph Forward Pass
1 for i  1 to N do
Let a1     am  1i
vi  fiva1     vam
Backward Computation Derivatives Backprop The backward pass begins by des-
ignating a node N with scalar 1 1 output as a loss-node and running forward computa-
tion up to that node The backward computation will computes the gradients with respect
 The backpropagation algorithm is
to that nodes value Denote by di the quantity
used to compute the values di for all nodes i
The backward pass lls a table di as follows
Algorithm 4 Computation Graph Backward Pass Backpropagation
1 dN   1
2 for i  N-1 to 1 do
cid80
di 
ji dj 
is the partial derivative of fj1j wrt the argument i  1j
The quantity
This value depends on the function fj and the values va1     vam where a1     am 
1j of its arguments which were computed in the forward pass
Thus in order to dene a new kind of node one need to dene two methods one for
calculating the forward value vi based on the nodes inputs and the another for calculating
for each x  1i
For further information on automatic dierentiation see Neidinger 2010 Section 7
Baydin et al 2015 For more in depth discussion of the backpropagation algorithm and
computation graphs also called ow graphs see Bengio et al 2015 Section 64 Lecun
et al 1998b Bengio 2012 For a popular yet technical presentation see Chris Olahs
description at httpcolahgithubioposts2015-08-Backprop
Software Several software packages implement the computation-graph model including
Theano18 Chainer19 penne20 and CNNpyCNN21 All these packages support all the es-
sential components node types for dening a wide range of neural network architectures
covering the structures described in this tutorial and more Graph creation is made almost
transparent by use of operator overloading The framework denes a type for representing
graph nodes commonly called expressions methods for constructing nodes for inputs and
18 httpdeeplearningnetsoftwaretheano
19 httpchainerorg
20 httpsbitbucketorgndnlppenne
21 httpsgithubcomclabcnn
parameters and a set of functions and mathematical operations that take expressions as
input and result in more complex expressions For example the python code for creating
the computation graph from Figure 3c using the pyCNN framework is
from pycnn import 
 model initialization
model  Model
modeladdparametersW1 20150
modeladdparametersb1 20
modeladdparametersW2 1720
modeladdparametersb2 17
modeladdlookupparameterswords 100 50
 Building the computation graph
renewcg  create a new graph
 Wrap the model parameters as graph-nodes
W1  parametermodelW1
b1  parametermodelb1
W2  parametermodelW2
b2  parametermodelb2
def getindexx return 1
 Generate the embeddings layer
vblack  lookupmodelwords getindexblack
 lookupmodelwords getindexdog
 lookupmodelwords getindexthe
 Connect the leaf nodes into a complete graph
x  concatenatevthe vblack vdog
output  softmaxW2tanhW1xb1b2
loss  -logpickoutput 5
lossvalue  lossforward
lossbackward  the gradient is computed
 and stored in the corresponding
 parameters
Most of the code involves various initializations the rst block denes model parameters
that are be shared between dierent computation graphs recall that each graph corresponds
to a specic training example The second block turns the model parameters into the graph-
node Expression types The third block retrieves the Expressions for the embeddings of the
input words Finally the fourth block is where the graph is created Note how transparent
the graph creation is  there is an almost a one-to-one correspondence between creating
the graph and describing it mathematically The last block shows a forward and backward
pass The other software frameworks follow similar patterns
Theano involves an optimizing compiler for computation graphs which is both a blessing
and a curse On the one hand once compiled large graphs can be run eciently on either
the CPU or a GPU making it ideal for large graphs with a xed structure where only the
inputs change between instances However the compilation step itself can be costly and it
makes the interface a bit cumbersome to work with In contrast the other packages focus on
building large and dynamic computation graphs and executing them on the y without a
compilation step While the execution speed may suer with respect to Theanos optimized
version these packages are especially convenient when working with the recurrent and
recursive networks described in Sections 10 12 as well as in structured prediction settings
as described in Section 8
Implementation Recipe Using the computation graph abstraction the pseudo-code for
a network training algorithm is given in Algorithm 5
Algorithm 5 Neural Network Training with Computation Graph Abstraction using mini-
batches of size 1
1 Dene network parameters
2 for iteration  1 to N do
for Training example xi yi in dataset do
loss node  build computation graphxi yi parameters
loss nodeforward
gradients  loss nodebackward
parameters  update parametersparameters gradients
8 return parameters
Here build computation graph is a user-dened function that builds the computation
graph for the given input output and network structure returning a single loss node
update parameters is an optimizer specic update rule The recipe species that a new
graph is created for each training example This accommodates cases in which the network
structure varies between training example such as recurrent and recursive neural networks
to be discussed in Sections 10  12 For networks with xed structures such as an MLPs it
may be more ecient to create one base computation graph and vary only the inputs and
expected outputs between examples
Network Composition As long as the networks output is a vector 1  k matrix it
is trivial to compose networks by making the output of one network the input of another
creating arbitrary networks The computation graph abstractions makes this ability explicit
a node in the computation graph can itself be a computation graph with a designated output
node One can then design arbitrarily deep and complex networks and be able to easily
evaluate and train them thanks to automatic forward and gradient computation This makes
it easy to dene and train networks for structured outputs and multi-objective training as
we discuss in Section 7 as well as complex recurrent and recursive networks as discussed
in Sections 1012
63 Optimization Issues
Once the gradient computation is taken care of the network is trained using SGD or another
gradient-based optimization algorithm The function being optimized is not convex and for
a long time training of neural networks was considered a black art which can only be done
by selected few Indeed many parameters aect the optimization process and care has to
be taken to tune these parameters While this tutorial is not intended as a comprehensive
guide to successfully training neural networks we do list here a few of the prominent issues
For further discussion on optimization techniques and algorithms for neural networks refer
to Bengio et al 2015 Chapter 8 For some theoretical discussion and analysis refer
to Glorot  Bengio 2010 For various practical tips and recommendations see LeCun
et al 1998a Bottou 2012
Initialization The non-convexity of the loss function means the optimization procedure
may get stuck in a local minimum or a saddle point and that starting from dierent initial
points eg dierent random values for the parameters may result in dierent results Thus
it is advised to run several restarts of the training starting at dierent random initializations
and choosing the best one based on a development set22 The amount of variance in the
results is dierent for dierent network formulations and datasets and cannot be predicted
in advance
The magnitude of the random values has an important eect on the success of training
An eective scheme due to Glorot and Bengio 2010 called xavier initialization after
Glorots rst name suggests initializing a weight matrix W  Rdindout as
cid34
W  U
din  dout
din  dout
cid35
where U a b is a uniformly sampled random value in the range a b This advice works
well on many occasions and is the preferred default initialization method by many
Analysis by He et al 2015 suggests that when using ReLU non-linearities the weights
should be initialized by sampling from a zero-mean Gaussian distribution whose standard
deviation is
 This initialization was found by He et al to work better than xavier
initialization in an image classication task especially when deep networks were involved
cid113 2
Vanishing and Exploding Gradients
In deep networks it is common for the error
gradients to either vanish become exceedingly close to 0 or explode become exceedingly
high as they propagate back through the computation graph The problem becomes more
severe in deeper networks and especially so in recursive and recurrent networks Pascanu
Mikolov  Bengio 2012 Dealing with the vanishing gradients problem is still an open
research question Solutions include making the networks shallower step-wise training rst
train the rst layers based on some auxiliary output signal then x them and train the upper
layers of the complete network based on the real task signal or specialized architectures
that are designed to assist in gradient ow eg the LSTM and GRU architectures for
recurrent networks discussed in Section 11 Dealing with the exploding gradients has
a simple but very eective solution clipping the gradients if their norm exceeds a given
threshold Let g be the gradients of all parameters in the network and cid107gcid107 be their L2
norm Pascanu et al 2012 suggest to set g  threshold
Saturation and Dead Neurons Layers with tanh and sigmoid activations can become
saturated  resulting in output values for that layer that are all close to one the upper-
limit of the activation function Saturated neurons have very small gradients and should
be avoided Layers with the ReLU activation cannot be saturated but can die  most
or all values are negative and thus clipped at zero for all inputs resulting in a gradient
of zero for that layer If your network does not train well it is advisable to monitor the
network for saturated or dead layers Saturated neurons are caused by too large values
g if cid107gcid107  threshold
cid107gcid107
22 When debugging and for reproducibility of results it is advised to used a xed random seed
entering the layer This may be controlled for by changing the initialization scaling the
range of the input values or changing the learning rate Dead neurons are caused by all
weights entering the layer being negative for example this can happen after a large gradient
update Reducing the learning rate will help in this situation For saturated layers another
option is to normalize the values in the saturated layer after the activation ie instead of
gh  tanhh using gh  tanhh
 Layer normalization is an eective measure for
cid107tanhhcid107
countering saturation but is also expensive in terms of gradient computation
Shuing The order in which the training examples are presented to the network is im-
portant The SGD formulation above species selecting a random example in each turn
In practice most implementations go over the training example in order It is advised to
shue the training examples before each pass through the data
Learning Rate Selection of the learning rate is important Too large learning rates
will prevent the network from converging on an eective solution Too small learning
rates will take very long time to converge As a rule of thumb one should experiment
with a range of initial learning rates in range 0 1 eg 0001 001 01 1 Monitor
the networks loss over time and decrease the learning rate once the network seem to be
stuck in a xed region Learning rate scheduling decrease the rate as a function of the
number of observed minibatches A common schedule is dividing the initial learning rate
by the iteration number Leon Bottou 2012 recommends using a learning rate of the form
t  01  0t1 where 0 is the initial learning rate t is the learning rate to use on
the tth training example and  is an additional hyperparameter He further recommends
determining a good value of 0 based on a small sample of the data prior to running on the
entire dataset
Minibatches Parameter updates occur either every training example minibatches of size
1 or every k training examples Some problems benet from training with larger minibatch
sizes In terms of the computation graph abstraction one can create a computation graph
for each of the k training examples and then connecting the k loss nodes under an averaging
node whose output will be the loss of the minibatch Large minibatched training can also
be benecial in terms of computation eciency on specialized computing architectures such
as GPUs This is beyond the scope of this tutorial
64 Regularization
Neural network models have many parameters and overtting can easily occur Overtting
can be alleviated to some extent by regularization A common regularization method is
L2 regularization placing a squared penalty on parameters with large values by adding
an additive 
2cid107cid1072 term to the objective function to be minimized where  is the set of
model parameters cid107  cid1072 is the squared L2 norm sum of squares of the values and  is a
hyperparameter controlling the amount of regularization
A recently proposed alternative regularization method is dropout Hinton Srivastava
Krizhevsky Sutskever  Salakhutdinov 2012 The dropout method is designed to prevent
the network from learning to rely on specic weights It works by randomly dropping set-
ting to 0 half of the neurons in the network or in a specic layer in each training example
Work by Wager et al 2013 establishes a strong connection between the dropout method
and L2 regularization Gal and Gharamani 2015 show that a multi-layer perceptron with
dropout applied at every layer can be interpreted as Bayesian model averaging
The dropout technique is one of the key factors contributing to very strong results of
neural-network methods on image classication tasks Krizhevsky Sutskever  Hinton
2012 especially when combined with ReLU activation units Dahl Sainath  Hinton
2013 The dropout technique is eective also in NLP applications of neural networks
7 Cascading and Multi-task Learning
The combination of online training methods with automatic gradient computations using
the computation graph abstraction allows for an easy implementation of model cascading
parameter sharing and multi-task learning
Model cascading is a powerful technique in which large networks are built by composing
them out of smaller component networks For example we may have a feed-forward network
for predicting the part of speech of a word based on its neighbouring words andor the
characters that compose it In a pipeline approach we would use this network for predicting
parts of speech and then feed the predictions as input features to neural network that does
syntactic chunking or parsing Instead we could think of the hidden layers of this network
as an encoding that captures the relevant information for predicting the part of speech In
a cascading approach we take the hidden layers of this network and connect them and not
the part of speech prediction themselves as the inputs for the syntactic network We now
have a larger network that takes as input sequences of words and characters and outputs a
syntactic structure The computation graph abstraction allows us to easily propagate the
error gradients from the syntactic task loss all the way back to the characters
To combat the vanishing gradient problem of deep networks as well as to make better
use of available training material the individual component networks parameters can be
bootstrapped by training them separately on a relevant task before plugging them in to
the larger network for further tuning For example the part-of-speech predicting network
can be trained to accurately predict parts-of-speech on a relatively large annotated corpus
before plugging its hidden layer into the syntactic parsing network for which less training
data is available In case the training data provide direct supervision for both tasks we can
make use of it during training by creating a network with two outputs one for each task
computing a separate loss for each output and then summing the losses into a single node
from which we backpropagate the error gradients
Model cascading is very common when using convolutional recursive and recurrent
neural networks where for example a recurrent network is used to encode a sentence into
a xed sized vector which is then used as the input of another network The supervision
signal of the recurrent network comes primarily from the upper network that consumes the
recurrent networks output as it inputs
Multi-task learning is used when we have related prediction tasks that do not neces-
sarily feed into one another but we do believe that information that is useful for one type
of prediction can be useful also to some of the other tasks For example chunking named
entity recognition NER and language modeling are examples of synergistic tasks Infor-
mation for predicting chunk boundaries named-entity boundaries and the next word in the
sentence all rely on some shared underlying syntactic-semantic representation Instead of
training a separate network for each task we can create a single network with several out-
puts A common approach is to have a multi-layer feed-forward network whose nal hidden
layer or a concatenation of all hidden layers is then passed to dierent output layers This
way most of the parameters of the network are shared between the dierent tasks Useful
information learned from one task can then help to disambiguate other tasks Again the
computation graph abstraction makes it very easy to construct such networks and compute
the gradients for them by computing a separate loss for each available supervision signal
and then summing the losses into a single loss that is used for computing the gradients In
case we have several corpora each with dierent kind of supervision signal eg we have
one corpus for NER and another for chunking the training procedure will shue all of the
available training example performing gradient computation and updates with respect to
a dierent loss in every turn Multi-task learning in the context of language-processing is
introduced and discussed in Collobert et al 2011
8 Structured Output Prediction
Many problems in NLP involve structured outputs cases where the desired output is not
a class label or distribution over class labels but a structured object such as a sequence
a tree or a graph Canonical examples are sequence tagging eg part-of-speech tagging
sequence segmentation chunking NER and syntactic parsing In this section we discuss
how feed-forward neural network models can be used for structured tasks In later sections
we discuss specialized neural network models for dealing with sequences Section 10 and
trees Section 12
81 Greedy Structured Prediction
The greedy approach to structured prediction is to decompose the structure prediction
problem into a sequence of local prediction problems and training a classier to perform
each local decision At test time the trained classier is used in a greedy manner Examples
of this approach are left-to-right tagging models Gimenez  Marquez 2004 and greedy
transition-based parsing Nivre 2008 Such approaches are easily adapted to use neural
networks by simply replacing the local classier from a linear classier such as an SVM or a
logistic regression model to a neural network as demonstrated in Chen  Manning 2014
Lewis  Steedman 2014
The greedy approaches suer from error propagation where mistakes in early decisions
carry over and inuence later decisions The overall higher accuracy achievable with non-
linear neural network classiers helps in osetting this problem to some extent In addition
training techniques were proposed for mitigating the error propagation problem by either
attempting to take easier predictions before harder ones the easy-rst approach Goldberg
 Elhadad 2010 or making training conditions more similar to testing conditions by
exposing the training procedure to inputs that result from likely mistakes Hal Daume III
Langford  Marcu 2009 Goldberg  Nivre 2013 These are eective also for training
greedy neural network models as demonstrated by Ma et al Ma Zhang  Zhu 2014
easy-rst tagger and  dynamic oracle training for greedy dependency parsing
82 Search Based Structured Prediction
The common approach to predicting natural language structures is search based For in-
depth discussion of search-based structure prediction in NLP see the book by Smith Smith
2011 The techniques can easily be adapted to use a neural-network In the neural-networks
literature such models were discussed under the framework of energy based learning LeCun
et al 2006 Section 7 They are presented here using setup and terminology familiar to
the NLP community
Search-based structured prediction is formulated as a search problem over possible struc-
predictx  arg max
yYx
scorex y
where x is an input structure y is an output over x in a typical example x is a sentence
and y is a tag-assignment or a parse-tree over the sentence Yx is the set of all valid
structures over x and we are looking for an output y that will maximize the score of the
x y pair
The scoring function is dened as a linear model
where  is a feature extraction function and w is a weight vector
scorex y  x y  w
In order to make the search for the optimal y tractable the structure y is decomposed
into parts and the feature function is dened in terms of the parts where p is a part-local
feature extraction function
cid88
x y 
ppartsxy
Each part is scored separately and the structure score is the sum of the component
parts scores
scorex y w  x y  w 
cid88
p 
cid88
w  p 
cid88
scorep
where p  y is a shorthand for p  partsx y The decomposition of y into parts is such
that there exists an inference algorithm that allows for ecient search for the best scoring
structure given the scores of the individual parts
One can now trivially replace the linear scoring function over parts with a neural-
network
scorex y 
cid88
where cp maps the part p into a din dimensional vector
In case of a one hidden-layer feed-forward network
scorep 
N N cp
cid88
cid88
cid88
scorex y 
N NM LP 1cp 
gcpW1  b1w
cp  Rdin W1  Rdind1 b1  Rd1 w  Rd1 A common objective in structured
prediction is making the gold structure y score higher than any other structure ycid48 leading
to the following generalized perceptron loss
ycid48 scorex ycid48  scorex y
In terms of implementation this means create a computation graph CGp for each of
the possible parts and calculate its score Then run inference over the scored parts to
nd the best scoring structure ycid48 Connect the output nodes of the computation graphs
corresponding to parts in the gold predicted structure y ycid48 into a summing node CGy
CGcid48y Connect CGy and CGcid48y using a minus node CGl and compute the gradients
As argued in LeCun et al 2006 Section 5 the generalized perceptron loss may not
be a good loss function when training structured prediction neural networks as it does not
have a margin and a margin-based hinge loss is preferred
max0 m  scorex y  max
cid54y
ycid48
scorex ycid48
It is trivial to modify the implementation above to work with the hinge loss
Note that in both cases we lose the nice properties of the linear model In particular the
model is no longer convex This is to be expected as even the simplest non-linear neural
network is already non-convex Nonetheless we could still use standard neural-network
optimization techniques to train the structured model
Training and inference is slower as we have to evaluate the neural network and take
gradients partsx y times
Structured prediction is a vast eld and is beyond the scope of this tutorial but loss func-
tions regularizers and methods described in eg Smith 2011 such as cost-augmented
decoding can be easily applied or adapted to the neural-network framework23
Probabilistic objective CRF
In a probabilistic framework CRF we treat each
of the parts scores as a clique potential see Smith 2011 and dene the score of each
structure y to be
cid80
cid80
py eN N cp
rameters of the network such that corpus conditional log likelihoodcid80
cid80
cid80
py escorep
The scoring function denes a conditional distribution P yx and we wish to set the pa-
scoreCRF x y  P yx 
ycid48
Yx
pycid48 eN N cp
ycid48
Yx
pycid48 escorep
xiyitraining log P yixi
cid80
cid80
is maximized
The loss for a given training example x y is then  log scoreCRF x y Taking the
gradient with respect to the loss is as involved as building the associated computation
graph The tricky part is the denominator the partition function which requires summing
over the potentially exponentially many structures in Y However for some problems a
dynamic programming algorithm exists for eciently solving the summation in polynomial
time When such an algorithm exists it can be adapted to also create a polynomial-size
computation graph
When an ecient enough algorithm for computing the partition function is not available
approximate methods can be used For example one may use beam search for inference
and for the partition function sum over the structures remaining in the beam instead of
over the exponentially large Yx
A hinge based approached was used by Pei et al 2015 for arc-factored dependency
parsing and the probabilistic approach by Durrett and Klein Durrett  Klein 2015 for a
CRF constituency parser The approximate beam-based partition function was eectively
used by Zhou et al 2015 in a transition based parser
Reranking When searching over all possible structures is intractable inecient or hard
to integrate into a model reranking methods are often used In the reranking framework
Charniak  Johnson 2005 Collins  Koo 2005 a base model is used to produce a
23 One should keep in mind that the resulting objectives are no longer convex and so lack the formal guar-
antees and bounds associated with convex optimization problems Similarly the theory learning bounds
and guarantees associated with the algorithms do not automatically transfer to the neural versions
list of the k-best scoring structures A more complex model is then trained to score the
candidates in the k-best list such that the best structure with respect to the gold one is
scored highest As the search is now performed over k items rather than over an exponential
space the complex model can condition on extract features from arbitrary aspects of the
scored structure Reranking methods are natural candidates for structured prediction using
neural-network models as they allow the modeler to focus on the feature extraction and
network structure while removing the need to integrate the neural network scoring into a
decoder Indeed reranking methods are often used for experimenting with neural models
that are not straightforward to integrate into a decoder such as convolutional recurrent
and recursive networks which will be discussed in later sections Works using the reranking
approach include Socher et al 2013 Auli et al 2013 Le  Zuidema 2014 Zhu et al
MEMM and hybrid approaches Other formulations are of course also possible For
example an MEMM McCallum Freitag  Pereira 2000 can be trivially adapted to the
neural network world by replacing the logistic regression Maximum Entropy component
with an MLP
Hybrid approaches between neural networks and linear models are also explored
particular Weiss et al Weiss et al 2015 report strong results for transition-based depen-
dency parsing in a two-stage model In the rst stage a static feed-forward neural network
MLP2 is trained to perform well on each of the individual decisions of the structured
problem in isolation In the second stage the neural network model is held xed and the
dierent layers output as well as hidden layer vectors for each input are then concatenated
and used as the input features of a linear structured perceptron model Collins 2002 that
is trained to perform beam-search for the best resulting structure While it is not clear
that such training regime is more eective than training a single structured-prediction neu-
ral network the use of two simpler isolated models allowed the researchers to perform a
much more extensive hyper-parameter search eg tuning layer sizes activation functions
learning rates and so on for each model than is feasible with more complicated networks
9 Convolutional Layers
Sometimes we are interested in making predictions based on ordered sets of items eg
the sequence of words in a sentence the sequence of sentences in a document and so on
Consider for example predicting the sentiment positive negative or neutral of a sentence
Some of the sentence words are very informative of the sentiment other words are less
informative and to a good approximation an informative clue is informative regardless
of its position in the sentence We would like to feed all of the sentence words into a
learner and let the training process gure out the important clues One possible solution is
feeding a CBOW representation into a fully connected network such as an MLP However
a downside of the CBOW approach is that it ignores the ordering information completely
assigning the sentences it was not good it was actually quite bad and it was not bad
it was actually quite good the exact same representation While the global position of the
indicators not good and not bad does not matter for the classication task the local
ordering of the words that the word not appears right before the word bad is very
important A naive approach would suggest embedding word-pairs bi-grams rather than
words and building a CBOW over the embedded bigrams While such architecture could be
eective it will result in huge embedding matrices will not scale for longer n-grams and will
suer from data sparsity problems as it does not share statistical strength between dierent
n-grams the embedding of quite good and very good are completely independent of
one another so if the learner saw only one of them during training it will not be able to
deduce anything about the other based on its component words The convolution-and-
pooling also called convolutional neural networks or CNNs architecture is an elegant and
robust solution to the this modeling problem A convolutional neural network is designed
to identify indicative local predictors in a large structure and combine them to produce a
xed size vector representation of the structure capturing these local aspects that are most
informative for the prediction task at hand
Convolution-and-pooling architectures LeCun  Bengio 1995 evolved in the neural
networks vision community where they showed great success as object detectors  recog-
nizing an object from a predened category cat bicycles regardless of its position in
the image Krizhevsky et al 2012 When applied to images the architecture is using 2-
dimensional grid convolutions When applied to text NLP we are mainly concerned with
1-d sequence convolutions Convolutional networks were introduced to the NLP commu-
nity in the pioneering work of Collobert Weston and Colleagues 2011 who used them for
semantic-role labeling and later by Kalchbrenner et al 2014 and Kim Kim 2014 who
used them for sentiment and question-type classication
91 Basic Convolution  Pooling
The main idea behind a convolution and pooling architecture for language tasks is to apply
a non-linear learned function over each instantiation of a k-word sliding window over
the sentence This function also called lter transforms a window of k words into a d
dimensional vector that captures important properties of the words in the window each
dimension is sometimes referred to in the literature as a channel Then a pooling
operation is used combine the vectors resulting from the dierent windows into a single
d-dimensional vector by taking the max or the average value observed in each of the d
channels over the dierent windows The intention is to focus on the most important
features in the sentence regardless of their location The d-dimensional vector is then
fed further into a network that is used for prediction The gradients that are propagated
back from the networks loss during the training process are used to tune the parameters
of the lter function to highlight the aspects of the data that are important for the task
the network is trained for Intuitively when the sliding window is run over a sequence the
lter function learns to identify informative k-grams
More formally consider a sequence of words x  x1     xn each with their correspond-
ing demb dimensional word embedding vxi A 1d convolution layer24 of width k works by
moving a sliding window of size k over the sentence and applying the same lter to each
window in the sequence vxi vxi1     vxik1 The lter function is usually a linear
transformation followed by a non-linear activation function
Let the concatenated vector of the ith window be wi  vxi vxi1 vxik1 wi 
Rkdemb Depending on whether we pad the sentence with k  1 words to each side we may
get either m  n k  1 narrow convolution or m  n  k  1 windows wide convolution
Kalchbrenner et al 2014 The result of the convolution layer is m vectors p1     pm
pi  Rdconv where
pi  gwiW  b
g is a non-linear activation function that is applied element-wise W  Rkdembdconv and
b  Rdconv are parameters of the network Each pi is a dconv dimensional vector encoding
the information in wi Ideally each dimension captures a dierent kind of indicative infor-
mation The m vectors are then combined using a max pooling layer resulting in a single
dconv dimensional vector c
cj  max
1im
pij denotes the jth component of pi The eect of the max-pooling operation is to get the
most salient information across window positions Ideally each dimension will specialize
in a particular sort of predictors and max operation will pick on the most important
predictor of each type
Figure 4 provides an illustration of the process
The resulting vector c is a representation of the sentence in which each dimension
reects the most salient information with respect to some prediction task c is then fed
into a downstream network layers perhaps in parallel to other vectors culminating in an
output layer which is used for prediction The training procedure of the network calculates
the loss with respect to the prediction task and the error gradients are propagated all the
way back through the pooling and convolution layers as well as the embedding layers 25
24 1d here refers to a convolution operating over 1-dimensional inputs such as sequences as opposed to 2d
convolutions which are applied to images
25 Besides being useful for prediction a by-product of the training procedure is a set of parameters W B
and embeddings v that can be used in a convolution and pooling architecture to encode arbitrary length
sentences into xed-size vectors such that sentences that share the same kind of predictive information
will be close to each other
Figure 4 1d convolutionpooling over the sentence the quick brown fox jumped over the
lazy dog This is a narrow convolution no padding is added to the sentence
with a window size of 3 Each word is translated to a 2-dim embedding vector
not shown The embedding vectors are then concatenated resulting in 6-dim
window representations Each of the seven windows is transfered through a 6 3
lter linear transformation followed by element-wise tanh resulting in seven
3-dimensional ltered representations Then a max-pooling operation is applied
taking the max over each dimension resulting in a nal 3-dimensional pooled
vector
While max-pooling is the most common pooling operation in text applications other
pooling operations are also possible the second most common operation being average
pooling taking the average value of each index instead of the max
92 Dynamic Hierarchical and k-max Pooling
Rather than performing a single pooling operation over the entire sequence we may want
to retain some positional information based on our domain understanding of the prediction
problem at hand To this end we can split the vectors pi into cid96 distinct groups apply
the pooling separately on each group and then concatenate the cid96 resulting dconv vectors
c1     ccid96 The division of the pis into groups is performed based on domain knowledge For
example we may conjecture that words appearing early in the sentence are more indicative
than words appearing late We can then split the sequence into cid96 equally sized regions
applying a separate max-pooling to each region For example Johnson and Zhang Johnson
 Zhang 2014 found that when classifying documents into topics it is useful to have 20
average-pooling regions clearly separating the initial sentences where the topic is usually
introduced from later ones while for a sentiment classication task a single max-pooling
operation over the entire sentence was optimal suggesting that one or two very strong
signals are enough to determine the sentiment regardless of the position in the sentence
thequickbrownquickbrownfoxbrownfoxjumpedfoxjumpedoverjumpedovertheoverthelazythelazydogMULtanhMULtanhMULtanhMULtanhMULtanhMULtanhMULtanhW63thequickbrownfoxjumpedoverthelazydogmaxconvolutionpoolingSimilarly in a relation extraction kind of task we may be given two words and asked to
determine the relation between them We could argue that the words before the rst word
the words after the second word and the words between them provide three dierent kinds
of information Chen et al 2015 We can thus split the pi vectors accordingly pooling
separately the windows resulting from each group
Another variation is performing hierarchical pooling in which we have a succession
of convolution and pooling layers where each stage applies a convolution to a sequence
pools every k neighboring vectors performs a convolution on the resulting pooled sequence
applies another convolution and so on This architecture allows sensitivity to increasingly
larger structures
Finally Kalchbrenner et al 2014 introduced a k-max pooling operation in which the
top k values in each dimension are retained instead of only the best one while preserving
the order in which they appeared in the text For example a consider the following matrix


cid21
cid209 6 3
A 1-max pooling over the column vectors will result in cid29 8 5cid3 while a 2-max pooling
cid29 6 3 7 8 5cid3
whose rows will then be concatenated to
will result in the following matrix
The k-max pooling operation makes it possible to pool the k most active indicators that
may be a number of positions apart it preserves the order of the features but is insensitive
to their specic positions It can also discern more nely the number of times the feature
is highly activated Kalchbrenner et al 2014
93 Variations
Rather than a single convolutional layer several convolutional layers may be applied in
parallel For example we may have four dierent convolutional layers each with a dierent
window size in the range 25 capturing n-gram sequences of varying lengths The result
of each convolutional layer will then be pooled and the resulting vectors concatenated and
fed to further processing Kim 2014
The convolutional architecture need not be restricted into the linear ordering of a sen-
tence For example Ma et al 2015 generalize the convolution operation to work over
syntactic dependency trees There each window is around a node in the syntactic tree
and the pooling is performed over the dierent nodes Similarly Liu et al 2015 apply a
convolutional architecture on top of dependency paths extracted from dependency trees Le
and Zuidema 2015 propose to perform max pooling over vectors representing the dierent
derivations leading to the same chart item in a chart parser
10 Recurrent Neural Networks  Modeling Sequences and Stacks
When dealing with language data it is very common to work with sequences such as words
sequences of letters sentences sequences of words and documents We saw how feed-
forward networks can accommodate arbitrary feature functions over sequences through the
use of vector concatenation and vector addition CBOW In particular the CBOW rep-
resentations allows to encode arbitrary length sequences as xed sized vectors However
the CBOW representation is quite limited and forces one to disregard the order of fea-
tures The convolutional networks also allow encoding a sequence into a xed size vector
While representations derived from convolutional networks are an improvement above the
CBOW representation as they oer some sensitivity to word order their order sensitivity is
restricted to mostly local patterns and disregards the order of patterns that are far apart
in the sequence
Recurrent neural networks RNNs Elman 1990 allow representing arbitrarily sized
structured inputs in a xed-size vector while paying attention to the structured properties
of the input
101 The RNN Abstraction
We use xij to denote the sequence of vectors xi     xj The RNN abstraction takes as
input an ordered list of input vectors x1  xn together with an initial state vector s0
and returns an ordered list of state vectors s1  sn as well as an ordered list of output
vectors y1  yn An output vector yi is a function of the corresponding state vector
si The input vectors xi are presented to the RNN in a sequential fashion and the state
vector si and output vector yi represent the state of the RNN after observing the inputs
x1i The output vector yi is then used for further prediction For example a model for
predicting the conditional probability of an event e given the sequence m1i can be dened
as pe  jx1i  sof tmaxyiW  bj The RNN model provides a framework for
conditioning on the entire history x1     xi without resorting to the Markov assumption
which is traditionally used for modeling sequences Indeed RNN-based language models
result in very good perplexity scores when compared to n-gram based models
Mathematically we have a recursively dened function R that takes as input a state
vector si and an input vector xi1 and results in a new state vector si1 An additional
function O is used to map a state vector si to an output vector yi When constructing an
RNN much like when constructing a feed-forward network one has to specify the dimension
of the inputs xi as well as the dimensions of the outputs yi The dimensions of the states
si are a function of the output dimension26
26 While RNN architectures in which the state dimension is independent of the output dimension are
possible the current popular architectures including the Simple RNN the LSTM and the GRU do not
follow this exibility
RN N s0 x1n s1n y1n
si  Rsi1 xi
yi  Osi
xi  Rdin yi  Rdout si  Rf dout
The functions R and O are the same across the sequence positions but the RNN keeps
track of the states of computation through the state vector that is kept and being passed
between invocations of R
Graphically the RNN has been traditionally presented as in Figure 5
Figure 5 Graphical representation of an RNN recursive
This presentation follows the recursive denition and is correct for arbitrary long sequences
However for a nite sized input sequence and all input sequences we deal with are nite
one can unroll the recursion resulting in the structure in Figure 6
Figure 6 Graphical representation of an RNN unrolled
While not usually shown in the visualization we include here the parameters  in order
to highlight the fact that the same parameters are shared across all time steps Dierent
ROxiyisisi1s0ROx1y1ROx2y2s1ROx3y3s2ROx4y4s3ROx5y5s4s5instantiations of R and O will result in dierent network structures and will exhibit dierent
properties in terms of their running times and their ability to be trained eectively using
gradient-based methods However they all adhere to the same abstract interface We will
provide details of concrete instantiations of R and O  the Simple RNN the LSTM and the
GRU  in Section 11 Before that lets consider modeling with the RNN abstraction
First we note that the value of si is based on the entire input x1  xi For example
by expanding the recursion for i  4 we get
s4 Rs3 x4
cid122
Rs2 x3 x4
cid125cid124
cid123
cid122
cid125cid124
cid123
cid125cid124
cid122
cid123
Rs1 x2 x3 x4
RRR
Rs0 x1 x2 x3 x4
Thus sn as well as yn could be thought of as encoding the entire input sequence27 Is
the encoding useful This depends on our denition of usefulness The job of the network
training is to set the parameters of R and O such that the state conveys useful information
for the task we are tying to solve
102 RNN Training
Viewed as in Figure 6 it is easy to see that an unrolled RNN is just a very deep neural
network or rather a very large computation graph with somewhat complex nodes in
which the same parameters are shared across many parts of the computation To train an
RNN network then all we need to do is to create the unrolled computation graph for a
given input sequence add a loss node to the unrolled graph and then use the backward
backpropagation algorithm to compute the gradients with respect to that loss This
procedure is referred to in the RNN literature as backpropagation through time or BPTT
Werbos 199028 There are various ways in which the supervision signal can be applied
Acceptor One option is to base the supervision signal only on the nal output vector
yn Viewed this way the RNN is an acceptor We observe the nal state and then decide
27 Note that unless R is specically designed against this it is likely that the later elements of the input
sequence have stronger eect on sn than earlier ones
28 Variants of the BPTT algorithm include unrolling the RNN only for a xed number of input symbols at
each time rst unroll the RNN for inputs x1k resulting in s1k Compute a loss and backpropagate
the error through the network k steps back Then unroll the inputs xk12k this time using sk as the
initial state and again backpropagate the error for k steps and so on This strategy is based on the
observations that for the Simple-RNN variant the gradients after k steps tend to vanish for large enough
k and so omitting them is negligible This procedure allows training of arbitrarily long sequences For
RNN variants such as the LSTM or the GRU that are designed specically to mitigate the vanishing
gradients problem this xed size unrolling is less motivated yet it is still being used for example when
doing language modeling over a book without breaking it into sentences
on an outcome29 For example consider training an RNN to read the characters of a word
one by one and then use the nal state to predict the part-of-speech of that word this is
inspired by Ling et al 2015b an RNN that reads in a sentence and based on the nal
state decides if it conveys positive or negative sentiment this is inspired by Wang et al
2015b or an RNN that reads in a sequence of words and decides whether it is a valid
noun-phrase The loss in such cases is dened in terms of a function of yn  Osn and
the error gradients will backpropagate through the rest of the sequence see Figure 730
The loss can take any familiar form  cross entropy hinge margin etc
Figure 7 Acceptor RNN Training Graph
Encoder Similar to the acceptor case an encoder supervision uses only the nal output
vector yn However unlike the acceptor where a prediction is made solely on the basis
of the nal vector here the nal vector is treated as an encoding of the information in the
sequence and is used as additional information together with other signals For example
an extractive document summarization system may rst run over the document with an
RNN resulting in a vector yn summarizing the entire document Then yn will be used
together with with other features in order to select the sentences to be included in the
summarization
then be L y1n y1n  cid80n
Transducer Another option is to treat the RNN as a transducer producing an output for
each input it reads in Modeled this way we can compute a local loss signal Llocal yi yi
for each of the outputs yi based on a true label yi The loss for unrolled sequence will
i1 Llocal yi yi or using another combination rather than a
sum such as an average or a weighted average see Figure 8 One example for such a
transducer is a sequence tagger in which we take xin to be feature representations for the
n words of a sentence and yi as an input for predicting the tag assignment of word i based
on words 1i A CCG super-tagger based on such an architecture provides state-of-the art
CCG super-tagging results Xu et al 2015
A very natural use-case of the transduction setup is for language modeling in which the
sequence of words x1i is used to predict a distribution over the i  1th word RNN based
29 The terminology is borrowed from Finite-State Acceptors However the RNN has a potentially innite
number of states making it necessary to rely on a function other than a lookup table for mapping states
to decisions
30 This kind of supervision signal may be hard to train for long sequences especially so with the Simple-
RNN because of the vanishing gradients problem It is also a generally hard learning task as we do not
tell the process on which parts of the input to focus
ROx1s0ROx2s1ROx3s2ROx4s3ROx5s4predictcalclossy5lossFigure 8 Transducer RNN Training Graph
language models are shown to provide much better perplexities than traditional language
models Mikolov et al 2010 Sundermeyer Schluter  Ney 2012 Mikolov 2012
Using RNNs as transducers allows us to relax the Markov assumption that is tradition-
ally taken in language models and HMM taggers and condition on the entire prediction
history The power of the ability to condition on arbitrarily long histories is demonstrated
in generative character-level RNN models in which a text is generated character by charac-
ter each character conditioning on the previous ones Sutskever Martens  Hinton 2011
The generated texts show sensitivity to properties that are not captured by n-gram language
models including line lengths and nested parenthesis balancing For a good demonstration
and analysis of the properties of RNN-based character level language models see Karpathy
Johnson  Li 2015
Encoder - Decoder Finally an important special case of the encoder scenario is the
Encoder-Decoder framework Cho van Merrienboer Bahdanau  Bengio 2014a Sutskever
et al 2014 The RNN is used to encode the sequence into a vector representation yn and
this vector representation is then used as auxiliary input to another RNN that is used as
a decoder For example in a machine-translation setup the rst RNN encodes the source
sentence into a vector representation yn and then this state vector is fed into a separate
decoder RNN that is trained to predict using a transducer-like language modeling ob-
jective the words of the target language sentence based on the previously predicted words
as well as yn The supervision happens only for the decoder RNN but the gradients are
propagated all the way back to the encoder RNN see Figure 9
Such an approach was shown to be surprisingly eective for Machine Translation Sutskever
et al 2014 using LSTM RNNs In order for this technique to work Sutskever et al found it
eective to input the source sentence in reverse such that xn corresponds to the rst word
of the sentence In this way it is easier for the second RNN to establish the relation be-
tween the rst word of the source sentence to the rst word of the target sentence Another
use-case of the encoder-decoder framework is for sequence transduction Here in order to
generate tags t1     tn an encoder RNN is rst used to encode the sentence x1n into xed
sized vector This vector is then fed as the initial state vector of another transducer RNN
which is used together with x1n to predict the label ti at each position i This approach
ROx1s0predictcalclossy1ROx2s1predictcalclossy2ROx3s2predictcalclossy3ROx4s3predictcalclossy4ROx5s4predictcalclossy5sumlossFigure 9 Encoder-Decoder RNN Training Graph
was used in Filippova Alfonseca Colmenares Kaiser  Vinyals 2015 to model sentence
compression by deletion
103 Multi-layer stacked RNNs
RNNs can be stacked in layers forming a grid Hihi  Bengio 1996 Consider k RNNs
RN N1     RN Nk where the jth RNN has states sj
1n The input for the
rst RNN are x1n while the input of the jth RNN j  2 are the outputs of the RNN
below it yj1
Such layered architectures are often called deep RNNs A visual representation of a 3-layer
RNN is given in Figure 10
1n  The output of the entire formation is the output of the last RNN yk
1n and outputs yj
While it is not theoretically clear what is the additional power gained by the deeper
architecture it was observed empirically that deep RNNs work better than shallower ones
on some tasks In particular Sutskever et al 2014 report that a 4-layers deep architec-
ture was crucial in achieving good machine-translation performance in an encoder-decoder
framework Irsoy and Cardie 2014 also report improved results from moving from a one-
layer BI-RNN to an architecture with several layers Many other works report result using
layered RNN architectures but do not explicitly compare to 1-layer RNNs
REOEx1se0REOEx2se1REOEx3se2REOEx4se3REOEx5se4se5RDODx1sd0predictcalclossy1RDODx2sd1predictcalclossy2RDODx3sd2predictcalclossy3RDODx4sd3predictcalclossy4RDODx5sd4predictcalclossy5sumlossFigure 10 A 3-layer deep RNN architecture
104 BI-RNN
A useful elaboration of an RNN is a bidirectional-RNN BI-RNN Schuster  Paliwal 1997
Graves 200831 Consider the task of sequence tagging over a sentence x1     xn An RNN
allows us to compute a function of the ith word xi based on the past  the words x1i up
to and including it However the following words xin may also be useful for prediction as
is evident by the common sliding-window approach in which the focus word is categorized
based on a window of k words surrounding it Much like the RNN relaxes the Markov
assumption and allows looking arbitrarily back into the past the BI-RNN relaxes the xed
window size assumption allowing to look arbitrarily far at both the past and the future
i and sb
for each input position i The forward state sf
Consider an input sequence x1n The BI-RNN works by maintaining two separate
states sf
i is based on x1 x2     xi
while the backward state sb
is based on xn xn1     xi The forward and backward states
are generated by two dierent RNNs The rst RNN Rf  Of  is fed the input sequence
x1n as is while the second RNN Rb Ob is fed the input sequence in reverse The state
representation si is then composed of both the forward and backward states
i  Obsb
i   Of sf
The output at position i is based on the concatenation of the two output vectors
yi  yf
i  taking into account both the past and the future The
vector yi can then be used directly for prediction or fed as part of the input to a more
complex network While the two RNNs are run independently of each other the error gra-
dients at position i will ow both forward and backward through the two RNNs A visual
representation of the BI-RNN architecture is given in Figure 11
The use of BI-RNNs for sequence tagging was introduced to the NLP community by
Irsoy and Cardie 2014
105 RNNs for Representing Stacks
Some algorithms in language processing including those for transition-based parsing Nivre
2008 require performing feature extraction over a stack
Instead of being conned to
31 When used with a specic RNN architecture such as an LSTM the model is called BI-LSTM
R1O1R2O2y11s10R3O3y21s20s30x1y1y31R1O1R2O2y12s11R3O3y22s21s31x2y2y32R1O1R2O2y13s12R3O3y23s22s32x3y3y33R1O1R2O2y14s13R3O3y24s23s33x4y4y34R1O1R2O2y15s14R3O3y25s24s34x5y5y35s15s25s35Figure 11 BI-RNN over the sentence the brown fox jumped 
looking at the k top-most elements of the stack the RNN framework can be used to provide
a xed-sized vector encoding of the entire stack
The main intuition is that a stack is essentially a sequence and so the stack state can be
represented by taking the stack elements and feeding them in order into an RNN resulting
in a nal encoding of the entire stack In order to do this computation eciently without
performing an On stack encoding operation each time the stack changes the RNN state
is maintained together with the stack state
If the stack was push-only this would be
trivial whenever a new element x is pushed into the stack the corresponding vector x
will be used together with the RNN state si in order to obtain a new state si1 Dealing
with pop operation is more challenging but can be solved by using the persistent-stack
data-structure Okasaki 1999 Goldberg Zhao  Huang 2013 Persistent or immutable
data-structures keep old versions of themselves intact when modied The persistent stack
construction represents a stack as a pointer to the head of a linked list An empty stack is
the empty list The push operation appends an element to the list returning the new head
The pop operation then returns the parent of the head but keeping the original list intact
From the point of view of someone who held a pointer to the previous head the stack did
not change A subsequent push operation will add a new child to the same node Applying
this procedure throughout the lifetime of the stack results in a tree where the root is an
empty stack and each path from a node to the root represents an intermediary stack state
Figure 12 provides an example of such a tree The same process can be applied in the
computation graph construction creating an RNN with a tree structure instead of a chain
structure Backpropagating the error from a given node will then aect all the elements
that participated in the stack when the node was created in order Figure 13 shows the
computation graph for the stack-RNN corresponding to the last state in Figure 12 This
modeling approach was proposed independently by Dyer et al and Watanabe et al Dyer
et al 2015 Watanabe  Sumita 2015 for transition-based dependency parsing
RfOfxtheconcatyf1sf0RfOfxbrownconcatyf2sf1RfOfxfoxconcatyf3sf2RfOfxjumpedconcatyf4sf3RfOfxconcatyf5sf4sf5RbObs0sb0yb1RbObs1sb1yb2RbObs2sb2yb3RbObs3sb3yb4RbObs4sb4yb5sb5ytheybrownyfoxyjumpedyFigure 12 An immutable stack construction for the sequence of operations push a push b
push c pop push d pop pop push e push f
Figure 13 The stack-RNN corresponding to the nal state in Figure 12
ahead1pushaabhead2pushbabchead3pushcabheadc4popabcdhead5pushdabheadcd6popaheadbcd7popabcdehead8pusheabcdefhead9pushfsoROyaxaROsayabxbROsabyacxcsacROsabyabdxdsabdROsayaexeROsaeyaefxfsaef11 Concrete RNN Architectures
We now turn to present three dierent instantiations of the abstract RN N architecture
discussed in the previous section providing concrete denitions of the functions R and O
These are the Simple RNN SRNN the Long Short-Term Memory LSTM and the Gated
Recurrent Unit GRU
111 Simple RNN
The simplest RNN formulation known as an Elman Network or Simple-RNN S-RNN was
proposed by Elman 1990 and explored for use in language modeling by Mikolov 2012
The S-RNN takes the following form
si RSRN N si1 xi  gxiWx  si1Ws  b
yi OSRN N si  si
si yi  Rds xi  Rdx Wx  Rdxds Ws  Rdsds b  Rds
That is the state at position i is a linear combination of the input at position i and
the previous state passed through a non-linear activation commonly tanh or ReLU The
output at position i is the same as the hidden state in that position32
In spite of its simplicity the Simple RNN provides strong results for sequence tagging
Xu et al 2015 as well as language modeling For comprehensive discussion on using
Simple RNNs for language modeling see the PhD thesis by Mikolov 2012
112 LSTM
The S-RNN is hard to train eectively because of the vanishing gradients problem Error
signals gradients in later steps in the sequence diminish in quickly in the back-propagation
process and do not reach earlier input signals making it hard for the S-RNN to capture
long-range dependencies The Long Short-Term Memory LSTM architecture Hochreiter
 Schmidhuber 1997 was designed to solve the vanishing gradients problem The main
idea behind the LSTM is to introduce as part of the state representation also memory
cells a vector that can preserve gradients across time Access to the memory cells is
controlled by gating components  smooth mathematical functions that simulate logical
gates At each input state a gate is used to decide how much of the new input should be
written to the memory cell and how much of the current content of the memory cell should
be forgotten Concretely a gate g  0 1n is a vector of values in the range 0 1 that is
multiplied component-wise with another vector v  Rn and the result is then added to
another vector The values of g are designed to be close to either 0 or 1 ie by using a
sigmoid function Indices in v corresponding to near-one values in g are allowed to pass
while those corresponding to near-zero values are blocked
32 Some authors treat the output at position i as a more complicated function of the state In our presen-
tation such further transformation of the output are not considered part of the RNN but as separate
computations that are applied to the RNNs output The distinction between the state and the output
are needed for the LSTM architecture in which not all of the state is observed outside of the RNN
Mathematically the LSTM architecture is dened as33
sj  RLST M sj1 xj cj hj
cj cj1 cid12 f  g cid12 i
hj  tanhcj cid12 o
i xjWxi  hj1Whi
f xjWxf  hj1Whf 
o xjWxo  hj1Who
g  tanhxjWxg  hj1Whg
yj  OLST M sj hj
sj  R2dh xi  Rdx cj hj i f  o g  Rdh Wx  Rdxdh Wh  Rdhdh
The symbol cid12 is used to denote component-wise product The state at time j is com-
posed of two vectors cj and hj where cj is the memory component and hj is the output
or state component There are three gates i f and o controlling for input f orget and
output The gate values are computed based on linear combinations of the current input
xj and the previous state hj1 passed through a sigmoid activation function An update
candidate g is computed as a linear combination of xj and hj1 passed through a tanh
activation function The memory cj is then updated the forget gate controls how much
of the previous memory to keep cj1 cid12 f  and the input gate controls how much of the
proposed update to keep g cid12 i Finally the value of hj which is also the output yj is
determined based on the content of the memory cj passed through a tanh non-linearity
and controlled by the output gate The gating mechanisms allow for gradients related to
the memory part cj to stay high across very long time ranges
For further discussion on the LSTM architecture see the PhD thesis by Alex Graves
2008 as well as Chris Olahs description34 For an analysis of the behavior of an LSTM
when used as a character-level language model see Karpathy et al 2015
LSTMs are currently the most successful type of RNN architecture and they are re-
sponsible for many state-of-the-art sequence modeling results The main competitor of the
LSTM-RNN is the GRU to be discussed next
Practical Considerations When training LSTM networks Jozefowicz et al 2015 strongly
recommend to always initialize the bias term of the forget gate to be close to one When
applying dropout to an RNN with an LSTM Zaremba et al 2014 found out that it is
33 There are many variants on the LSTM architecture presented here For example forget gates were not
part of the original proposal in Hochreiter  Schmidhuber 1997 but are shown to be an important
part of the architecture Other variants include peephole connections and gate-tying For an overview
and comprehensive empirical comparison of various LSTM architectures see Gre Srivastava Koutnk
Steunebrink  Schmidhuber 2015
34 httpcolahgithubioposts2015-08-Understanding-LSTMs
crucial to apply dropout only on the non-recurrent connection ie only to apply it between
layers and not between sequence positions
113 GRU
The LSTM architecture is very eective but also quite complicated The complexity of the
system makes it hard to analyze and also computationally expensive to work with The
gated recurrent unit GRU was recently introduced by Cho et al 2014b as an alternative
to the LSTM It was subsequently shown by Chung et al 2014 to perform comparably to
the LSTM on several non textual datasets
Like the LSTM the GRU is also based on a gating mechanism but with substantially
fewer gates and without a separate memory component
sj  RGRU sj1 xj 1  z cid12 sj1  z cid12 h
z xjWxz  hj1Whz
r xjWxr  hj1Whr
h  tanhxjWxh  hj1 cid12 rWhg
yj  OLST M sj sj
sj  Rdh xi  Rdx z r h  Rdh Wx  Rdxdh Wh  Rdhdh
One gate r is used to control access to the previous state sj1 and compute a proposed
update h The updated state sj which also serves as the output yj is then determined based
on an interpolation of the previous state sj1 and the proposal h where the proportions of
the interpolation are controlled using the gate z
The GRU was shown to be eective in language modeling and machine translation
However the jury between the GRU the LSTM and possible alternative RNN architectures
is still out and the subject is actively researched For an empirical exploration of the GRU
and the LSTM architectures see Jozefowicz et al 2015
114 Other Variants
The gated architectures of the LSTM and the GRU help in alleviating the vanishing gradi-
ents problem of the Simple RNN and allow these RNNs to capture dependencies that span
long time ranges Some researchers explore simpler architectures than the LSTM and the
GRU for achieving similar benets
Mikolov et al 2014 observed that the matrix multiplication si1Ws coupled with the
nonlinearity g in the update rule R of the Simple RNN causes the state vector si to undergo
large changes at each time step prohibiting it from remembering information over long
time periods They propose to split the state vector si into a slow changing component ci
context units and a fast changing component hi35 The slow changing component ci is
35 We depart from the notation in Mikolov et al 2014 and reuse the symbols used in the LSTM descrip-
updated according to a linear interpolation of the input and the previous component ci 
1  xiWx1  ci1 where   0 1 This update allows ci to accumulate the previous
inputs The fast changing component hi is updated similarly to the Simple RNN update
rule but changed to take ci into account as well36 hi  xiWx2  hi1Wh  ciWc
Finally the output yi is the concatenation of the slow and the fast changing parts of the
state yi  ci hi Mikolov et al demonstrate that this architecture provides competitive
perplexities to the much more complex LSTM on language modeling tasks
The approach of Mikolov et al can be interpreted as constraining the block of the matrix
Ws in the S-RNN corresponding to ci to be a multiply of the identity matrix see Mikolov
et al 2014 for the details Le et al Le Jaitly  Hinton 2015 propose an even simpler
approach set the activation function of the S-RNN to a ReLU and initialize the biases b
as zeroes and the matrix Ws as the identify matrix This causes an untrained RNN to copy
the previous state to the current state add the eect of the current input xi and set the
negative values to zero After setting this initial bias towards state copying the training
procedure allows Ws to change freely Le et al demonstrate that this simple modication
makes the S-RNN comparable to an LSTM with the same number of parameters on several
tasks including language modeling
36 The update rule diverges from the S-RNN update rule also by xing the non-linearity to be a sigmoid
function and by not using a bias term However these changes are not discussed as central to the
proposal
12 Modeling Trees  Recursive Neural Networks
The RNN is very useful for modeling sequences In language processing it is often natural
and desirable to work with tree structures The trees can be syntactic trees discourse trees
or even trees representing the sentiment expressed by various parts of a sentence Socher
et al 2013 We may want to predict values based on specic tree nodes predict values
based on the root nodes or assign a quality score to a complete tree or part of a tree In
other cases we may not care about the tree structure directly but rather reason about spans
in the sentence In such cases the tree is merely used as a backbone structure which help
guide the encoding process of the sequence into a xed size vector
The recursive neural network RecNN abstraction Pollack 1990 popularized in NLP
by Richard Socher and colleagues Socher Manning  Ng 2010 Socher Lin Ng  Man-
ning 2011 Socher et al 2013 Socher 2014 is a generalization of the RNN from sequences
to binary trees37
Much like the RNN encodes each sentence prex as a state vector the RecNN encodes
each tree-node as a state vector in Rd We can then use these state vectors either to predict
values of the corresponding nodes assign quality values to each node or as a semantic
representation of the spans rooted at the nodes
The main intuition behind the recursive neural networks is that each subtree is repre-
sented as a d dimensional vector and the representation of a node p with children c1 and c2
is a function of the representation of the nodes vecp  f vecc1 vecc2 where f is a
composition function taking two d-dimensional vectors and returning a single d-dimensional
vector Much like the RNN state si is used to encode the entire sequence x1  i the RecNN
state associated with a tree node p encodes the entire subtree rooted at p See Figure 14
for an illustration
121 Formal Denition
Consider a binary parse tree T over an n-word sentence As a reminder an ordered
unlabeled tree over a string x1     xn can be represented as a unique set of triplets i k j
st i  k  j Each such triplet indicates that a node spanning words xij is parent of the
nodes spanning xik and xk1j Triplets of the form i i i correspond to terminal symbols
at the tree leaves the words xi Moving from the unlabeled case to the labeled one we can
represent a tree as a set of 6-tuples A  B C i k j whereas i k and j indicate the spans
as before and A B and C are the node labels of of the nodes spanning xij xik and xk1j
respectively Here leaf nodes have the form A  A A i i i where A is a pre-terminal
symbol We refer to such tuples as production rules For an example consider the syntactic
tree for the sentence the boy saw her duck
37 While presented in terms of binary parse trees the concepts easily transfer to general recursively-dened
data structures with the major technical challenge is the denition of an eective form for R the
combination function
Figure 14 Illustration of a recursive neural network The representations of V and NP1
are combined to form the representation of VP The representations of VP and
NP2 are then combined to form the representation of S
Its corresponding unlabeled and labeled representations are 
Unlabeled
111
222
333
444
555
445
335
112
125
Labeled
Det Det Det 1 1 1
Corresponding Span
x11 the
Noun Noun Noun 2 2 2 x22 boy
Verb Verb Verb 3 3 3
Det Det Det 4 4 4
Noun Noun Noun 5 5 5 duck
NP Det Noun 4 4 5
VP Verb NP 3 3 5
NP Det Noun 1 1 2
S NP VP 1 2 5
her duck
saw her duck
the boy
the boy saw her duck
The set of production rules above can be uniquely converted to a set tree nodes qA
indicating a node with symbol A over the span xij by simply ignoring the elements
VNP1combineVPNP2combineSB C k in each production rule We are now in position to dene the Recursive Neural
Network
A Recursive Neural Network RecNN is a function that takes as input a parse tree
over an n-word sentence x1     xn Each of the sentences words is represented as a d-
dimensional vector xi and the tree is represented as a set T of production rules A 
B C i j k Denote the nodes of T by qA
ij The RecNN returns as output a correspond-
ij  Rd represents the
ing set of inside state vectors sA
corresponding tree node qA
ij and encodes the entire structure rooted at that node Like
the sequence RNN the tree shaped RecNN is dened recursively using a function R where
the inside vector of a given node is dened as a function of the inside vectors of its direct
children38 Formally
ij where each inside state vector sA
RecN N x1     xnT  sA
ii vxi
ij RA B C sB
ij  Rd  qA
ij  T 
ik sC
ik  T  qC
k1j  T
The function R usually takes the form of a simple linear transformation which may or
may not be followed by a non-linear activation function g
RA B C sB
ik sC
k1j  gsB
ik sC
k1jW
This formulation of R ignores the tree labels using the same matrix W  R2dd for all
combinations This may be a useful formulation in case the node labels do not exist eg
when the tree does not represent a syntactic structure with clearly dened labels or when
they are unreliable However if the labels are available it is generally useful to include them
in the composition function One approach would be to introduce label embeddings vA
mapping each non-terminal symbol to a dnt dimensional vector and change R to include
the embedded symbols in the combination function
RA B C sB
ik sC
k1j  gsB
ik sC
k1j vA vBW
here W  R2d2dntd Such approach is taken by Qian Tian Huang Liu Zhu 
Zhu 2015 An alternative approach due to Socher et al 2013 is to untie the weights
according to the non-terminals using a dierent composition matrix for each B C pair of
symbols39
RA B C sB
ik sC
k1j  gsB
ik sC
k1jWBC
38 Le and Zuidema 2014 extend the RecNN denition such that each node has in addition to its inside
state vector also an outside state vector representing the entire structure around the subtree rooted
at that node Their formulation is based on the recursive computation of the classic inside-outside
algorithm and can be thought of as the BI-RNN counterpart of the tree RecNN For details see Le 
Zuidema 2014
39 While not explored in the literature a trivial extension would condition the transformation matrix also
This formulation is useful when the number of non-terminal symbols or the number of
possible symbol combinations is relatively small as is usually the case with phrase-structure
parse trees A similar model was also used by Hashimoto et al 2013 to encode subtrees
in semantic-relation classication task
122 Extensions and Variations
As all of the denitions of R above suer from the vanishing gradients problem of the
Simple RNN several authors sought to replace it with functions inspired by the Long Short-
Term Memory LSTM gated architecture resulting in Tree-shaped LSTMs Tai Socher 
Manning 2015 Zhu Sobhani  Guo 2015b The question of optimal tree representation
is still very much an open research question and the vast space of possible combination
functions R is yet to be explored Other proposed variants on tree-structured RNNs includes
a recursive matrix-vector model Socher Huval Manning  Ng 2012 and recursive neural
tensor network Socher et al 2013
In the rst variant each word is represented as a
combination of a vector and a matrix where the vector denes the words static semantic
content as before while the matrix acts as a learned operator for the word allowing
more subtle semantic compositions than the addition and weighted averaging implied by
the concatenation followed by linear transformation function In the second variant words
are associated with vectors as usual but the composition function becomes more expressive
by basing it on tensor instead of matrix operations
123 Training Recursive Neural Networks
The training procedure for a recursive neural network follows the same recipe as training
other forms of networks dene a loss spell out the computation graph compute gradients
using backpropagation40 and train the parameters using SGD
With regard to the loss function similar to the sequence RNN one can associate a loss
either with the root of the tree with any given node or with a set of nodes in which case
the individual nodes losses are combined usually by summation The loss function is based
on the labeled training data which associates a label or other quantity with dierent tree
Additionally one can treat the RecNN as an Encoder whereas the inside-vector associ-
ated with a node is taken to be an encoding of the tree rooted at that node The encoding
can potentially be sensitive to arbitrary properties of the structure The vector is then
passed as input to another network
For further discussion on recursive neural networks and their use in natural language
tasks refer to the PhD thesis of Richard Socher 2014
40 Before the introduction of the computation graph abstraction the specic backpropagation procedure
for computing the gradients in a RecNN as dened above was referred to as the Back-propagation trough
Structure BPTS algorithm Goller  Kuchler 1996
13 Conclusions
Neural networks are powerful learners providing opportunities ranging from non-linear
classication to non-Markovian modeling of sequences and trees We hope that this expo-
sition help NLP researchers to incorporate neural network models in their work and take
advantage of their power
References
Adel H Vu N T  Schultz T 2013 Combination of Recurrent Neural Networks and
Factored Language Models for Code-Switching Language Modeling In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics Vol-
ume 2 Short Papers pp 206211 Soa Bulgaria Association for Computational
Linguistics
Ando R  Zhang T 2005a A High-Performance Semi-Supervised Learning Method
for Text Chunking In Proceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics ACL05 pp 19 Ann Arbor Michigan Association for
Computational Linguistics
Ando R K  Zhang T 2005b A framework for learning predictive structures from
multiple tasks and unlabeled data The Journal of Machine Learning Research 6
18171853
Auli M Galley M Quirk C  Zweig G 2013 Joint Language and Translation Mod-
eling with Recurrent Neural Networks
In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing pp 10441054 Seattle Washing-
ton USA Association for Computational Linguistics
Auli M  Gao J 2014 Decoder Integration and Expected BLEU Training for Recurrent
Neural Network Language Models In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics Volume 2 Short Papers pp 136142
Baltimore Maryland Association for Computational Linguistics
Ballesteros M Dyer C  Smith N A 2015 Improved Transition-based Parsing by
Modeling Characters instead of Words with LSTMs In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language Processing pp 349359 Lisbon
Portugal Association for Computational Linguistics
Bansal M Gimpel K  Livescu K 2014 Tailoring Continuous Word Representations
for Dependency Parsing In Proceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics Volume 2 Short Papers pp 809815 Baltimore
Maryland Association for Computational Linguistics
Baydin A G Pearlmutter B A Radul A A  Siskind J M 2015 Automatic
dierentiation in machine learning a survey arXiv150205767 cs
Bengio Y 2012 Practical recommendations for gradient-based training of deep architec-
tures arXiv12065533 cs
Bengio Y Ducharme R Vincent P  Janvin C 2003 A Neural Probabilistic Lan-
guage Model J Mach Learn Res 3 11371155
Bengio Y Goodfellow I J  Courville A 2015 Deep Learning Book in preparation
for MIT Press
Bitvai Z  Cohn T 2015 Non-Linear Text Regression with a Deep Convolutional
Neural Network In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Lan-
guage Processing Volume 2 Short Papers pp 180185 Beijing China Association
for Computational Linguistics
Botha J A  Blunsom P 2014 Compositional Morphology for Word Representations
In Proceedings of the 31st International Conference on
and Language Modelling
Machine Learning ICML Beijing China Award for best application paper
Bottou L 2012 Stochastic gradient descent tricks In Neural Networks Tricks of the
Trade pp 421436 Springer
Charniak E  Johnson M 2005 Coarse-to-Fine n-Best Parsing and MaxEnt Discrim-
inative Reranking In Proceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics ACL05 pp 173180 Ann Arbor Michigan Association
for Computational Linguistics
Chen D  Manning C 2014 A Fast and Accurate Dependency Parser using Neural
Networks In Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing EMNLP pp 740750 Doha Qatar Association for Compu-
tational Linguistics
Chen Y Xu L Liu K Zeng D  Zhao J 2015 Event Extraction via Dynamic
Multi-Pooling Convolutional Neural Networks
In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing Volume 1 Long Papers pp 167
176 Beijing China Association for Computational Linguistics
Cho K van Merrienboer B Bahdanau D  Bengio Y 2014a On the Properties of
Neural Machine Translation EncoderDecoder Approaches In Proceedings of SSST-
8 Eighth Workshop on Syntax Semantics and Structure in Statistical Translation
pp 103111 Doha Qatar Association for Computational Linguistics
Cho K van Merrienboer B Gulcehre C Bahdanau D Bougares F Schwenk H 
Bengio Y 2014b Learning Phrase Representations using RNN EncoderDecoder for
Statistical Machine Translation In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing EMNLP pp 17241734 Doha Qatar
Association for Computational Linguistics
Chrupala G 2014 Normalizing tweets with edit scripts and recurrent neural embeddings
In Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-
guistics Volume 2 Short Papers pp 680686 Baltimore Maryland Association for
Computational Linguistics
Chung J Gulcehre C Cho K  Bengio Y 2014 Empirical Evaluation of Gated
Recurrent Neural Networks on Sequence Modeling arXiv14123555 cs
Collins M 2002 Discriminative Training Methods for Hidden Markov Models Theory
In Proceedings of the 2002 Confer-
and Experiments with Perceptron Algorithms
ence on Empirical Methods in Natural Language Processing pp 18 Association for
Computational Linguistics
Collins M  Koo T 2005 Discriminative Reranking for Natural Language Parsing
Computational Linguistics 31 1 2570
Collobert R  Weston J 2008 A unied architecture for natural language processing
Deep neural networks with multitask learning In Proceedings of the 25th international
conference on Machine learning pp 160167 ACM
Collobert R Weston J Bottou L Karlen M Kavukcuoglu K  Kuksa P 2011
Natural language processing almost from scratch The Journal of Machine Learning
Research 12 24932537
Crammer K  Singer Y 2002 On the algorithmic implementation of multiclass kernel-
based vector machines The Journal of Machine Learning Research 2 265292
Creutz M  Lagus K 2007 Unsupervised Models for Morpheme Segmentation and
Morphology Learning ACM Trans Speech Lang Process 4 1 31334
Cybenko G 1989 Approximation by superpositions of a sigmoidal function Mathematics
of Control Signals and Systems 2 4 303314
Dahl G Sainath T  Hinton G 2013 Improving deep neural networks for LVCSR
using rectied linear units and dropout In 2013 IEEE International Conference on
Acoustics Speech and Signal Processing ICASSP pp 86098613
de Gispert A Iglesias G  Byrne B 2015 Fast and Accurate Preordering for SMT
using Neural Networks In Proceedings of the 2015 Conference of the North American
Chapter of the Association for Computational Linguistics Human Language Technolo-
gies pp 10121017 Denver Colorado Association for Computational Linguistics
Dong L Wei F Tan C Tang D Zhou M  Xu K 2014 Adaptive Recursive Neural
Network for Target-dependent Twitter Sentiment Classication
In Proceedings of
the 52nd Annual Meeting of the Association for Computational Linguistics Volume
2 Short Papers pp 4954 Baltimore Maryland Association for Computational
Linguistics
Dong L Wei F Zhou M  Xu K 2015 Question Answering over Freebase with
In Proceedings of the 53rd Annual
Multi-Column Convolutional Neural Networks
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing Volume 1 Long Papers pp 260
269 Beijing China Association for Computational Linguistics
dos Santos C  Gatti M 2014 Deep Convolutional Neural Networks for Sentiment
Analysis of Short Texts In Proceedings of COLING 2014 the 25th International Con-
ference on Computational Linguistics Technical Papers pp 6978 Dublin Ireland
Dublin City University and Association for Computational Linguistics
dos Santos C Xiang B  Zhou B 2015 Classifying Relations by Ranking with
Convolutional Neural Networks
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing Volume 1 Long Papers pp 626634 Beijing
China Association for Computational Linguistics
Duchi J Hazan E  Singer Y 2011 Adaptive subgradient methods for online learning
and stochastic optimization The Journal of Machine Learning Research 12 2121
Duh K Neubig G Sudoh K  Tsukada H 2013 Adaptation Data Selection us-
ing Neural Language Models Experiments in Machine Translation In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics Vol-
ume 2 Short Papers pp 678683 Soa Bulgaria Association for Computational
Linguistics
Durrett G  Klein D 2015 Neural CRF Parsing In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing Volume 1 Long Papers pp 302
312 Beijing China Association for Computational Linguistics
Dyer C Ballesteros M Ling W Matthews A  Smith N A 2015 Transition-
Based Dependency Parsing with Stack Long Short-Term Memory In Proceedings of
the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing Volume 1 Long
Papers pp 334343 Beijing China Association for Computational Linguistics
Elman J L 1990 Finding Structure in Time Cognitive Science 14 2 179211
Faruqui M  Dyer C 2014 Improving Vector Space Word Representations Using Mul-
tilingual Correlation In Proceedings of the 14th Conference of the European Chapter
of the Association for Computational Linguistics pp 462471 Gothenburg Sweden
Association for Computational Linguistics
Filippova K Alfonseca E Colmenares C A Kaiser L  Vinyals O 2015 Sentence
Compression by Deletion with LSTMs
In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing pp 360368 Lisbon Portugal
Association for Computational Linguistics
Gal Y  Ghahramani Z 2015 Dropout as a Bayesian Approximation Representing
Model Uncertainty in Deep Learning arXiv150602142 cs stat
Gao J Pantel P Gamon M He X  Deng L 2014 Modeling Interestingness with
Deep Neural Networks In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing EMNLP pp 213 Doha Qatar Association for
Computational Linguistics
Gimenez J  Marquez L 2004 SVMTool A general POS tagger generator based on
Support Vector Machines In Proceedings of the 4th LREC Lisbon Portugal
Glorot X  Bengio Y 2010 Understanding the diculty of training deep feedforward
neural networks In International conference on articial intelligence and statistics
pp 249256
Glorot X Bordes A  Bengio Y 2011 Deep sparse rectier neural networks
International Conference on Articial Intelligence and Statistics pp 315323
Goldberg Y  Elhadad M 2010 An Ecient Algorithm for Easy-First Non-Directional
Dependency Parsing In Human Language Technologies The 2010 Annual Conference
of the North American Chapter of the Association for Computational Linguistics pp
742750 Los Angeles California Association for Computational Linguistics
Goldberg Y  Levy O 2014 word2vec Explained deriving Mikolov et als negative-
sampling word-embedding method arXiv14023722 cs stat
Goldberg Y  Nivre J 2013 Training Deterministic Parsers with Non-Deterministic
Oracles Transactions of the Association for Computational Linguistics 1 0 403
Goldberg Y Zhao K  Huang L 2013 Ecient Implementation of Beam-Search
Incremental Parsers In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics Volume 2 Short Papers pp 628633 Soa Bulgaria
Association for Computational Linguistics
Goller C  Kuchler A 1996 Learning Task-Dependent Distributed Representations
by Backpropagation Through Structure In In Proc of the ICNN-96 pp 347352
Graves A 2008 Supervised sequence labelling with recurrent neural networks PhD
thesis Technische Universitat Munchen
Gre K Srivastava R K Koutnk J Steunebrink B R  Schmidhuber J 2015
LSTM A Search Space Odyssey arXiv150304069 cs
Hal Daume III Langford J  Marcu D 2009 Search-based Structured Prediction
Machine Learning Journal MLJ
Harris Z 1954 Distributional Structure Word 10 23 146162
Hashimoto K Miwa M Tsuruoka Y  Chikayama T 2013 Simple Customization
of Recursive Neural Networks for Semantic Relation Classication
In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing pp
13721376 Seattle Washington USA Association for Computational Linguistics
He K Zhang X Ren S  Sun J 2015 Delving Deep into Rectiers Surpassing
Human-Level Performance on ImageNet Classication arXiv150201852 cs
Henderson M Thomson B  Young S 2013 Deep Neural Network Approach for the
Dialog State Tracking Challenge In Proceedings of the SIGDIAL 2013 Conference
pp 467471 Metz France Association for Computational Linguistics
Hermann K M  Blunsom P 2013 The Role of Syntax in Vector Space Models of
Compositional Semantics
In Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics Volume 1 Long Papers pp 894904 Soa
Bulgaria Association for Computational Linguistics
Hermann K M  Blunsom P 2014 Multilingual Models for Compositional Distributed
Semantics In Proceedings of the 52nd Annual Meeting of the Association for Com-
putational Linguistics Volume 1 Long Papers pp 5868 Baltimore Maryland
Association for Computational Linguistics
Hihi S E  Bengio Y 1996 Hierarchical Recurrent Neural Networks for Long-Term
Dependencies In Touretzky D S Mozer M C  Hasselmo M E Eds Advances
in Neural Information Processing Systems 8 pp 493499 MIT Press
Hinton G E Srivastava N Krizhevsky A Sutskever I  Salakhutdinov R R
2012 Improving neural networks by preventing co-adaptation of feature detectors
arXiv12070580 cs
Hochreiter S  Schmidhuber J 1997 Long short-term memory Neural computation
9 8 17351780
Hornik K Stinchcombe M  White H 1989 Multilayer feedforward networks are
universal approximators Neural Networks 2 5 359366
Irsoy O  Cardie C 2014 Opinion Mining with Deep Recurrent Neural Networks
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing EMNLP pp 720728 Doha Qatar Association for Computational Lin-
guistics
Iyyer M Boyd-Graber J Claudino L Socher R  Daume III H 2014a A Neural
Network for Factoid Question Answering over Paragraphs In Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing EMNLP pp
633644 Doha Qatar Association for Computational Linguistics
Iyyer M Enns P Boyd-Graber J  Resnik P 2014b Political Ideology Detection
Using Recursive Neural Networks In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics Volume 1 Long Papers pp 11131122
Baltimore Maryland Association for Computational Linguistics
Iyyer M Manjunatha V Boyd-Graber J  Daume III H 2015 Deep Unordered
Composition Rivals Syntactic Methods for Text Classication In Proceedings of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing Volume 1 Long Pa-
pers pp 16811691 Beijing China Association for Computational Linguistics
Johnson R  Zhang T 2014 Eective Use of Word Order for Text Categorization with
Convolutional Neural Networks arXiv14121058 cs stat
Johnson R  Zhang T 2015 Eective Use of Word Order for Text Categorization with
Convolutional Neural Networks In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computational Linguistics Human Lan-
guage Technologies pp 103112 Denver Colorado Association for Computational
Linguistics
Jozefowicz R Zaremba W  Sutskever I 2015 An Empirical Exploration of Recur-
rent Network Architectures In Proceedings of the 32nd International Conference on
Machine Learning ICML-15 pp 23422350
Kalchbrenner N Grefenstette E  Blunsom P 2014 A Convolutional Neural Network
for Modelling Sentences In Proceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics Volume 1 Long Papers pp 655665 Baltimore
Maryland Association for Computational Linguistics
Karpathy A Johnson J  Li F-F 2015 Visualizing and Understanding Recurrent
Networks arXiv150602078 cs
Kim Y 2014 Convolutional Neural Networks for Sentence Classication In Proceed-
ings of the 2014 Conference on Empirical Methods in Natural Language Processing
EMNLP pp 17461751 Doha Qatar Association for Computational Linguistics
Kingma D  Ba J
2014
Adam A Method for Stochastic Optimization
arXiv14126980 cs
Krizhevsky A Sutskever I  Hinton G E 2012 ImageNet Classication with Deep
Convolutional Neural Networks In Pereira F Burges C J C Bottou L  Wein-
berger K Q Eds Advances in Neural Information Processing Systems 25 pp
10971105 Curran Associates Inc
Kudo T  Matsumoto Y 2003 Fast Methods for Kernel-based Text Analysis
Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -
Volume 1 ACL 03 pp 2431 Stroudsburg PA USA Association for Computational
Linguistics
Le P  Zuidema W 2014 The Inside-Outside Recursive Neural Network model for
Dependency Parsing In Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing EMNLP pp 729739 Doha Qatar Association for
Computational Linguistics
Le P  Zuidema W 2015 The Forest Convolutional Network Compositional Distri-
butional Semantics with a Neural Chart and without Binarization
In Proceedings
of the 2015 Conference on Empirical Methods in Natural Language Processing pp
11551164 Lisbon Portugal Association for Computational Linguistics
Le Q V Jaitly N  Hinton G E 2015 A Simple Way to Initialize Recurrent Networks
of Rectied Linear Units arXiv150400941 cs
LeCun Y  Bengio Y 1995 Convolutional Networks for Images Speech and Time-
Series In Arbib M A Ed The Handbook of Brain Theory and Neural Networks
MIT Press
LeCun Y Bottou L Orr G  Muller K 1998a Ecient BackProp In Orr G 
K M Eds Neural Networks Tricks of the trade Springer
Lecun Y Bottou L Bengio Y  Haner P 1998b Gradient Based Learning Applied
to Pattern Recognition
LeCun Y Chopra S Hadsell R Ranzato M  Huang F 2006 A tutorial on energy-
based learning Predicting structured data 1 0
LeCun Y  Huang F 2005 Loss functions for discriminative training of energybased
models AIStats
Levy O  Goldberg Y 2014a Dependency-Based Word Embeddings In Proceedings of
the 52nd Annual Meeting of the Association for Computational Linguistics Volume
2 Short Papers pp 302308 Baltimore Maryland Association for Computational
Linguistics
Levy O  Goldberg Y 2014b Neural Word Embedding as Implicit Matrix Factoriza-
tion In Ghahramani Z Welling M Cortes C Lawrence N D  Weinberger
K Q Eds Advances in Neural Information Processing Systems 27 pp 21772185
Curran Associates Inc
Levy O Goldberg Y  Dagan I 2015
Improving Distributional Similarity with
Lessons Learned from Word Embeddings Transactions of the Association for Com-
putational Linguistics 3 0 211225
Lewis M  Steedman M 2014 Improved CCG Parsing with Semi-supervised Supertag-
ging Transactions of the Association for Computational Linguistics 2 0 327338
Li J Li R  Hovy E 2014 Recursive Deep Models for Discourse Parsing In Proceed-
ings of the 2014 Conference on Empirical Methods in Natural Language Processing
EMNLP pp 20612069 Doha Qatar Association for Computational Linguistics
Ling W Dyer C Black A W  Trancoso I 2015a TwoToo Simple Adaptations of
Word2Vec for Syntax Problems In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computational Linguistics Human Lan-
guage Technologies pp 12991304 Denver Colorado Association for Computational
Linguistics
Ling W Dyer C Black A W Trancoso I Fermandez R Amir S Marujo L 
Luis T 2015b Finding Function in Form Compositional Character Models for
Open Vocabulary Word Representation In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing pp 15201530 Lisbon Portugal
Association for Computational Linguistics
Liu Y Wei F Li S Ji H Zhou M  WANG H 2015 A Dependency-Based Neural
Network for Relation Classication In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing Volume 2 Short Papers pp 285290 Beijing
China Association for Computational Linguistics
Ma J Zhang Y  Zhu J 2014 Tagging The Web Building A Robust Web Tagger
with Neural Network
In Proceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics Volume 1 Long Papers pp 144154 Baltimore
Maryland Association for Computational Linguistics
Ma M Huang L Zhou B  Xiang B 2015 Dependency-based Convolutional Neural
Networks for Sentence Embedding In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing Volume 2 Short Papers pp 174179 Beijing
China Association for Computational Linguistics
McCallum A Freitag D  Pereira F C 2000 Maximum Entropy Markov Models for
Information Extraction and Segmentation In ICML Vol 17 pp 591598
Mikolov T Chen K Corrado G  Dean J 2013 Ecient Estimation of Word
Representations in Vector Space arXiv13013781 cs
Mikolov T Joulin A Chopra S Mathieu M  Ranzato M 2014 Learning Longer
Memory in Recurrent Neural Networks arXiv14127753 cs
Mikolov T Karaat M Burget L Cernocky J  Khudanpur S 2010 Recurrent
neural network based language model In INTERSPEECH 2010 11th Annual Con-
ference of the International Speech Communication Association Makuhari Chiba
Japan September 26-30 2010 pp 10451048
Mikolov T Kombrink S Lukas Burget Cernocky J H  Khudanpur S 2011 Ex-
tensions of recurrent neural network language model In Acoustics Speech and Signal
Processing ICASSP 2011 IEEE International Conference on pp 55285531 IEEE
Mikolov T Sutskever I Chen K Corrado G S  Dean J 2013 Distributed Rep-
resentations of Words and Phrases and their Compositionality In Burges C J C
Bottou L Welling M Ghahramani Z  Weinberger K Q Eds Advances in
Neural Information Processing Systems 26 pp 31113119 Curran Associates Inc
Mikolov T 2012 Statistical language models based on neural networks PhD thesis Ph
D thesis Brno University of Technology
Mnih A  Kavukcuoglu K 2013 Learning word embeddings eciently with noise-
contrastive estimation In Burges C J C Bottou L Welling M Ghahramani Z
 Weinberger K Q Eds Advances in Neural Information Processing Systems 26
pp 22652273 Curran Associates Inc
Mrksic N O Seaghdha D Thomson B Gasic M Su P-H Vandyke D Wen T-H
 Young S 2015 Multi-domain Dialog State Tracking using Recurrent Neural
Networks In Proceedings of the 53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint Conference on Natural Language
Processing Volume 2 Short Papers pp 794799 Beijing China Association for
Computational Linguistics
Neidinger R 2010
Introduction to Automatic Dierentiation and MATLAB Object-
Oriented Programming SIAM Review 52 3 545563
Nguyen T H  Grishman R 2015 Event Detection and Domain Adaptation with
Convolutional Neural Networks
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing Volume 2 Short Papers pp 365371 Beijing
China Association for Computational Linguistics
Nivre J 2008 Algorithms for Deterministic Incremental Dependency Parsing Compu-
tational Linguistics 34 4 513553
Okasaki C 1999 Purely Functional Data Structures Cambridge University Press Cam-
bridge UK New York
Pascanu R Mikolov T  Bengio Y 2012 On the diculty of training Recurrent
Neural Networks arXiv12115063 cs
Pei W Ge T  Chang B 2015 An Eective Neural Network Model for Graph-based
Dependency Parsing In Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing Volume 1 Long Papers pp 313322 Beijing China Associa-
tion for Computational Linguistics
Pennington J Socher R  Manning C 2014 Glove Global Vectors for Word Rep-
resentation In Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing EMNLP pp 15321543 Doha Qatar Association for Com-
putational Linguistics
Pollack J B 1990 Recursive Distributed Representations Articial Intelligence 46
77105
Polyak B T 1964 Some methods of speeding up the convergence of iteration methods
USSR Computational Mathematics and Mathematical Physics 4 5 1  17
Qian Q Tian B Huang M Liu Y Zhu X  Zhu X 2015 Learning Tag Embeddings
and Tag-specic Composition Functions in Recursive Neural Network In Proceedings
of the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing Volume 1 Long
Papers pp 13651374 Beijing China Association for Computational Linguistics
Rong X 2014 word2vec Parameter Learning Explained arXiv14112738 cs
Rumelhart D E Hinton G E  Williams R J 1986 Learning representations by
back-propagating errors Nature 323 6088 533536
Santos C D  Zadrozny B 2014 Learning Character-level Representations for Part-
of-Speech Tagging pp 18181826
Schuster M  Paliwal K K 1997 Bidirectional recurrent neural networks
Transactions on Signal Processing 45 11 26732681
Shawe-Taylor J  Cristianini N 2004 Kernel Methods for Pattern Analysis Cambridge
University Press
Smith N A 2011 Linguistic Structure Prediction Synthesis Lectures on Human Lan-
guage Technologies Morgan and Claypool
Socher R 2014 Recursive Deep Learning For Natural Language Processing and Computer
Vision PhD thesis Stanford University
Socher R Bauer J Manning C D  Andrew Y N 2013 Parsing with Compositional
Vector Grammars In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics Volume 1 Long Papers pp 455465 Soa Bulgaria
Association for Computational Linguistics
Socher R Huval B Manning C D  Ng A Y 2012 Semantic Compositionality
through Recursive Matrix-Vector Spaces In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning pp 12011211 Jeju Island Korea Association for Computational
Linguistics
Socher R Lin C C-Y Ng A Y  Manning C D 2011 Parsing Natural Scenes
and Natural Language with Recursive Neural Networks In Getoor L  Scheer T
Eds Proceedings of the 28th International Conference on Machine Learning ICML
2011 Bellevue Washington USA June 28 - July 2 2011 pp 129136 Omnipress
Socher R Manning C  Ng A 2010 Learning Continuous Phrase Representations
and Syntactic Parsing with Recursive Neural Networks In Proceedings of the Deep
Learning and Unsupervised Feature Learning Workshop of NIPS 2010 pp 19
Socher R Perelygin A Wu J Chuang J Manning C D Ng A  Potts C 2013
Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing pp 16311642 Seattle Washington USA Association for Computational
Linguistics
Sordoni A Galley M Auli M Brockett C Ji Y Mitchell M Nie J-Y Gao J
 Dolan B 2015 A Neural Network Approach to Context-Sensitive Generation
of Conversational Responses
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computational Linguistics Human Lan-
guage Technologies pp 196205 Denver Colorado Association for Computational
Linguistics
Sundermeyer M Alkhouli T Wuebker J  Ney H 2014 Translation Modeling
with Bidirectional Recurrent Neural Networks In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing EMNLP pp 1425 Doha
Qatar Association for Computational Linguistics
Sundermeyer M Schluter R  Ney H 2012 LSTM Neural Networks for Language
Modeling In INTERSPEECH
Sutskever I Martens J Dahl G  Hinton G 2013 On the importance of initialization
and momentum in deep learning In Proceedings of the 30th international conference
on machine learning ICML-13 pp 11391147
Sutskever I Martens J  Hinton G E 2011 Generating text with recurrent neural
networks In Proceedings of the 28th International Conference on Machine Learning
ICML-11 pp 10171024
Sutskever I Vinyals O  Le Q V V 2014 Sequence to Sequence Learning with
Neural Networks In Ghahramani Z Welling M Cortes C Lawrence N D 
Weinberger K Q Eds Advances in Neural Information Processing Systems 27 pp
31043112 Curran Associates Inc
Tai K S Socher R  Manning C D 2015 Improved Semantic Representations From
Tree-Structured Long Short-Term Memory Networks In Proceedings of the 53rd An-
nual Meeting of the Association for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Processing Volume 1 Long Papers
pp 15561566 Beijing China Association for Computational Linguistics
Tamura A Watanabe T  Sumita E 2014 Recurrent Neural Networks for Word
Alignment Model
In Proceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics Volume 1 Long Papers pp 14701480 Baltimore
Maryland Association for Computational Linguistics
Tieleman T  Hinton G 2012 Lecture 65RmsProp Divide the gradient by a running
average of its recent magnitude COURSERA Neural Networks for Machine Learning
Van de Cruys T 2014 A Neural Network Approach to Selectional Preference Acquisi-
tion In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan-
guage Processing EMNLP pp 2635 Doha Qatar Association for Computational
Linguistics
Vaswani A Zhao Y Fossum V  Chiang D 2013 Decoding with Large-Scale Neu-
ral Language Models Improves Translation In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing pp 13871392 Seattle Washing-
ton USA Association for Computational Linguistics
Wager S Wang S  Liang P S 2013 Dropout Training as Adaptive Regularization
In Burges C J C Bottou L Welling M Ghahramani Z  Weinberger K Q
Eds Advances in Neural Information Processing Systems 26 pp 351359 Curran
Associates Inc
Wang P Xu J Xu B Liu C Zhang H Wang F  Hao H 2015a Semantic Cluster-
ing and Convolutional Neural Network for Short Text Categorization In Proceedings
of the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing Volume 2 Short
Papers pp 352357 Beijing China Association for Computational Linguistics
Wang X Liu Y SUN C Wang B  Wang X 2015b Predicting Polarities of Tweets
by Composing Word Embeddings with Long Short-Term Memory In Proceedings of
the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing Volume 1 Long
Papers pp 13431353 Beijing China Association for Computational Linguistics
Watanabe T  Sumita E 2015 Transition-based Neural Constituent Parsing In Pro-
ceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing Volume
1 Long Papers pp 11691179 Beijing China Association for Computational Lin-
guistics
Weiss D Alberti C Collins M  Petrov S 2015 Structured Training for Neural
Network Transition-Based Parsing In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing Volume 1 Long Papers pp 323333 Beijing
China Association for Computational Linguistics
Werbos P J 1990 Backpropagation through time What it does and how to do it
Proceedings of the IEEE 78 10 1550  1560
Weston J Bordes A Yakhnenko O  Usunier N 2013 Connecting Language and
Knowledge Bases with Embedding Models for Relation Extraction
In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing pp
13661371 Seattle Washington USA Association for Computational Linguistics
Xu W Auli M  Clark S 2015 CCG Supertagging with a Recurrent Neural Network
In Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference on Natural Language Processing
Volume 2 Short Papers pp 250255 Beijing China Association for Computational
Linguistics
Yin W  Schutze H 2015 Convolutional Neural Network for Paraphrase Identication
In Proceedings of the 2015 Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics Human Language Technologies pp 901911
Denver Colorado Association for Computational Linguistics
Zaremba W Sutskever I  Vinyals O 2014 Recurrent Neural Network Regularization
arXiv14092329 cs
Zeiler M D 2012 ADADELTA An Adaptive Learning Rate Method arXiv12125701
Zeng D Liu K Lai S Zhou G  Zhao J 2014 Relation Classication via Convolu-
tional Deep Neural Network In Proceedings of COLING 2014 the 25th International
Conference on Computational Linguistics Technical Papers pp 23352344 Dublin
Ireland Dublin City University and Association for Computational Linguistics
Zhou H Zhang Y Huang S  Chen J 2015 A Neural Probabilistic Structured-
Prediction Model for Transition-Based Dependency Parsing
In Proceedings of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing Volume 1 Long Pa-
pers pp 12131222 Beijing China Association for Computational Linguistics
Zhu C Qiu X Chen X  Huang X 2015a A Re-ranking Model for Dependency
Parser with Recursive Convolutional Neural Network
In Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics and the 7th Inter-
national Joint Conference on Natural Language Processing Volume 1 Long Papers
pp 11591168 Beijing China Association for Computational Linguistics
Zhu X Sobhani P  Guo H 2015b Long Short-Term Memory Over Tree Structures
arXiv150304881 cs
