Shannon Capacity of Signal Transduction for
Multiple Independent Receptors
Dept of Mathematics Applied Mathematics and Statistics
Dept of Electrical Engineering and Computer Science
Peter J Thomas
Andrew W Eckford
Dept of Electrical Engineering and Computer Science
Dept of Biology
CWRU Cleveland Ohio USA
Email pjthomascaseedu
York University
Toronto Ontario Canada
Email aeckfordyorkuca
AbstractCyclic adenosine monophosphate cAMP is consid-
ered a model system for signal transduction the mechanism by
which cells exchange chemical messages Our previous work
calculated the Shannon capacity of a single cAMP receptor
however a typical cell may have thousands of receptors operating
in parallel In this paper we calculate the capacity of a cAMP
signal transduction system with an arbitrary number of inde-
pendent indistinguishable receptors By leveraging prior results
on feedback capacity for a single receptor we show somewhat
unexpectedly that the capacity is achieved by an IID input
distribution and that the capacity for n receptors is n times
the capacity for a single receptor
I INTRODUCTION
In multicellular organisms specialized cells must communi-
cate with one another in order to coordinate their action The
means by which a cell receives such signals is known as signal
transduction numerous receptors on the cells surface bind to
signal-bearing molecules known as ligands a bound receptor
then relays the signal across the cell wall by producing second
messengers which induce the cell to act The instructions
encoded in signal transduction may govern tasks such as cell
growth apoptosis differentiation and many others
The Dictyostelium amoeba has on the order of 80000 recep-
tors uniformly distributed across its cell membrane which are
believed to act independently to transduce the cyclic adenosine
monophosphate cAMP signal 1 We recently introduced a
nite state channel model based on ligand-receptor binding
the BIND channel 2 3 for which we 1 rigorously
obtained the capacity in a discrete time setting 2 showed
that the capacity is achieved by IID inputs in discrete time
and 3 obtained a non-rigorous asymptotic expression for
the mutual information rate in continuous time
There is a long history of research at the intersection of
information theory and biology including notable work by
Attneave 4 and Barlow 5 on sensory systems Yockey
6 on ionizing radiation and mutagenesis and Berger 7
on the efciency of organ systems Recent progress on the
computational and mathematical aspects of biology has led to
This work was in part supported by NSF grant DMS-1413770 and the
Natural Sciences and Engineering Research Council NSERC The authors
thank the ve anonymous referees for their insightful comments only some
of which could be addressed in the space available
a surge of interest in biological information theory in general
8 and in information-theoretic analysis of signal transduction
in particular eg 914 There has been much recent and
related work on information-theoretic tools with application to
biological channels such as unit output memory channels 15
an example of which is the Previous Output is the STate
POST channel 16 a type of Markov channel that may be
used to model receptors in signal transduction A recent paper
17 considers generalizations of the BIND channel to the case
of multiple receptors
In the present paper we extend our previous results on
the single receptor case to systems with multiple receptors
where these receptors are indistinct independent and statisti-
cally identical As in earlier work we consider discrete-time
Markov models as an approximation of the receptor kinetics
modeled in continuous time with the master equation thus we
analyze the case where the discrete time step   0 Our main
result is to show somewhat unexpectedly that the capacity
for n  1 receptors is achieved by an IID input distribution
as   0 while the capacity for n receptors is n times the
capacity for a single receptor Our analysis provides a closed
form solution for the mutual information rate valid for all n
that complements the asymptotic results and capacity bounds
obtained in 17
II SYSTEM MODEL
A Model for a single receptor
For a single receptor we use a nite-state Markov channel
model 18 identical to those described in 2 3 The cAMP
receptor has two states it may be unbound awaiting the arrival
of a cAMP molecule or it may be bound to cAMP transducing
the signal into second messengers We refer to these states
as U and B respectively There exist far more complicated
receptors with larger state spaces an advantage of analyzing
cAMP is its simplicity
For an individual receptor let pUt denote the probability
that the receptor is in state U at time t resp pBt in state
B It is known that this probability evolves according to a
Fig 1
State transition diagram for a single cAMP receptor with discrete
time step  arrows are marked with their transition probabilities The input-
sensitive state transition is indicated by a bold arrow See also 20
differential equation pair see also 2 19
 kctpUt  kpBt
 kctpUt  kpBt
where ct is the concentration of cAMP and k and k
are rate constants corresponding to the U  B and U  B
reactions respectively Following the principle of mass action
U  B requires a cAMP molecule
therefore its rate is
proportional to ct however U  B requires no external
molecules and its rate is therefore independent of ct
The model in 1-2 can be approximated by a discrete-time
Markov chain and we take advantage of this discretization
in obtaining our results We assume that the concentration
ct is binary ct  L H where L is the lowest possible
concentration and H is the highest possible concentration1 Let
H  kH L  kL and   k further let  represent
a discrete time step The discrete-time approximation for the
differential equation pair 1-2 is given by
pUt     1   HLpUt   pBto 
pBt      HLpUt  1   pBto 
where HL should be replaced with either H or L
depending on the concentration and f  o  means
lim0f    0 Neglecting terms o  the channel
state can be represented as a discrete-time Markov chain with
transition probability matrix
cid20 1   HL
cid21
 HL
1   
potentially be bound or unbound at any one time we require
that the individual receptor binding or unbinding probabilities
n For a given set
be sufciently small ie max H    1
of reaction rates this condition can be met by reducing the
size of  the discrete time increment Therefore for systems
with large numbers of receptors the continuous time setting is
ultimately more natural than discrete time moreover molecu-
lar communication systems do not generally have access to
a reference clock as is normally the case in macroscopic
engineered systems Following our previous work however
we begin the analysis assuming a small discrete time step
and later consider the   0 limit of our mutual information
results
is a sequence of ligand
concentrations Xi  L H The output also the state is
the number of bound receptors Yi  0 1     n The state
kk1  nk HL 0  k  n1
kk1  k  1  k  n see the next section for
more details We require  n maxH   1 This channel
which we call the BINDnL H  channel is a member
of the set of Chen-Berger unit output memory channels 15
For all such channels the feedback-capacity-achieving input
distribution has the form
transitions obeycid0PHL
andcid0PHL
Channel denition The input
cid1
cid1
pX mY m 
pXiYi1 xi  yi1
where  represents causal conditioning further y0 is null That
is each input is dependent only on the previous state of the
channel and no other past inputs or states
Now consider an encoding scheme exploiting feedback in
which the probability of sending input H when the channel is
in state k is pk  pXiYi1H  k for 0  k  n When the
channel is in the fully bound state Y  n each individual
receptor releases its ligand with probability  independent of
the input concentration so the choice of pn has no effect on
information transmission see also 3 The capacity therefore
requires optimizing over the n free parameters p0     pn1
mcid89
The state transition diagram for cAMP is given in Figure 1
A Mutual information for two receptors
III RESULTS
B Multiple receptors
Suppose we have n identical independent receptors each
with individual binding probabilities  L  H and   as
dened above We consider therefore a model with n  1
distinct states representing a population of n indistinguishable
receptors state k refers to the system with k out of n receptors
bound to signaling ligand molecules If each receptor binds
or unbinds signaling molecules independently of the other
receptors then the state transition probabilities are as given
in Figure 2 From the gure because all n receptors could
1In 2 we show that binary inputs achieve capacity for a single receptor
discrete time case We expect this result to hold for an arbitrary number of
receptors but in this paper we restrict attention to binary input distributions
for simplicity
First we consider the case n  2 in detail and then
generalize to arbitrary n With two receptors we have
2 HL
cid10
 HL
1 cid10 2
Thus we have a transition probability matrix
 1  2 HL
2 HL
1     HL
 HL
1  2 
 8
Using the capacity-achieving input distribution
the output
states Y n form a Markov chain letting k  L  H 
n HL
cid10
n  1 HL
cid10
n  k HL
cid10
k  1 
 n  1 cid10 n
 HL
Fig 2
State transition diagram for n independent indistinguishable cAMP receptors obeying mass-action kinetics
Lpk
 1  2 0
 9
2 0
1     1
1  2 
Thus with binary inputs H L the state is completely described
by two parameters represented by pk for k  0 1 For k 
2 the fully bound case there is no information transmission
The steady-state distribution of Y is given by the Perron-
Frobenius eigenvector Letting i  PrY  i
0  PrY  0 
1  PrY  1 
2  PrY  2 
2  20  01
2  20  01
2  20  01
The mutual information rate is given by
IX Y   lim
m
1  Y m
 HYi  Yi1  HYi  Xi Yi1 for arbitrary i
Let p represent the partial entropy function where
p 
p log p p cid54 0
we use natural
logarithms throughout so information is
measured in nats Further let H3p q represent the triple
entropy function where
H3p q  p  q  1  p  q
dened for p  q  1 Note that H3p 0 reduces to the
binary entropy function Then
HYi  Yi1  0  H32 0 0
HYi  Yi1  1  H3 1  
HYi  Yi1  2  H32  0
cid26
B As   0 capacity-achieving input distribution is IID
In order for the capacity-achieving distribution to be IID
it must be true that 19 is maximized with p0  p1 which
implies 0  1 It can be shown via numerical examples
that the capacity-achieving input distribution is not IID for
arbitrary values of L H and  and nite   0 However
we are interested in the limiting case where   0
The reader may check that the state occupancy probabilities
i are independent of  However 19 becomes
IX Y  
 p0H32 H 0  1  p0H32 L 0
cid16H32 0 0
cid16H3 1  
cid17
cid17
 p1H3 H    1  p1H3 L  
Although terms such as H32 0 0 diverge as   0 the
divergent terms cancel in 20 and IX Y  remains nite in
the limit Straightforward application of lHopitals rule yields
cid16H32 0 0
cid16
cid16H3 1  
cid16
 p0H32 H 0  1  p0H32 L 0
0  p0H  1  p0L
 p1H3 H    1  p1H3 H  
1  p1H  1  p1L
cid17
cid17
cid17
cid17
Finally letting Z  2  20  01
cid16
HYiXi Yi1  0  p0H32 H 0  1  p0H32 L 0
HYiXi Yi1  1  p1H3 H    1  p1H3 L  
HYiXi Yi1  2  p2H32  0  1  p2H32  0
In the preceding equation we can reduce to HYiXi Yi1 
2  H32  0 the same as HYi  Yi1  2 in 18
Finally
IX Y  
cid16H32 0 0  p0H32 H 0  1  p0H32 L 0
cid17
cid16H3 1    p1H3 H    1  p1H3 L  
cid17
IX Y  
0  01
 p0  0p1H
 0    p0  0p1L
cid17
To proceed we assume that the input distribution is IID
and show that this is optimal Under the IID constraint we
have p0  p1  p which leads to several simplications The
stationary probabilities become binomial writing   0 
1  2
cid182
cid19 k2k
  2  and Z    2
kp 
and the continuous time information rate reduces to
cid19
IX Y pkp
cid18 
  
  pH  1  pL 
The IID capacity of this channel is given by
CIID  max
IX Y pkp
cid18 
cid19
 2 max
  
  pH  1  pL 
Thus we may state
Proposition 1 For two receptors as   0 the capacity-
achieving input distribution is IID
Proof Let C represent the capacity If we take CIID2
in 27 we obtain exactly the capacity for a single receptor as
  0 from 2 3
We know that C  CIID The capacity two independent
receptors can be no greater than twice the capacity of a single
receptor so C  2CIID2  CIID Since C is bounded
above and below by CIID the result follows
We give a numerical example of this result in Figure 3
where the global maximum of the MI rate indeed lies on the
diagonal p0  p1  p
Fig 3 Mutual information rate for two independent receptors with feedback
as a function of the high-input probability when unbound p0 and when singly
bound p1 The high-input probability when the receptor is doubly bound p2
does not affect the MI rate For this example the continuous time transition
rates are L  1 H  10   20 All rates are in Hz The optimal MI rate
of 357367 nats per second is attained when p0  p1  0371696 Diagonal
inside the zoomed-in box marks the line p0  p1
In 2 3 we showed rigorously that the feedback capacity
and the IID capacity for the single receptor were identical
this result held for all values of   0 ie all settings of the
parameters On the other hand the feedback capacity for the
two receptors cannot exceed twice the feedback capacity of a
single receptor when the receptors are independent because
we could consider the two independent receptors separately
Therefore the system with two identical independent receptors
inherits the property that feedback capacity  IID capacity 
capacity
C Generalizing to n  2
Returning to arbitrary n  2 we adopt
the following
notation Let kpk  L  pkH  L be the average
per capita transition rate from state k to state k  1 Dene
cid26 n
cid0n
ncid88
cid1nkcid81k1
j0 jpj 1  k  n
Zp0     pn1 
cid52
 PrY  k  AkZ is the stationary probability
Then k
of the k-bound state If we take pk  p for all k the case
of IID inputs then we have the same per capita binding rate
in each state k    L  pH  L In this case the
stationary distribution is binomial
cid18n
cid19cid18 
cid19kcid18 
cid19nk
  
  
where    is the equilibrium probability of any given
receptor being in the bound state
Fixing some n  1 we write Ip the average mutual
information rate per time step for IL H  p0     pn1
ie we leave the dependence on the transition probabilities
cid124 represents the
implicit The vector p  p0 p1     pn1
high versus low input probabilities for the feedback case
The IID case corresponds to p  1p  p p     p
cid124 Ip
may be written as a sum over the information rates contributed
by each edge I cid80n1
k0 Ik cid80n1
k0 kk where
cid17
k  n  kk
cid16
n  kpk
1  pkn  kL  pkn  kH
pH1pL
with p  
Here we have used the product rule property of the partial
entropy kp  kp  pk
pH 1pL
cid17cid16
cid16
cid17
As in the case of n  2 receptors the IID case simplies
I1p 
kn  kp  p
n1cid88
cid18
 p
cid19 knk
cid18n
n1cid88
  n n  k
cid18 
cid19
  
cid19
n  n
  
 np
which is n times the mutual information rate for a single
receptor This leads to the following
Proposition 2 For n receptors as   0 capacity C is n
times the single-receptor capacity and the capacity-achieving
input distribution is IID
Proof By a similar argument to Proposition 1 consider-
ing n independent distinguishable receptors receiving the same
032034036038040042032034036038040042p0p10011042042032032037037input signal it is clear that the n-receptor information rate
with feedback cannot exceed n times the single receptor rate
with feedback Therefore for n indistinguishable receptors
CIID  C for arbitrary n and the result follows
In contrast non-independent receptors can have feedback
capacity greater than IID capacity Fig 4 shows this effect for
a channel for which ligand binding is cooperative instead of
independent eg with transition probability matrix cf 8
 1   HL
  34
 HL
1     HL
 HL
1   
Fig 4 Mutual information rate with feedback for two receptors as a
function p0 and p1 when the receptors are not independent Total transition
rates i  i  1 iH  10Hz or iL  1Hz i  i  1 i  20Hz
The optimal MI rate of 21026 nats per second is attained when p0  0407
p1  0364 Zoom-in box and p0  p1 line positions same as Fig 3
IV DISCUSSION
Analysis of the single-receptor BIND channel introduced in
2 3 turns on the observation that it falls in the class of
channels satisfying the Chen-Berger condition 15 21 At
the same time it may be viewed as an instance of a POST
channel Previous Output is the STate 16 Here we show that
a ligand-binding system comprising n identical independent
receptors also satises these conditions The n-receptor BIND
channel has a mutual information rate equal to n times the
mutual information rate of the single receptor BIND channel
and its capacity is realized by an IID input source with optimal
high concentration probability p that is the same for one
receptor as it is for n receptors Under the IID input condition
the receptors stationary state distribution becomes binomial
suggesting a relation to the well-studied binomial channel 22
As discussed in 2 the physical channel model breaks down
as   0 in the sense that the concentration at the receiver
will not remain IID at arbitrarily ne time scales The way in
which biophysical constraints restrict the input ensemble will
be system specic and a topic for future investigations
REFERENCES
1 T Jin N Zhang Y Long C A Parent and P N Devreotes Local-
ization of the G protein  complex in living cells during chemotaxis
Science vol 287 pp 10341036 2000
2 P J Thomas and A W Eckford Capacity of a simple intercellular
signal transduction channel httparxivorgabs14111650 2015
3 A Eckford and P Thomas Capacity of a simple intercellular signal
transduction channel in Proc IEEE Intl Symp on Information Theory
ISIT pp 18341838 2013
4 F Attneave Some informational aspects of visual perception Psycho-
logical review vol 61 no 3 p 183 1954
5 H B Barlow Sensory Communication MIT Press ch 13 Possible
principles underlying the transformations of sensory messages pp 217
234 1961
6 H P Yockey A study of aging thermal killing and radiation damage
by information theory in Symposium on Information Theory in Biology
H P Yockey R P Platzman and H Quastler Eds New York London
Pergamon Press pp 297316 1958
7 T Berger Rate distortion theory Mathematical basis for data compres-
sion Prentice Hall 1971
8 U Mitra and A W Eckford Editorial Inaugural issue of the IEEE
Transactions on Molecular Biological and Multi-Scale Communica-
tions IEEE Trans Molecular Biological and Multi-Scale Communi-
cations vol 1 no 1 pp 13 Mar 2015
9 B W Andrews and P A Iglesias An information-theoretic char-
acterization of the optimal gradient sensing response of cells PLoS
Computational Biology vol 3 no 8 2007
10 P J Thomas D J Spencer S K Hampton P Park and J P Zurkus
The diffusion-limited biochemical signal-relay channel in Advances
in Neural Information Processing Systems 2003
11 J M Kimmel R M Salter and P J Thomas An information theoretic
framework for eukaryotic gradient sensing in Advances in neural
information processing systems pp 705712 2006
12 M Pierobon and I F Akyildiz Noise analysis in ligand-binding
reception for molecular communication in nanonetworks IEEE Trans
Signal Processing vol 59 no 9 pp 41684182 Sep 2011
13 A Rhee R Cheong and A Levchenko The application of information
theory to biochemical signaling systems Physical Biology vol 9 no 4
p 045011 2012
14 H Mahdavifar and A Beirami Diffusion channel with poisson recep-
tion process capacity results and applications in Proc IEEE Intl Symp
on Information Theory ISIT 2015
15 J Chen and T Berger The capacity of nite-state Markov channels
with feedback IEEE Trans Info Theory vol 51 no 3 pp 780798
Mar 2005
16 H H Permuter H Asnani and T Weissman Capacity of a POST
channel with and without feedback IEEE Trans Info Theory vol 60
no 10 pp 60416057 2014
17 M Tahmasbi and F Fekri On the capacity achieving probability
measures for molecular receivers in Proc IEEE Information Theory
Workshop pp 109113 2015
18 A Goldsmith and P Varaiya Capacity mutual information and coding
for nite-state markov channels IEEE Trans Info Theory vol 42
no 3 pp 868886 May 1996
19 D J Higham Modeling and simulating chemical reactions SIAM
review vol 50 no 2 pp 347368 2008
20 A W Eckford and P J Thomas Information theory of intercellular
signal transduction in Proc 49th Annual Asilomar Conference on
Signals Systems and Computers 2015
21 T Berger and Y Ying Characterizing optimum input output pro-
cesses for nite-state channels with feedback in Proc IEEE Intl Symp
on Information Theory ISIT p 117 2003
22 C Komninakis L Vandenberghe and R D Wesel Capacity of the
binomial channel or minimax redundancy for memoryless sources in
Proc IEEE Intl Symp on Information Theory ISIT pp 127127 2001
042042032032037037p0p10011