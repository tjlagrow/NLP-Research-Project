Applying Explanation-based Learning to Control and Speeding-up
Natural Language Generation
Gunter Neumann
DFKI GmbH
Stuhlsatzenhausweg 3
66123 Saarbrucken Germany
neumanndfkiuni-sbde
Abstract
This paper presents a method for the au-
tomatic extraction of subgrammars to con-
trol and speeding-up natural language gen-
eration NLG The method is based on
explanation-based learning EBL The main
advantage for the proposed new method
for NLG is that the complexity of the
grammatical decision making process dur-
ing NLG can be vastly reduced because
the EBL method supports the adaption of
a NLG system to a particular use of a lan-
1 Introduction
a Machine Learning
In recent years
nique known as Explanation-based Learning EBL
Mitchell Keller and Kedar-Cabelli 1986 van
Harmelen and Bundy 1988 Minton et al 1989 has
successfully been applied to control and speeding-up
natural language parsing Rayner 1988 Samuelsson
and Rayner 1991 Neumann 1994a Samuelsson
1994 Srinivas and Joshi 1995 Rayner and Carter
1996 The core idea of EBL is to transform the
derivations or explanations computed by a prob-
lem solver eg a parser to some generalized and
compact forms which can be used very eciently
for solving similar problems in the future EBL has
primarily been used for parsing to automatically spe-
cialize a given source grammar to a specic domain
In that case EBL is used as a method for adapting a
general grammar andor parser to the sub-language
dened by a suitable training corpus Rayner and
Carter 1996
A specialized grammar can be seen as describ-
ing a domain-specic set of prototypical construc-
tions Therefore the EBL approach is also very
interesting for natural language generation NLG
Informally NLG is the production of a natural
language text from computer-internal representa-
tion of information where NLG can be seen as
a complexpotentially cascadeddecision making
process Commonly a NLG system is decomposed
into two major components viz the strategic com-
ponent which decides what to say and the tacti-
cal component which decides how to say the result
of the strategic component The input of the tacti-
cal component is basically a semantic representation
computed by the strategic component Using a lexi-
con and a grammar its main task is the computation
of potentially all possible strings associated with a
semantic input Now in the same sense as EBL is
used in parsing as a means to control the range of
possible strings as well as their degree of ambigu-
ity it can also be used for the tactical component
to control the range of possible semantic input and
their degree of paraphrases
In this paper we present a novel method for the
automatic extraction of subgrammars for the control
and speeding-up of natural language generation Its
main advantage for NLG is that the complexity of
the linguistically oriented decision making process
during natural language generation can be vastly re-
duced because the EBL method supports adaption
of a NLG system to a particular language use The
core properties of this new method are
 prototypical occuring grammatical construc-
tions can automatically be extracted
 generation of these constructions is vastly sped
up using simple but ecient mechanisms
 the new method supports partial matching in
the sense that new semantic input need not be
completely covered by previously trained exam-
 it can easily be integrated with recently de-
veloped chart-based generators as described in
eg Neumann 1994b Kay 1996 Shemtov
used for machine translation Copestake et al 1996
Shemtov 19962
The method has been completely implemented
and tested with a broad-coverage HPSG-based
grammar for English see sec 5 for more details
2 Foundations
The main focus of this paper is tactical generation
ie the mapping of structures usually represent-
ing semantic information eventually decorated with
some functional features to strings using a lexicon
and a grammar Thus stated we view tactical gen-
eration as the inverse process of parsing Informally
EBL can be considered as an intelligent storage unit
of example-based generalized parts of the grammat-
ical search space determined via training by the tac-
tical generator1 Processing of similar new input is
then reduced to simple lookup and matching oper-
ations which circumvent re-computation of this al-
ready known search space
We concentrate on constraint-based grammar for-
malism following a sign-based approach consider-
ing linguistic objects ie words and phrases as
utterance-meaning associations Pollard and Sag
1994 Thus viewed a grammar is a formal state-
ment of the relation between utterances in a natu-
ral language and representations of their meanings
in some logical or other articial language where
such representations are usually called logical forms
Shieber 1993 The result of the tactical generator
is a feature structure or a set of such structures in
the case of multiple paraphrases containing among
others the input logical form the computed string
and a representation of the derivation
In our current implementation we are using TDL
a typed feature-based language and inference system
for constraint-based grammars Krieger and Schafer
1994 TDL allows the user to dene hierarchically-
ordered types consisting of type and feature con-
straints As shown later a systematic use of type
information leads to a very compact representation
of the extracted data and supports an elegant but
ecient generalization step
We are adapting a at representation of log-
ical forms as described in Kay 1996 Copestake
et al 1996 This is a minimally structured but
descriptively adequate means to represent seman-
tic information which allows for various types of
under-overspecication facilitates generation and
the specication of semantic transfer equivalences
1 In case a reversible grammar is used the parser can
even be used for processing the training corpus
Informally a at representation is obtained by
the use of extra variables which explicitly repre-
sent the relationship between the entities of a logical
form and scope information In our current system
we are using the framework called minimal recur-
sion semantics MRS described in Copestake et
al 1996 Using their typed feature structure nota-
tion gure 1 displays a possible MRS of the string
Sandy gives a chair to Kim abbreviated where
convenient
The value of the feature liszt is actually treated
like a set ie the relative order of the elements is
immaterial The feature handel is used to repre-
sent scope information and index plays much the
same role as a lambda variable in conventional rep-
resentations for more details see Copestake et al
1996
3 Overview of the method
retrieve 
instantiate
extract 
generalize 
grammar
semantic
grammatical
processor
grammar
results
Figure 3 A blueprint of the architecture
The above gure displays the overall architecture
of the EBL learning method The right-hand part
of the diagram shows the linguistic competence base
LCB and the left the EBL-based subgrammar pro-
cessing component SGP
LCB corresponds to the tactical component of a
general natural language generation system NLG In
this paper we assume that the strategic component
of the NLG has already computed the MRS repre-
sentation of the information of an underlying com-
puter program SGP consists of a training module
TM an application module AM and the subgram-
2 But note our approach does not depend on a at
representation of logical forms However in the case
of conventional representation form the mechanisms for
indexing the trained structures would require more com-
plex abstract data types see sec 4 for more details
handel h1
index e2
liszt SandyRelhhandel h4
Tohandel h12
ChairRelhhandel h10

GiveRel
handel h1
preparg x6

 TempOverhhandel h1
event e2i 
x6  KimRelhhandel h14

handel h9
restr h10
scope h11

Figure 1 The MRS of the string Sandy gives a chair to Kim
lisztDSandyRelhandel h4 
ChairRelhandel h10 
 GiveRelhandel h1 
 Tohandel h12 
 TempOverhandel h1 
 Somehandel h9 
 KimRelhandel h14 E
Figure 2 The generalized MRS of the string Sandy gives a chair to Kim
mar automatically determined by TM and applied
Briey the ow of control is as follows During
the training phase of the system a new logical form
mrs is given as input to the LCB After grammatical
processing the resulting feature structure f smrs
ie a feature structure that contains among others
the input MRS the computed string and a repre-
sentation of the derivation tree is passed to TM
TM extracts and generalizes the derivation tree of
f smrs which we call the template templmrs
of f smrs
templmrs is then stored in a deci-
sion tree where indices are computed from the MRS
found under the root of templmrs During the ap-
plication phase a new semantic input mrs is used
for the retrieval of the decision tree If a candidate
template can be found and successfully instantiated
the resulting feature structure f smrs constitutes
the generation result of mrs
Thus described the approach seems to facilitate
only exact retrieval and matching of a new seman-
tic input However before we describe how partial
matching is realized we will demonstrate in more de-
tail the exact matching strategy using the example
MRS shown in gure 1
Training phase The training module TM starts
right after the resulting feature structure f s for the
input MRS mrs has been computed
In the rst
phase TM extracts and generalizes the derivation
tree of f s called the template of f s Each node of
the template contains the rule name used in the cor-
responding derivation step and a generalization of
the local MRS A generalized MRS is the abstrac-
tion of the liszt value of a MRS where each element
only contains the lexical semantic type and han-
del information the handel information is used
for directing lexical choice see below
In our example mrs gure 2 displays the gener-
alized MRS mrsg For convenience we will use the
more compact notation
SandyRel h4 GiveRel h1
TempOver h1 Some h9
ChairRel h10 To h12 KimRel h14
Using this notation gure 4 see next page dis-
plays the template templmrs obtained from f s
Note that it memorizes not only the rule application
structure of a successful process but also the way the
grammar mutually relates the compositional parts of
the input MRS
In the next step of the training module TM the
generalized MRS mrsg information of the root node
of templmrs is used for building up an index in
a decision tree Remember that the relative order
of the elements of a MRS is immaterial For that
reason the elements of mrsg are alphabetically or-
dered so that we can treat it as a sequence when
used as a new index in the decision tree
The alphabetic ordering has two advantages
Firstly we can store dierent templates under a
common prex which allows for ecient storage and
retrieval Secondly it allows for a simple ecient
treatment of MRS as sets during the retrieval phase
of the application phase
SubjhD
SandyRel h4 GiveRel h1 TempOver h1
Some h9 ChairRel h10 To h12 KimRel h14
ProperLe
SandyRel h4
HCompNc
GiveRel h1 TempOver h1
Some h9 ChairRel h10 To h12 KimRel h14
HCompNc
GiveRel h1 TempOver h1
Some h9 ChairRel h10
To h12 KimRel h14
PrepNoModLe
To h12
ProperLe
KimRel h14
MvToDitransLe
GiveRel h1
TempOver h1
Some h9
ChairRel h10
DetSgLe
Some h9
IntrNLe
ChairRel h10
Figure 4 The template templmrs Rule names
are in bold
Application phase The application module AM
basically performs the following steps
1 Retrieval For a new MRS mrs we rst con-
struct the alphabetically sorted generalized MRS
g is then used as a path description
for traversing the decision tree For reasons we
will explain soon traversal is directed by type
subsumption Traversal is successful if mrs
been completely processed and if the end node
in the decision tree contains a template Note
that because of the alphabetic ordering the rel-
ative order of the elements of new input mrs is
immaterial
2 Expansion A successfully retrieved template
templ is expanded by deterministically applying
the rules denoted by the non-terminal elements
from the top downwards in the order specied
by templ In some sense expansion just re-plays
the derivation obtained in the past This will
result in a grammatically fully expanded fea-
ture structure where only lexical specic infor-
mation is still missing But note that through
structure sharing the terminal elements will al-
ready be constrained by syntactic information3
3 It is possible to perform the expansion step o-line
as early as the training phase in which case the applica-
tion phase can be sped up however at the price of more
memory being taken up
3 Lexical lookup From each terminal element of
the unexpanded template templ the type and
handel information is used to select the cor-
responding element from the input MRS mrs
note that in general the MRS elements of the
mrs are much more constrained than their cor-
responding elements in the generalized MRS
g The chosen input MRS element is then
used for performing lexical lookup where lexi-
cal elements are indexed by their relation name
In general this will lead to a set of lexical can-
didates
4 Lexical instantiation In the last step of the ap-
plication phase the set of selected lexical el-
ements is unied with the constraints of the
terminal elements in the order specied by the
terminal yield We also call this step terminal-
matching
In our current system terminal-
matching is performed from left to right Since
the ordering of the terminal yield is given by the
template it is also possible to follow other se-
lection strategies eg a semantic head-driven
strategy which could lead to more ecient
terminal-matching because the head element is
supposed to provide selectional restriction in-
formation for its dependents
A template together with its corresponding index
describes all sentences of the language that share
the same derivation and whose MRS are consistent
with that of the index Furthermore the index and
the MRS of a template together dene a normaliza-
tion for the permutation of the elements of a new
input MRS The proposed EBL method guarantees
soundness because retaining and applying the orig-
inal derivation in a template enforces the full con-
straints of the original grammar
Achieving more generality So far the applica-
tion phase will only be able to re-use templates for
a semantic input which has the same semantic type
information However it is possible to achieve more
generality if we apply a further abstraction step on
a generalized MRS This is simply achieved by se-
lecting a supertype of a MRS element instead of the
given specialized type
The type abstraction step is based on the stan-
dard assumption that the word-specic lexical se-
mantic types can be grouped into classes represent-
ing morpho-syntactic paradigms These classes de-
ne the upper bounds for the abstraction process In
our current system these upper bounds are directly
used as the supertypes to be considered during the
type abstraction step More precisely for each el-
ement x of a generalized MRS mrsg it is checked
whether its type Tx is subsumed by an upper bound
Ts we assume disjoint sets Only if this is the case
Ts replaces Tx in mrsg4 Applying this type abstrac-
tion strategy on the MRS of gure 1 we obtain
However it will not be able to generate a sentence
like A man gives a book to Kim since the retrieval
phase will already fail In the next section we will
show how to overcome even this kind of restriction
Named h4 ActUndPrep h1
TempOver h1 Some h9
RegNom h10 To h12 Named h14
Named h4 ActUndPrep h1 TempOver h1
Some h9 RegNom h10 To h12 Named h14
ProperLe
Named h4
HCompNc
ActUndPrep h1 TempOver h1
Some h9 RegNom h10 To h12 Named h14
HCompNc
ActUndPrep h1 TempOver h1
Some h9 RegNom h10
To h12 Name h14
PrepNoModLe
To h12
ProperLe
Name h14
MvToDitransLe
ActUndPrep h1
TempOver h1
Some h9
RegNom h10
DetSgLe
Some h9
IntrNLe
RegNom h10
Figure 5 The more generalized derivation tree dtg
where eg Named is the common supertype of
SandyRel and KimRel and ActUndPrep is the
supertype of GiveRel Figure 5 shows the tem-
plate templg obtained from f s using the more gen-
eral MRS information Note that the MRS of the
root node is used for building up an index in the
decision tree
Now if retrieval of the decision tree is directed
by type subsumption the same template can be re-
trieved and potentially instantiated for a wider range
of new MRS input namely for those which are type
compatible wrt
subsumption relation Thus the
template templg can now be used to generate eg
the string Kim gives a table to Peter as well as
the string Noam donates a book to Peter
4 Of course
if a very ne-grained lexical seman-
tic type hierarchy is dened then a more careful selec-
tion would be possible to obtained dierent degrees of
type abstraction and to achieve a more domain-sensitive
determination of the subgrammars However more
complex type abstraction strategies are then needed
which would be able to nd appropriate supertypes
automatically
4 Partial Matching
The core idea behind partial matching is that in case
an exact match of an input MRS fails we want at
least as many subparts as possible to be instantiated
Since the instantiated template of a MRS subpart
corresponds to a phrasal sign we also call it a phrasal
template For example assuming that the training
phase has only to be performed for the example in
gure 1 then for the MRS of A man gives a book to
Kim a partial match would generate the strings a
man and gives a book to Kim5 The instantiated
phrasal templates are then combined by the tactical
component to produce larger units if possible see
below
Extended training phase The training module
is adapted as follows Starting from a template
templ obtained for the training example in the man-
ner described above we extract recursively all pos-
sible subtrees templs also called phrasal templates
Next each phrasal template is inserted in the deci-
sion tree in the way described above
It is possible to direct the subtree extraction pro-
cess with the application of lters which are ap-
plied to the whole remaining subtree in each recur-
sive step By using these lters it is possible to re-
strict the range of structural properties of candidate
phrasal templates eg extract only saturated NPs
or subtrees having at least two daughters or sub-
trees which have no immediate recursive structures
These lters serve the same means as the chunking
criteria described in Rayner and Carter 1996
During the training phase it is recognized for each
phrasal template templs whether the decision tree
already contains a path pointing to a previously ex-
tracted and already stored phrasal template templ
such that templs  templ
s In that case templs is
not inserted and the recursion stops at that branch
Extended application phase For the applica-
tion module only the retrieval operation of the de-
cision tree need be adapted
Remember that the input of the retrieval opera-
tion is the sorted generalized MRS mrsg of the input
MRS mrs Therefore mrsg can be handled like a
sequence The task of the retrieval operation in the
5 If we would allow for an exhaustive partial match
see below then the strings a book and Kim would
additionally be generated
case of a partial match is now to potentially nd all
subsequences of mrsg which lead to a template
In case of exact matching strategy the decision
tree must be visited only once for a new input In
the case of partial matching however the decision
tree describes only possible prexes for a new input
Hence we have to recursively repeat retrieval of the
decision tree as long as the remaining sux is not
In other words the decision tree is now a
nite representation of an innite structure because
implicitly each endpoint of an index bears a pointer
to the root of the decision tree
Assuming that the following templateindex pairs
have been inserted into the decision tree hab t1i
habcd t2i hbcd t3i Then retrieval using the path
abcd will return all three templates retrieval using
aabbcd will return template t1 and t3 and abc will
only return t16
Interleaving with normal processing Our
EBL method can easily be integrated with normal
processing because each instantiated template can
be used directly as an already found sub-solution
In case of an agenda-driven chart generator of the
kind described in Neumann 1994a Kay 1996 an
instantiated template can be directly added as a
passive edge to the generators agenda
If passive
edges with a wider span are given higher priority
than those with a smaller span the tactical gener-
ator would try to combine the largest derivations
before smaller ones ie it would prefer those struc-
tures determined by EBL
5 Implementation
The EBL method just described has been fully im-
plemented and tested with a broad coverage HPSG-
based English grammar including more than 2000
fully specied lexical entries7 The TDL grammar
formalism is very powerful supporting distributed
disjunction full negation as well as full boolean type
In our current system an ecient chart-based
bidirectional parser is used for performing the train-
ing phase During training the user can interac-
tively select which of the parsers readings should
be considered by the EBL module In this way the
user can control which sort of structural ambigui-
ties should be avoided because they are known to
6 It is possible to parameterize our system to per-
form an exhaustive or a non-exhaustive strategy In the
non-exhaustive mode the longest matching prexes are
preferred
7This grammar has been developed at CSLI Stan-
ford and kindly be provided to the author
cause misunderstandings For interleaving the EBL
application phase with normal processing a rst pro-
totype of a chart generator has been implemented
using the same grammar as used for parsing
First tests has been carried out using a small test
set of 179 sentences Currently a parser is used for
processing the test set during training Generation
of the extracted templates is performed solely by
the EBL application phase ie we did not consid-
ered integration of EBL and chart generation The
application phase is very ecient The average pro-
cessing time for indexing and instantiation of a sen-
tence level template determined through parsing of
an input MRS is approximately one second8 Com-
pared to parsing the corresponding string the factor
of speed up is between 10 to 20 A closer look to
the four basic EBL-generation steps
indexing in-
stantiation lexical lookup and terminal matching
showed that the latter is the most expensive one up
to 70 of computing time The main reasons are
that 1 lexical lookup often returns several lexical
readings for an MRS element which introduces lex-
ical non-determinism and 2 the lexical elements
introduce most of the disjunctive constraints which
makes unication very complex Currently termi-
nal matching is performed left to right However
we hope to increase the eciency of this step by us-
ing head-oriented strategies since this might help to
re-solve disjunctive constraints as early as possible
6 Discussion
The only other approach I am aware of which
also considers EBL for NLG is Samuelsson 1995a
Samuelsson 1995b However he focuses on the
compilation of a logic grammar using LR-compiling
techniques where EBL-related methods are used to
optimize the compiled LR tables in order to avoid
spurious non-determinisms during normal genera-
tion He considers neither the extraction of a spe-
cialized grammar for supporting controlled language
generation nor strong integration with the normal
generator
However these properties are very important for
achieving high applicability Automatic grammar
extraction is worthwhile because it can be used to
support the denition of a controlled domain-specic
language use on the basis of training with a gen-
eral source grammar Furthermore in case exact
matching is requested only the application module
is needed for processing the subgrammar
In case
8 EBL-based generation of all possible templates of
an input MRS is less than 2 seconds The tests have
been performed using a Sun UltraSparc
of normal processing our EBL method serves as a
speed-up mechanism for those structures which have
actually been used or uttered However complete-
ness is preserved
We view generation systems which are based on
canned text and linguistically-based systems sim-
ply as two endpoints of a contiguous scale of possible
system architectures see also Dale et al 1994
Thus viewed our approach is directed towards the
automatic creation of application-specic generation
systems
7 Conclusion and Future Directions
We have presented a method of automatic extrac-
tion of subgrammars for controlling and speeding up
natural language generation NLG The method is
based on explanation-based learning EBL which
has already been successfully applied for parsing
We showed how the method can be used to train
a system to a specic use of grammatical and lexical
We already have implemented a similar EBL
method for parsing which supports on-line learn-
ing as well as statistical-based management of ex-
tracted data In the future we plan to combine EBL-
based generation and parsing to one uniform EBL
approach usable for high-level performance strate-
gies which are based on a strict interleaving of pars-
ing and generation cf Neumann and van Noord
1994 Neumann 1994a
8 Acknowledgement
The research underlying this paper was supported
by a research grant from the German Bundesmin-
isterium fur Bildung Wissenschaft Forschung
und Technologie BMBF to the DFKI project
paradime FKZ ITW 9704
I would like to thank the HPSG people from CSLI
Stanford for their kind support and for providing the
HPSG-based English grammar In particular I want
to thank Dan Flickinger and Ivan Sag Many thanks
also to Walter Kasper for fruitful discussions
References
Copestake et al1996 Copestake A D Flickinger
R Malouf S Riehemann and I Sag
Translation using minimal recursion semantics
In Proceedings 6th International Conference on
Theoretical and Methodological Issues in Machine
Translation
1994 Report from working group 2 Lexi-
calization and architecture
In W Hoeppner
H Horacek and J Moore editors Principles of
Natural Language Generation Dagstuhl-Seminar-
Report 93 Schlo Dagstuhl Saarland Germany
Europe pages 3039
Kay1996 Kay M 1996 Chart generation In 34th
Annual Meeting of the Association for Computa-
tional Linguistics Santa Cruz Ca
Krieger and Schafer1994 Krieger Hans-Ulrich and
Ulrich Schafer 1994 TDLa type description
language for constraint-based grammars In Pro-
ceedings of the 15th International Conference on
Computational Linguistics COLING-94 pages
893899
Minton et al1989 Minton S J G Carbonell
C A Knoblock D RKuokka O Etzioni and
YGi 1989 Explanation-based learning A prob-
lem solving perspective Articial Intelligence
4063115
Mitchell Keller and Kedar-Cabelli1986
Mitchell T R Keller and S Kedar-Cabelli
1986 Explanation-based generalization a uni-
fying view Machine Learning 14780
Neumann1994a Neumann G 1994a Application
of explanation-based learning for ecient process-
ing of constraint based grammars In Proceedings
of the Tenth IEEE Conference on Articial Intel-
ligence for Applications pages 208215 San An-
tonio Texas March
Neumann1994b Neumann G 1994b A Uniform
Computational Model for Natural Language Pars-
ing and Generation PhD thesis Universitat des
Saarlandes Germany Europe November
Neumann and van Noord1994 Neumann G and
1994 Reversibility and self-
G van Noord
monitoring in natural language generation
Tomek Strzalkowski editor Reversible Grammar
in Natural Language Processing Kluwer pages
5996
Pollard and Sag1994 Pollard C and I M Sag
1994 Head-Driven Phrase Structure Grammar
Center for the Study of Language and Informa-
tion Stanford
Rayner1988 Rayner M
Applying
explanation-based generalization to natural lan-
guage processing In Proceedings of the Interna-
tional Conference on Fifth Generation Computer
Systems Tokyo
Dale et al1994 Dale R W Finkler R Kittredge
N Lenke G Neumann C Peters and M Stede
Rayner and Carter1996 Rayner M and D Carter
1996 Fast parsing using pruning and grammar
specialization In 34th Annual Meeting of the As-
sociation for Computational Linguistics Morris-
town New Jersey
Samuelsson1994 Samuelsson C
Natural-Language Parsing Using Explanation-
Based Learning PhD thesis Swedish Institute
of Computer Science Kista Sweden Europe
Samuelsson1995a Samuelsson C 1995a An e-
cient algorithm for surface generation In Proceed-
ings of the 14th International Joint Conference on
Articial Intelligence pages 14141419 Montreal
Canada
Samuelsson1995b Samuelsson C 1995b Example-
based optimization of surface-generation tables
In Proceedings of Recent Advances in Natural Lan-
guage Processing Velingrad Bulgaria Europe
Samuelsson and Rayner1991 Samuelsson C and
M Rayner
1991 Quantitative evaluation of
explanation-based learning as an optimization
tool for a large-scale natural language system In
IJCAI-91 pages 609615 Sydney Australia
Shemtov1996 Shemtov H
1996 Generation of
Paraphrases from Ambiguous Logical Forms In
Proceedings of the 16th International Conference
on Computational Linguistics COLING pages
919924 Kopenhagen Denmark Europe
Shieber1993 Shieber S M 1993 The problem of
logical-form equivalence Computational Linguis-
tics 19179190
Srinivas and Joshi1995 Srinivas B and A Joshi
1995 Some novel applications of explanation-
based learning
lexicalized tree-
adjoining grammars
In 33th Annual Meeting
of the Association for Computational Linguistics
Cambridge MA
to parsing
van Harmelen and Bundy1988 van Harmelen F
and A Bundy 1988 Explanation-based general-
izationpartial evaluation Articial Intelligence
36401412
Applying Explanation-based Learning to Control and Speeding-up
Natural Language Generation
Gunter Neumann
DFKI GmbH
Stuhlsatzenhausweg 3
66123 Saarbrucken Germany
neumanndfkiuni-sbde
Abstract
This paper presents a method for the au-
tomatic extraction of subgrammars to con-
trol and speeding-up natural language gen-
eration NLG The method is based on
explanation-based learning EBL The main
advantage for the proposed new method
for NLG is that the complexity of the
grammatical decision making process dur-
ing NLG can be vastly reduced because
the EBL method supports the adaption of
a NLG system to a particular use of a lan-
1 Introduction
a Machine Learning
In recent years
nique known as Explanation-based Learning EBL
Mitchell Keller and Kedar-Cabelli 1986 van
Harmelen and Bundy 1988 Minton et al 1989 has
successfully been applied to control and speeding-up
natural language parsing Rayner 1988 Samuelsson
and Rayner 1991 Neumann 1994a Samuelsson
1994 Srinivas and Joshi 1995 Rayner and Carter
1996 The core idea of EBL is to transform the
derivations or explanations computed by a prob-
lem solver eg a parser to some generalized and
compact forms which can be used very eciently
for solving similar problems in the future EBL has
primarily been used for parsing to automatically spe-
cialize a given source grammar to a specic domain
In that case EBL is used as a method for adapting a
general grammar andor parser to the sub-language
dened by a suitable training corpus Rayner and
Carter 1996
A specialized grammar can be seen as describ-
ing a domain-specic set of prototypical construc-
tions Therefore the EBL approach is also very
interesting for natural language generation NLG
Informally NLG is the production of a natural
language text from computer-internal representa-
tion of information where NLG can be seen as
a complexpotentially cascadeddecision making
process Commonly a NLG system is decomposed
into two major components viz the strategic com-
ponent which decides what to say and the tacti-
cal component which decides how to say the result
of the strategic component The input of the tacti-
cal component is basically a semantic representation
computed by the strategic component Using a lexi-
con and a grammar its main task is the computation
of potentially all possible strings associated with a
semantic input Now in the same sense as EBL is
used in parsing as a means to control the range of
possible strings as well as their degree of ambigu-
ity it can also be used for the tactical component
to control the range of possible semantic input and
their degree of paraphrases
In this paper we present a novel method for the
automatic extraction of subgrammars for the control
and speeding-up of natural language generation Its
main advantage for NLG is that the complexity of
the linguistically oriented decision making process
during natural language generation can be vastly re-
duced because the EBL method supports adaption
of a NLG system to a particular language use The
core properties of this new method are
 prototypical occuring grammatical construc-
tions can automatically be extracted
 generation of these constructions is vastly sped
up using simple but ecient mechanisms
 the new method supports partial matching in
the sense that new semantic input need not be
completely covered by previously trained exam-
 it can easily be integrated with recently de-
veloped chart-based generators as described in
eg Neumann 1994b Kay 1996 Shemtov
used for machine translation Copestake et al 1996
Shemtov 19962
The method has been completely implemented
and tested with a broad-coverage HPSG-based
grammar for English see sec 5 for more details
2 Foundations
The main focus of this paper is tactical generation
ie the mapping of structures usually represent-
ing semantic information eventually decorated with
some functional features to strings using a lexicon
and a grammar Thus stated we view tactical gen-
eration as the inverse process of parsing Informally
EBL can be considered as an intelligent storage unit
of example-based generalized parts of the grammat-
ical search space determined via training by the tac-
tical generator1 Processing of similar new input is
then reduced to simple lookup and matching oper-
ations which circumvent re-computation of this al-
ready known search space
We concentrate on constraint-based grammar for-
malism following a sign-based approach consider-
ing linguistic objects ie words and phrases as
utterance-meaning associations Pollard and Sag
1994 Thus viewed a grammar is a formal state-
ment of the relation between utterances in a natu-
ral language and representations of their meanings
in some logical or other articial language where
such representations are usually called logical forms
Shieber 1993 The result of the tactical generator
is a feature structure or a set of such structures in
the case of multiple paraphrases containing among
others the input logical form the computed string
and a representation of the derivation
In our current implementation we are using TDL
a typed feature-based language and inference system
for constraint-based grammars Krieger and Schafer
1994 TDL allows the user to dene hierarchically-
ordered types consisting of type and feature con-
straints As shown later a systematic use of type
information leads to a very compact representation
of the extracted data and supports an elegant but
ecient generalization step
We are adapting a at representation of log-
ical forms as described in Kay 1996 Copestake
et al 1996 This is a minimally structured but
descriptively adequate means to represent seman-
tic information which allows for various types of
under-overspecication facilitates generation and
the specication of semantic transfer equivalences
1 In case a reversible grammar is used the parser can
even be used for processing the training corpus
Informally a at representation is obtained by
the use of extra variables which explicitly repre-
sent the relationship between the entities of a logical
form and scope information In our current system
we are using the framework called minimal recur-
sion semantics MRS described in Copestake et
al 1996 Using their typed feature structure nota-
tion gure 1 displays a possible MRS of the string
Sandy gives a chair to Kim abbreviated where
convenient
The value of the feature liszt is actually treated
like a set ie the relative order of the elements is
immaterial The feature handel is used to repre-
sent scope information and index plays much the
same role as a lambda variable in conventional rep-
resentations for more details see Copestake et al
1996
3 Overview of the method
retrieve 
instantiate
extract 
generalize 
grammar
semantic
grammatical
processor
grammar
results
Figure 3 A blueprint of the architecture
The above gure displays the overall architecture
of the EBL learning method The right-hand part
of the diagram shows the linguistic competence base
LCB and the left the EBL-based subgrammar pro-
cessing component SGP
LCB corresponds to the tactical component of a
general natural language generation system NLG In
this paper we assume that the strategic component
of the NLG has already computed the MRS repre-
sentation of the information of an underlying com-
puter program SGP consists of a training module
TM an application module AM and the subgram-
2 But note our approach does not depend on a at
representation of logical forms However in the case
of conventional representation form the mechanisms for
indexing the trained structures would require more com-
plex abstract data types see sec 4 for more details
handel h1
index e2
liszt SandyRelhhandel h4
Tohandel h12
ChairRelhhandel h10

GiveRel
handel h1
preparg x6

 TempOverhhandel h1
event e2i 
x6  KimRelhhandel h14

handel h9
restr h10
scope h11

Figure 1 The MRS of the string Sandy gives a chair to Kim
lisztDSandyRelhandel h4 
ChairRelhandel h10 
 GiveRelhandel h1 
 Tohandel h12 
 TempOverhandel h1 
 Somehandel h9 
 KimRelhandel h14 E
Figure 2 The generalized MRS of the string Sandy gives a chair to Kim
mar automatically determined by TM and applied
Briey the ow of control is as follows During
the training phase of the system a new logical form
mrs is given as input to the LCB After grammatical
processing the resulting feature structure f smrs
ie a feature structure that contains among others
the input MRS the computed string and a repre-
sentation of the derivation tree is passed to TM
TM extracts and generalizes the derivation tree of
f smrs which we call the template templmrs
of f smrs
templmrs is then stored in a deci-
sion tree where indices are computed from the MRS
found under the root of templmrs During the ap-
plication phase a new semantic input mrs is used
for the retrieval of the decision tree If a candidate
template can be found and successfully instantiated
the resulting feature structure f smrs constitutes
the generation result of mrs
Thus described the approach seems to facilitate
only exact retrieval and matching of a new seman-
tic input However before we describe how partial
matching is realized we will demonstrate in more de-
tail the exact matching strategy using the example
MRS shown in gure 1
Training phase The training module TM starts
right after the resulting feature structure f s for the
input MRS mrs has been computed
In the rst
phase TM extracts and generalizes the derivation
tree of f s called the template of f s Each node of
the template contains the rule name used in the cor-
responding derivation step and a generalization of
the local MRS A generalized MRS is the abstrac-
tion of the liszt value of a MRS where each element
only contains the lexical semantic type and han-
del information the handel information is used
for directing lexical choice see below
In our example mrs gure 2 displays the gener-
alized MRS mrsg For convenience we will use the
more compact notation
SandyRel h4 GiveRel h1
TempOver h1 Some h9
ChairRel h10 To h12 KimRel h14
Using this notation gure 4 see next page dis-
plays the template templmrs obtained from f s
Note that it memorizes not only the rule application
structure of a successful process but also the way the
grammar mutually relates the compositional parts of
the input MRS
In the next step of the training module TM the
generalized MRS mrsg information of the root node
of templmrs is used for building up an index in
a decision tree Remember that the relative order
of the elements of a MRS is immaterial For that
reason the elements of mrsg are alphabetically or-
dered so that we can treat it as a sequence when
used as a new index in the decision tree
The alphabetic ordering has two advantages
Firstly we can store dierent templates under a
common prex which allows for ecient storage and
retrieval Secondly it allows for a simple ecient
treatment of MRS as sets during the retrieval phase
of the application phase
SubjhD
SandyRel h4 GiveRel h1 TempOver h1
Some h9 ChairRel h10 To h12 KimRel h14
ProperLe
SandyRel h4
HCompNc
GiveRel h1 TempOver h1
Some h9 ChairRel h10 To h12 KimRel h14
HCompNc
GiveRel h1 TempOver h1
Some h9 ChairRel h10
To h12 KimRel h14
PrepNoModLe
To h12
ProperLe
KimRel h14
MvToDitransLe
GiveRel h1
TempOver h1
Some h9
ChairRel h10
DetSgLe
Some h9
IntrNLe
ChairRel h10
Figure 4 The template templmrs Rule names
are in bold
Application phase The application module AM
basically performs the following steps
1 Retrieval For a new MRS mrs we rst con-
struct the alphabetically sorted generalized MRS
g is then used as a path description
for traversing the decision tree For reasons we
will explain soon traversal is directed by type
subsumption Traversal is successful if mrs
been completely processed and if the end node
in the decision tree contains a template Note
that because of the alphabetic ordering the rel-
ative order of the elements of new input mrs is
immaterial
2 Expansion A successfully retrieved template
templ is expanded by deterministically applying
the rules denoted by the non-terminal elements
from the top downwards in the order specied
by templ In some sense expansion just re-plays
the derivation obtained in the past This will
result in a grammatically fully expanded fea-
ture structure where only lexical specic infor-
mation is still missing But note that through
structure sharing the terminal elements will al-
ready be constrained by syntactic information3
3 It is possible to perform the expansion step o-line
as early as the training phase in which case the applica-
tion phase can be sped up however at the price of more
memory being taken up
3 Lexical lookup From each terminal element of
the unexpanded template templ the type and
handel information is used to select the cor-
responding element from the input MRS mrs
note that in general the MRS elements of the
mrs are much more constrained than their cor-
responding elements in the generalized MRS
g The chosen input MRS element is then
used for performing lexical lookup where lexi-
cal elements are indexed by their relation name
In general this will lead to a set of lexical can-
didates
4 Lexical instantiation In the last step of the ap-
plication phase the set of selected lexical el-
ements is unied with the constraints of the
terminal elements in the order specied by the
terminal yield We also call this step terminal-
matching
In our current system terminal-
matching is performed from left to right Since
the ordering of the terminal yield is given by the
template it is also possible to follow other se-
lection strategies eg a semantic head-driven
strategy which could lead to more ecient
terminal-matching because the head element is
supposed to provide selectional restriction in-
formation for its dependents
A template together with its corresponding index
describes all sentences of the language that share
the same derivation and whose MRS are consistent
with that of the index Furthermore the index and
the MRS of a template together dene a normaliza-
tion for the permutation of the elements of a new
input MRS The proposed EBL method guarantees
soundness because retaining and applying the orig-
inal derivation in a template enforces the full con-
straints of the original grammar
Achieving more generality So far the applica-
tion phase will only be able to re-use templates for
a semantic input which has the same semantic type
information However it is possible to achieve more
generality if we apply a further abstraction step on
a generalized MRS This is simply achieved by se-
lecting a supertype of a MRS element instead of the
given specialized type
The type abstraction step is based on the stan-
dard assumption that the word-specic lexical se-
mantic types can be grouped into classes represent-
ing morpho-syntactic paradigms These classes de-
ne the upper bounds for the abstraction process In
our current system these upper bounds are directly
used as the supertypes to be considered during the
type abstraction step More precisely for each el-
ement x of a generalized MRS mrsg it is checked
whether its type Tx is subsumed by an upper bound
Ts we assume disjoint sets Only if this is the case
Ts replaces Tx in mrsg4 Applying this type abstrac-
tion strategy on the MRS of gure 1 we obtain
However it will not be able to generate a sentence
like A man gives a book to Kim since the retrieval
phase will already fail In the next section we will
show how to overcome even this kind of restriction
Named h4 ActUndPrep h1
TempOver h1 Some h9
RegNom h10 To h12 Named h14
Named h4 ActUndPrep h1 TempOver h1
Some h9 RegNom h10 To h12 Named h14
ProperLe
Named h4
HCompNc
ActUndPrep h1 TempOver h1
Some h9 RegNom h10 To h12 Named h14
HCompNc
ActUndPrep h1 TempOver h1
Some h9 RegNom h10
To h12 Name h14
PrepNoModLe
To h12
ProperLe
Name h14
MvToDitransLe
ActUndPrep h1
TempOver h1
Some h9
RegNom h10
DetSgLe
Some h9
IntrNLe
RegNom h10
Figure 5 The more generalized derivation tree dtg
where eg Named is the common supertype of
SandyRel and KimRel and ActUndPrep is the
supertype of GiveRel Figure 5 shows the tem-
plate templg obtained from f s using the more gen-
eral MRS information Note that the MRS of the
root node is used for building up an index in the
decision tree
Now if retrieval of the decision tree is directed
by type subsumption the same template can be re-
trieved and potentially instantiated for a wider range
of new MRS input namely for those which are type
compatible wrt
subsumption relation Thus the
template templg can now be used to generate eg
the string Kim gives a table to Peter as well as
the string Noam donates a book to Peter
4 Of course
if a very ne-grained lexical seman-
tic type hierarchy is dened then a more careful selec-
tion would be possible to obtained dierent degrees of
type abstraction and to achieve a more domain-sensitive
determination of the subgrammars However more
complex type abstraction strategies are then needed
which would be able to nd appropriate supertypes
automatically
4 Partial Matching
The core idea behind partial matching is that in case
an exact match of an input MRS fails we want at
least as many subparts as possible to be instantiated
Since the instantiated template of a MRS subpart
corresponds to a phrasal sign we also call it a phrasal
template For example assuming that the training
phase has only to be performed for the example in
gure 1 then for the MRS of A man gives a book to
Kim a partial match would generate the strings a
man and gives a book to Kim5 The instantiated
phrasal templates are then combined by the tactical
component to produce larger units if possible see
below
Extended training phase The training module
is adapted as follows Starting from a template
templ obtained for the training example in the man-
ner described above we extract recursively all pos-
sible subtrees templs also called phrasal templates
Next each phrasal template is inserted in the deci-
sion tree in the way described above
It is possible to direct the subtree extraction pro-
cess with the application of lters which are ap-
plied to the whole remaining subtree in each recur-
sive step By using these lters it is possible to re-
strict the range of structural properties of candidate
phrasal templates eg extract only saturated NPs
or subtrees having at least two daughters or sub-
trees which have no immediate recursive structures
These lters serve the same means as the chunking
criteria described in Rayner and Carter 1996
During the training phase it is recognized for each
phrasal template templs whether the decision tree
already contains a path pointing to a previously ex-
tracted and already stored phrasal template templ
such that templs  templ
s In that case templs is
not inserted and the recursion stops at that branch
Extended application phase For the applica-
tion module only the retrieval operation of the de-
cision tree need be adapted
Remember that the input of the retrieval opera-
tion is the sorted generalized MRS mrsg of the input
MRS mrs Therefore mrsg can be handled like a
sequence The task of the retrieval operation in the
5 If we would allow for an exhaustive partial match
see below then the strings a book and Kim would
additionally be generated
case of a partial match is now to potentially nd all
subsequences of mrsg which lead to a template
In case of exact matching strategy the decision
tree must be visited only once for a new input In
the case of partial matching however the decision
tree describes only possible prexes for a new input
Hence we have to recursively repeat retrieval of the
decision tree as long as the remaining sux is not
In other words the decision tree is now a
nite representation of an innite structure because
implicitly each endpoint of an index bears a pointer
to the root of the decision tree
Assuming that the following templateindex pairs
have been inserted into the decision tree hab t1i
habcd t2i hbcd t3i Then retrieval using the path
abcd will return all three templates retrieval using
aabbcd will return template t1 and t3 and abc will
only return t16
Interleaving with normal processing Our
EBL method can easily be integrated with normal
processing because each instantiated template can
be used directly as an already found sub-solution
In case of an agenda-driven chart generator of the
kind described in Neumann 1994a Kay 1996 an
instantiated template can be directly added as a
passive edge to the generators agenda
If passive
edges with a wider span are given higher priority
than those with a smaller span the tactical gener-
ator would try to combine the largest derivations
before smaller ones ie it would prefer those struc-
tures determined by EBL
5 Implementation
The EBL method just described has been fully im-
plemented and tested with a broad coverage HPSG-
based English grammar including more than 2000
fully specied lexical entries7 The TDL grammar
formalism is very powerful supporting distributed
disjunction full negation as well as full boolean type
In our current system an ecient chart-based
bidirectional parser is used for performing the train-
ing phase During training the user can interac-
tively select which of the parsers readings should
be considered by the EBL module In this way the
user can control which sort of structural ambigui-
ties should be avoided because they are known to
6 It is possible to parameterize our system to per-
form an exhaustive or a non-exhaustive strategy In the
non-exhaustive mode the longest matching prexes are
preferred
7This grammar has been developed at CSLI Stan-
ford and kindly be provided to the author
cause misunderstandings For interleaving the EBL
application phase with normal processing a rst pro-
totype of a chart generator has been implemented
using the same grammar as used for parsing
First tests has been carried out using a small test
set of 179 sentences Currently a parser is used for
processing the test set during training Generation
of the extracted templates is performed solely by
the EBL application phase ie we did not consid-
ered integration of EBL and chart generation The
application phase is very ecient The average pro-
cessing time for indexing and instantiation of a sen-
tence level template determined through parsing of
an input MRS is approximately one second8 Com-
pared to parsing the corresponding string the factor
of speed up is between 10 to 20 A closer look to
the four basic EBL-generation steps
indexing in-
stantiation lexical lookup and terminal matching
showed that the latter is the most expensive one up
to 70 of computing time The main reasons are
that 1 lexical lookup often returns several lexical
readings for an MRS element which introduces lex-
ical non-determinism and 2 the lexical elements
introduce most of the disjunctive constraints which
makes unication very complex Currently termi-
nal matching is performed left to right However
we hope to increase the eciency of this step by us-
ing head-oriented strategies since this might help to
re-solve disjunctive constraints as early as possible
6 Discussion
The only other approach I am aware of which
also considers EBL for NLG is Samuelsson 1995a
Samuelsson 1995b However he focuses on the
compilation of a logic grammar using LR-compiling
techniques where EBL-related methods are used to
optimize the compiled LR tables in order to avoid
spurious non-determinisms during normal genera-
tion He considers neither the extraction of a spe-
cialized grammar for supporting controlled language
generation nor strong integration with the normal
generator
However these properties are very important for
achieving high applicability Automatic grammar
extraction is worthwhile because it can be used to
support the denition of a controlled domain-specic
language use on the basis of training with a gen-
eral source grammar Furthermore in case exact
matching is requested only the application module
is needed for processing the subgrammar
In case
8 EBL-based generation of all possible templates of
an input MRS is less than 2 seconds The tests have
been performed using a Sun UltraSparc
of normal processing our EBL method serves as a
speed-up mechanism for those structures which have
actually been used or uttered However complete-
ness is preserved
We view generation systems which are based on
canned text and linguistically-based systems sim-
ply as two endpoints of a contiguous scale of possible
system architectures see also Dale et al 1994
Thus viewed our approach is directed towards the
automatic creation of application-specic generation
systems
7 Conclusion and Future Directions
We have presented a method of automatic extrac-
tion of subgrammars for controlling and speeding up
natural language generation NLG The method is
based on explanation-based learning EBL which
has already been successfully applied for parsing
We showed how the method can be used to train
a system to a specic use of grammatical and lexical
We already have implemented a similar EBL
method for parsing which supports on-line learn-
ing as well as statistical-based management of ex-
tracted data In the future we plan to combine EBL-
based generation and parsing to one uniform EBL
approach usable for high-level performance strate-
gies which are based on a strict interleaving of pars-
ing and generation cf Neumann and van Noord
1994 Neumann 1994a
8 Acknowledgement
The research underlying this paper was supported
by a research grant from the German Bundesmin-
isterium fur Bildung Wissenschaft Forschung
und Technologie BMBF to the DFKI project
paradime FKZ ITW 9704
I would like to thank the HPSG people from CSLI
Stanford for their kind support and for providing the
HPSG-based English grammar In particular I want
to thank Dan Flickinger and Ivan Sag Many thanks
also to Walter Kasper for fruitful discussions
References
Copestake et al1996 Copestake A D Flickinger
R Malouf S Riehemann and I Sag
Translation using minimal recursion semantics
In Proceedings 6th International Conference on
Theoretical and Methodological Issues in Machine
Translation
1994 Report from working group 2 Lexi-
calization and architecture
In W Hoeppner
H Horacek and J Moore editors Principles of
Natural Language Generation Dagstuhl-Seminar-
Report 93 Schlo Dagstuhl Saarland Germany
Europe pages 3039
Kay1996 Kay M 1996 Chart generation In 34th
Annual Meeting of the Association for Computa-
tional Linguistics Santa Cruz Ca
Krieger and Schafer1994 Krieger Hans-Ulrich and
Ulrich Schafer 1994 TDLa type description
language for constraint-based grammars In Pro-
ceedings of the 15th International Conference on
Computational Linguistics COLING-94 pages
893899
Minton et al1989 Minton S J G Carbonell
C A Knoblock D RKuokka O Etzioni and
YGi 1989 Explanation-based learning A prob-
lem solving perspective Articial Intelligence
4063115
Mitchell Keller and Kedar-Cabelli1986
Mitchell T R Keller and S Kedar-Cabelli
1986 Explanation-based generalization a uni-
fying view Machine Learning 14780
Neumann1994a Neumann G 1994a Application
of explanation-based learning for ecient process-
ing of constraint based grammars In Proceedings
of the Tenth IEEE Conference on Articial Intel-
ligence for Applications pages 208215 San An-
tonio Texas March
Neumann1994b Neumann G 1994b A Uniform
Computational Model for Natural Language Pars-
ing and Generation PhD thesis Universitat des
Saarlandes Germany Europe November
Neumann and van Noord1994 Neumann G and
1994 Reversibility and self-
G van Noord
monitoring in natural language generation
Tomek Strzalkowski editor Reversible Grammar
in Natural Language Processing Kluwer pages
5996
Pollard and Sag1994 Pollard C and I M Sag
1994 Head-Driven Phrase Structure Grammar
Center for the Study of Language and Informa-
tion Stanford
Rayner1988 Rayner M
Applying
explanation-based generalization to natural lan-
guage processing In Proceedings of the Interna-
tional Conference on Fifth Generation Computer
Systems Tokyo
Dale et al1994 Dale R W Finkler R Kittredge
N Lenke G Neumann C Peters and M Stede
Rayner and Carter1996 Rayner M and D Carter
1996 Fast parsing using pruning and grammar
specialization In 34th Annual Meeting of the As-
sociation for Computational Linguistics Morris-
town New Jersey
Samuelsson1994 Samuelsson C
Natural-Language Parsing Using Explanation-
Based Learning PhD thesis Swedish Institute
of Computer Science Kista Sweden Europe
Samuelsson1995a Samuelsson C 1995a An e-
cient algorithm for surface generation In Proceed-
ings of the 14th International Joint Conference on
Articial Intelligence pages 14141419 Montreal
Canada
Samuelsson1995b Samuelsson C 1995b Example-
based optimization of surface-generation tables
In Proceedings of Recent Advances in Natural Lan-
guage Processing Velingrad Bulgaria Europe
Samuelsson and Rayner1991 Samuelsson C and
M Rayner
1991 Quantitative evaluation of
explanation-based learning as an optimization
tool for a large-scale natural language system In
IJCAI-91 pages 609615 Sydney Australia
Shemtov1996 Shemtov H
1996 Generation of
Paraphrases from Ambiguous Logical Forms In
Proceedings of the 16th International Conference
on Computational Linguistics COLING pages
919924 Kopenhagen Denmark Europe
Shieber1993 Shieber S M 1993 The problem of
logical-form equivalence Computational Linguis-
tics 19179190
Srinivas and Joshi1995 Srinivas B and A Joshi
1995 Some novel applications of explanation-
based learning
lexicalized tree-
adjoining grammars
In 33th Annual Meeting
of the Association for Computational Linguistics
Cambridge MA
to parsing
van Harmelen and Bundy1988 van Harmelen F
and A Bundy 1988 Explanation-based general-
izationpartial evaluation Articial Intelligence
36401412
