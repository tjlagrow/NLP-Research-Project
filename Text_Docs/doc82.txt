Robust Processing of Natural Language
Wolfgang Menzel
Fachbereich Informatik Universitat Hamburg
Vogt-Kolln-Strae 30 22527 Hamburg Germany
Abstract Previous approaches to robustness in natural language pro-
cessing usually treat deviant input by relaxing grammatical constraints
whenever a successful analysis cannot be provided by normal means
This schema implies that error detection always comes prior to error
handling a behaviour which hardly can compete with its human model
where many erroneous situations are treated without even noticing them
The paper analyses the necessary preconditions for achieving a higher
degree of robustness in natural language processing and suggests a quite
dierent approach based on a procedure for structural disambiguation
It not only oers the possibility to cope with robustness issues in a more
natural way but eventually might be suited to accommodate quite dif-
ferent aspects of robust behaviour within a single framework
1 Robustness in Natural Language Processing
The notion of robustness in natural language processing is a rather broad one
and lacks a precise denition Usually it is taken to describe a kind of mono-
tonic behaviour which should be guaranteed whenever a system is exposed to
some sort of non-standard input data A comparatively small deviation from a
predened ideal should lead to no or only minor disturbances in the systems
response whereas a total failure might only be accepted for suciently distorted
Under this informal notion robustness may well be interpreted as a systems
indierence to a wide range of external disruptive factors including
 the inherent uncertainty of real world input eg speech or hand writing
 noisy environments
 the variance between speakers for instance idiolectal dialectal or sociolectal
 erroneous input with respect to some normative standard
 an insucient competence of the processing system if eg exposed to a
non-native language or new terminology
 highly varying speech rates and
 resource limitations due to the parallel execution of several mental activities
One of the most impressive features of human language processing is the
ability to retain its basic capabilities even if it is exposed to a combination of
adverse factors Technical solutions on the other hand are likely to have serious
problems if confronted with only a single type of distortion apart from the
fundamental diculties to supply the desired monotonic behaviour at all
Accordingly problems of robustness in NLP have almost never been consid-
ered from a unifying perspective so far A number of very specic techniques for
some of those dierent aspects has been developed which hardly can be related
to each other
Robustness for instance is a key issue in speech recognition where reliable
recognition results for a variety of speakers and speaking conditions are desired
Two basic technologies attempt to support this goal
 robust stochastic modelling techniques which are able to capture generaliza-
tions across the individual variety1 and
 sophisticated search procedures which select among huge amounts of compet-
ing recognition hypotheses by comparing probability estimations for signal
segments of increasing length
Special signal enhancement techniques are used to suppress stationary environ-
mental noise There are other aspects of robustness which even have not been
treated at all including the exible adaptation to external time constraints or
internal resource limitations
Traditionally the notion of robustness has been strongly connected to the
processing of ill-formed input2 where ill-formedness can be dened both in
terms of human standards of grammaticality or in terms of unexpected input
Most of the work has been concerned with the problem from a purely syntactic
point of view and usually relied on two basic techniques error anticipation and
constraint relaxation
Error anticipation identies a number of common mistakes and tries to in-
tegrate them into the existing grammar by devising dedicated extensions to its
coverage Therefore the method is limited to a few selected types of deviant
constructions which are notorious and therefore predictable namely
 stereotypical spelling mistakes comittee rigth etc
 performance phenomena in spoken language like restarts cf 6 and
 interference-based competence errors in early phases of second language
learning cf 1
Obviously the complete innovative potential and the individual creativity for
producing ill-formed input cannot be adequately captured by such means alone
On the other hand constraint relaxation techniques rely on a systematic vari-
ation of existing grammar rules written for standard input Initially the idea
was restricted to the stepwise retraction of eg agreement conditions in syntac-
tic rules It can easily be extended to incorporate arbitrary rule transformations
in order to allow for the insertion deletion substitution and transposition of
1 The diculties with a straightforward generalization of this approach to eg syntactic
or semantic anomalies are obvious It would require huge amounts of suciently de-
viant utterances being available as training data This renders the approach technically
infeasible and cognitively implausible
For similar reasons connectionist approaches are not considered here At the moment
they seem to be limited to approximate solutions for at representations cf 27
2 For a good overview see 25
elements The dierence vanishes completely within modern constraint-based
formalisms 26 2 where a transposition of constituents can be interpreted
equally well as a relaxation of linear precedence constraints Furthermore con-
straints can be annotated by their degree of vulnerability hence allowing to
include aspects of error anticipation into the relaxation framework
Since both error anticipation and constraint relaxation considerably enlarge
the generative capacity of the original grammar they will lead to spurious am-
biguities and serious search problems This restricts their application to a kind
of post mortem analysis3 Only if a failure of the standard analysis procedure
indicates the presence of non-standard input error rules or relaxation techniques
are activated to integrate the fragmentary results obtained so far
Even a supercial comparison with human processing principles shows the
fundamental decit of these approaches A human reader or listener accepts ill-
formed input to a wide degree often without noticing an error at all This is
particularly true if strong expectations concerning the content of the utterance
are involved or if heavy time constraints restrict the processing depth
Obviously there is a fundamental parallelism between robustness issues and
time considerations which syntactically oriented solutions lack so far Robust-
ness in human language processing does not amount to an additional eort
but instead facilitates both insensitivity to ill-formed input as well as a exible
adaptation to temporal restrictions
This basic pattern is much better modelled by semantically oriented ap-
proaches based on the slot-and-ller-principle Here highly domain specic ex-
pectations are coded by means of frame-like structures and checked against the
input for satisfaction The schema can be successfully extended to a kind of skim-
ming understanding bringing together the question of robustness against syn-
tactically ill-formed input and some simple considerations concerning resource
limitations
This advantage of a semantically guided analysis however is won by the
cost of excluding another important robustness feature namely the ability to
cope with unexpected input eg a change of topic beyond the narrow limita-
tions of the domain or the violation of selectional restrictions in metaphorical
expressions
2 Observations from Human Language Processing
Psycholinguistic evidence provides a contradictory picture of human language
processing Some observations clearly support a rather strong modular organi-
zation with processing units of great autonomy like syntax and semantics 4
5 On the other hand there is a considerable semantic inuence on the assign-
ment of syntactic structure 20 which suggests a highly integrated processing
architecture
3 There are exceptions to every rule For language learning purposes 17 propose an
initial analysis based on a moderately weak grammar and followed by a more rigid
second pass
Robust behaviour in natural language understanding seems to require both
 the autonomy between parallel lines of processing which embodies redun-
dancy and allows to compensate partial insuciencies and
 the interactive nature of informational exchange which allows to relate par-
tial structures on dierent levels of granularity
Functional autonomy undoubtedly is of fundamental importance for robustness
It allows to yield an at least vague interpretation even in cases of extremely
distorted input
1 A semantically almost empty sentence can be analysed quite well by syn-
tactic means alone delivering a hypothetical interpretation in terms of a
possible world with highly underspecied referential object descriptions and
possibly ambiguous thematic roles
 und grausig gutzt der Golz23
2 Syntactically ill-formed utterances are interpreted based on semantic and
background knowledge even if subcategorization regularities or other gram-
matical constraints are violated
Although both processing units are  at least partially  able to generate some
useful interpretation independently of the other one best results of course are
to be expected if they combine their eorts in a systematic way
Parallel and autonomous structures in language processing have not only
evolved between syntactic and semantic aspects of language They can be ob-
served equally well at the level of speech comprehension where auditory hearing
and visual lip-reading clues are usually combined to achieve a reliable recogni-
tion result Again both systems  in principle  are able to work independently
but synergy occurs if both are activated concurrently
A second group of observations related to the question of robustness concerns
the expectation-driven nature of human language understanding Here expecta-
tions come to play at two dierent dimensions
 Syntactic semantic and pragmatic predictions about future input derived
from previous parts of the utterance or dialogue
 Expectations exchanged between parallel and autonomous processing struc-
tures for syntax and semantics
The role of dynamic expectations has mostly been investigated from the view-
point of a possible search space reduction in prediction based parsing strategies
namely left or head corner algorithms If used to select between competing
hypotheses in speech recognition the predictive capacity of a grammar can con-
tribute additionally to an enhanced robustness of the overall system 10 7
Although the importance of predictions for robustness is beyond question
here the second type of expectations shall be examined as a matter of prior-
ity since they are expected to establish the attempted informational coupling
between parallel processing units As the simple examples above have shown
no predened direction for this exchange of information can be assumed Cer-
tain syntactic constructions may trigger specic semantic interpretations a view
which is strongly supported by the traditional perspective on the relation be-
tween syntax and semantics In the opposite direction semantic relations eg
derived from background knowledge can not only be used to disambiguate be-
tween preestablished syntactic readings but moreover are able to actively pro-
pose suitable syntactic structures This bidirectionality of interaction seems to
be of great importance for the ability to provide the mutual compensation nec-
essary to treat deviant constructions of dierent kind
Of course the expectation-based nature of natural language processing can-
not guarantee a failure-proof performance under all circumstances4 There cer-
tainly are situations in which strong expectations may override even sensory
data Such a situation can easily be studied in everyday conversation when-
ever eg pragmatic expectations are predominant A similar problem occurs in
experimental settings using intentionally desynchronised video input where lip
reading information sometimes overrides even the auditory stimulus The prob-
lem is witnessed as well by the diculties usually encountered in proof-reading
ones own text Extremely strong expectations concerning the content usually
cause minor mistakes to be passed unnoticed
Typically expectations are contradictory and will be of dierent impact on
the progress of the analysis procedure Hence there is a third principle of ro-
bust language processing upon which the human model builds It concerns the
preference-based selection between both competing interpretations as well as
dierent expectations 12 Expectations have to be ranked according to their
particular strength and weighted against each other
Recently linguistic research has shown a remarkable trend towards the de-
velopment of integrated models of language structure One of the more popular
examples surely is Head-Driven Phrase Structure Grammar HPSG 24 where
syntactic and semantic descriptions are uniquely related to each other by coref-
erential pointers within the framework of typed feature structures The strong
coupling on the level of representation and on the level of processing ie within
unication completely lacks autonomy The construction of a logical form is
always mediated by syntactic descriptions taken eg from subcategorization in-
formation Since syntactic and semantic restrictions are conjunctively combined
the overall vulnerability against arbitrary impairment of the input utterances
even increases An analysis may now fail due to syntactic as well as due to
semantic reasons
A quite similar conclusion can be drawn for construction grammar 3 an-
other integrated approach It combines syntactic semantic and even pragmatic
information in a single representation named construction Again autonomy of
individual description levels is missing and even if constructions are supplied
with preferential weightings derived from their frequency of use as realized in
4 Note that perfect performance is not necessarily covered by the informal notion of
robustness introduced earlier
SAL 13 robustness does not increase
A clearcut separation of representational levels has actually been realized in
the cognitively motivated parser COMPERE 11 18 The system aims at mod-
elling error recovery techniques for garden-path sentences It uses an arbitration
mechanism to decide in case of a conict situation which alternative reading
should be backed up This allows to combine early commitment decisions with
the possibility to switch to another interpretation if necessary later on Although
the parser is guided in its decisions by dierent kinds of preferences the map-
ping between syntactic and semantic representations seems to be a strict one
Accordingly it does not provide the necessary means for conict resolution in all
those cases of non-standard input for which no interpretation can be established
In particular three dierent cases can be distinguished
1 failure on a single level syntax or semantics
2 failure on both levels syntax and semantics
3 no consistent mapping between levels
Whereas the rst case might be easily accommodated by the arbitration mech-
anism the latter two require the abandonment of the strict mapping and its
replacement by a preference-based module interaction
3 Disambiguation by Constraint Propagation
A suitable combination of the three principles discussed above might in fact
provide the foundation for an eective use of redundancy in parallel processing
structures
 autonomy guarantees a fall-back behaviour for failures of a single module
 expectancy-oriented analysis facilitates the informational exchange and
 preference-based processing guides the analysis towards a promising interpre-
tation and establishes a loose coupling between modules
These principles even if taken together do not explain the almost unconscious
treatment of errors in everyday communication To simulate a similar behaviour
a selective constraint invocation strategy will become necessary Then parsing
is understood as a disambiguation procedure which activates only specic parts
of the grammar if this is deemed to be unavoidable for solving a particular
disambiguation problem The procedure can be terminated if a suciently reli-
able disambiguation has been achieved even if certain conditions of the grammar
have never been checked so far Robustness is not introduced by a post mortem
retraction of constraints but rather by their careful invocation
Along these lines a rudimentary kind of robustness has been achieved in the
Constraint Grammar framework 15 a system for parsing large amounts of un-
restricted text Constraint Grammar CG attempts to establish a dependency
description which is underspecied with respect to the precise identity of modi-
ees Initially it assigns a set of morphologically justied syntactic labels to each
word form in the input sentence Possible labels among others are
FMAINV
the nite verb of a clause
a grammatical subject
a direct object
a determiner modifying a noun to the right
a noun modifying a noun to the right
The initial set of labels is successively reduced by applying compatibility
and surface ordering constraints until a unique interpretation has been reached
or the set of available constraints is exhausted In the latter case a total dis-
ambiguation cannot be achieved by purely syntactic means as in the following
attachment example
SUBJ FMAINV DN AN OBJ
In contrast to traditional grammars of the phrase structure type which li-
cense well-formed structures according to their rule system constraint grammar
rather happens to be an eliminative approach5 Instead of imposing a normative
description on the input data it takes them as starting point and tries to nd a
plausible interpretation for them
This proceeding is motivated by the nding that language is an open-ended
system and so grammar formalisms based on a rigid and idealized conception
of grammatical correctness are bound to leak 14 p 37 Parsing if understood
as a disambiguation procedure is put down to the principle of parsimony The
more eort is spent the better disambiguation results can be expected and 14
p 39 points to the important psycholinguistic parallel
Mental eort is needed for achieving clarity precision and maximal
information Less eorts imply retention of unclarity and ambiguity
ie information decrease In several types of parsers rule applications
create rather than discard ambiguities the more processing the less
unambiguous information
Parsing as disambiguation can well be extended to deal with fully specied de-
pendency structures without loosing its promising characteristics A complete
disambiguation of structural descriptions has rst been described for Constraint
Dependency Grammar CDG 21 and simply requires to replace the monadic
categories of CG by pairs consisting of the relation name and the exactly speci-
ed modiee The mutual compatibility of modifying relations is checked against
a set of constraints and thus the set of possible modications is successively
reduced by a constraint propagation mechanism Further extensions of the ap-
proach concern the inclusion of feature descriptions valency specications and
valency saturation conditions 8
5 With this respect a strong parallel between the eliminative nature of disambiguation
and cohort modelling ideas for spoken word recognition 19 becomes visible
Though a closer inspection of the kind of robustness feature introduced by
the eliminative mode of operation reveals that its nature is quite accidental so
far Which types of deviation can be tolerated indeed strongly depends on the
rather arbitrary sequence of constraint applications This shortcoming seems to
be closely connected to the fact that both formalisms lack the notion of prefer-
ence so far and therefore do not have the possibility to model the quality of
a constraint6 Hence adding a preference-based selection strategy will be one of
the most pressing needs for further improvement Such an extension will be pro-
posed in section 5 Before we turn to this topic section 4 introduces a modular
representation schema along the traditional syntax-semantics distinction It sup-
ports the desired functional autonomy as well as a highly interactive exchange
of expectations between the two layers
4 Representation Layers
Whereas Constraint Grammar restricts itself to purely syntactic means an in-
tegration of simple semantic criteria into Constraint Dependency Grammar has
been proposed recently 9 It takes into account sortal restrictions only attach-
ing them to surface syntactic relations without aiming at modularity and au-
tonomous behavior In order to facilitate functional independence it will become
necessary to establish separate layers for structural description and constraint
propagation
 a syntactic layer relating word forms according to functional surface struc-
ture notions eg subject-of dir-object-of prep-modifier-of etc
and using constraints on ordering agreement valency and valency satura-
tion to select among competing structural conguration and
 a semantic layer building sentence structures by means of thematic roles like
agent-of instrument-of time-of etc thereby relying upon the argu-
ment structure of semantic predicates and their corresponding selectional
restrictions7
The following small and rather rigid sample grammar illustrates the dierent
types of constraints needed
1 licensing conditions for modication relations
sy1 catdepXN
 catsynmodXV  synlabXSUBJOBJ 8
A noun can modify a verb either as a subject or as a direct object
6 CG at least includes heuristic constraints which may be activated at a particular stage
of the disambiguation procedure
7 The proposed separation quite closely corresponds with the one chosen in 11
8 depX refers to the modier of a relation syndomX and semdomX to its modi-
ees synlabX and semlabX are the respective relation names catX numX
sempropX    denote properties of the corresponding node
2 agreement conditions
sy2 synlabXSUBJ  numdepXnumsyndomX
A subject agrees with its modiee with respect to number
3 linear ordering constraints
sy3 synlabXSUBJ  posdepXpossyndomX
The subject precedes the nite verb
4 compatibility constraints9
sy4 syndomXsyndomY  synlabX 6 synlabY
cannot be modied twice by the same relation
A word form
sy1 through sy3 are unary constraints sy4 is a binary one Note that constraints
refer to modifying relations instead of word forms Therefore they are able to
express admissibility conditions on local congurations consisting of up to three
nodes Note as well that sy3  even for German main clauses  has a strong
heuristic appearance and simply states a preference condition which additionally
requires a suitable exception handling mechanism
In a very similar fashion semantic constraints comprise
1 licensing conditions
se1 catdepXN  catsemdomXV  semlabXAGPAT
2 selectional restrictions
se2 wordsemdomXfressen  sempropmodXanimal
 semlabXAG
Animals do eat
se3 wordsemdomXfressen  sempropmodXplant
 semlabXPAT
Plants are to be eaten
3 compatibility constraints10
se4 semdomXsemdomY  semlabX6semlabY
Adhering to the principle of autonomy both layers are designed in a way
which allows them to propagate constraints in a completely independent man-
ner Each modier is specied for two possibly dierent modiees and no cross-
reference between the layers has been used so far
In order to nally mediate the interaction between layers a set of mapping
constraints has to be provided which sets up bidirectional correspondences
9 In fact there is another general compatibility constraint implicitly built into the decision
procedure and excluding ambiguous modifying relations from being consistent
sygen  syndomXsyndomY  depXdepY
10 Again supplemented by a general semantic uniqueness constraint
segen  semdomXsemdomY  depXdepY
ss1 syndomXsemdomX  synlabXSUBJ semlabAG
The subject of a verb is always identical to its agens
ss2 syndomXsemdomX  synlabXOBJ semlabPAT
The direct object of a verb is always identical to its patiens
It should have become obvious that the selectional restrictions as well as the
mapping constraints at best can be taken to stand for a preferential interpreta-
tion They surely are much to rigid to be sensibly used within a framework of
strict reasoning
Semantic constraints need not be restricted to linguistically motivated ie
universally valid ones In particular domain-specic restrictions play a crucial
role in semantic disambiguation and should urgently be incorporated whenever
possible Here the semantic layer oers a convenient interface to a knowledge
representation component which on demand can contribute constraints from
eg specialized ontologies referential instantiations or temporal reasoning
5 Weakening Constraints
So far one of the most striking shortcomings has been the strictly binary nature
of constraint satisfaction Not surprisingly it turned out to be most inappro-
priate within the area of semantic modelling where hardly a constraint can be
formulated without restricting oneself to a particular preferential reading
In what follows preferences are not modelled in the usual direct manner
by emphasizing particular well-formed interpretations but rather indirectly by
putting a penalty on all remaining alternatives which violate a constraint For this
purpose each constraint gets a penalty factor pf assigned reducing the condence
score in negative cases Penalty factors may range from zero to one where
species a strict constraint in the classical sense and
indicates a soft constraint accepting contradictory cases
with a condence value proportional to pf
Obviously a value of one is meaningless because it neutralizes the constraint
Penalty factors are combined multiplicatively ie compatibility matrices within
the constraint satisfaction problem no longer contain binary categories but con-
dence scores also ranging from zero for impossible combinations up to one
for combinations not even violating a single constraint
The indirect treatment of preference by penalty factors oers a consistent
extension to the basic paradigm of constraint satisfaction It does not sacrice
the eliminative nature of constraints but simply softens it Inappropriate readings
are excluded only if they violate strict constraints In all other cases they are
downgraded to a certain degree
In particular the penalty-based approach helps to tackle some normaliza-
tion problems otherwise inherently connected with the constraint satisfaction
approach Most modifying relations or combinations of them will pass a con-
straint simply because it is irrelevant for that particular conguration An in-
crease of goodness estimates for these cases would yield a highly undesirable
since unjustied reinforcement
By assigning the penalty factors pfsy1pfsy40 to the constraints sy1
and sy4 from section 3 both are declared to be strict ones a fact obviously
being valid for the toy-size sample grammar which does not take into account
coordinative structures Using pfsy201 the agreement condition is treated
as a rather strong one which allows exceptions only occasionally pfsy303
on the other hand results in a much more permissive constraint justied by the
fact that sy2 is meant to exclude ungrammatical utterances but sy3 only to
disfavour a marked ordering
On the semantic layer only the licensing constraint se1 is declared as a strict
one The compatibility constraint se4 is weakened considerably in order to ac-
count for double modication as in the case of anaphoric reference The two
selectional constraints receive penalty factors of dierent strength in order to
model the lower probability of a plant eating something se2 compared to an
animal being eaten se3
pfse100
pfse201
pfse307
pfse405
The mapping constraints nally are weighted in a way which strongly favours
the subject-agens and object-patiens pairings nevertheless allowing alter-
native interpretations eg in passive sentences
pfss102
pfss203
Alternative readings can but need not be specied explicitly In more realistic
applications though it is recommended to aim at a considerably richer modelling
otherwise an unbalanced penalty factor as between se2 and se3 may create a
sometimes undesired strong bias by default If both constraints appear to be of
no relevance the patience reading is clearly prioritized In the example chosen
this corresponds to the acceptable interpretation that an arbitrary thing is more
likely to be eaten than to eat
After having introduced penalty factors as a means of modelling preferences
constraint propagation can be extended from the classical case of strictly bi-
nary decisions to the handling of condence scores The application of penalty-
weighted constraints to a disambiguation problem now consists of two steps
1 the calculation of initial condence scores for all combinations of syntactic
and semantic modication relations and
2 a selection procedure pruning the search space by sorting out unlikely inter-
pretations
The selection procedure is based on a local assessment function heuristically
identifying relations to be pruned In order to not select promising hypothe-
ses assessment rst of all should only take into account modication relations
characterized by the following three criteria
 being close to the global minimum for all modication relations combined
 an as possible as high contrast to alternative relations and
 a low contrast between all the condence scores supporting the relation in
question
For experimental purposes a selection procedure based on the sum of quadratic
errors for setting scores to zero has been used Hence structural interpretations
violating a high number of rather strong constraints are pruned rst
Using the toy grammar specied above together with its penalty scores the
arbitration process between syntactic and semantic evidence in simple disam-
biguation problems can be studied Thus in a sentence like
Pferde fressen Gras
Horses eat grass
both layers uniformly support a single interpretation
fressen
Due to the strong semantic support the interpretation remains unchanged if a
marked ordering topicalization of the direct object is chosen 2b an agree-
ment error is introduced 2c and both deviations are combined nally 2d
GrasP AT fressen PferdeAG
PferdAG fressen GrasP AT 
GrasP AT fressen PferdAG
The interpretation is retained even if its semantic support is neutralized as in
the following utterance containing a twofold type shift
AutosAG fressen GeldP AT 
GeldP AT fressen AutosAG
AutoAG fressen GeldP AT 
Cars eat money
It switches to the alternative interpretation only in the case of combined syn-
tactic distortions
GeldAG fressen AutoP AT 
Even for the counterintuitive example
GraserAG fressen PferdP AT 
which if desired could be taken as a headline-style utterance syntactic evidence
will gain the upper hand against the violation of two selectional constraints This
interpretation however happens to be a rather fragile one and breaks immedi-
ately under arbitrary syntactic variation
Since the selection procedure operates on a global assessment of local struc-
tural congurations it cannot guarantee to nd an optimal and globally consis-
tent interpretation The partially local mode of operation on the other hand
can be expected to provide a quite natural explanation for human garden-path
phenomena Within the framework of preference-based disambiguation they turn
out to be a special case of contradictory situations which manifest themselves as
expectation violations The consequences of a pruning decision may not coincide
with local condence estimations elsewhere in the constraint network
Expectation violations not necessarily do indicate an erroneous situation
They are frequently used as a speakers intentionally chosen means to attract
the attention of the audience This happens for instance by deviating from an
unmarked ordering to emphasize a topicalized constituent cf 3b or by oth-
erwise producing unexpected utterances
On the other hand there are the typical erroneous situations which in case of
 internal diculties eg due to early commitment strategies in garden path
situations might oer the possibility to initiate a reanalysis and
 external reasons eg ill-formed input can be used to track down the error
to nd a possible remedy for it
Note that in the latter case the situation coincides with basic observations for
the human model Finding an interpretation for erroneous utterances will be
easier  in terms of eort to spent  than detecting the error which in turn will
be less demanding than localizing or even correcting it
As with human language processing there will be no predened direction for
the general ow of expectations during arbitration Whether syntactic evidence
is propagated from the syntactic to the semantic layer or vice versa depends
only on the available information This seems to be in accordance with recent
psycholinguistic ndings which contest the existence of purely structural disam-
biguation principles 16
6 Preference-based Reasoning
Eliminating implausible interpretations by locally pruning less favoured modi-
cation relations represents only one though fundamental method for the dis-
ambiguation of natural language utterances By selecting among modifying rela-
tions according to negative evidence from maximally dispreferred hypotheses the
technique ts quite well into the constraint satisfaction approach and achieves
its robust behaviour by avoiding extremely risky decisions on a locally topmost
reading Taking this as a starting point the basic way of reasoning can well be
complemented by a second propagation principle based on preference-induced
constraints These are activated only in situations where enough positive evi-
dence can be derived from almost uniquely determined preferences Since the
existence of convincing preferences in realistic disambiguation tasks represents
rather the exception than the rule the nature of this propagation principle is
secondary
Preference-induced constraints consist of implications P p C which given
enough evidence for the unary precondition P  require the possibly binary con-
straint C to hold Constraints of this type can be used to model eg the higher-
order conditions which in many mapping situations involve more than two rela-
pss1 wordsyndomXim  synlabXPHEAD
 sempropdepXTEMPLOC
p synlabYPMOD  depYsyndomX
 semdomZsydomX  semlabZPART-OF
A prepositional phrase headed by the word form im lls a semantic
part-of slot
In postnominal positions like
Dann nehmen wir die erste Woche im Mai
Lets take the rst week in may
this constraint puts a preference on the lower attachment since a part-of relation
usually is not licensed as an argument position for verbs
Preference-induced constraints can also be used to modify value assignments
at certain nodes in the constraint network without the necessity to copy them
By applying this technique phrasal feature projections can be modelled in order
to build up descriptions for partial dependency trees
pss2 synlabXDET
p casesyndomXcasesyndomX  casedepX
A noun group carries the intersection of possible case features
found at its members
Preference-induced constraints introduce a kind of inhibitory mechanism to the
disambiguation procedure Already preferred interpretations is given the chance
to propagate their consequences over the network thus possibly leading to further
suppression of alternative readings
7 Conclusion
Combining the eliminative nature of a disambiguation procedure with a system
architecture supporting bidirectional arbitration between syntactic and semantic
evidence has turned out to be a key factor for achieving a higher level of robust-
ness in language understanding While the disambiguation paradigm provided
the basic fall-back behaviour an arbitrary utterance will get a description as-
signed and the possibility to prune the search space towards a least disfavoured
reading the parallel arrangement of modules allows to interactively exchange
expectations and thus bypassing local interpretation diculties By modelling
a preference distribution based on penalty factors the desired robust behaviour
can be demonstrated at least for very simple sample utterances Although no
conclusive judgement about the feasibility of the approach can be given until
the experimental setting has been scaled up to a fairly realistic problem size
a remarkable qualitative advance over comparable approaches becomes evident
even on this elementary level
 The approach departs from a predened sequential arrangement of modules
in favour of a strictly symmetrical architecture consisting of autonomous
components for syntax and semantics
 It allows to treat syntactic ill-formedness and semantic deviations by pro-
viding a mechanism for mutual compensation Syntactically anomalous ut-
terances can be understood as long as there is enough semantic andor
pragmatic evidence In order to communicate novel or unusual content a
suciently high degree of syntactic support is required
 Insucient modelling information on any one of the processing layers might
well result in the selection of an odd interpretation but will not cause the
language processing unit to break down entirely
 Robustness is not an add-on feature of an otherwise temperamental proce-
dure but falls out from the basic properties of the processing mechanism
Since structural disambiguation by constraint satisfaction likewise lends itself to
the creation of time sensitive parsing procedures 22 in the long run it might
provide a unifying foundation to build language processing systems upon which
embody aspects of robustness against such dierent disruptive factors as syntac-
tically ill-formed input metaphorical use and dynamic time constraints
References
1 M E Catt
Intelligent diagnosis of ungrammaticality in computer-assisted lan-
guage instruction Technical Report CSRI-218 Computer Systems Research Insti-
tute University of Toronto 1988
2 G Erbach Towards a theory of degrees of grammaticality Report 34 Universitat
des Saarlandes Computerlinguistik Saarbrucken 1993
3 C J Fillmore P Kay and M C OConnor Regularity and idiomaticity in gram-
matical constructions The case of let alone Language 64501538 1988
4 K I Forster Levels of processing and the structure of the of the language proces-
sor In W E Cooper and E C T Walker eds Sentence Processing Psycholin-
guistic studies presented to Merrill Garret p 2785 Lawrence Erlbaum Hillsdale
NJ 1979
5 L Frazier Theories of sentence processing
In J L Gareld ed Modularity
in Knowledge Representation and Natural Language Understanding p 291307
MIT-Press Cambridge MA 1987
6 S Goeser Chart parsing of robust grammars In Proc 14th Int Conf on Com-
putational Linguistics Coling 92 p 120126 Nancy France 1992
7 G Hanrieder and G Gorz Robust parsing of spoken dialogue using contextual
knowledge and recognition probabilities In Proc of the ESCA Workshop on Spo-
ken Dialogue Systems Denmark 1995
8 M P Harper and R A Helzerman Managing multiple knowledge sources in
constraint-based parsing of spoken language Technical Report EE-94-16 School
of Electrical Engineering Purdue University West Lafayette 1994
9 M P Harper L H Jamieson C B Zoltowski L L McPheters and R A Helz-
erman Semantics and constraint parsing In Proc of the Int Conf on Acoustics
Speech and Signal Processing ICASSP-92 p II6366 1992
10 A Hauenstein and H Weber An investigation of tightly coupled time syn-
chronous speech language interfaces using a unication grammar Verbmobil Re-
port 9 1994
11 J K Holbrook K P Eiselt and K Mahesh A unied process model of syntactic
and semantic error recovery in sentence understanding In Proc of the 14th Annual
Conf of the Cognitive Science Society p 195200 Bloomington IN 1992
12 R S Jackendo Semantics and Cognition MIT Press Cambridge 1983
13 D Jurafsky A cognitive model of sentence interpretation The construction gram-
mar approach Technical Report TR-93-077 International Computer Science In-
stitute Berkeley 1993
14 F Karlsson Designing a parser for unrestricted text In F Karlsson A Vouti-
lainen J Heikkila and A Anttila eds Constraint Grammar  A Language-
Independent System for Parsing Unrestricted Text p 140 Mouton de Gruyter
Berlin New York 1995
15 F Karlsson A Voutilainen J Heikkila and A Anttila eds Constraint Gram-
mar  A Language-Independent System for Parsing Unrestricted Text Mouton de
Gruyter Berlin New York 1995
16 L Konieczny B Hemforth and N Voelker The impact of context and semantic
bias on constituent attachment in reading In J J Quantz and B Schmitz eds
Ambiguity and Strategies of Disambiguation p 105126 KIT-Report 120 Berlin
17 I Kudo H Koshino M Chung and T Morimoto Schema method A frame-
work for correcting grammatically ill-formed input In Proc 12th Int Conf on
Computational Linguistics Coling 88 p 341347 Budapest 1988
18 K Mahesh and K P Eiselt Uniform representations for syntax-semantic arbi-
In Proc of the 16th Annual Conf of the Cognitive Science Society p
tration
589594 1994
19 W Marslen-Wilson Functional parallelism in spoken word-recognition Cogni-
tion 2571102 1987
20 W Marslen-Wilson and L K Tyler Against modularity In J L Gareld ed
Modularity in Knowledge Representation and Natural Language Understanding p
3762 MIT-Press Cambridge MA 1987
21 H Maruyama Structural disambiguation with constraint propagation In Proc
28th Annual Meeting of the ACL p 3138 1990
22 W Menzel Parsing of spoken language under time constraints In T Cohn ed
Proc 11th Europ Conf on Articial Intelligence p 560564 Amsterdam 1994
23 C Morgenstern Alle Galgenlieder Cassirer Berlin 1933
24 C Pollard and I A Sag Head-Driven Phrase Structure Grammar The Univer-
sity of Chicago Press Chicago 1994
25 M Stede The search for robustness in natural language understanding Articial
Intelligence Review 64383414 1992
26 H Uszkoreit Strategies for adding control information to declarative grammars
Computerlinguistik Report 10 Universitat des Saarlandes Saarbrucken 1991
27 S Wermter and V Weber
Learning Fault-tolerant Speech Parsing with
SCREEN In Proc of the 12th Nat Conf on Articial Intelligence p 670675
Seattle 1994
