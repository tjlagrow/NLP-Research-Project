Apparent criticality and calibration issues
in the Hawkes self-excited point process model
application to high-frequency nancial data
Vladimir Filimonova Didier Sornetteab
aDept of Management Technology and Economics ETH Zurich Zurich Switzerland
bSwiss Finance Institute co University of Geneva
Abstract
We present a careful analysis of possible issues on the application of the self-excited Hawkes pro-
cess to high-frequency nancial data We carefully analyze a set of eects leading to signicant
biases in the estimation of the criticality index n that quanties the degree of endogeneity of
how much past events trigger future events We report the following model biases i evidence
of strong upward biases on the estimation of n when using power law memory kernels in the
presence of outliers ii strong eects on n resulting from the form of the regularization part of
the power law kernel iii strong edge eects on the estimated n when using power law kernels
and iv the need for an exhaustive search of the absolute maximum of the log-likelihood func-
tion due to its complicated shape Moreover we demonstrate that the calibration of the Hawkes
process on mixtures of pure Poisson process with changes of regime leads to completely spurious
apparent critical values for the branching ratio n  1 while the true value is actually n  0
More generally regime shifts on the parameters of the Hawkes model andor on the generating
process itself are shown to systematically lead to a signicant upward bias in the estimation of
the branching ratio We demonstrate the importance of the preparation of the high-frequency
nancial data in particular a the impact of overnight trading in the analysis of long-term
trends b intraday seasonality and detrending of the data and c vulnerability of the anal-
ysis to day-to-day non-stationarity and regime shifts Special care is given to the decrease of
quality of the timestamps of tick data due to latency and grouping of messages to packets by
the stock exchange Altogether our careful exploration of the caveats of the calibration of the
Hawkes process stresses the need for considering all the above issues before any conclusion can
be sustained In this respect because the above eects are plaguing their analyses the claim
by Hardiman Bercot and Bouchaud 2013 that nancial market have been continuously func-
tioning at or close to criticality n  1 cannot be supported In contrast our previous results
on E-mini SP 500 Futures Contracts and on major commodity future contracts are upheld
Keywords Hawkes process Poisson process endogeneity reexivity branching ratio
outliers memory kernel high-frequency data criticality statistical biases power laws
regime shifts
Email addresses vfilimonovethzch Vladimir Filimonov dsornetteethzch Didier
Sornette
Preprint submitted to arXiv
July 4 2014
1 Introduction
The Hawkes self-excited Poisson process is the simplest extension of the Poisson point
process in which past events inuence future events through a memory kernel Its broad
domain of applications from biology geology to economics and nance invites a thorough
understanding of the issues associated with its calibration to real data and in particular in
the possible biases that arise in the estimation of one of its key parameters the branching
ratio n that quanties the degree of endogeneity of how much past events trigger future
events
We present a careful analysis of a set of eects that lead to signicant biases in the
estimation of the branching ratio n arguably the key parameter of the Hawkes self-excited
Poisson process The motivation of our study stems from the meaning of n as a direct
measure of endogeneity or reexivity since n is exactly equal to the fraction of the
average number of endogenously generated events among all events 1 2 for stationary
time series Concretely the measure n  07  08 reported in our recent studies 2 3
means that 70 to 80 of all trades in the E-mini SP 500 Futures Contracts and in
major commodity future contracts are due in recent years to past trades rather than to
external eects or exogenous news This result has important implications concerning
the ecient market hypothesis and the stability of nancial markets in the presence of
increasing trading frequency and volume Our motivation is further increased by the
recent claim based also on the calibration of the Hawkes process that nancial market
have been continuously functioning at or close to criticality n  1 over the last decades
4 a result in contradiction with our other studies 2 3
The article is structured as follows
In section 2 we introduce the Hawkes model
and briey discuss its properties and in particular provide the rigorous denition of the
branching ratio n We also explain how the calibration of the Hawkes model to empir-
ical time series is performed and present the residual analysis as a statistical goodness
of t Section 3 discusses the common issues appearing in the calibration of the Hawkes
process which are divided in four classes i the impact of outliers ii the somewhat
surprising impact of the regularization part of power law kernels iii the edge eect that
is particularly important for long memory power law kernels and iv the often present
multiple extrema of the likelihood function Section 4 studies in detail how some mi-
crostructure patterns of the high-frequency nancial data are the source of signicant
estimation biases of the branching ratio In particular we analyze the problem of distin-
guishing between regular Trading Hours and overnight trading the impact of recording
latency of the grouping of timestamps and the bundling of timestamps We show that
the intraday seasonality leads to a non-stationary behavior of the exogenous component
of the Hawkes process which is very dicult to remove and is the source of large biases
in the estimation of n The section ends by emphasizing how non-stationarity regime
shifts and the mixing of dierent phases leads to extraordinary large spurious calibration
results such as a mixture of Poisson processes with n  0 by denition for which the
calibration concludes that n is close to critical And section 5 concludes by stressing the
need to revisit many previous studies that have been concerned with inter-event times
2 Hawkes model and measure of endogeneity
21 Hawkes model and its kernel specication
The methodology for estimating the endogeneity or reexivity present in the dy-
namics of a given point process developed in 2 and later exploited in 3 4 5 is based
on the self-excited conditional Poisson model introduced by 6 7 Being a point process
the linear Hawkes model is dened with the conditional intensity
tFt  lim
Prcid2Nt  h  Nt  0Ftcid3
where tiiN is an ordered set of event times ti  tj for i  j Nt  maxi  ti  t
is the corresponding counting process and Ft  t1     ti  ti  t is the ltration
that represents the history of the process until time t Dening t as the background
intensity which is a deterministic function of time that accounts for the intensity of arrival
of exogenous events not dependent on history the conditional intensity of the Hawkes
process takes the following general form
tFt  t Z t
t  sdNs
A deterministic kernel function t which should satisfy causality t  0 for t  0
models the endogenous feedback mechanism memory of the process The integral of
t which is called the branching ratio
n Z 
tdt  0
plays a crucial role for the dynamics of the process which will be elaborated later In
particular stationarity of the Hawkes process 2 requires that n  1 To emphasize the
importance of this parameter we rewrite 2 as
tFt  t  nXtit
ht  ti
where we have also accounted for the fact that each event arrives instantaneously and
the dierential of the counting process dNt can be represented in the form of a sum of
delta-functions dNt  Ptit t  tidt Here ht is the normalized kernel function
ht  tn such that R 
The shape of the kernel function ht denes the correlation properties of the process
Financial applications traditionally use an exponential kernel 8 9 10 2 3
0 htdt  1
expcid18
cid19 t 
This exponential form has been originally suggested by 6 and ensures Markovian prop-
erties of the model 11 The Heaviside function t ensures the validity of the causality
principle In the geophysical applications of the Hawkes model in the form of its spatio-
temporal extension called the Epidemic-Type Aftershock sequence ETAS 12 13 14 15
the memory kernel ht has a power law time-dependence
t  c1 t
which describes the modied Omori-Utsu law of aftershock rates 16 17 The time
constant c regularizes the behavior of the power law kernel at very short times Kagan
and Knopo introduced another regularization for the memory kernel 18 19
 
t1 t  0 
This expression 7 was also used recently in 4 and was approximated as a sum of
exponential functions
Z M 1
expcid18
icid19  S expcid18
1cid19 
so that h0  0 and R 
where the coecients obey a power law i  0mi and the coecients S and Z are chosen
0 htdt  1 In their empirical calibration 4 has xed M  15
and m  5 For values of  close to zero the resulting function describes approximately
a power-law form with tail exponent 1   while the negative exponential term provides
a smooth cut-o at short times
22 The branching ratio
The linear structure of the intensity of the Hawkes process 4 allows one to consider
it as a cluster process in which the random process of cluster centers tc
i iN0 is the
Poisson process with rate t All clusters associated with centers tc
i  are mutually
independent by construction and can be considered as a generalized branching process 20
In this context each event ti can be either an immigrant or a descendant The rate of
immigration is determined by the background intensity t and results in an exogenous
random process Once an immigrant event occurs it generates a whole cluster of events
Namely a zeroth-order event which we will call the mother event can trigger one or
more rst-order events daughter events Each of these daughters in turn may trigger
several second-order events the grand-daughters of the initial mother and so on
In this context the branching ratio n is dened as the average number of daughter
triggered events of rst generation per mother event Depending on the
events ie
branching ratio there are three regimes i sub-critical n  1 ii critical n  1
and iii super-critical or explosive n  1 Starting from a single mother event or
immigrant at time t1 the process dies out with probability 1 in the sub-critical and
critical regimes and has a nite probability to explode to an innite number of events in
the super-critical regime The critical regime for n  1 separates the two main regimes
and is characterized by power law statistics of the number of events and in the number
of generations before extinction 21 For n  1 the process is stationary in the presence
of a Poissonian or more generally stationary ux of immigrants
In the case of a constant background intensity t    const and in the sub-
critical regime n  1 the branching ratio is exactly equal to the average fraction of
the number of descendants in the whole population of events 1 2
In other words
the branching ratio is equal to the average proportion of endogenously generated events
among all events and can be considered as an eective measure of endogeneity of the
system
23 Estimation of the degree of endogeneity n
There are several routes to estimate n from real data One is to reverse-engineer the
clusters and calculate the average number of direct descendants to any given event This
can be done via the stochastic declustering parametric 22 and non-parametric 23 24
method which amounts to reconstruct from the sequence of events the original cluster
branching structure or at least distinguish between descendants and immigrants but
this may have severe limitations in the presence of long-memory kernels 25 A simpler
way is to just use the Maximum Likelihood Estimation method which benets from
the fact that the log-likelihood function is known in closed form for Hawkes processes
26 27 Namely the parameters of the model 4 with any specied kernel 58 can
be determined by numerical maximization of the following log-likelihood function
log Lt1     tN   Z T
tFtdt 
log tiFti
where ti  0 T  is the set of observation times of the events In general the calculation
of the log-likelihood function 9 has computational complexity ON 2 However for
exponential 5 and approximate power law 8 kernels it can be reduced to ON by
taking advantage of a recursive relation 27
It should be noted that the calibration of the Hawkes model on nite samples always
results in an underestimation of the real value of the branching ratio n
Indeed the
events occurring before the time window of calibration that could trigger events within the
window are not taken into account This results in an overestimation of the background
rate  and therefore an underestimation of the observed n In other words neglecting
past events before the windows and their triggering eect leads to the misattribution that
many of the endogenous events are exogenous 28 Moreover for any given event not
all its daughter events are observed within the given window 0 T  especially for mother
events that happen to be close to the right-end boundary T  This eect which also results
in underestimating the observed secondary events and thus n becomes more pronounced
for larger memories of the process as determined by the shape and characteristics time
of the kernel function ht
Finally the calibration of the Hawkes model on the data should be validated with
the goodness-of-t using residual analysis 14 which consists in studying the residual
process dened as the nonparametric transformation of the initial series of the event time
stamps ti into
i Z ti
ttdt
where tt is the conditional intensity of the Hawkes process 4 estimated with the
maximum likelihood method As it was shown in 29 under the null hypothesis that the
data has been generated by the Hawkes process 4 with selected kernel ht the residual
process i should be Poisson with unit intensity The goodness-of-t can then be veried
both by i visual cusum plot or Q-Q plot analysis and ii rigorous statistical tests
such as independence tests applied to the sequence of i andor tests of the exponential
distribution of the transformed inter-event times i  i1 which amounts to testing the
uniform distribution of the random variables Ui  1  expi  i1 in the interval
0 1 The null hypothesis of uniform probability distribution of variables Ui can be tested
using the Kolmogorov-Smirnov test and the null hypothesis of absence of autocorrelations
in sequence U1     UN 1 can be addressed with the Ljung-Box test
24 Implementation of the Maximum Likelihood Estimation method
Consider the Maximum Likelihood Estimation of the Hawkes process 4 where the
background intensity is constant t    const and the kernel ht is parametrized
with a parameter set      for the exponential kernel 5   c  for the Omori-
type kernel 6 and   0  for the exponential kernel 8 The expression for the
log-likelihood function 9 in this case can be written in the form
log Lt1     tN   T  nH1 
logcid0  nH2 ticid1
H1 
Xi1 Z T
ht  tidt
hti  tj
H2 ti  Xtj ti
The most computationally intensive part of the log-likelihood calculation is the summation
over all past events in H2 ti We propose here to partition the search space into two
subspaces and to subordinate one to the other as demonstrated recently in another similar
application 30 We thus reformulate the optimization problem
 n   arg min
nh  log L n t1     tN i
into the two-step optimization problems
  arg min
St1     tN 
St1     tN   min
n h  log L n t1     tN i
n T  nH1 
logcid0  nH2 ticid1 
In other words the cost function St1     tN  of the parameters set  of kernel function
ht is equal to the value of the original cost function  log L when the parameters
 n are selected as the best ones for a given value of  This reformulation achieves
three important goals i similarly to 30 in some cases it decreases the number of local
minima ii it allows us to present an illustrative visualization of the search space see
Section 34 and iii it dramatically decreases the computational cost of the calibration
by using dynamic programming Fixing the parameter set  of the kernel function allows
us to solve eciently the optimization problem 15 by computing H1 and H2 ti
only once for each step of problem 14
Further in the subordinated optimization over the space  n we can simplify 15
by an analytical determination of one of the parameters using the method proposed
by Lyubushin and Pisarenko 31 This method is based on the fact that the log-
likelihood 11 satises the following equation
 log L
 log L
 T  nH1  N
where N is the total number of observed events within 0 T  At the extremum of the
log-likelihood for    n  n and    the partial derivatives are vanishing
 log L   log Ln  0 and equation 16 provides a relation between the optimal
parameter values
T  nH1   N
Taking into account this last expression 17 one can replace the optimization prob-
lem 15 by the equivalent problem
St1     tN   min
n N 
logcid18 N
T H2 ti  H1
cid19 
Despite the fact that the proles of the cost functions in 18 and 15 are dierent
they have the same minimal value The optimization procedure now consists of the
subordinated optimization 14 and 18 that yields the values of  and  while the
value of  is derived from 17   NT  nH1 T 
Note that the quality of the parameter estimation depends on the specic parametriza-
tion of the model 2 The classical econometric literature such as 9 32 33 34 35
employ the Hawkes model with the exponential kernel t   expt which is not
normalized and require the estimation of the two parameters   which just have to
obey the conditions    0 In contrast the explicit denition of the branching ratio
n   see Eq 4 which is bounded for stationary process 0  n  1 provides
much more robust estimations For similar reasons in the subordinated procedure 15
we suggest inferring the background activity   0 from 17 while minimizing the
modied cost function with respect to n as in 18 and not vice versa
3 Common issues of the calibration of the Hawkes process
In this and following sections we discuss a number of issues arising in the treatment
of high frequency nancial data and in numerical procedures for the calibration of the
Hawkes model that bias the estimation of the parameters of the model Some of these
issues are common to all types of problems and some are specic to the work 4 which
claims that nancial markets are functioning systematically at criticality We document
and quantify precisely a series of biases associated with the estimation of the branching
ratio n that rationalize the spurious claim many eects concur to give the impression of
an apparent criticality and these eects need to be understand identied and corrected
before any solid conclusion can be drawn
31 Impact of outliers for dierent memory kernels
The rst issue concerns the robustness of the estimation of the branching ratio n in the
presence of a small fraction of outliers and for dierent memory kernels This problem
is motivated by the observed distribution of inter-quote durations for mid-quote price
changes of E-mini SP 500 Futures Contracts during Regular Trading Hours from 930
to 1615 CDT which forms the basis of the contradictory claims presented in 4 versus
By comparing Tables 1 and 2 we observe the existence of genuine outliers in the data
as compared with what is expected from time series generated purely with the Hawkes
process Specically we can observe that the relation between quantiles of inter-quote
durations eg fractions Q95Q90 or Q99Q95 is similar for theory Table 2 and empirical
data Table 1 However the relation of the maximal observed duration to the quantiles
MaxQ99 is dierent When in theory the maximum observed inter-quote duration
is only  3 times larger than Q99 the empirical observations show that the maximum
observed inter-quote duration is more than ten times larger than the 99 quantile Q99
even in the years of active trading This reects a highly irregular trading activity in the
market that is not fully captured at the extremes by the best Hawkes process calibrated
on the same data
To illustrate potential biases that result from this eect we introduce a few outliers
extreme inter-event intervals in synthetic time series generated by the Hawkes process
so as to mimic the phenomenon observed in Table 1 compared with Table 2 We create
dierent synthetic time series of the Hawkes process with duration 0 105  104 seconds
and xed exogenous intensity   03 and branching ratio n  07 using
i the exponential kernel 5 with   01 or
ii the power law kernel 6 with c  01 and   05 or
iii the approximate power law kernel 8 with 0  01 and   05
In order to get rid of the edge eects we burn the initial period 0 105 seconds we
discuss the impact of the edge eect in details in section 33
In each synthetic time
series we record the maximum observed duration and then replace a small fraction of
randomly selected durations with values that are M-times M  1 2 and 5 larger than
Table 1 Descriptive statistics of inter-quote durations between consecutive mid-quote price changes
during Regular trading Hours of E-mini SP 500 Futures Contracts in dierent time periods empirical
quantiles Q90 Q95 Q99 maximum value Max total number of observed mid-quote durations N
number of durations that are at least twice greater than Q99 N2Q99  and fraction of latest in the overall
sample Fraction N2Q99 N  100
Date from
Date to
February 1 2002 April 1 2002
February 1 2006 April 1 2006
February 1 2009 April 1 2009
February 1 2012 April 1 2012
Q90 Q95 Q99 Max
N2Q99 Fraction
Table 2 Theoretical quantiles and maximum values of inter-event durations for time series generated
with the Hawkes process with the approximate power law kernel 8 for   002   015 and n and
0 given in the rst two columns parameters are taken from 4 The data is obtained by numerical
simulation of the Hawkes process on the interval 0 108  105 with burning of the interval 0 108
Q90 Q95
Q99 Max
the initial maximum observed value On these time series with a small fraction of outliers
we calibrate the Hawkes model with the same kernel as the one used to initially generate
the synthetic time series This is repeated 100 times to obtain a statistical average and
standard deviation of the estimated branching ratio n
The results are shown in Figure 1 which gives the estimated branching ratio as a
function of the fraction of introduced outliers for the three types of memory kernels One
can observe that the estimations of the time series generated with an exponential kernel
Fraction of modified durations 
Figure 1 Estimated branching ratio n obtained by calibrating the Hawkes model on synthetic time series
generated with the Hawkes process for the three dierent memory kernels discussed in the text in the
presence of a small fraction of inter-quote duration outliers The outliers are generated by replacing a
fraction given in the abscissa of the durations by durations that are M times larger than the largest
duration of the simulation where M  1 green lines M  2 red lines and M  5 blue lines The
lower black continuous line corresponds to the exponential kernel 6 with M  5 Solid blue and red
lines correspond to the approximate power law kernel 8 and dashed blue and red lines correspond to
the power law kernel 6 Shaded areas cover the  one standard deviation of the statistical estimation
of n over 100 realizations The horizontal dashed line at n  07 is the true value used in the synthetic
generation of all time series The critical value n  1 is also indicated as a horizontal dashed line
are robust to the introduction of outliers as the estimated n remains within one standard
deviation of the true value 07 used to generate the synthetic time series Remarkably
a completely dierent behavior occurs for power law kernels for which the estimated
branching ratio is strongly biased upward even for small fraction of outliers So that for
M  2 1 of outliers introduce an upward bias of approximately 028 or 40 in relative
value resulting in an almost critical estimated n  098 This bias is much stronger for
M  5 where just 017 of outliers are sucient to lead to the spurious conclusion that
the system is at or close to critical n  1 or even super-critical in the case of exact
power law kernels The intuition behind this result is that large outlier time intervals are
interpreted incorrectly within the Hawkes model with power law kernel as waiting times
that reect a genuine endogenous triggering activity The scale-free power law kernels are
exible enough to endogenize these outliers In contrast the more rigid form of the
exponential kernel which is characterized by a single characteristic time scale   leads to
a much smaller inuence of the outliers in the calibration
In real data as we observe from the Table 1 approximately 0104 of inter-event
durations are at least twice larger than the 99 quantile Though the exact value of the
bias for n depends on the distribution and location of extreme durations the synthetic
case for M  1 can be considered as a conservative estimate of the bias with M  2
providing a reasonable order-of-magnitude value
32 Eect of the regularization part of power law kernels
In section 21 presenting the Hawkes model 4 we have introduced three dierent
versions of the memory kernel ht with power law tail which dier in the way the
regularization at short times is introduced as shown in gure 2
a the Omori law 6 with a smooth regulation
b the power law kernel with a sharp ultraviolet cut-o 7 and
c the approximation of the power law kernel with a sum of exponentials 8
While their tail are identical asymptotically they dier signicantly at short times In
particular while the Omori law kernel is strictly decaying from t  0 the other two are
non-monotonous and are characterized by a maximum at some tmax  0 corresponding
to the most probable waiting time between a mother and its rst-generation triggered
daughters
Figure 2 Illustration of the dierences between the three power law kernels implemented within the
Hawkes model a the Omori law 6 continuous green line b the power law kernel with a cut-
o 7 dashed red line and c the approximation of the power law kernel with a sum of exponents 8
dotted-dashed blue line
These apparently innocuous dierences have actually a signicant impact on the es-
timation of the branching ratio n leading to important biases when the kernel is not
specied correctly In order to illustrate this we have numerically simulated the Hawkes
process with one of the power law kernels presented in Fig 2 and calibrated the Hawkes
model with another of these power law kernel on this synthetic data We have considered
the following cases
i we simulated the Hawkes process with the approximate power law kernel 8 for
0  1 and   05 and calibrated the obtained time series with the Hawkes process
with the Omori law kernel 6
ii we simulated the Hawkes process with the Omori law kernel 6 for c  1 and
  05 and calibrated the obtained time series with the Hawkes process with the
approximate power law kernel 8
iii nally to assess the possible bias of the estimation procedure we simulated the
Hawkes process with the approximate power law kernel 8 for 0  1 and   05
and calibrated the obtained time series with the Hawkes process with the same
kernel
We have xed the background activity   01 and spanned the branching ratio n in the
interval 0 1 In order to get rid of edge eects we simulated time series with duration
0 108  105 seconds and burned the initial period 0 108 seconds to only analyze the
interval 108  1 108  105
Branching ratio n
Figure 3 Illustration of the bias in estimating the branching ratio which is caused by the misspecication
of the long-memory kernel see text i Estimated branching ratio n using the Hawkes model with the
Omori law kernel 6 when the generating process has the approximate power law kernel 8 blue curve
with circles ii estimated branching ratio using the Hawkes model with the approximate power law
kernel when the generating process has the Omori law kernel red curve with squares iii estimated
branching ratio in the case when both generating model and estimating model have the approximate
power law kernel black curve with dots Solid lines show the average value of n obtained over 100
estimations The shaded areas cover the  one standard deviation of the statistical estimation over the
same 100 realizations
Figure 3 shows the results of the numerical estimations of n The straight diagonal
line and the narrow condence bands for case iii conrms the excellent quality of the
calibration in the case of a correct specication of the Hawkes model However when the
memory kernel of the generating process diers from the kernel used in the calibration
procedure signicant biases are observed
In case i when the true kernel is the ap-
proximate power law kernel 8 and the calibration procedure uses the Omori law 6 we
observe a slight n  n  007 underestimation of the branching ratio However in the
opposite case ii when the true kernel is the Omori law and the estimation is performed
using the approximate power law kernel 8 the overestimation of the branching ratio is
large of the order of n  n  02 for n  03 In fact the misspecication of the kernel in
this case may lead to the incorrect conclusion of criticality when the real branching ratio
is subcritical n  08
The above observations suggest that the choice of the kernel should be a subject of a
careful analysis for any empirical calibration in which a long-memory kernel is used Our
above tests also challenges the claims of 4 since they have been based on the use of the
approximate power law kernel 8 for the calibration of the empirical data which leads
to the largest upward bias for n even leading to spurious criticality
The question of the proper form of the power law kernel is not that trivial For
example even in the synthetic cases discussed here we nd that residual analysis is
usually unable to reject the false model in the sub-critical regime especially far from
criticality n  07 Nested statistical tests 36 are not applicable here since none of the
models 68 can be embedded into another one We nd that the Akaike information
criterion 37 can successfully select the correct model in our synthetic cases However
the usefulness and selectivity of the Akaike criterion for the Hawkes model in the presence
of noise remains open and should be further investigated
More specic ways of recovering the kernel involve non-parametric estimation meth-
ods which however also exhibit severe limitations The parametric and non-parametric
Expectation Maximization EM methods 38 23 39 24 typically penalize irregularity
of estimated functions to avoid large uctuations which will pose a problem when the
kernel of the Hawkes process is a power law with a cut-o 7 or the approximated power
law 8 with a small value of 0 Moreover the power of these methods is small when the
clusters of the Hawkes process overlap signicantly 25 which is typically the case with
nancial high frequency data
The recently proposed nonparametric method 40 based on the estimation of the
autocorrelation function of the counting process dNt which is also used in 4 is free of
this limitation However its numerical implementation faces severe short-comings when
implemented in short time windows The counting process is bursty by nature and its
autocorrelation function is long-ranged mimicking the long-memory property of nancial
volatility Therefore its estimation on short intervals is strongly biased Moreover since
it does not decay to zero within the interval of observation the Discrete Fourier Transform
of the sample will be contaminated by high frequencies because of the truncation the
so-called spectrum leakage Because the Fourier spectrum of the correlation function
is transformed into a nonlinear formula for the Fourier spectrum of the kernel higher
harmonics of the spectrum of the kernel which are responsible for the behavior of the
kernel ht for small values of t appear due to a nonlinear transformation of the higher
harmonics of the spectrum of the correlation function see for instance Eq 33 in 40
These higher harmonics are in general not estimated reliably in comparison with the
lower frequencies which themselves are responsible for the long-term behavior of the
kernel ht Enlarging the time window in this case is not generally possible due to the
presence of strong intraday non-stationarity The problem of estimating the kernel at
short lags is illustrated in Fig 1 of 40 where the strictly decaying exponential kernel 5
was estimated with the condition of having a narrow cut-o at short lags
Finally in nancial applications all methods described above are seriously aected
by a strong noise due to the nature of the nancial data feeds As we will discuss in sec-
tion 42 even though the timestamps have millisecond resolution the eective resolution
of the data is much lower up to seconds in early 2000s which makes all estimations at
short lags particularly unreliable
33 The edge eect and issues concerning numerical simulations of long memory pro-
two characteristic times T095 and T099 dened respectively by R T095
R T099
For the exponential kernel 5 and power law kernels 68 with suciently large
exponents   1 and   1 the decay of the kernel as time increases is suciently fast
so that transient periods aecting simulations are relatively short However in the case
of small values of the exponents  and  the decay of the power law kernels 6 and 8
is very slow which implies that events far in the past continue to inuence the triggering
of events far in the future To quantify this slow decay Table 3 gives the values of
htdt  095 and
htdt  099 Intuitively a given event occurring at some time ti has still a 5
resp 1 probability of triggering new events at times larger than T095 resp T099
Taking the typical values 0  1 second and   015 we have T095  11  107 seconds
and T099  13  109 seconds for approximate power law kernel 8 which is about 4888
resp 577777 trading days where each trading day consists of 625 hours For the
exact Omori-type kernel 6 these values are much larger T095  49  108 seconds and
T099  23  1013 seconds Even for the large windows of two months considered in 4
if one considers the power law kernels with small exponent  as capturing the real long
memory of the empirical time series this implies that events that occurred years before a
given window still exert a signicant inuence on the occurrence of events in that window
For numerical simulations this implies that edge eects play a dominant role and may
lead to signicant biases in parameter estimations if not accounted for properly by using
extremely long realizations and burning out a very long transient
The resulting edge eect is illustrated in Fig 4 which shows that the transient period
which is characterized by a strong non-stationarity lasts much longer than the charac-
teristic times T095 and T099 As a rule of thumb one needs to wait at least 100 times
more than T099 in order to reach the quasi-stationary regime
Such long transient periods obviously aect the estimation of the parameters assuming
that the underlying data is generated by the Hawkes process with long memory kernels
such as 68 In order to illustrate this we have simulated almost critical n  099
Hawkes processes with the approximate power law kernel 8 in the very long interval
0 109  106 We have then burned a rst interval of length 109 seconds As seen from
Table 3 and Fig 4 such a long interval is sucient to avoid the transient dynamics for
kernels with   05 But it is denitely not sucient to get rid of edge eects for   02
Our simulation experiment is aimed at determining possible biases in the estimation
of the branching ratio n when the Hawkes model used for the estimation has a short-
Table 3 Characteristic times T095 and T099 dened respectively by R T095
R T099
htdt  099 for the approximate power law kernel 8 and Omori-type kernel 6
htdt  095 and
  
Approximate PL kernel 8
T0950
16  108
11  107
65  105
92  103
2  102
T0990
43  109
13  109
20  108
16  106
5  103
Omori-type kernel 6
T0950
T0990
1  1013
10  1020
49  108
23  1013
32  106
10  1010
23  104
48  106
42  102
1  104
Figure 4 Dependence of the average number of events in 10 seconds intervals as a function of time where
the origin of time t  0 is the start of the simulation with no preceding ancestors The simulations are
performed for an almost critical n  099 Hawkes process with approximate power law kernel 8 for
  1 0  1 and   1 upper panel   05 middle panel and   02 lower panel Dashed and
solid vertical lines corresponds to the values T095 and T099 respectively The curves are obtained by
averaging over 2000 realizations
range exponential kernel 5 while the data is generated with the long-range generating
kernel 8 As described above we have used the interval of T  106 seconds which
corresponds approximately to 2 months of trading 44 trading days of 625 hours each
We split the interval of 106 seconds into subintervals of length 1800 seconds 30 minutes
 in total 555 subintervals however we did not apply any randomization to the times-
tamps the eect of randomization will be considered in the following section We then
calibrated the Hawkes model with an exponential kernel 5 in each of the 1800 seconds
intervals The upper panel of Fig 5 shows that notwithstanding the dierence of the
kernels using in the generating and in the estimating Hawkes process the calibration
recovers correctly the near critical values of the branching ratio n  09 for   05
ie when the edge eect was successfully removed However for   02 when the tran-
sient eect is strong and only a minor fraction of the ancestors fall within the window
of analysis the estimations present signicant downward biases The slow increase of
the estimated branching ratio n as 0 decreases reects the corresponding decrease of the
transient period see Table 3
Figure 5 Results of the estimation of the branching ratio n on synthetic time series generated by the
Hawkes process with the approximate power law kernel 8 with   002 and n  099 The parameter 0
is spanned in the interval 103 1 x-axis Dierent lines correspond to dierent values of the exponent
 bottom-up   01 black circles   015 red squares   02 blue triangles   05 magenta
crosses and   1 green diamonds The gure corresponds to the case when the estimation is performed
by using the Hawkes process with the exponential kernel 5 and the whole interval 0 106 seconds is
split into intervals of 30 minutes each for calibration Solid lines represent averages over 555 estimations
The condence intervals are extremely narrow and not presented the maximum standard deviation of
the estimation of n is approximately 006 for the upper panel and 0013 for the lower panel
Simulation of long time-series can be made computationally ecient The traditional
simulation method of Hawkes self-excited processes uses the modied thinning proce-
dure 41 42 which is based on simulating the homogeneous Poisson process with su-
ciently high intensity and then accepting or rejecting points with a probability given by
the Hawkes model The numerical complexity of this method is ON 2 which prevents
its use for simulating long time-series of near-critical Hawkes processes A much more
ecient simulation method benets from the branching structure representation of the
Hawkes process discussed in section 22 43 44 One can construct the Hawkes process
as a combination of the homogeneous Poisson process with intensity t   that de-
scribes immigration and of a set of non-homogeneous Poisson processes with intensities
it  nht  ti which represent descendants Using a vectorized construction of the
ancestors for each generation of descendants one can reach a numerical complexity of
OT  M where T is a window size and M is the number of generations that t this
window
34 Multiple extrema of the likelihood function and suboptimal solutions
The calibration of the Hawkes model requires nding the numerical solution of the
minimization of  log L negative of the log-likelihood function given by expression
9 in the parameter space  n c  or  n 0  for the power law kernels When
the data is generated by the Hawkes model 4 with the same memory kernel as that used
for the calibration as can be ensured in synthetic cases the 4-dimensional cost function
 log L typically has one pronounced minimum that can be easily found with almost
any local minimization algorithm However in some cases when the memory kernel used
for the estimation diers from the kernel used to generate the data the cost function
may have a very at valley similar to that associated with the Rosenbrock function 45
which requires adaptive numerical descent methods For real data in the presence of noise
and possible outliers and the highly probable fact that the generating process does not
coincide exactly with the Hawkes process the cost function is in general non-trivial with
several local minima As a consequence solutions found by local optimization methods are
highly sensitive to the starting point of the method In this case local descent methods are
not sucient and should be complemented with some metaheuristics 46 the simplest
of which is running local minimization methods starting from multiple starting points
scattered within the whole search space and choosing the best solution among them
Nevertheless there is typically no guarantee that the solution thus found is the best one
In order to illustrate the problem associated with the existence of multiple local minima
and its consequence we visualize the cost function in a two-dimensional space obtained
by partitioning the four-dimensional search space  n 0  into subspaces  n and
0  and subordinating the rst to the second as described in Section 24 We consider
the concrete example of the E-mini SP 500 Futures Contracts with millisecond resolu-
tion where periods of two months are used for the calibration in which only the Regular
Trading Hours 930 to 1615 CDT are kept for each day which are glued together in a
single continuous time series In addition a kind of detrending is performed to remove
the intraday U-shape lunch eect by using a function wt that we estimated over the
same two months interval see section 43 Before calibration we additionally applied a
randomization procedure to the timestamps within millisecond intervals1
Figure 6 shows the typical surface of the cost function S0 t1     tN  for the period
19982005 Even the reduced cost function 15 keeps a rather complex structure with
several minima and very long valleys where the minimization algorithm can be stuck The
global minimum of the cost function pointed out by the vertical black line on the left of
g 6 gives   03031 n  00751 0  000028   24604 and the corresponding value
of the cost function negative log-likelihood is S  209105 However if the starting point
for the minimization algorithm is chosen incorrectly for instance if the minimization is
started from the point 0  1   1 then the optimization procedure converges to
the local minimum vertical black line on the right of g 6 which gives   00150
n  11054 0  28089   01442 and a much higher value of the cost function
1In the sections 4144 we discuss in details every step of this data preprocessing and show that all
of them result in signicant upward biases for the estimated branching ratio
Figure 6 Surface of the cost function S0 t1     tN  eq 15 used in the calibration of the Hawkes
model for the period March 1 2001  April 30 2001 see text for details The two black vertical lines
point to the locations of the two local minima global on the left and local on the right
S  285  105 These two solutions belong to completely dierent regimes of the process
while the optimal set of parameters points to a subcritical regime n  00751  1 the
sub-optimal solution diagnoses a super-critical regime n  11054  1
On the same dataset and using the same procedure 4 have reported estimates for
n 0 and  that are very close to those corresponding to the local suboptimal minimum
n  11054 leading these authors to incorrectly conclude about the existence of criticality
or super-criticality at all times
The dierence between the local suboptimal and global optimal minimization of the
cost function is illustrated in Figure 7 This allows us to compare the suboptimal
branching ratios reported in 4 with the optimal ratio obtained by exhaustive explo-
ration of the parameter space One can see that the estimates presented in 4 are far
from being optimal and this leads to the incorrect conclusion that the system has been
critical and super critical over most of time since 1998 while the global optimal solution
shows a low branching ratio until 2002 which grows progressively until 2006 and then
later uctuates between 08 and 11 approximately
Finally we stress that the issue raised here is even more crucial for the multivariate
Hawkes model The monovariate Hawkes process with for instance exponential kernel 5
and constant background activity is fully described with a set of only 3 parameters 
n and   With the addition of one dimension and accounting for the cross-excitation
bi-variate model that is used for instance in 9 47 34 35 48 the parameter set is
increased to 10 parameters when no symmetry is imposed The six-variate Hawkes model
suggested in 10 is parametrized by 78 parameters in the general case The 10-variate
Figure 7 Bottom Estimate of the branching ratio for the global minimum red circles and suboptimal
estimates presented in 4 black squares
Hawkes model 49 in general would require the calibration of 210 parameters however
49 reduces the number of calibrated parameters to 32 Obviously the augmentation
of the parameter set makes the cost function even more complicated This increases
the chances of multiple extrema and thus poses a numerical challenge to the calibration
procedure
4 Microstructure of the high-frequency nancial data and biases
The following four subsections discuss some properties of the high-frequency nan-
cial data that should be taken into account in any analysis using point process models
Since they have been been inadequately addressed in 4 and are rarely mentioned in the
literature it is useful to develop them for future use
Before proceeding we need to note that high-frequency data cleaning is one of the
most important aspects of any modeling since presence of outliers or misrecorded data
may substantially bias estimates and forecasts Issues of ltering and cleaning the high-
frequency will not be discussed here and we refer reader to the existing literature 50 51
52 53
41 Regular Trading Hours and overnight trading
The rst question concerns the choice of the period of analysis In our original work 2
we have calibrated the Hawkes process within short intervals of 10 to 30 minutes that we
scanned over the whole history of E-mini SP 500 Futures Contracts Then we collected
the estimates within Regular Trading Hours RTH 930 to 1615 CDT in each month
and averaged them to construct the reexivity index for the given month 2 3
contrast 4 considered long intervals of two months where data was concatenate from
the Regular Trading Hours of each day to construct one single continuous point process
Choosing RTH is motivated by two important reasons First the trading activity
within the RTH is typically larger than within the overnight period Second the trading
activity within each 24 hours is highly non-stationary where both intensity and dynamics
within RTH and overnight periods dier signicantly When using short time intervals of
10 to 30 minutes there is no problem with focusing only on the RTH In contrast throwing
away a signicant part of the day 1625 hours versus 625 RTH and concatenating data
from day to day over a two-month period for the kind of long-term analysis performed in
4 may lead to signicant and uncontrollable biases
The electronic trading of E-mini SP500 Futures is active 23 hours a day And while
the volume traded over night is lower than the volume traded in RTH this dierence is not
that extreme As seen from g 8 left in recent years almost 20 of the daily volume
is traded outside of the RTH Moreover the number of limit orders submitted over night
and the number of mid-quote price changes triggered by them which are the input of the
analysis in 2 3 4 is huge As seen in g 8 right since 2002 the number of mid-quote
price changes over night consistently amounts to more than 3040 of all mid-quote price
changes and in recent years it reached the value of 50 and even 67 in 2010 In other
words in 2010 the number of events mid-quote price changes observed over night was
up to 2 times larger than the number of events observed in RTH and considered by 4
Throwing away the over night data is thus simply unwarranted especially when using
long memory power law kernels that link the activity across days as done in 4
Figure 8 Left Fraction of total daily volume that is traded outside of Regular Trading Hours 930 to
1615 CDT on E-mini SP 500 Futures Contracts Right Fraction of total daily number of mid-quote
price changes that is observed outside Regular Trading Hours on E-mini SP 500 Futures Contracts
Each point represents an average value calculated over two months intervals
By considering only regular trading hours RTH and by constructing single contin-
uous time series one ignores a substantial part of the data This data manipulation no
doubt will aect the calibration of the model and in particular the estimates of the branch-
ing ratio Indeed the ignored periods might contain zero-order events that are ancestors
of many of the events observed during RTH which will result in an underestimation of
the background activity and an overestimation of the branching ratio For example dur-
ing pre-market auctions the important information about corporate and macroeconomic
events which is usually released before trading hours is being digested by the equity
prices Thus not accounting for this process will overestimate the total reexivity of
all equity futures and derivatives markets Or on the other hand if most of zero-order
events are concentrated in RTH the endogenous part of the intensity and the branch-
ing ratio will be underestimated However as we will see further accounting for this
non-stationarity is not trivial
Moreover practically speaking there exists three substantially dierent trading peri-
ods for E-mini SP 500 Futures Contracts i Regular Trading Hours of the US markets
9001600 EST ii Regular Trading Hours of the Asian markets 2000300 EST and
iii Regular Trading Hours of the European markets 3001100 EST Market partic-
ipants and their strategies dier substantially over these trading periods It is dicult
to expect that a single model with xed parameters can describe all three regimes si-
multaneously and arguably it is better to consider the three intervals iiii separately
While treating the three intervals separately is feasible within the approach of 2 3 that
considers short analysis windows to construct an eective monthly index out of these es-
timates in contrast the approach of Hardiman et at 4 is inapplicable here as it requires
the construction of long continuous time series
42 Latency grouping of timestamps and the bundling eect
Despite the fact that many high frequency data providers give millisecond or even
sometimes microsecond precision for tick timestamps a rather large number of transac-
tions and quote changes have identical timestamps The origin of this phenomenon lies
in the nature of the data feed from the exchange which is obtained by the FASTFIX
protocol which is nowadays the most commonly used protocol for communicating nan-
cial data The protocol bundles multiple updates of multiple instruments within a single
message by an algorithm designed by the exchange Then the package is sent to the data
provider collection system and which stamps the time of events on the package arrival
but not to the time when the transactions quotes were actually executed and recorded
by the exchange The exchange time which is the only reliable timestamp is coded in the
FASTFIX protocol and stamped with a resolution of seconds due to the protocol limi-
tation The millisecond timestamp of the data provider enriches the second resolution
however it is subjected to an uncertainty which arrises from
i overhead brought by processing data on both sides
ii latency of the message traveling from the exchange to data providers collection
system
iii grouping multiple events into single FASTFIX packages
Data processing i include time for packingunpacking FASTFIX package which is of
order of several tens of microseconds 54 and overhead brought by operating with large-
scale databases which vary from milliseconds to hundreds of milliseconds or even seconds
depending on the implementation and scale The latency to the exchange ii is usually
of the order of tens of milliseconds These two factors introduce an eective bias to the
timestamps which in most cases is more-or-less regular and could be considered to be
constant Therefore it is not usually relevant in the calibration of the Hawkes process
which is invariant with respect to a time shift However as we will see further these
factors can uctuate a lot which may aect the calibration of the parameters
In most cases the strongest bias to the timestamps is introduced by the bundling of
events into FASTFIX packages iii which results in the fact that the the actual time
of any tick is uncertain within a range that is larger than or equal to the time between
two consecutive FASTFIX packages In contrast to i and ii this uncertainty is both
irregular for any particular event and has much higher scales For most exchanges this
time varies from tens of milliseconds in recent years to several hundreds of milliseconds
or even seconds in early 19982003 3
Time between consecutive quotes ms
Figure 9 Histogram of the non-zero inter-quote time for E-mini SP 500 Futures Contracts on October
2 2002 over Regular Trading Hours Data source Thompson Reuters Tick History
Figure 9 illustrates the irregularity in the time between consecutive quotes recorded
by the Thompson Reuters Tick History TRTH One can observe a strong peak of the
histogram at the inter-quote time of approximately 350 ms which reveals the high irregu-
larity of the data on the arrival times due to the mechanisms described above bundling of
multiple events in packages and quasi-regular delivery of these packages from the exchange
to data collection system Naively one could assume that this peak at 350 ms marks the
time between deliveries of consecutive FASTFIX packages However this assumption is
very far from being correct
In order to ensure data integrity each FASTFIX package that is sent from the ex-
change carries a unique within a trading day sequential message number Some data
providers such as Thompson Reuters Tick History  TRTH record these numbers along
with the information about the quote or trade itself which allow us to retrospectively eval-
uate the uncertainty of timestamps of quotes or trades The time between FASTFIX
packages can be estimated as the dierence in timestamps of the rst recorded quotes in
successive packages Another important characteristic that we analyze here is the over-
head for data processing This is particularly important for the TRTH and similar data
provider which timestamp events not at the moment when the FASTFIX packet arrives
to the collecting system but at the moment when the actual trade or quote is written
to the database The procedure of the data processing in TRTH is the following the
collection system receives packages from many exchanges containing updates for many
instruments All these packages are uncompressed and events trades and quotes are
dispatched to the queue from where they are written to the database The additional
uncertainty due to this pipeline might be signicant In order to estimate this data pro-
cessing overhead we considered the dierence in timestamps between the last and the rst
quotes with identical sequential package number ie events within the same package
Time between FASTFIX packages msec   
Processing time msec  
Figure 10 Histograms of the time between consecutive FASTFIX packages left panels and overhead for
the data processing right panels for E-mini SP 500 Futures Contracts over Regular Trading Hours on
dierent dates topbottom October 02 2002 October 08 2004 October 10 2006 October 10 2008
February 12 2009 October 08 2010 October 10 2011 and October 10 2012 Data source Thompson
Reuters Tick History
Such an analysis is illustrated in Fig 9 which uncovers that the inter-package times
are much larger than naively estimated above at about 350 ms Fig 10 upper panels
We observe that the time between consecutive FASTFIX packages for the same date vary
from 300 to 1700 milliseconds having an average of approximately 1 second Moreover
the processing times is also very large  one package could take up to 1300 milliseconds
In a worst case scenario this can sum up to 3 seconds Note that this eective resolution
can be signicantly lower than the resolution of timestamps from the exchanges 1 second
Then the peak in Fig 9 can be simply explained as corresponding not to the inter-package
time but to the time dierence between the end of processing a previous package and the
start of processing the next one We veried this explanation directly by analyzing the
corresponding histogram
Fig 10 presents the histograms of waiting times between packets and of processing
times in dierent years for the TRTH database Before 2009 both sources of uncertainties
in the timestamps were enormous having scales of 5001000 milliseconds In the second
half of 2009 the data quality increased signicantly and these uncertainties dropped to
values of tens of milliseconds Note however that these uncertainties of tens of milliseconds
are still larger than the often claimed millisecond resolution
The frequency of package arrivals depends on multiple factors First it obviously
depend on the protocol itself The FAST FIX Adapted for Streaming protocol was in-
troduced in 2006 and had two revisions in 2007 revision 11 and 2009 revision 12
The underlying FIX Financial Information eXchange protocol also passed through sev-
eral evolutionary steps from its introduction in 1992 to the modern versions FIX 43
20012002 FIX 44 2003 and FIX 50 200620082009 The adoption time of pro-
tocols is dierent in dierent exchanges and each exchange may implement some specic
rules for collecting messages However for the same exchange all customers with simi-
lar subscription types will receive quote updates with the same frequency In contrast
additional overhead for data processing may vary from one data provider to another and
depend on the design of the collection system Therefore our estimates are indeed con-
servative in the sense that latency and waiting time depend on the infrastructure of the
collecting system and the type of data-feed subscription
In practice one can expect
that the exchange gateway can send up to tens of thousands of FASTFIX messages per
second However such throughput is available for co-located collecting system
Two possibilities can be considered for dealing with the uncertainty in the timestamps
resulting from the FASTFIX protocol One is to consider only the timestamps provided
by the exchange with resolution of seconds as a reliable source of data The other is to
use the enriched millisecond timestamps of TRTH while accounting for the uncertainty
due to bundling updates in FASTFIX packages as for instance in 3 In any case it
is unacceptable to ignore the uncertainty in the timestamps by assuming a millisecond
resolution of timestamps
In order to account for timestamp uncertainties in the calibration of the Hawkes pro-
cess we proposed a randomization procedure which consists in uniformly redistributing
timestamps within the intervals of uncertainty 2 Essentially this amounts to assuming
that each event occurring within the interval of uncertainty is independent of all the others
within the same interval but not between dierent intervals This raises the important
question of what will happen if the interval in which the randomization is employed is
dierent from the real time interval in which events are bundled to packages In order to
illustrate the possible bias that could stem from an improper assumption we develop the
following numerical simulations
We assume that there is no overhead for data processing or it is constant and all
events are bundled by the exchange in packages every second and transmitted to the data
collection system However for the calibration we randomize the timestamps within
a smaller intervals  eg   1 millisecond as in 4 Obviously this will result
in spurious clustering of events Fig 11 that will be reected in the estimation of the
branching ratio It turns out that the resulting overestimation of the branching ratio is
similar to the eect of the outliers that we considered in section 31
1 second
FASTFIX Package 2
FASTFIX Package 3
Events at
the Exchange
Packages at
the Collection
randomized
Figure 11 Illustration of the packaging of events at the exchange and the randomization procedure see
For illustration of the possible resulting bias we have simulated the Hawkes process
with   35 n  05 and i exponential kernel 5 with   06 and ii approximate
power law kernel 8 with   1 0  03 We have also considered iii a Poisson process
with intensity 0  7 as a generating model All the parameters are chosen to mimic
the distribution of the number of events per second observed for E-mini SP 500 Futures
Contracts on October 2 2002 Poisson-like distribution with mean 68 median 7 and
standard deviation 37 We grouped events into packages over 1 second intervals and
randomized them in intervals of duration  where  is varied from 1 millisecond to 1
second Then we calibrated these data with the Hawkes model with the exponential
kernel in case i and with the approximate power law kernel in cases ii and iii
Fig 12 shows the dependence with  of the estimated branching ratio n in all three
cases We see that the estimated n monotonously increases when decreasing  having
a cusp at   05 seconds ie when the randomization interval becomes smaller than
one half of the original bundling interval For smaller values of  the estimated
branching ratio reaches an asymptotic value which is independent of the kernel used
for the estimation of the branching ratio as well as of the underlying generating model
Indeed the Poisson model represented by the black line in Fig 12 has the same spurious
value n  0856 as does the subcritical Hawkes model with n  05 This spurious value
is only dependent on the average number of events per FASTFIX package which are
aggregated over 1 second interval in this case Fig 13 illustrates this dependence from
which one can conclude that in the worst case scenario one can observe near-critical
values of the branching ratio when in fact the underlying model is pure Poisson with
independent events n  0 This observation strongly suggests that any sound analysis
should either conne itself to using only second resolution timestamps of the exchange
as in 2 or be complemented with an extensive robustness analysis implying model
calibration with dierent assumptions on  as for instance performed in 3
 msec
Figure 12 Illustration of the bias that results from improper assumptions on the duration  of ran-
domization intervals see text when the generating model is i the Hawkes process with an exponential
kernel red line with circles ii the Hawkes process with the approximate power law kernel blue line
with squares and iii a Poisson process black line with dots Each point represents an average over
100 realizations of length T  105 seconds the initial interval of length 106 seconds was burned The
condence intervals are extremely narrow and not presented the maximum standard deviation of the
estimation of n is 004
Average number of events per FASTFIX package
Figure 13 Dependence of the asymptotic value of the estimated branching ratio n when the randomization
interval duration   0 see text The graph is obtained by numerical simulations of the homogeneous
Poisson process with dierent intensities 0 randomizing the data as described in the text and by
calibration of the Hawkes model with exponential kernel 5 on the Poisson generated data
Interestingly in the randomization procedure discussed above the overhead for the
data processing when present may mitigate the negative impact of bundling events into
packages
Indeed instead of being recorded at exactly the same time with periods of
roughly 1 second Fig 10 left panels which leads to spurious clustering when being
randomized over 1 millisecond intervals the events of the same package are being more or
less uniformly distributed across most of the interval between two consecutive packages
Thus only a fraction of the inter-event intervals corresponding to the peak in Fig 9 are
aected by the incorrect procedure In other words for the TRTH database these two
biases present in the data somehow partially compensate each other in the calibration of
the Hawkes model However this compensation is not necessarily present for other data
providers that stamp events at their times of arrival to the collection system
Finally we draw attention to the fact that the ndings of the present section challenge
all studies concerned with inter-event times such as 55 56 57 58 59 60 61 62 and
others where the observations of heavy tailed distributions long memory correlation
functions and multifractal scaling of the inter-event durations may be strongly biased by
the bursty nature of the raw data subjected to the bundling eects
43 Intraday seasonality and detrending of the data
As already mentioned for a constant background intensity t    const and
in the stationary case n  1 the branching ratio n is exactly equal to the fraction of
the average number of endogenously generated events among all events 1 2 We can
therefore interpret n as a reexivity index which quanties the degree of endogeneity
or reexivity of the nancial market 63 When t is time varying a correct estimation
of n is made much more dicult by the fact the we do not observe t independently but
only the sum of the exogenous component t rst term in the rhs of expression 2
and of the triggered events second integral term in the rhs of expression 2 If the
dynamics of t is convoluted many parameters may be needed to capture its complexity
which makes the estimation less robust and often degenerate several solutions of very
dierent parameter values compete at the same level of the likelihood function As a
consequence the determination of the branching ratio n may be severely hindered and
biased
The exogenous component t is known unfortunately to exhibit complicated struc-
tures such as the U-shape intraday seasonality reecting the fact that the trading
activity varies signicantly during a typical day being large at the open low at lunch
time and larger towards the close To address this problem one needs to impose some
knowledge on t either coming from some expert advice a priori information or em-
pirical treatment Once a model M t is chosen each day activity can then be corrected
by using this M t in order to obtain an eective activity with an assumed constant
eective  The hope is to remove in this way the U-shape intraday seasonality and other
patterns This detrending method to apply this correction uses the same time-transform
theorem utilized for the residual analysis in Eq 10 Namely if t is the intensity of
the generating process for ti then the integral 10 transforms the process ti into
a stationary Poisson process i with constant intensity 1 Similarly one can assume
0 M t
will transform the Hawkes generated time series ti with the time-varying background
intensity into the Hawkes process i with constant background intensity which could
even be assumed equal to 1 if the estimation of M t is faithful thus further reducing
the number of unknown parameters While intuitive note that this transformation is
that if M t is the intensity of the background process then the integral i R ti
a Hawkes model This results from the fact that the time transformation i  R ti
only an approximation because the time series ti and i cannot be both generated by
0 M t
distorts the kernel of the process
if for instance all events ti have the same mem-
ory function t  ti the events i will have memory functions   i ti M t
that depends both on the background intensity and on the original event time ti We
expect this approach to become less and less reliable as n is closer to 1 We nd that
when t varies slowly and weakly the detrending provides reasonable estimates for n
However it does not perform when in the presence of strong non-stationarity Similar
results are obtained by just dividing each daily activity by the model exogenous activity
M t Fundamentally this results from the fact that no detrending method can fully
disentangle the exogenous from the endogenous events in the presence of multiple gener-
ations of triggered events other than by reconstructing the full sets of genealogical trees
38 23 39 24 which is in general extremely dicult for time-varying t A misspec-
ication of t clearly leads to biases in the estimation of the branching ratio n if one
underestimates the true background intensity t at some times exogenous events will
then be attributed to the self-excitation component of the intensity thus leading to an
over-estimation of n in contrast choosing t as being similar to the total unconditional
intensity will lead to interpret most endogenous events as exogenous and will push n close
Let us illustrate concretely these problems by using a standard approach as done
in 4 which superimposes the trading activity of each day by matching the intraday
time and taking the average over a large set of days for each intraday time This leads
to dene an average intensity wt over a given time period eg a day and assume
that the background intensity follows this path while particular features of each trading
day are averaged out In 4 wt was assumed to be a perfect periodic function with
a period of one day over the whole two-months interval of the analysis Unfortunately
this assumption seems to be quite far from reality The left panels of Fig 14 illustrate
that the assumed U-shape daily seasonality holds only on average while every particular
day has its own large scale deviation from this trend ie the intraday activity cannot be
viewed as just noise decorating the average assumed average intraday pattern Moreover
for some days such as September 17 18 or October 11 29 in Fig 14 the dynamics of
trading activity does not follow a U-shape at all having no signicant drop over lunch
time As a result the detrending method using an average unconditional intensity does
not fully remove the non-stationarity in the time series as illustrated by the right panels
of the Fig 10
A severe problem inherent on such seasonality detrending is that by construction it
is supposed that all deviations from the average trading activity are the result of endoge-
nous self-excitations in the system The case of September 18 2007 shown in Fig 14 is an
excellent illustration that this assumption may often be incorrect At around 1400 EST
one can observe a large spike of trading activity that was accompanied almost immedi-
ately by a spike in price not shown in the picture In fact this is a direct consequence
of the announcement of the Federal Interest rate by the FOMC commission which sur-
prised investors the announced rate 475 was lower than the market expectations
5 according to the survey of Bloomberg LP In other words the price jump and the
Raw data
Time EST
After detrending
Time EST
Figure 14 Estimated in non-overlapping 5-minute intervals unconditional intensity in events-per-second
of ow of mid-quote price changes of E-mini SP 500 Futures Contracts on some dates of September
October 2007 Left panels present the raw data black bars and the average intensity over the period
of September 1October 30 2007 red line Right panels present the unconditional intensity after
detrending using the average intensity
spike in volatility are unambiguous exogenous event that resulted from the release of an
unpredicted piece of information In contrast the average intensity approach used in 4
amounts to count it in signicant part as being endogenous Note also that this single
event September 18 2007 weights in signicantly in the determination of wt as can be
seen by comparing the dierent scales in the ordinates of the panels on the left where
the spike at around 1400 EST is not really representative of the trading activities of the
other days while it constitutes a prominent feature of wt due to the impact of the large
activity on September 18 2007 In fact the release of almost all macroeconomic news
results in similar patterns of trading activity  spikes at release times often preceded
by a freezing of trading and a vanishing liquidity from the order book right before the
announcement see for instance 64
Informed by synthetic simulations we will see in the following section that even small
regime shifts lead to large overestimations of the branching ratio if it is not accounted
for in the model of the background activity For this reason spikes such as the one
that occurred in September 18 2007 thus tend to drive the estimated branching ratio
upward
In order to avoid such non-stationarity issues in our analysis of short-term
reexivity 2 3 we have ignored all days with scheduled macroeconomic announcements
such as the FOMC rate decision or in the case of oil futures the US Energy Information
Administration weekly report
44 Non-stationarity regime shifts and mixing
Stationarity is extremely important in statistics The Hawkes model is no exception
and in fact this model is very sensitive to the presence of any non-stationarity present
in the time series As discussed in section 43 intraday trading is subjected to the U-
shape activity pattern and spikes of activity on release of news However there is another
perhaps even more important source of non-stationarity in the fact that trading activity
uctuates signicantly from day to day Fig 15 shows the variations of the number of
mid-quote price changes within a two month window in three dierent years One can
observe that the activity uctuates strongly within a factor 2 to 5 in each such two month
windows Therefore constructing a continuous time series in two month windows out
of such non-stationary data would mix up dierent regimes which again would seriously
aect the calibration of the model
Figure 15 Dynamics of daily numbers of mid-quote price changes counted over Regular Trading Hours
for the Front Month Contract of the E-mini SP 500 Futures from February 1 to April 1 in three dierent
We investigate the inuence of such non-stationarity of the activity with synthetic time
series Consider two independent realizations of the Hawkes process with identical kernels
and identical parameters but with dierent branching ratios n1 and n2 If we concatenate
them and calibrate the Hawkes model on the resulting time series we will not estimate a
branching ratio n for the combined time series that is somewhere between n1 and n2 as
we might expect intuitively The regime switch will be interpreted by the Hawkes model
as an excess clustering Therefore the estimated n which is also an eective measure of
clustering for the Hawkes model will tend to be higher so as to account for the regime
switch In fact we nd n always to be no less than the highest branching ratio of the
individual sets
We illustrate this point with the following numerical simulation We simulate two
independent synthetic time series of the Hawkes process Ft1 and Ft2 with approximate
power law kernel 8 for parameters 1  2  1 1  2  1 01  02  1 second
Fixing the branching ratio n1  05 we span the branching ratio n2 within the interval
005 095 The length of realization is T  105 seconds however as before we simulate
realization on the interval 0 105  106 seconds and burn the initial period 0 106 to
get rid of edge eects Then we concatenated the two time series to obtain continuous
realizations Ft1 Ft2 and Ft2 Ft1 and calibrated Hawkes process with the same ap-
proximate power law kernel 8 on the newly created realizations of length T  2  105
seconds Results are presented in g 16 left2 The branching ratio is always estimated
to be larger than 05 reaches the critical value n  1 and even becomes super-critical
n  1 when n2 is signicantly smaller than n1  05 The dependence is especially
steep for n2  05 when concatenating realizations with n1  05 and n2  03 results in
the estimation n  08 For n2  02 the estimated branching ratio of the concatenated
realization is critical n  1
Similarly for two independent Hawkes process with identical branching ratio n but
with dierent background activity 1 and 2 concatenation results in a signicant over-
estimation of the branching ratio n Similarly to the previous simulation we consider
now independent synthetic time series of the Hawkes process Ft1 and Ft2 with identi-
cal branching ratio n1  n2  05 but with dierent values of the background activity
1  05 and 2 spanning the interval 0 2 Results are presented in g 16 right Sim-
ilarly to the previous case even small dierences between 1 and 2 result in signicant
overestimations of n For example a 40 dierence results in the estimation n  09
when the true value is n  05 a 60 dierence results in estimating the critical value
n  1 for branching ratio
The impact of the regime shift from one set of parameter values to another set of
parameter values is so strong that a strong upward bias of the branching ratio occurs even
in total absence of clustering and of triggering This is shown by using several independent
Poisson processes with dierent intensities i and by generating a continuous time
series of events with regime shifts from one intensity to the next in subsequent time
windows By construction such a time series should be characterized by a vanishing
branching ratio as there is no triggering and no clustering Calibrating the Hawkes model
on such a time series with any kind of kernel actually leads to quite high values of n
depending on the amplitude of dierent Poisson intensities i Keeping in mind that the
2Note that in contrast to gure 1 both lengths T of the realizations and the initial burned period
are larger and the exponent  is smaller which results in more robust estimations
Figure 16 Results of the estimation of the branching ratio n on the synthetic time-series Ft1 Ft2 blue
line with circles and Ft2 Ft1  red line with squares where Ft1 and Ft2 are Hawkes processes with
approximate power law kernel 8 The left panel corresponds to the case of concatenating processes with
identical background activity 1  2  1 but dierent branching ratios n1  05 and n2 spanning the
interval 005 095 The right panel corresponds to the case of concatenating processes with identical
branching ratio n1  n2  05 but dierent background activities 1  1 and 2 spanning the interval
0 2 All lines are obtained as averages over 100 dierent realizations The condence intervals are
extremely narrow and are not presented the maximum standard deviation of the estimation of n in 100
dierent realizations is about 0025
variability of the real intensity over a two months period is extremely high as illustrated
in Fig 15 we expect a signicant upward bias in the estimation of n just as result of the
spurious interpretation of clustering by the Hawkes calibration exercise
In order to verify this prediction we have constructed sets of synthetic time series
in the manner of 4 but instead of real data we have considered independent Poisson
processes for each trading day where the intensity i was estimated from the real data as
equal to i  NiT with Ni being the number of observed mid-quote price changes in the
i-th day and T  625 hours is the duration of the Regular Trading Hours RTH Fig 17
shows the estimated branching ratio in successive two-month windows from 1998 to 2012
While the true branching ratio is zero by construction the calibration by the Hawkes
model gives a value n hovering around the critical value 1 Strikingly while the truth
is zero self-excitation the calibration diagnoses criticaility over almost the whole 14
years interval To test the robustness of this result we also calibrated the Hawkes model
on the time series in which days where trading was stopped before 1615 CDT were ignored
some of these days correspond to the rollover dates At these dates the total activity is
extremely low see for instance the rst day of the third weeks in Fig 15 and such strong
regime shift will drive the estimated n up However even by removing these days the
estimated branching ratio remains very high above 08 and close to the critical value 1
after 2007 as seen from the black continuous curve in g 17
This synthetic non-stationary Poisson time series is too simple to embody the complex
phenomenology reported in 4 such as the values of the exponent  of the memory kernel
which is estimated in the range 0    1 as compared to the structureless Poisson process
Figure 17 Results of the estimation of the branching ratio n on the synthetic time-series concatenated
from independent poisson processes with intensities taken from the real data red curve The black
curve with squares corresponds to the case where days with trading durations smaller than 5 hours
are ignored The blue curve with triangles corresponds to the case where one continuous time series is
constructed from independent Hawkes process with exponential kernel 5 with n  03   10 seconds
and  proportional to the intensity taken from the real data
We have thus added some structure in our synthetics by considering time series generated
by the Hawkes model with an exponential kernel 5 n  03 mild self-excitation   10
seconds and  proportional to the intensity taken from the real data low-activity days
were ignored The blue curve in Fig 17 shows that the estimated branching ratio often
becomes super-critical n  1 ie very far from its true value n  03 In this case
the exponent  is estimated to be 01    03 which is similar to what is reported by
4 The existence of regime shift thus strongly bias all parameters and leads to utterly
spurious conclusions
Moreover the residual analysis of these calibrations cannot reject the null hypothe-
sis of Poisson residuals 10 Hence residual analysis does not reject the null that time
series truly generated by regime-switching non-homogeneous Poisson processes would be
generated by the Hawkes process at criticality notwithstanding the fundamental dier-
ences between generating and calibrating models The same lack of rejection of the null
is obtained when the true generating process is the regime-switching Hawkes model with
an approximate power law kernel 8 n  05 0  1 second and   05 which is
undistinguishable according to the residual analysis from a stationary Hawkes process at
or above criticality with branching ratio n  1 and with the exponent 01    03
Finally we stress that the spurious criticality reported here is not unique to the ap-
proximate power law kernel 8 In the presence of non-stationarity a calibration using
the Hawkes model with any kernel including short-memory exponential kernel 5 will
result in signicant biases on the estimated branching ratio n
5 Conclusion
We have presented a careful analysis of a set of eects that lead to signicant biases
in the estimation of the branching ratio n arguably the key parameter of the Hawkes
self-excited Poisson process The motivation of our study stems from the meaning of n as
a direct measure of endogeneity or reexivity since n is exactly equal to the fraction of
the average number of endogenously generated events among all events 1 2 for stationary
time series Concretely the measure n  07  08 reported in our recent studies 2 3
means that 70 to 80 of all trades in the E-mini SP 500 Futures Contracts and in
major commodity future contracts are due in recent years to past trades rather than to
external eects or exogenous news This result has important implications concerning
the ecient market hypothesis and the stability of nancial markets in the presence of
increasing trading frequency and volume The detailed and careful study of the possible
biases attached to the estimation of n has been additionally catalyzed by the recent
claim based also on the calibration of the Hawkes process that nancial market have been
continuously functioning at or close to criticality n  1 over the last decades 4 a result
in contradiction with our other studies 2 3
Our overall conclusion is that calibrating the Hawkes process is akin to an excursion
within a mineeld that requires expert and careful testing before any conclusive step
can be taken We have identied ve main sources of generally upward biases for the
branching ratio which are arguably present in high-frequency nancial data The size
of the biases are found to be largely sucient to explain the discrepancy between our
previous results and the claim of 4 We nd in fact that all the biases are likely to be
present in the analysis of 4 which can be seen as a showcase of the diculties inherent
in the calibration of the Hawkes process
Our careful analysis of the microstructure of high-frequency nancial data the impact
of regular trading hours and overnight trading the eect of latency of trade recording
the grouping of trade timestamps and their bundling suggest that most if not all studies
published until now that have been concerned with inter-event times should be revisited
In particular our investigations suggest that the observations and estimations of heavy
tailed distributions presented in other papers the reports of long memory correlation
functions and of multifractal scaling of the inter-event durations may be strongly biased
by the bursty nature of the raw data and may be subjected to the bundling eects and
non-stationarity due to day-to-day regime shifts
Given the many distortions occurring in trade recording by various stock market ex-
changes robust choices ought to be made before any analysis is performed In particular
our results presented here suggests that it is very dicult if not impossible to estimate
reliably long-memory eects using high-frequency nancial data In previous works 2 3
we have bypassed this issue of long memory to focus solely on the branching ratio at
time scales of no more than tens of minutes for which one can show that most of the
problematic biasing eects can be tamed
On the conceptual level reported long-term memory and critical branching structures
of the triggering process are heavily biased by and can even result entirely from the non-
stationarity in the high-frequency nancial time series Such ndings can result from
the misuse of the mono-scaling statistical tools such as correlation function and Hawkes
process applied to intrinsically multi-scaling generated data With the increase of the
duration of the time intervals beyond the intraday time scales for which approximate
stationarity holds to days and months these analyses suer from the problem of mix-
ing dierent feedback mechanisms The existence of many dierent kind of strategies
from those used by the long-term investors to the behaviors of hedgers chartists high-
frequency traders and other market participants creates non-trivial multifractal scaling
of the price time series see for instance 65 Thus the application of the Hawkes model
with a single memory kernel on datasets at large time scales weekly monthly and so
on is not consistent with the complexity of the system Here one should consider some
multifractal extension of the Hawkes model for example in the spirit of Self-Excited
Multifractal Process 66 that combines explicitly self-exciting feedback together with a
nonlinear multi-scaling response function However no multifractal point process models
have yet been introduced
Acknowledgements
We are grateful to Spencer Wheatley for helpful discussions and suggestions while
preparing this manuscript as well as for pointing out the impact of outliers section 31
We also thank Igor Artyukhin for the help in numerical tests We are grateful to Stephen
Hardiman and Jean-Philippe Bouchaud for many long and fruitful discussions while
preparing the manuscript as well as for pointing out some inconsistencies and mistakes
in preliminary versions of this article
We would like to express our deep gratitude to professor Alexander Saichev for many
fruitful discussions and collaborations on the critical regime of branching processes Pro-
fessor Saichev who passed away on 8 June 2013 has been an outstanding contributor to
the general theory and to the practical applications of self-excited processes that we have
been developing at ETH Zurich
References
1 A Helmstetter D Sornette Importance of direct and indirect triggered seismicity
in the ETAS model of seismicity Geophysical Research Letters 30 11 2003 1576
2 V Filimonov D Sornette Quantifying reexivity in nancial markets Toward a
prediction of ash crashes Physical Review E 85 5 2012 056108
3 V Filimonov D Bicchetti N Maystre D Sornette Quantication of the High Level
of Endogeneity and of Structural Regime Shifts in Commodity Markets UNCTAD
Discussion Paper No 212 2013 146
4 S J Hardiman N Bercot J-P Bouchaud Critical reexivity in nancial markets
a Hawkes process analysis The European Physical Journal B - Condensed Matter
and Complex Systems 86 10 2013 442
5 V Filimonov S Wheatley D Sornette Eective measure of reexivity of the self-
excited Hawkes and Autoregressive Conditional Duration point processes Working
paper 2013 114
6 A G Hawkes Spectra of some self-exciting and mutually exciting point processes
Biometrika 58 1 1971 8390
7 A G Hawkes Point Spectra of Some Mutually Exciting Point Processes Journal of
the Royal Statistical Society Series B Methodological 33 3 1971 438443
8 P Hewlett Clustering of order arrivals price impact and trade path optimisation
In Workshop on Financial Modeling with Jump processes Ecole Polytechnique
9 C G Bowsher Modelling security market events in continuous time Intensity based
multivariate point process models Journal of Econometrics 141 2 2007 876912
10 R Cont Statistical Modeling of High Frequency Financial Data Facts Models and
Challenges IEEE Signal Processing 28 5 2011 1625
11 D Oakes The Markovian Self-Exciting Process Applied Probability Trust 12 1
1975 6977
12 D Vere-Jones T Ozaki Some examples of statistical estimation applied to earth-
quake data I Cyclic Poisson and self-exciting models Annals of the Institute of
Statistical Mathematics 34 1 1982 189207
13 D Vere-Jones Stochastic Models for Earthquake Occurrence Journal of the Royal
Statistical Society Series B Methodological 32 1 1970 162
14 Y Ogata Statistical models for earthquake occurrences and residual analysis for
point processes Journal of the American Statistical Association 83 401 1988 9
15 A Helmstetter D Sornette Subcritical and supercritical regimes in epidemic models
of earthquake aftershocks Journal of Geophysical Research 107 B10 2002 2237
16 T Utsu A statistical study of the occurrence of aftershocks Geophysical Magazine
30 1961 521605
17 T Utsu Y Ogata The centenary of the Omori formula for a decay law of aftershock
activity Journal of Physics of the Earth 41 1 1995 133
18 L Knopo Y Y Kagan Stochastic synthesis of earthquake catalogs Journal of
Geophysical Research 86 B4 1981 28532862
19 Y Y Kagan L Knopo Statistical short-term earthquake prediction Science
236 4808 1987 15631563
20 A G Hawkes D Oakes A Cluster Process Representation of a Self-Exciting Process
Journal of Applied Probability 11 3 1974 493503
21 A Saichev A Helmstetter D Sornette Anomalous Scaling of Ospring and Gen-
eration Numbers in Branching Processes Pure and Applied Geophysics 162 2005
11131134
22 J Zhuang Y Ogata D Vere-Jones Stochastic declustering of space-time earthquake
occurrences Journal of the American Statistical Association 97 458 2002 369380
23 D Marsan O Lengline Extending Earthquakes Reach Through Cascading Science
319 5866 2008 10761079
24 E Lewis G O Mohler A Nonparametric EM algorithm for Multiscale Hawkes
Processes 2011 116
25 D Sornette S Utkin Limits of declustering methods for disentangling exogenous
from endogenous events in time series with foreshocks main shocks and aftershocks
Physical Review E 79 6 2009 061110
26 Y Ogata The asymptotic behaviour of maximum likelihood estimators for stationary
point processes Annals of the Institute of Statistical Mathematics 30 1 1978 243
27 T Ozaki Maximum likelihood estimation of Hawkes self-exciting point processes
Annals of the Institute of Statistical Mathematics 31 1 1979 145155
28 D Sornette M J Werner Apparent clustering and apparent background earth-
quakes biased by undetected seismicity Journal of Geophysical Research 110 B9
2005 B09303
29 F Papangelou Integrability of Expected Increments of Point Processes and a Related
Random Change of Scale Transactions of the American Mathematical Society 165
1972 483506
30 V Filimonov D Sornette A Stable and Robust Calibration Scheme of the Log-
Periodic Power Law Model Physica A Statistical Mechanics and its Applications
392 17 2013 36983707
31 A Lyubushin V Pisarenko Research on Seismic Regime Using Linear Model of
Intensity of Interacting Point Processes Izvestiya Physics of the Solid Earth 29
1994 11081113
32 L Bauwens N Hautsch Modelling Financial High Frequency Data Using Point
Processes in T Mikosch J-P Krei R A Davis T G Andersen Eds Handbook
of Financial Time Series Springer 2009 pp 953979
33 E Errais K Giesecke L R Goldberg Ane Point Processes and Portfolio Credit
Risk SIAM Journal on Financial Mathematics 1 1 2010 642
34 Y At-Sahalia J Cacho-Diaz R J A Laeven Modeling nancial contagion using
mutually exciting jump processes
35 P Embrechts T Liniger L Lu Multivariate Hawkes Processes an Application to
Financial Data J Appl Probab 48A 2011 367378
36 S S Wilks The Large-Sample Distribution of the Likelihood Ratio for Testing Com-
posite Hypotheses The Annals of Mathematical Statistics 9 1 1938 6062
37 H Akaike A new look at the statistical model identication IEEE Transactions on
Automatic Control 19 6 1974 716723
38 J Zhuang Y Ogata D Vere-Jones Analyzing earthquake clustering features by
using stochastic reconstruction Journal of Geophysical Research 109 B5 2004
B05301
39 A Veen F P Schoenberg Estimation of SpaceTime Branching Process Models
in Seismology Using an EMType Algorithm Journal of the American Statistical
Association 103 482 2008 614624
40 E Bacry K Dayri J-F Muzy Non-parametric kernel estimation for symmetric
Hawkes processes Application to high frequency nancial data The European Phys-
ical Journal B - Condensed Matter and Complex Systems 85 5 2012 157
41 P A W Lewis G S Shedler Simulation of nonhomogeneous poisson processes by
thinning Naval Research Logistics Quarterly 26 3 1979 403413
42 Y Ogata On Lewis simulation method for point processes IEEE Transactions on
Information Theory 27 1 1981 2331
43 J Mller J G Rasmussen Perfect simulation of Hawkes processes Advances in
applied probability 37 3 2005 629646
44 J Mller J G Rasmussen Approximate Simulation of Hawkes Processes Method-
ology and Computing in Applied Probability 8 1 2006 5364
45 H H Rosenbrock An Automatic Method for Finding the Greatest or Least Value
of a Function The Computer Journal 3 3 1960 175184
46 E-G Talbi Metaheuristics from design to implementation Wiley 2009
47 I M Toke Market making in an order book model and its impact on the spread
in Econophysics of Order-Driven Markets Springer Verlag 2011 pp 4964
48 E Bacry S Delattre M Homann J-F Muzy Modeling microstructure noise with
mutually exciting point processes Quantitative Finance 13 1 2013 6577
49 J Large Measuring the resiliency of an electronic limit order book Journal of Fi-
nancial Markets 10 1 2007 125
50 M M Dacorogna R Gencay U A Muller R B Olsen O V Pictet An Introduc-
tion to High-Frequency Finance Academic Press 2001
51 T N Falkenberry High Frequency Data Filtering Tech rep Tick Data Inc Sep
52 C T Brownlees G M Gallo Financial econometric analysis at ultra-high frequency
Data handling concerns Physica A Statistical Mechanics and its Applications 51 4
2006 22322245
53 O E Barndor-Nielsen P R Hansen A Lunde N Shephard Realized kernels in
practice trades and quotes The Econometrics Journal 12 3 2009 C1C32
54 J W Lockwood A Gupte N Mehta M Blott T English K Vissers A Low-
Latency Library in FPGA Hardware for High-Frequency Trading HFT in 2012
IEEE 20th Annual Symposium on High-Performance Interconnects HOTI IEEE
2012 pp 916
55 C Gourieroux J Jasiak Nonlinear Autocorrelograms an Application to Inter-Trade
Durations Journal of Time Series Analysis 23 2
56 Z-Q Jiang W Chen W-X Zhou Scaling in the distribution of intertrade durations
of Chinese stocks Physica A Statistical Mechanics and its Applications 387 23
2008 58185825
57 E Scalas T Kaizoji M Kirchler J Huber A Tedeschi Waiting times between
orders and trades in double-auction markets Physica A Statistical Mechanics and
its Applications 366 2006 463471
58 P Oswiecimka J Kwapien S Drozdz Multifractality in the stock market price in-
crements versus waiting times Physica A Statistical Mechanics and its Applications
347 2005 626638
59 Z Eisler J Kertesz Size matters some stylized facts of the stock market revisited
The European Physical Journal B - Condensed Matter and Complex Systems 51 1
2006 145154
60 Z-Q Jiang W Chen W-X Zhou Detrended uctuation analysis of intertrade
durations Physica A Statistical Mechanics and its Applications 388 4 2009 433
61 P C C Ivanov A Yuen B Podobnik Y Lee Common scaling patterns in intertrade
times of U S stocks Physical Review E 69 5 2004 056107
62 M Politi E Scalas Fitting the empirical distribution of intertrade durations Phys-
ica A Statistical Mechanics and its Applications 387 8-9 2008 20252034
63 G Soros The Alchemy of Finance Reading the Mind of the Market John Wiley 
Sons NY 1987
64 R Almgren High-Frequency Event Analysis in Eurex Interest Rate Futures
65 A Arneodo J-F Muzy D Sornette Direct causal cascade in the stock market
The European Physical Journal B - Condensed Matter and Complex Systems 2 2
1998 277282
66 V Filimonov D Sornette Self-excited multifractal dynamics Europhysics Letters
94 4 2011 46003
