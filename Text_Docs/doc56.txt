Process-Oriented Parallel Programming with an Application to
Data-Intensive Computing
Edward Givelberg
July 22 2014
Abstract
We introduce process-oriented programming as a natural extension of object-oriented programming
for parallel computing It is based on the observation that every class of an object-oriented language
can be instantiated as a process accessible via a remote pointer The introduction of process pointers
requires no syntax extension identies processes with programming objects and enables processes to
exchange information simply by executing remote methods Process-oriented programming is a high-level
language alternative to multithreading MPI and many other languages environments and tools currently
used for parallel computations It implements natural object-based parallelism using only minimal syntax
extension of existing languages such as C and Python and has therefore the potential to lead to
widespread adoption of parallel programming We implemented a prototype system for running processes
using C with MPI and used it to compute a large three-dimensional Fourier transform on a computer
cluster built of commodity hardware components Three-dimensional Fourier transform is a prototype
of a data-intensive application with a complex data-access pattern The process-oriented code is only
a few hundred lines long and attains very high data throughput by achieving massive parallelism and
maximizing hardware utilization
1 Introduction
The rst commercially available microprocessor CPUs appeared in the early 1970s These were single-
processor devices that operated with a clock rate of less than 1 MHz Over the course of the following three
decades increasingly faster and cheaper CPUs were built This was achieved in large part by persistently
increasing the clock rate By 2004 the CPU clock rates reached the 3-4 GHz range and heat dissipation
became a major problem
In order to continue improving the performance and the cost microprocessor
designers turned to parallel computing Today nearly all computing devices servers tablets phones etc
are built using processors with multiple computing cores whose operating frequency is less than 35 GHz
The industry move to parallel computing succeeded because practically every application contains tasks
that can be executed in parallel And yet a decade later the vast majority of computer programs are still
being written for execution on a single processor and parallel computation is being realized primarily using
threads sequential processes that share memory
Parallel programming is generally recognized as dicult and has been a subject of extensive research
see 3 9 11 and references therein The problem with threads is eloquently described in 10 The
author paints a bleak scenario
If  programmers make more intensive use of multithreading the next generation of computers
will become nearly unusable
In the scientic computing community parallel programs are typically written in Fortran and C with OpenMP
2 and MPI 1 Dozens of high-level languages for parallel programming have also been developed but
presently none of them is widely used Even the so-called embarrassingly parallel computations are not
embarrassingly easy to implement
In this paper we develop a new framework for parallel programming which we call process-oriented
programming
It is based on the fundamental observation that any class in an object-oriented language
can be instantiated as a process accessible via a remote pointer 6 Such a process instantiates an object
of the class and acts as a server to other processes remotely executing the class interface methods on
this object The introduction of remote pointers enables a straightforward extension of object-oriented
programming languages to process-oriented programming with hardly any syntax additions We show that
process-oriented programming is an ecient framework for parallel programming and we propose it as a
replacement for multithreading MPI and many other languages environments and tools currently used for
parallel computations We implemented a prototype system for running processes using C with MPI and
investigated process-oriented programming in the context of data-intensive computing
The rapid growth of generated and collected data in business and academia creates demand for increas-
ingly complex and varied computations with very large data sets The data sets are typically stored on
hard drives so the cost of accessing and moving small portions of the data set is high Nevertheless a large
number of hard drives can be used in parallel to signicantly reduce the amortized cost of data access In
7 we argued that a data-intensive computer can be built using widely available commodity hardware
components to solve general computational problems involving very large data sets
The primary challenge in the construction of the data-intensive computer lies in software engineering
The software framework must balance programmer productivity with ecient code execution ie big data
applications with complex data access patterns must be realizable using a small amount of code and this
code must attain high data throughput using massive parallelism In this paper we aim to demonstrate that
process-oriented programming is the right framework for the realization of the data-intensive computer
We chose the computation of a large three-dimensional Fourier transform as the subject of our study
primarily because it can be considered as a prototype of a dicult data-intensive problem We show that
using processes our application can be realized with only a few hundred lines of code which are equivalent to
approximately 15000 lines of C with MPI We also show that even with the complex data access patterns
required for the computation of the 3D Fourier transform our code attains very high data throughput by
achieving massive parallelism and maximizing hardware utilization
In section 2 we describe a simple model of storage of a data set as a collection of data pages on mul-
tiple hard-drives This model is used in the examples in section 3 where we introduce processes The
process-oriented implementation of the Fourier transform is described in section 4 and the eciency of the
computation is analyzed in section 5 We conclude with a discussion in section 6
Our presentation uses C but can be easily applied to any object-oriented language size t is a large
non-negative integer type used in C to represent the size of a data object in bytes
2 The Data Set
21 Data Pages
We represent an N1N2N3 array of complex double precision numbers as a collection of N P1N P2N P3
pages where each page is a small complex double precision array of size n1  n2  n3 First we dene a
Page class which stores n bytes of unstructured data
class Page
public 
Page  sizet n  u n s i g n e d char  data  
 Page  
p r o t e c t e d 
sizet n 
u n s i g n e d char  data 
The ArrayPage class is derived from the Page class to handle three-dimensional complex double-precision
array blocks
class A r r a y P a g e 
public Page
public 
A r r a y P a g e 
int n1  int n2  int n3 
double  data
 c o n s t r u c t o r that a l l o c a t e s data 
A r r a y P a g e  int n1  int n2  int n3  
void t r a n s p o s e 1 3  
void t r a n s p o s e 2 3  
private 
int n1  n2  n3 
The ArrayPage class may include functions for local computation with the array page data such as the trans-
pose functions that are important in the computation of the Fourier transform see section 42 Throughout
this paper the arrays have equal dimensions and the distinct variables n1 n2 and n3 are maintained only
for the clarity of exposition
Storage devices
We store data pages on hard drives using a single large le for every available hard drive The following
PageDevice class controls the hard drive IO
class P a g e D e v i c e
public 
P a g e D e v i c e 
string filename 
sizet NumberOfPages 
sizet P a g e S i z e
 P a g e D e v i c e  
void write  Page  p  sizet P a g e I n d e x  
void read  Page  p  sizet P a g e I n d e x  
p r o t e c t e d 
string f i l e n a m e 
sizet N u m b e r O f P a g e s 
sizet P a g e S i z e 
private 
int f i l e  d e s c r i p t o r 
The implementation of this class creates a le filename of NumberOfPages  PageSize bytes Pages of
data are stored in the PageDevice object using a PageIndex address where PageIndex ranges from 0 to
NumberOfPages - 1 The write method copies a data page of size PageSize to the location with an oset
PageIndex  PageSize from the beginning of the le filename Similarly the read method reads a page
of data stored at a given integer address in the PageDevice Linux direct IO functions are used in the class
implementation For example the Linux open function used with the O DIRECT ag attempts to minimize
cache eects of the IO to and from the specied le
For array pages we dene the ArrayPageDevice which extends PageDevice as follows
class A r r a y P a g e D e v i c e 
public P a g e D e v i c e
public 
A r r a y P a g e D e v i c e 
string filename 
sizet NumberOfPages 
int nn1  int nn2  int nn3
n1  nn1   n2  nn2   n3  nn3  
P a g e D e v i c e 
filename 
NumberOfPages 
2  n1  n2  n3  sizeof  double 
void w r i t e  t r a n s p o s e 1 3  Page  p  sizet P a g e I n d e x  
void r e a d  t r a n s p o s e 1 3  Page  p  sizet P a g e I n d e x  
void w r i t e  t r a n s p o s e 2 3  Page  p  sizet P a g e I n d e x  
void r e a d  t r a n s p o s e 2 3  Page  p  sizet P a g e I n d e x  
private 
int n1  int n2  int n3 
The implementation of the transpose methods is very simple For example
void A r r a y P a g e D e v i c e 
r e a d  t r a n s p o s e 1 3 
Page  p  sizet P a g e I n d e x
read p  P a g e I n d e x  
p -  t r a n s p o s e 1 3  
In addition to the transpose methods the ArrayPageDevice class may provides various other methods for
computing with n1  n2  n3 blocks of complex double precision data Furthermore in section 43 we
extend ArrayPageDevice to include caching
23 Page map
Since we use multiple hard drives to store a single large array object we introduce the PageMap class to
specify the storage layout of a given array The PageMap translates logical array page indices into storage
addresses A storage address consists of an id of the server corresponding to the hard drive where the data
page is stored and the page index indicating the address of the page on that hard drive
typedef
int s e r v e r  i d 
sizet p a g e  i n d e x 
address 
struct PageMap
virtual address P a g e A d d r e s s 
int i1  int i2  int i3
In most applications the PageMap would be a simple on-the-y computable function but it is also possible
to implement it using an array of pre-computed values
3 Processes
In this section we introduce processes into object-oriented programming languages and show that a complete
framework for parallel programming with processes is obtained using only very small syntax extension
31 Process creation destruction and remote pointers
Programming objects can be naturally interpreted as processes Upon creation such a process instantiates
the object and proceeds to act as a server to other processes remotely executing the class interface methods on
this object For example a program running on the computing node machine0 can create a new PageDevice
process on machine1 as follows
sizet n u m b e r  o f  p a g e s  1024
sizet p a g e  s i z e  32  1024  1024
char  r e m o t e  m a c h i n e   m a c h i n e 1  
P a g e D e v i c e  storage
 new  r e m o t e  m a c h i n e 
P a g e D e v i c e 
 p a g e f i l e  
n um b e r o f p a ge s 
p a g e  s i z e
It can then generate a page of data and store it on the remote machine1 using the remote pointer storage
Page  page  G e n e r a t e D a t a P a g e  
sizet P a g e A d d r e s s  17
storage -  write  page  P a g e A d d r e s s  
The new PageDevice process on machine1 acts as a server which listens on a communications port accepts
commands from other processes executes them and sends results back to the clients The client-server
protocol is generated by the compiler from the class desccription Remote pointer dereferencing triggers a
sequence of events that includes several client-server communications data transfer and execution of code
on both the client and the server machines
Process semantics and remote pointers extend naturally to simple objects as shown in the following
example
double  data
 new  r e m o t e  m a c h i n e  double 1024
data 7  31415
double x  data 2
When this code is executed on machine0 a new process is created on remote machine This process
allocates a block of 1024 doubles and deploys a server that communicates with the parent client running
on machine0 The execution of data7  31415 requires communication between the client and the
server including sending the numbers 7 and 31415 from the client to the server Similarly the execution
of the following command leads to an assignment of the local variable x with a copy of the remote double
data2 obtained over the network using client-server communications We emphasize that code execution
is sequential each instruction and all communications associated with it is completed before the following
instruction is executed
Finally we remark that the notion of the class destructor in C extends natually to process objects
destruction of a remote object causes termination of the remote process and completion of the correspoding
client-server communications
32 Parallel computation
The sequential programming model requires an execution of an instruction to complete before the next
instruction is executed We dened remote method execution to conform to this model and therefore the
calling process is kept idle until it is notied that the remote method has completed In order to enable
parallel computation we introduce the start keyword to indicate that the calling process may proceed with
the execution of the next statement without waiting for the current statement to complete
Example A shared memory computation is constructed by providing access to the previously dened
data block to several computing processes
const int N  64
class C o m p u t i n g P r o c e s s 
C o m p u t i n g P r o c e s s  p r o c e s s  g r o u p  N 
for  int i  0
i  N 
start p r o c e s s  g r o u p  i   new  machine  i 
C o m p u t i n g P r o c e s s  data  
The start keyword can be used also for remote method and remote function calls as is shown below
An array of remote pointers denes a group of processes It is easy to assign ids to the processes and to
make them aware of the other processes in the group This enables subsequent inter-process communication
by remote method execution Extending the example above we assume that ComputingProcess is derived
from the following ProcessGroupMember class
class P r o c e s s G r o u p M e m b e r
public 
int ID  const
 return id  
int N u m b e r O f P r o c e s s e s  const
 return N  
P r o c e s s G r o u p M e m b e r  P r o c e s s G r o u p  const
 return group  
p r o t e c t e d 
virtual void S e t P r o c e s s G r o u p 
int myid 
int myN 
P r o c e s s G r o u p M e m b e r  m y  g r o u p
private 
int id 
int N 
P r o c e s s G r o u p M e m b e r  group 
The parameter my group of the SetProcessGroup method is a remote pointer to an array of remote processes
so a shallow copy implementation of SetProcessGroup will result in redundant future communications The
following deep copy implementation of SetProcessGroup which copies the entire remote array of remote
pointers to a local array of remote pointers is preferable
void P r o c e s s G r o u p M e m b e r  S e t P r o c e s s G r o u p 
int myid 
int myN 
P r o c e s s G r o u p M e m b e r  m y  g r o u p
id  myid 
N  myN 
group  new P r o c e s s G r o u p M e m b e r   N 
for  int i  0
i  N 
 remote copy 
group  i   m y  g r o u p  i 
After instantiating the processes in process group the master process can form a process group as
follows
for  int id  0
id  N 
p r o c e s s  g r o u p  i  -  S e t P r o c e s s G r o u p 
i  N  p r o c e s s  g r o u p
Alternatively the master process can execute the SetProcessGroup calls in parallel but this requires syn-
chronizing the processes at the end A standard way to do this is using barrier functions A process executing
the barrier function call must wait until all other processes in the group execute a barrier function call before
proceeding with the execution of the next statement
for  int id  0
id  N 
start p r o c e s s  g r o u p  i  -  S e t P r o c e s s G r o u p 
i  N  p r o c e s s  g r o u p
processgroup -  barrier  
Figure 1 Cluster conguration for data intensive computing The nodes are interconnected by a high speed
network 10 Gbsec Ethernet Each node has a 12 core Intel Xeon processor with 48 GB of RAM and 24
attached 1 TB hard-drives Each hard drive has an IO throughput in the range of 100-150 MBsec
33 Persistent objects and processes
We view a large data set as a collection of persistent processes which provide access to portions of the data
set as well as methods for computing with it Persistent processes are objects that can be destroyed only
by explicitly calling the destructor The runtime system is responsible for storing process representation
and activating and de-activating processes as needed Processes can be accessed using a symbolic object
address similar to addresses used by the Data Access Protocol DAP 5 for example
P a g e D e v i c e  p a g e  d e v i c e 
 http  data  set  P a g e D e v i c e 34  
Because persistence is especially important for large data objects in this paper we assume that all processes
and all objects are persistent
4 A program to compute the Fourier transform
In this section we show how to use processes in a data-intensive application on a cluster such as the one
shown in Figure 1 We chose a large three-dimensional Fourier transform as an example of a data-intensive
application and we give an extensive description of the process-oriented code We consider the situation
where the array data set is represented by a group of processes and the Fourier transform is a separate
application whose processes interact with the array processes In section 43 we show how the eciency of
the parallel computation can be improved using server-side caching
41 The Array
We described a method of storing a large data set on multiple hard drives in section 2 We now dene the
Array class that is used in the Fourier transform computation
626141dualIntelXeonX565012physicalcoresmulthreading48GBRAM24 x 1 TB 24 x100 MBsec node0networkswitch10 Gbsec  Ethernet  node1node2node3The Array class describes a complex double precision three-dimensional array
It species the array
domain its storage layout and data access methods We rst dene an auxiliary class to describe rectangular
three-dimensional domains
class Domain
public 
Domain 
int N11  int N12 
int N21  int N22 
int N31  int N32
The collection of hard drives attached to the cluster nodes can be turned into a distributed disk-based
random access memory by launching a PageDevice process for every hard-drive on the cluster node to which
this hard-drive is attached but for computations with arrays we launch ArrayPageDevice processes that
enable us to perform some of the array computations close to the data
A r r a y P a g e D e v i c e  p a g e  s e r v e r 
new A r r a y P a g e D e v i c e   N u m b e r O f S e r v e r s 
int i  0
i  N u m b e r O f S e r v e r s 
p a g e  s e r v e r  i   new  machine  i 
A r r a y P a g e D e v i c e 
f i l e n a m e  i  
NumberOfPages 
N1  N2  N3
The ArrayPageDevice processes could be launched instead of the PageDevice processes however in most
applications it would be advantageous to launch them alongside existing PageDevice processes In this case
ArrayPageDevice must include a constructor that takes a pointer to an existing PageDevice process as
a parameter There would be essentially no communication overhead when the ArrayPageDevice process
is launched on the same node as the corresponding PageDevice process The PageDeviceread method
for example will copy a page from a hard drive directly into a memory buer that is accessible by the
corresponding ArrayPageDevice process
Array storage layout is determined by the PageMap object The assignment of array pages to servers may
aect the degree of parallelism that can be achieved in the computation We use the circulant map which
assigns array page i1 i2 i3 to the server i1  i2  i3  NumberOfServers To complete the
specication of the PageMap we assign the rst available PageIndex address within the target PageServer
The Array class provides methods for a client process to compute over a small array subdomain The
client may use a small eg 4 GB memory buer to assemble the subdomain from array pages that reside
within page server processes as determined by the array pagemap
class Array
 complex double
public 
Array 
Domain  ArrayDomain 
Domain  PageDomain 
int N u mb e rO f S er v er s 
A r r a y P a g e D e v i c e  pageserver 
PageMap  pagemap
 Array  
void read  Domain  d  double  buffer  
void write  Domain  d  double  buffer  
void r e a d  t r a n s p o s e 1 3 
Domain  d  double  buffer
void w r i t e  t r a n s p o s e 1 3 
Domain  d  double  buffer
void r e a d  t r a n s p o s e 2 3 
Domain  d  double  buffer
void w r i t e  t r a n s p o s e 2 3 
Domain  d  double  buffer
private 
Domain  A r r a y D o m a i n 
Domain  P a g e D o m a i n 
int N u m b e r O f S e r v e r s 
A r r a y P a g e D e v i c e  p a g e  s e r v e r 
PageMap  pagemap 
An Array object is constructed by a single process which can then pass the object pointer to any group
of processes Because an Array is a persistent object it can also be accessed using a symbolic address as
described in section 33
Transpose IO methods are used to compute with long and narrow array subdomains that are not
aligned with the third dimension In section 42 we show an application of the transpose methods to the
computation of the Fourier transform The transpose of array pages can be computed either on the client
or on the servers For example the implementation of Arrayread transpose13 can assemble the data by
executing ArrayPageDeviceread transpose13 on the appropriate page server processes Alternatively
it can execute ArrayPageDeviceread methods on these servers followed by ArrayPagetranspose13 on
the received pages
FFT Processes
We compute the three-dimensional Fourier transform using three separate functions fft1 fft2 and fft3
each performing one-dimensional Fourier transforms along the corresponding dimension The functions
fft1 and fft2 are similar to fft3 except that subdomains are read and written using the IO transpose
operations of the Array class Having read and transposed the data into a local memory buer Fourier
transforms are computed along the third dimension The result is then transposed back and written to the
array We therefore restrict our description below to the fft3 function
The computation of the fft3 function is performed in parallel using several FFT3 client processes
class FFT3 
public P r o c e s s G r o u p M e m b e r
public 
FFT3  int sign  Array  a  
 FFT3   delete buffer  
void C o m p u t e T r a n s f o r m  
private 
double  buffer 
int sign 
Array  a 
We divide the array into n slabs along the rst dimension The master process launches n FFT3 processes
assigning each process to a slab
FFT3  fft  new FFT3   n 
for  int i  0
i  n 
fft  i   new  node  i  FFT3  sign  a  
for  int i  0
i  n 
fft  i  -  S e t P r o c e s s G r o u p i  n  fft  
for  int i  0
i  n 
start fft  i  -  C o m p u t e T r a n s f o r m  
Each process maintains a buer that can hold a page line where a page line is a collection of N P3 pages
with page indices
i1 i2 i3 
i3  0 1     N P3  1 
For complex double precision array of 1283 pages with each page consisting of 1283 points the page line
buer is 4 GB Each process computes
void FFT3  C o m p u t e T r a n s f o r m 
Domain  P a g e L i n e 
for every P a g e L i n e in the slab
a -  read  PageLine  buffer  
call FFTW sign  buffer  
a -  write  PageLine  buffer  
P r o c e s s G r o u p  - barrier  
An fft process assembles a page line by reading the pages from appropriate page servers For each page line
one FFTW 4 function call computes a set of n1  n2 one-dimensional complex double Fourier transforms of
size N3 each The result pages are then sent back to the page servers and stored on hard drives Although
the fft processes do not communicate with each other they share a common network and a common pool
of page servers The barrier synchronization at the end of each iteration is not strictly necessary
43 Parallel execution caching
Each page line read and write operation consists of disk IO and network transfer with disk IO being
signicantly more time consuming We implement caching on the page server in order to carry out part of
the disk IO in parallel with the FFTW computation
class A r r a y D e v i c e 
public A r r a y P a g e D e v i c e
public 
A r r a y P a g e D e v i c e 
string filename 
int NumberOfPages 
int n1  int n2  int n3 
A r r a y P a g e D e v i c e 
filename 
NumberOfPages 
n1  n2  n3 
N u m b e r O f C a c h e P a g e s  Nc 
void R e a d I n t o C a c h e  sizet p a g e  i n d e x  
void read  Page  p  sizet p a g e  i n d e x  
private 
int N u m b e r O f C a c h e P a g e s 
Page  cache 
An ArrayDevice server is congured with a cache that can hold a line of pages In order to use ArrayDevice
instead of ArrayPageDevice we add the following ReadIntoServerCache method to the Array class
void Array  R e a d I n t o S e r v e r C a c h e 
Domain  domain
for every page  i1  i2  i3  in domain
address a 
pagemap -  P a g e A d d r e s s  i1  i2  i3  
int id  a  s e r v e r  i d 
sizet i  a  p a g e  i n d e x 
start server  id  -  R e a d I n t o C a c h e  i  
The ArrayDeviceread method executes ArrayPageDeviceread if the requested page is not found in its
cache otherwise it returns sends over the network the cached page
The transform computation is now reorganized so that instructions to read the next line of pages are
sent to the page servers before the FFT of the current page line is started
void FFT3  C o m p u t e T r a n s f o r m 
Domain  P a g e L i n e 
for every P a g e L i n e in the slab
a -  read  PageLine  buffer  
Domain  N e x t P a g e L i n e  next page line
start a -  R e a d I n t o S e r v e r C a c h e  N e x t P a g e L i n e  
call FFTW sign  buffer  
a -  write  PageLine  buffer  
P r o c e s s G r o u p  - barrier  
After the completion of the FFT the servers are instructed to write pages to hard drives An execution of
the write method will take place only after the server has completed ArrayDeviceReadIntoCache The
eciency of server-side caching depends on the relative timing of the FFT computation and the page IO If
necessary write and WriteFromCache methods can be implemented in the ArrayDevice class analogously
5 Computation of the Fourier Transform
In this section we describe the computation of a large 64 TB data-intensive Fourier transform on a small
8 nodes 96 cores cluster We describe our prototype implementation of the process framework and analyze
the performance of the Fourier transform computation
Software Implementation of Processes
The Fourier transform application was developed by completing the process-oriented code which is sketched
in sections 2 and 4 translating it into C with MPI and linking it against an auxiliary library of tools
for implementation of basic process functionality We describe the procedure for ArrayDevice processes In
order to instantiate ArrayDevice processes we created an ArrayDevice server class and a C program
le ArrayDevice processcpp containing the following code
 start the server and d i s c o n n e c t from parent process
int main  int argc  char  argv 
M P I  I n i t  argc   argv  
M P I  C o m m parent 
M P I  C o m m  g e t  p a r e n t  parent  
A r r a y D e v i c e  s e r v e r  s 
new A r r a y D e v i c e  s e r v e r  
M P I  C o m m  d i s c o n n e c t  parent  
M P I  F i n a l i z e  
In addition we implemented the function
void L a u n c h P r o c e s s 
const string  p r o c e s s  f i l e  n a m e 
const string  m a c hi n e a d dr e ss 
string  s e r v e r  a d d r e s s
LaunchProcess uses MPI Comm spawn to create an MPI process by running the executable process file name
on the remote machine specied by machine address The launched process starts a server which uses
MPI Open port to open a port and return the port address in the server address output parameter The
server process then disconnects from the launching process
In order to implement remote method execution we also created an ArrayDevice client class The
ArrayDevice client and the ArrayDevice server classes are automatically constructed from the
ArrayDevice class For the purposes of this example we use a simple name mangling scheme to rst generate
the following ArrayDevice interface class
class A r r a y D e v i c e  i n t e r f a c e
public 
virtual void A r r a y D e v i c e  c r e a t e 
 c o n s t r u c t o r
string filename 
sizet numberofpages 
sizet p a g e s i z e
virtual void A r r a y D e v i c e  d e s t r o y  
 d e s t r u c t o r
virtual void A r r a y D e v i c e  w r i t e  Page  p  sizet p a g e  i n d e x  
virtual void A r r a y D e v i c e  R e a d I n t o C a c h e 
int n 
sizet  p a g e  i n d e x
 c o m m a n d s used in client - server p r o t o c o l
enum A r r a y D e v i c e  c o m m a n d
c r e a t e  C M D  0 
d e s t r o y  C M D  1 
w r i t e  C M D  2 
R e a d I n t o C a c h e  C M D  3
The ArrayDevice interface class contains meta-information obtained from the ArrayDevice class Both
the ArrayDevice client and the ArrayDevice server classes are derived from ArrayDevice interface
ArrayDevice client is also derived from a general ProcessClient class and similarly ArrayDevice server
is derived from ProcessServer Jointly the ArrayDevice client ArrayDevice server pair implement
remote procedure calls RPC for ArrayDevice The client-server communications protocol for ArrayDevice
uses the ArrayDevice interface class and the general-purpose functionality implemented in ProcessClient
and ProcessServer The client-server implementation uses a simple object serialization library to send
method parameters and results over the network This serialization library is also used to implement a
rudimentary le-based object persistence mechanism Information is sent over the network using an MPI-
based communications software layer
The translation procedure we implemented for the processes of the Fourier transform application can be
extended and incorporated into a C compiler It converts several hundred lines of process-oriented code
into a CMPI application for computing Fourier transform which is approximately 15000 lines long A
very small subset of MPI functions is used The following is the almost complete list
 MPI Send MPI Recv  for inter-process communication
 MPI barrier  for process synchronization
 MPI Open port MPI Close port MPI Comm accept  to implement client-server functionality
 MPI Comm spawn MPI Info create MPI Info set  to spawn processes on specied target ma-
chines
 MPI Comm connect MPI Comm disconnect MPI Comm get parent  to manage process connections
Both MPICH and Intel MPI were used in our computation and with both implementations we encoun-
tered problems with some MPI functions The most signicant bugs were found in MPI Comm spawn and
MPI Comm disconnect
Fourier Transform Computation
We used the process prototype implementation to carry out a computation of the Fourier transform of a
16K3-point array of complex double precision numbers We use the notation 1K  1024 16K  16384 
128  128 The total size of the array is 64 TB The computations were carried out on a cluster of 8 nodes
interconnected with a 10 Gbsec Ethernet similar to the cluster depicted in Figure 1 Each computational
node has an Intel Rcid13 Xeon Rcid13 X5650 12 core CPU with hyperthreading rated at 267GHz and 24 attached
1 TB hard drives The hard drives are manufactured by Samsung model SpinPoint F3 HD103SJ with
manufacturer listed latency of 414 ms and average seek time of 89 ms Benchamark hard drive read and
write throughput is reported at over 100 MBsec
The 16 K3-point array was broken up into 1283 ArrayPage pages of 128  128  128 points each The
resulting page size is 32 MB The choice of the page size is constrained by the latency and seek time of the
systems hard drives the smaller the page size the lower the overall disk IO throughput We measured a
typical page readwrite time in the range of 025-035 sec
We used 4 of the 8 available nodes to store the Array object creating 24 ArrayDevice processes on each
node one process for each available hard drive Because ArrayDevice processes are primarily dedicated to
disk IO it is possible to run 24 processes on a 12-core node The process framework makes it easy to shift
computation closer to data by extending the ArrayDevice class and in such case relatively more powerful
CPUs may be needed to run the server processes
We used the other 4 nodes to run 16 processes of the Fourier transform application 4 processes per node
Each of the 16 Fourier transform processes was assigned an array slab of 8  128  128 pages The process
computes the transform of its slab line by line in 8  128  1024 iterations Although the 16 processes are
independent of each other they compete among themselves for service from 96 page servers
The wall clock time for a single iteration of ComputeTransform see section 43 generally ranged from 68
to 78 seconds with the average of approximately 73 seconds The total speed of data processing including
reading computing and writing the data has been therefore close to 1 GBsec
In the next section we
analyze the performance in detail and indicate a number of ways to substantially improve it
53 Performance analysis
The computation of the Fourier transform was completed in the course of several long over 10 hours
continuous runs Our implementation of persistence mechanism for processes made it possible to stop and
restart the computation multiple times The primary reason for long runs was to test the stability and
robustness of our implementation We instrumented the code to measure the utilization of the systems
components the network the hard drives and the processors Because the results did not vary substantially
over the course of the computation we present a detailed analysis of a typical iteration of ComputeTransform
The synchronization of the processes at the end of each iteration is not strictly necessary but we found
that it did not signicantly aect performance and made the code easier to analyze On the other hand we
found that introducing additional barriers within the iteration would slow down the computation signicantly
We now present detailed measurements of the component phases of the iteration
At the beginning of every iteration each process reads a page line consisting of 128 pages which are evenly
distributed among the 96 servers by the circulant map see section 41 Except for the rst iteration the
required pages have already been read from the hard drive and placed in the memory of the corresponding
servers Each server has either 21 or 22 pages to send to the clients In total 64 GB of data is sent from
the servers to the clients Accordingly the combined size of the server caches in each of the server nodes is
slightly more than 16 GB We timed the parallel reading of page lines by the 16 client processes in a typical
time step
15 Array  read 11520 -11647 x 2432 -2559 x 0 -16383 4 GB  173638 sec  235894 MB  sec
12 Array  read 9216 -9343 x 2432 -2559 x 0 -16383 4 GB  186695 sec  219395 MB  sec
0 Array  read 0 -127 x 2432 -2559 x 0 -16383 4 GB  216104 sec  189538 MB  sec
1 Array  read 768 -895 x 2432 -2559 x 0 -16383 4 GB  222821 sec  183824 MB  sec
11 Array  read 8448 -8575 x 2432 -2559 x 0 -16383 4 GB  224682 sec  182302 MB  sec
8 Array  read 6144 -6271 x 2432 -2559 x 0 -16383 4 GB  227035 sec  180412 MB  sec
14 Array  read 10752 -10879 x 2432 -2559 x 0 -16383 4 GB  229213 sec  178698 MB  sec
5 Array  read 3840 -3967 x 2432 -2559 x 0 -16383 4 GB  271039 sec  151122 MB  sec
6 Array  read 4608 -4735 x 2432 -2559 x 0 -16383 4 GB  270914 sec  151192 MB  sec
2 Array  read 1536 -1663 x 2432 -2559 x 0 -16383 4 GB  272604 sec  150255 MB  sec
4 Array  read 3072 -3199 x 2432 -2559 x 0 -16383 4 GB  281986 sec  145256 MB  sec
3 Array  read 2304 -2431 x 2432 -2559 x 0 -16383 4 GB  283351 sec  144556 MB  sec
7 Array  read 5376 -5503 x 2432 -2559 x 0 -16383 4 GB  289623 sec  141425 MB  sec
10 Array  read 7680 -7807 x 2432 -2559 x 0 -16383 4 GB  29027 sec  14111 MB  sec
9 Array  read 6912 -7039 x 2432 -2559 x 0 -16383 4 GB  295628 sec  138553 MB  sec
13 Array  read 9984 -10111 x 2432 -2559 x 0 -16383 4 GB  301746 sec  135743 MB  sec
The rst number in each row is the client process id followed by the description of the domain domain
size the time it took to read it and the corresponding throughput The fastest process completes the
execution of the Arrayread method including sending the command and the parameter to page servers
in approximately 175 seconds the slowest  in about 30 seconds The faster processes proceed to start
the execution of ArrayReadIntoServerCache immediately after completion of Arrayread The typical
aggregate throughput during the parallel execution of Arrayread is therefore signicantly larger than 2
GBsec The maximal possible throughput to the four nodes computing the transform is approximately 4
GBsec
In the ArrayReadIntoServerCache phase of the computation 16 clients send small messages to 96
page servers with instructions to read a total of 16  128 pages These commands are queued for execution
in the servers so that the clients do not wait for the completion of the execution The time measurements of
the parallel execution of start ArrayReadIntoServerCache by the 16 client processes in a typical time
step were
14 Array  R e a d I n t o S e r v e r C a c h e 10752 -10879 x 2560 -2687 x 0 -16383 936148 sec
13 Array  R e a d I n t o S e r v e r C a c h e 9984 -10111 x 2560 -2687 x 0 -16383 210771 sec
10 Array  R e a d I n t o S e r v e r C a c h e 7680 -7807 x 2560 -2687 x 0 -16383 325717 sec
12 Array  R e a d I n t o S e r v e r C a c h e 9216 -9343 x 2560 -2687 x 0 -16383 95796 sec
8 Array  R e a d I n t o S e r v e r C a c h e 6144 -6271 x 2560 -2687 x 0 -16383 981484 sec
11 Array  R e a d I n t o S e r v e r C a c h e 8448 -8575 x 2560 -2687 x 0 -16383 136131 sec
9 Array  R e a d I n t o S e r v e r C a c h e 6912 -7039 x 2560 -2687 x 0 -16383 267452 sec
5 Array  R e a d I n t o S e r v e r C a c h e 3840 -3967 x 2560 -2687 x 0 -16383 516807 sec
2 Array  R e a d I n t o S e r v e r C a c h e 1536 -1663 x 2560 -2687 x 0 -16383 50223 sec
4 Array  R e a d I n t o S e r v e r C a c h e 3072 -3199 x 2560 -2687 x 0 -16383 408604 sec
0 Array  R e a d I n t o S e r v e r C a c h e 0 -127 x 2560 -2687 x 0 -16383 106741 sec
3 Array  R e a d I n t o S e r v e r C a c h e 2304 -2431 x 2560 -2687 x 0 -16383 394899 sec
1 Array  R e a d I n t o S e r v e r C a c h e 768 -895 x 2560 -2687 x 0 -16383 998827 sec
6 Array  R e a d I n t o S e r v e r C a c h e 4608 -4735 x 2560 -2687 x 0 -16383 516787 sec
15 Array  R e a d I n t o S e r v e r C a c h e 11520 -11647 x 2560 -2687 x 0 -16383 149185 sec
7 Array  R e a d I n t o S e r v e r C a c h e 5376 -5503 x 2560 -2687 x 0 -16383 327696 sec
These results are signicantly worse than expected Our implementation of processes repeatedly establishes
and breaks client-server connections We found the MPI Comm connect function to be very fast but with
increasing number of client-server connections it sporadically performed hundreds of times slower than usual
An implementation of caching connections is likely to reduce the total time for this phase of the computation
to about 3 seconds
We timed the execution of the FFTW function call by every processor
553611 sec
586336 sec
14 fftw 10752 -10879 x 2432 -2559 x 0 -16383
0 fftw 0 -127 x 2432 -2559 x 0 -16383
6 fftw 4608 -4735 x 2432 -2559 x 0 -16383
4 fftw 3072 -3199 x 2432 -2559 x 0 -16383
8 fftw 6144 -6271 x 2432 -2559 x 0 -16383
12 fftw 9216 -9343 x 2432 -2559 x 0 -16383
10 fftw 7680 -7807 x 2432 -2559 x 0 -16383
2 fftw 1536 -1663 x 2432 -2559 x 0 -16383
11 fftw 8448 -8575 x 2432 -2559 x 0 -16383
7 fftw 5376 -5503 x 2432 -2559 x 0 -16383
15 fftw 11520 -11647 x 2432 -2559 x 0 -16383
3 fftw 2304 -2431 x 2432 -2559 x 0 -16383
9 fftw 6912 -7039 x 2432 -2559 x 0 -16383
13 fftw 9984 -10111 x 2432 -2559 x 0 -16383
5 fftw 3840 -3967 x 2432 -2559 x 0 -16383
1 fftw 768 -895 x 2432 -2559 x 0 -16383
591158 sec
599485 sec
601364 sec
603876 sec
606033 sec
607081 sec
638444 sec
638421 sec
642032 sec
642975 sec
14001 sec
145631 sec
145582 sec
14878 sec
The last 4 processes 9135 and 1 ran on the same node Throughout our computation these 4 processes
executed the FFTW library function call signicantly slower than the processes running on other nodes
Additional investigation of the conguration of this node is needed to speed up the computation
The reading of the pages on the server side is done concurrently with the FFTW computation We
include the measurements for a few of the 96 servers
34 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  559864 sec  125745 MB  sec
25 A r r a y D e v i c e  R e a d I n t o C a c h e  21 pages  672 MB  5614 sec  119701 MB  sec
57 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  558319 sec  126093 MB  sec
52 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  559719 sec  125777 MB  sec
40 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  561367 sec  125408 MB  sec
73 A r r a y D e v i c e  R e a d I n t o C a c h e  21 pages  672 MB  557943 sec  120442 MB  sec
21 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  564487 sec  124715 MB  sec
33 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  563134 sec  125015 MB  sec
89 A r r a y D e v i c e  R e a d I n t o C a c h e  21 pages  672 MB  556771 sec  120696 MB  sec
46 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  562876 sec  125072 MB  sec
The reading throughput is close to the maximal throughput for this type of hard drives For every server the
page reading commands are scheduled before the page writing commands of the last phase of the iteration
The writing of the pages will therefore start only after the completion of the page read commands The
clients write pages in parallel with the typical timing as follows
8 Array  write 6144 -6271 x 2432 -2559 x 0 -16383 4 GB  223279 sec  183448 MB  sec
4 Array  write 3072 -3199 x 2432 -2559 x 0 -16383 4 GB  22784 sec  179776 MB  sec
11 Array  write 8448 -8575 x 2432 -2559 x 0 -16383 4 GB  227165 sec  18031 MB  sec
12 Array  write 9216 -9343 x 2432 -2559 x 0 -16383 4 GB  23083 sec  177447 MB  sec
15 Array  write 11520 -11647 x 2432 -2559 x 0 -16383 4 GB  230278 sec  177872 MB  sec
7 Array  write 5376 -5503 x 2432 -2559 x 0 -16383 4 GB  232437 sec  17622 MB  sec
0 Array  write 0 -127 x 2432 -2559 x 0 -16383 4 GB  237938 sec  172145 MB  sec
14 Array  write 10752 -10879 x 2432 -2559 x 0 -16383 4 GB  245307 sec  166975 MB  sec
10 Array  write 7680 -7807 x 2432 -2559 x 0 -16383 4 GB  242152 sec  16915 MB  sec
6 Array  write 4608 -4735 x 2432 -2559 x 0 -16383 4 GB  246648 sec  166066 MB  sec
2 Array  write 1536 -1663 x 2432 -2559 x 0 -16383 4 GB  247291 sec  165635 MB  sec
3 Array  write 2304 -2431 x 2432 -2559 x 0 -16383 4 GB  253921 sec  16131 MB  sec
9 Array  write 6912 -7039 x 2432 -2559 x 0 -16383 4 GB  218906 sec  187113 MB  sec
13 Array  write 9984 -10111 x 2432 -2559 x 0 -16383 4 GB  220268 sec  185956 MB  sec
1 Array  write 768 -895 x 2432 -2559 x 0 -16383 4 GB  217936 sec  187945 MB  sec
5 Array  write 3840 -3967 x 2432 -2559 x 0 -16383 4 GB  222789 sec  183851 MB  sec
The results for page writing are fairly uniform with the total time for each process between 22 and 255
seconds The aggregate througfhput for this phase is therefore over 25 GBsec We did not implement
explicit caching for writing pages The implementation of Arraywrite is analogous to the implementation
of ArrayReadIntoServerCache see section 43
void Array  write  Domain  domain 
for every page in domain
start write page to the appropriate server
There is a limited caching eect as a result of the start command having transmitted the page to the
server the client disconnects and proceeds to transmit pages to other servers while the server starts writing
the page to disk only after the client has disconnected We found that writing pages to hard drive with the
O DIRECT ag is about twice as fast as reading presumably because of buering The typical throughput
measured was between 230 and 240 MBsec
22 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 6 9 5 8 sec  233648 MB  sec
78 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 5 6 1 5 sec  235962 MB  sec
73 A r r a y D e v i c e  write  1 page  32 MB  013452 sec  237883 MB  sec
64 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 6 4 8 4 sec  23446 MB  sec
8 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 7 0 4 2 sec  233505 MB  sec
54 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 6 5 3 3 sec  234376 MB  sec
87 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 6 2 9 3 sec  234788 MB  sec
The conclusion of our performance analysis is that the presented computation could be sped up by 25
or more but greater benets can be derived from a more balanced hardware conguration The aggregate
throughput of the 24 hard drives of a cluster node is about 3 GBsec about 3 times the capacity of the
incoming network connection Furthermore the FFTW computation takes only 10-20 of the total iteration
time It appears that a 24-fold increase in the network capacity of the present cluster is likely to result
in a more balanced system with better hardware utilization and a total runtime of under 20 seconds per
iteration
6 Discussion and Conclusions
In this paper we introduced process-oriented programming as a natural extension of object-oriented pro-
gramming for parallel computing We implemented a prototype of the process framework and carried out
a data-intensive computation We have shown that a complex and ecient application can be built using
only a few hundred lines of process-oriented code which is equivalent to many thousands of lines of object-
oriented code with MPI The process-oriented code in this paper is an extension of C but processes can
be introduced into any object-oriented language The syntax extension is minimal In C for example it
amounts to adding a parameter to the new operator and introducing the keyword start Combined with the
fact that creating a process can be thought of as simply placing an object on a remote machine it suggests
that a lot of existing code can be easily modied to run in parallel Potentially the most important impact of
the process-oriented extension of languages such as C and Python is a widespread adoption of parallel
programming as application developers realize the ability to easily create processes instead of using thread
libraries and to place dierent objects on dierent CPU cores
The process-oriented programming model is based on a simple hardware abstraction the computer
consists of a collection of processors interconnected by a network where each processor is capable of running
multiple processes The run-time system is responsible for mapping the abstract model onto a concrete
hardware system and must provide the programmer with system functions describing the state of the
hardware The hardware abstraction of the process-oriented model makes it possible to create portable
parallel applications and applications that run in the cloud
Processes are accessible by remote pointers Syntactically executing a method on a remote process
is not dierent from method execution on an object Any class of an object-oriented language can be
interpreted as a process but even more importantly in the process-oriented framework only processes that
are class instances are allowed We argue that object-based parallelism is a high level abstraction which is
naturally suitable for reasoning about parallelism Although shared memory and message passing can be
realized in a process-oriented language these are lower implementation-level models Process inheritance
is a powerful aspect of object-based parallelism as it enables the denition of new processes in terms of
previously dened processes In combination with process pointers it gives the programmer the exibility
to adapt the computation to the hardware by adding simple methods to class denitions see the comments
at the end of section 41 about the computation of array page transpose
A process-oriented program like a typical sequential program starts with a single main process The
main process may launch new processes on remote machines as easily as it can create objects In contrast
with MPI processes are explicitly managed by the programmer Process launching is part of the program
and is not determined by the runtime command line parameters Using process pointers and language data
structures the programmer can form groups of processes assign process ids and perform tasks that in MPI
would require using communicators
Processes exchange information by executing remote methods rather than via shared memory or message
passing Wed like to use the analogy that writing programs with message passing today is like writing
programs with GOTOs fty years ago it is easy to write intractable code And just like GOTO statements
are used in assembly languages message passing is a low-level language construct underlying remote method
execution in process-oriented programming
We introduced the start keyword to enable parallel execution of remote methods In order to use the
keyword the programmer must decide whether there is a need to wait for the remote task to complete before
proceeding with the computation In general this decision is easy and intuitive but keeping track of task
dependencies is not Each process executes only one method at a time and remote method execution requests
are queued The programmer must keep in mind the state of the execution queue for every process This is
possible only for very simple scenarios We used barriers to synchronize processes Barrier synchronization
helps the programmer to keep track of the execution queues of the processes but it may reduce the parallelism
of the computation
We view a large data object as a collection of persistent processes For a large data object a negligible
amount of additional storage space is needed to store serialized processes alongside the data Process
persistence is needed to enable stopping and restarting a computation and to make a data set accessible to
several simultaneous applications It is also needed to develop basic mechanisms for fault tolerance In this
paper we showed that the process-oriented view of a large data object is very powerful using only a small
amount of code the programmer can copy and reformat very large data objects and even carry out complex
operations such as the Fourier transform Yet software users and application developers tend to see a data
set as consisting of just data and being independent of a specic programming language We stop short
of suggesting a solution for this problem but in this context it is worth recalling the CORBA standard 8
The introduction of process-oriented programming was motivated by our research in data-intensive com-
puting The data-intensive Fourier transform computation was carried out on a small cluster of 8 nodes
96 cores Our measurements indicate that Petascale data-intensive computations can be eciently carried
out on a larger cluster with more nodes and signicantly increased network bandwidth Such a cluster can
serve as a general-purpose data-intensive computer whose operating system and applications are developed
as process-oriented programs
The introduction of process-oriented programming in this paper is far from complete It is merely the
rst step of an extensive research program We tested a substantial subset of the process framework in a
prototype implementation A full-edged implementation must include a compiler and a run-time system
that substantially expand the basic prototype
7 Acknowledgements
I am grateful to J J Bunn for discussions that signicantly improved the presentation of the material
References
1 MPI A Message-Passing Interface Standard version 30 httpwwwmpi-forumorg
2 OpenMP 40 Specications httpopenmporgwpopenmp-specifications
3 J Diaz C Munoz-Caro and A Nino A survey of parallel programming models and tools in the multi
and many-core era Parallel and Distributed Systems IEEE Transactions on 23813691386 2012
4 M Frigo and S G Johnson The design and implementation of tw3 Proceedings of the IEEE
932216231 2005
5 J Gallagher N Potter T Sgouros S Hankin and G Flierl The data access protocol-DAP 20 2004
6 E Givelberg Object-oriented parallel programming April 2014 arXiv14044666 csPL
7 E Givelberg A Szalay K Kanov and R Burns An architecture for a data-intensive computer In
Proceedings of the rst international workshop on Network-aware data management pages 5764 ACM
8 M Henning The rise and fall of CORBA Queue 452834 June 2006
9 M Herlihy and N Shavit The Art of Multiprocessor Programming Revised Reprint Elsevier 2012
10 E A Lee The problem with threads Computer 3953342 2006
11 T G Mattson B A Sanders and B L Massingill Patterns for parallel programming Pearson
Education 2004
Process-Oriented Parallel Programming with an Application to
Data-Intensive Computing
Edward Givelberg
July 22 2014
Abstract
We introduce process-oriented programming as a natural extension of object-oriented programming
for parallel computing It is based on the observation that every class of an object-oriented language
can be instantiated as a process accessible via a remote pointer The introduction of process pointers
requires no syntax extension identies processes with programming objects and enables processes to
exchange information simply by executing remote methods Process-oriented programming is a high-level
language alternative to multithreading MPI and many other languages environments and tools currently
used for parallel computations It implements natural object-based parallelism using only minimal syntax
extension of existing languages such as C and Python and has therefore the potential to lead to
widespread adoption of parallel programming We implemented a prototype system for running processes
using C with MPI and used it to compute a large three-dimensional Fourier transform on a computer
cluster built of commodity hardware components Three-dimensional Fourier transform is a prototype
of a data-intensive application with a complex data-access pattern The process-oriented code is only
a few hundred lines long and attains very high data throughput by achieving massive parallelism and
maximizing hardware utilization
1 Introduction
The rst commercially available microprocessor CPUs appeared in the early 1970s These were single-
processor devices that operated with a clock rate of less than 1 MHz Over the course of the following three
decades increasingly faster and cheaper CPUs were built This was achieved in large part by persistently
increasing the clock rate By 2004 the CPU clock rates reached the 3-4 GHz range and heat dissipation
became a major problem
In order to continue improving the performance and the cost microprocessor
designers turned to parallel computing Today nearly all computing devices servers tablets phones etc
are built using processors with multiple computing cores whose operating frequency is less than 35 GHz
The industry move to parallel computing succeeded because practically every application contains tasks
that can be executed in parallel And yet a decade later the vast majority of computer programs are still
being written for execution on a single processor and parallel computation is being realized primarily using
threads sequential processes that share memory
Parallel programming is generally recognized as dicult and has been a subject of extensive research
see 3 9 11 and references therein The problem with threads is eloquently described in 10 The
author paints a bleak scenario
If  programmers make more intensive use of multithreading the next generation of computers
will become nearly unusable
In the scientic computing community parallel programs are typically written in Fortran and C with OpenMP
2 and MPI 1 Dozens of high-level languages for parallel programming have also been developed but
presently none of them is widely used Even the so-called embarrassingly parallel computations are not
embarrassingly easy to implement
In this paper we develop a new framework for parallel programming which we call process-oriented
programming
It is based on the fundamental observation that any class in an object-oriented language
can be instantiated as a process accessible via a remote pointer 6 Such a process instantiates an object
of the class and acts as a server to other processes remotely executing the class interface methods on
this object The introduction of remote pointers enables a straightforward extension of object-oriented
programming languages to process-oriented programming with hardly any syntax additions We show that
process-oriented programming is an ecient framework for parallel programming and we propose it as a
replacement for multithreading MPI and many other languages environments and tools currently used for
parallel computations We implemented a prototype system for running processes using C with MPI and
investigated process-oriented programming in the context of data-intensive computing
The rapid growth of generated and collected data in business and academia creates demand for increas-
ingly complex and varied computations with very large data sets The data sets are typically stored on
hard drives so the cost of accessing and moving small portions of the data set is high Nevertheless a large
number of hard drives can be used in parallel to signicantly reduce the amortized cost of data access In
7 we argued that a data-intensive computer can be built using widely available commodity hardware
components to solve general computational problems involving very large data sets
The primary challenge in the construction of the data-intensive computer lies in software engineering
The software framework must balance programmer productivity with ecient code execution ie big data
applications with complex data access patterns must be realizable using a small amount of code and this
code must attain high data throughput using massive parallelism In this paper we aim to demonstrate that
process-oriented programming is the right framework for the realization of the data-intensive computer
We chose the computation of a large three-dimensional Fourier transform as the subject of our study
primarily because it can be considered as a prototype of a dicult data-intensive problem We show that
using processes our application can be realized with only a few hundred lines of code which are equivalent to
approximately 15000 lines of C with MPI We also show that even with the complex data access patterns
required for the computation of the 3D Fourier transform our code attains very high data throughput by
achieving massive parallelism and maximizing hardware utilization
In section 2 we describe a simple model of storage of a data set as a collection of data pages on mul-
tiple hard-drives This model is used in the examples in section 3 where we introduce processes The
process-oriented implementation of the Fourier transform is described in section 4 and the eciency of the
computation is analyzed in section 5 We conclude with a discussion in section 6
Our presentation uses C but can be easily applied to any object-oriented language size t is a large
non-negative integer type used in C to represent the size of a data object in bytes
2 The Data Set
21 Data Pages
We represent an N1N2N3 array of complex double precision numbers as a collection of N P1N P2N P3
pages where each page is a small complex double precision array of size n1  n2  n3 First we dene a
Page class which stores n bytes of unstructured data
class Page
public 
Page  sizet n  u n s i g n e d char  data  
 Page  
p r o t e c t e d 
sizet n 
u n s i g n e d char  data 
The ArrayPage class is derived from the Page class to handle three-dimensional complex double-precision
array blocks
class A r r a y P a g e 
public Page
public 
A r r a y P a g e 
int n1  int n2  int n3 
double  data
 c o n s t r u c t o r that a l l o c a t e s data 
A r r a y P a g e  int n1  int n2  int n3  
void t r a n s p o s e 1 3  
void t r a n s p o s e 2 3  
private 
int n1  n2  n3 
The ArrayPage class may include functions for local computation with the array page data such as the trans-
pose functions that are important in the computation of the Fourier transform see section 42 Throughout
this paper the arrays have equal dimensions and the distinct variables n1 n2 and n3 are maintained only
for the clarity of exposition
Storage devices
We store data pages on hard drives using a single large le for every available hard drive The following
PageDevice class controls the hard drive IO
class P a g e D e v i c e
public 
P a g e D e v i c e 
string filename 
sizet NumberOfPages 
sizet P a g e S i z e
 P a g e D e v i c e  
void write  Page  p  sizet P a g e I n d e x  
void read  Page  p  sizet P a g e I n d e x  
p r o t e c t e d 
string f i l e n a m e 
sizet N u m b e r O f P a g e s 
sizet P a g e S i z e 
private 
int f i l e  d e s c r i p t o r 
The implementation of this class creates a le filename of NumberOfPages  PageSize bytes Pages of
data are stored in the PageDevice object using a PageIndex address where PageIndex ranges from 0 to
NumberOfPages - 1 The write method copies a data page of size PageSize to the location with an oset
PageIndex  PageSize from the beginning of the le filename Similarly the read method reads a page
of data stored at a given integer address in the PageDevice Linux direct IO functions are used in the class
implementation For example the Linux open function used with the O DIRECT ag attempts to minimize
cache eects of the IO to and from the specied le
For array pages we dene the ArrayPageDevice which extends PageDevice as follows
class A r r a y P a g e D e v i c e 
public P a g e D e v i c e
public 
A r r a y P a g e D e v i c e 
string filename 
sizet NumberOfPages 
int nn1  int nn2  int nn3
n1  nn1   n2  nn2   n3  nn3  
P a g e D e v i c e 
filename 
NumberOfPages 
2  n1  n2  n3  sizeof  double 
void w r i t e  t r a n s p o s e 1 3  Page  p  sizet P a g e I n d e x  
void r e a d  t r a n s p o s e 1 3  Page  p  sizet P a g e I n d e x  
void w r i t e  t r a n s p o s e 2 3  Page  p  sizet P a g e I n d e x  
void r e a d  t r a n s p o s e 2 3  Page  p  sizet P a g e I n d e x  
private 
int n1  int n2  int n3 
The implementation of the transpose methods is very simple For example
void A r r a y P a g e D e v i c e 
r e a d  t r a n s p o s e 1 3 
Page  p  sizet P a g e I n d e x
read p  P a g e I n d e x  
p -  t r a n s p o s e 1 3  
In addition to the transpose methods the ArrayPageDevice class may provides various other methods for
computing with n1  n2  n3 blocks of complex double precision data Furthermore in section 43 we
extend ArrayPageDevice to include caching
23 Page map
Since we use multiple hard drives to store a single large array object we introduce the PageMap class to
specify the storage layout of a given array The PageMap translates logical array page indices into storage
addresses A storage address consists of an id of the server corresponding to the hard drive where the data
page is stored and the page index indicating the address of the page on that hard drive
typedef
int s e r v e r  i d 
sizet p a g e  i n d e x 
address 
struct PageMap
virtual address P a g e A d d r e s s 
int i1  int i2  int i3
In most applications the PageMap would be a simple on-the-y computable function but it is also possible
to implement it using an array of pre-computed values
3 Processes
In this section we introduce processes into object-oriented programming languages and show that a complete
framework for parallel programming with processes is obtained using only very small syntax extension
31 Process creation destruction and remote pointers
Programming objects can be naturally interpreted as processes Upon creation such a process instantiates
the object and proceeds to act as a server to other processes remotely executing the class interface methods on
this object For example a program running on the computing node machine0 can create a new PageDevice
process on machine1 as follows
sizet n u m b e r  o f  p a g e s  1024
sizet p a g e  s i z e  32  1024  1024
char  r e m o t e  m a c h i n e   m a c h i n e 1  
P a g e D e v i c e  storage
 new  r e m o t e  m a c h i n e 
P a g e D e v i c e 
 p a g e f i l e  
n um b e r o f p a ge s 
p a g e  s i z e
It can then generate a page of data and store it on the remote machine1 using the remote pointer storage
Page  page  G e n e r a t e D a t a P a g e  
sizet P a g e A d d r e s s  17
storage -  write  page  P a g e A d d r e s s  
The new PageDevice process on machine1 acts as a server which listens on a communications port accepts
commands from other processes executes them and sends results back to the clients The client-server
protocol is generated by the compiler from the class desccription Remote pointer dereferencing triggers a
sequence of events that includes several client-server communications data transfer and execution of code
on both the client and the server machines
Process semantics and remote pointers extend naturally to simple objects as shown in the following
example
double  data
 new  r e m o t e  m a c h i n e  double 1024
data 7  31415
double x  data 2
When this code is executed on machine0 a new process is created on remote machine This process
allocates a block of 1024 doubles and deploys a server that communicates with the parent client running
on machine0 The execution of data7  31415 requires communication between the client and the
server including sending the numbers 7 and 31415 from the client to the server Similarly the execution
of the following command leads to an assignment of the local variable x with a copy of the remote double
data2 obtained over the network using client-server communications We emphasize that code execution
is sequential each instruction and all communications associated with it is completed before the following
instruction is executed
Finally we remark that the notion of the class destructor in C extends natually to process objects
destruction of a remote object causes termination of the remote process and completion of the correspoding
client-server communications
32 Parallel computation
The sequential programming model requires an execution of an instruction to complete before the next
instruction is executed We dened remote method execution to conform to this model and therefore the
calling process is kept idle until it is notied that the remote method has completed In order to enable
parallel computation we introduce the start keyword to indicate that the calling process may proceed with
the execution of the next statement without waiting for the current statement to complete
Example A shared memory computation is constructed by providing access to the previously dened
data block to several computing processes
const int N  64
class C o m p u t i n g P r o c e s s 
C o m p u t i n g P r o c e s s  p r o c e s s  g r o u p  N 
for  int i  0
i  N 
start p r o c e s s  g r o u p  i   new  machine  i 
C o m p u t i n g P r o c e s s  data  
The start keyword can be used also for remote method and remote function calls as is shown below
An array of remote pointers denes a group of processes It is easy to assign ids to the processes and to
make them aware of the other processes in the group This enables subsequent inter-process communication
by remote method execution Extending the example above we assume that ComputingProcess is derived
from the following ProcessGroupMember class
class P r o c e s s G r o u p M e m b e r
public 
int ID  const
 return id  
int N u m b e r O f P r o c e s s e s  const
 return N  
P r o c e s s G r o u p M e m b e r  P r o c e s s G r o u p  const
 return group  
p r o t e c t e d 
virtual void S e t P r o c e s s G r o u p 
int myid 
int myN 
P r o c e s s G r o u p M e m b e r  m y  g r o u p
private 
int id 
int N 
P r o c e s s G r o u p M e m b e r  group 
The parameter my group of the SetProcessGroup method is a remote pointer to an array of remote processes
so a shallow copy implementation of SetProcessGroup will result in redundant future communications The
following deep copy implementation of SetProcessGroup which copies the entire remote array of remote
pointers to a local array of remote pointers is preferable
void P r o c e s s G r o u p M e m b e r  S e t P r o c e s s G r o u p 
int myid 
int myN 
P r o c e s s G r o u p M e m b e r  m y  g r o u p
id  myid 
N  myN 
group  new P r o c e s s G r o u p M e m b e r   N 
for  int i  0
i  N 
 remote copy 
group  i   m y  g r o u p  i 
After instantiating the processes in process group the master process can form a process group as
follows
for  int id  0
id  N 
p r o c e s s  g r o u p  i  -  S e t P r o c e s s G r o u p 
i  N  p r o c e s s  g r o u p
Alternatively the master process can execute the SetProcessGroup calls in parallel but this requires syn-
chronizing the processes at the end A standard way to do this is using barrier functions A process executing
the barrier function call must wait until all other processes in the group execute a barrier function call before
proceeding with the execution of the next statement
for  int id  0
id  N 
start p r o c e s s  g r o u p  i  -  S e t P r o c e s s G r o u p 
i  N  p r o c e s s  g r o u p
processgroup -  barrier  
Figure 1 Cluster conguration for data intensive computing The nodes are interconnected by a high speed
network 10 Gbsec Ethernet Each node has a 12 core Intel Xeon processor with 48 GB of RAM and 24
attached 1 TB hard-drives Each hard drive has an IO throughput in the range of 100-150 MBsec
33 Persistent objects and processes
We view a large data set as a collection of persistent processes which provide access to portions of the data
set as well as methods for computing with it Persistent processes are objects that can be destroyed only
by explicitly calling the destructor The runtime system is responsible for storing process representation
and activating and de-activating processes as needed Processes can be accessed using a symbolic object
address similar to addresses used by the Data Access Protocol DAP 5 for example
P a g e D e v i c e  p a g e  d e v i c e 
 http  data  set  P a g e D e v i c e 34  
Because persistence is especially important for large data objects in this paper we assume that all processes
and all objects are persistent
4 A program to compute the Fourier transform
In this section we show how to use processes in a data-intensive application on a cluster such as the one
shown in Figure 1 We chose a large three-dimensional Fourier transform as an example of a data-intensive
application and we give an extensive description of the process-oriented code We consider the situation
where the array data set is represented by a group of processes and the Fourier transform is a separate
application whose processes interact with the array processes In section 43 we show how the eciency of
the parallel computation can be improved using server-side caching
41 The Array
We described a method of storing a large data set on multiple hard drives in section 2 We now dene the
Array class that is used in the Fourier transform computation
626141dualIntelXeonX565012physicalcoresmulthreading48GBRAM24 x 1 TB 24 x100 MBsec node0networkswitch10 Gbsec  Ethernet  node1node2node3The Array class describes a complex double precision three-dimensional array
It species the array
domain its storage layout and data access methods We rst dene an auxiliary class to describe rectangular
three-dimensional domains
class Domain
public 
Domain 
int N11  int N12 
int N21  int N22 
int N31  int N32
The collection of hard drives attached to the cluster nodes can be turned into a distributed disk-based
random access memory by launching a PageDevice process for every hard-drive on the cluster node to which
this hard-drive is attached but for computations with arrays we launch ArrayPageDevice processes that
enable us to perform some of the array computations close to the data
A r r a y P a g e D e v i c e  p a g e  s e r v e r 
new A r r a y P a g e D e v i c e   N u m b e r O f S e r v e r s 
int i  0
i  N u m b e r O f S e r v e r s 
p a g e  s e r v e r  i   new  machine  i 
A r r a y P a g e D e v i c e 
f i l e n a m e  i  
NumberOfPages 
N1  N2  N3
The ArrayPageDevice processes could be launched instead of the PageDevice processes however in most
applications it would be advantageous to launch them alongside existing PageDevice processes In this case
ArrayPageDevice must include a constructor that takes a pointer to an existing PageDevice process as
a parameter There would be essentially no communication overhead when the ArrayPageDevice process
is launched on the same node as the corresponding PageDevice process The PageDeviceread method
for example will copy a page from a hard drive directly into a memory buer that is accessible by the
corresponding ArrayPageDevice process
Array storage layout is determined by the PageMap object The assignment of array pages to servers may
aect the degree of parallelism that can be achieved in the computation We use the circulant map which
assigns array page i1 i2 i3 to the server i1  i2  i3  NumberOfServers To complete the
specication of the PageMap we assign the rst available PageIndex address within the target PageServer
The Array class provides methods for a client process to compute over a small array subdomain The
client may use a small eg 4 GB memory buer to assemble the subdomain from array pages that reside
within page server processes as determined by the array pagemap
class Array
 complex double
public 
Array 
Domain  ArrayDomain 
Domain  PageDomain 
int N u mb e rO f S er v er s 
A r r a y P a g e D e v i c e  pageserver 
PageMap  pagemap
 Array  
void read  Domain  d  double  buffer  
void write  Domain  d  double  buffer  
void r e a d  t r a n s p o s e 1 3 
Domain  d  double  buffer
void w r i t e  t r a n s p o s e 1 3 
Domain  d  double  buffer
void r e a d  t r a n s p o s e 2 3 
Domain  d  double  buffer
void w r i t e  t r a n s p o s e 2 3 
Domain  d  double  buffer
private 
Domain  A r r a y D o m a i n 
Domain  P a g e D o m a i n 
int N u m b e r O f S e r v e r s 
A r r a y P a g e D e v i c e  p a g e  s e r v e r 
PageMap  pagemap 
An Array object is constructed by a single process which can then pass the object pointer to any group
of processes Because an Array is a persistent object it can also be accessed using a symbolic address as
described in section 33
Transpose IO methods are used to compute with long and narrow array subdomains that are not
aligned with the third dimension In section 42 we show an application of the transpose methods to the
computation of the Fourier transform The transpose of array pages can be computed either on the client
or on the servers For example the implementation of Arrayread transpose13 can assemble the data by
executing ArrayPageDeviceread transpose13 on the appropriate page server processes Alternatively
it can execute ArrayPageDeviceread methods on these servers followed by ArrayPagetranspose13 on
the received pages
FFT Processes
We compute the three-dimensional Fourier transform using three separate functions fft1 fft2 and fft3
each performing one-dimensional Fourier transforms along the corresponding dimension The functions
fft1 and fft2 are similar to fft3 except that subdomains are read and written using the IO transpose
operations of the Array class Having read and transposed the data into a local memory buer Fourier
transforms are computed along the third dimension The result is then transposed back and written to the
array We therefore restrict our description below to the fft3 function
The computation of the fft3 function is performed in parallel using several FFT3 client processes
class FFT3 
public P r o c e s s G r o u p M e m b e r
public 
FFT3  int sign  Array  a  
 FFT3   delete buffer  
void C o m p u t e T r a n s f o r m  
private 
double  buffer 
int sign 
Array  a 
We divide the array into n slabs along the rst dimension The master process launches n FFT3 processes
assigning each process to a slab
FFT3  fft  new FFT3   n 
for  int i  0
i  n 
fft  i   new  node  i  FFT3  sign  a  
for  int i  0
i  n 
fft  i  -  S e t P r o c e s s G r o u p i  n  fft  
for  int i  0
i  n 
start fft  i  -  C o m p u t e T r a n s f o r m  
Each process maintains a buer that can hold a page line where a page line is a collection of N P3 pages
with page indices
i1 i2 i3 
i3  0 1     N P3  1 
For complex double precision array of 1283 pages with each page consisting of 1283 points the page line
buer is 4 GB Each process computes
void FFT3  C o m p u t e T r a n s f o r m 
Domain  P a g e L i n e 
for every P a g e L i n e in the slab
a -  read  PageLine  buffer  
call FFTW sign  buffer  
a -  write  PageLine  buffer  
P r o c e s s G r o u p  - barrier  
An fft process assembles a page line by reading the pages from appropriate page servers For each page line
one FFTW 4 function call computes a set of n1  n2 one-dimensional complex double Fourier transforms of
size N3 each The result pages are then sent back to the page servers and stored on hard drives Although
the fft processes do not communicate with each other they share a common network and a common pool
of page servers The barrier synchronization at the end of each iteration is not strictly necessary
43 Parallel execution caching
Each page line read and write operation consists of disk IO and network transfer with disk IO being
signicantly more time consuming We implement caching on the page server in order to carry out part of
the disk IO in parallel with the FFTW computation
class A r r a y D e v i c e 
public A r r a y P a g e D e v i c e
public 
A r r a y P a g e D e v i c e 
string filename 
int NumberOfPages 
int n1  int n2  int n3 
A r r a y P a g e D e v i c e 
filename 
NumberOfPages 
n1  n2  n3 
N u m b e r O f C a c h e P a g e s  Nc 
void R e a d I n t o C a c h e  sizet p a g e  i n d e x  
void read  Page  p  sizet p a g e  i n d e x  
private 
int N u m b e r O f C a c h e P a g e s 
Page  cache 
An ArrayDevice server is congured with a cache that can hold a line of pages In order to use ArrayDevice
instead of ArrayPageDevice we add the following ReadIntoServerCache method to the Array class
void Array  R e a d I n t o S e r v e r C a c h e 
Domain  domain
for every page  i1  i2  i3  in domain
address a 
pagemap -  P a g e A d d r e s s  i1  i2  i3  
int id  a  s e r v e r  i d 
sizet i  a  p a g e  i n d e x 
start server  id  -  R e a d I n t o C a c h e  i  
The ArrayDeviceread method executes ArrayPageDeviceread if the requested page is not found in its
cache otherwise it returns sends over the network the cached page
The transform computation is now reorganized so that instructions to read the next line of pages are
sent to the page servers before the FFT of the current page line is started
void FFT3  C o m p u t e T r a n s f o r m 
Domain  P a g e L i n e 
for every P a g e L i n e in the slab
a -  read  PageLine  buffer  
Domain  N e x t P a g e L i n e  next page line
start a -  R e a d I n t o S e r v e r C a c h e  N e x t P a g e L i n e  
call FFTW sign  buffer  
a -  write  PageLine  buffer  
P r o c e s s G r o u p  - barrier  
After the completion of the FFT the servers are instructed to write pages to hard drives An execution of
the write method will take place only after the server has completed ArrayDeviceReadIntoCache The
eciency of server-side caching depends on the relative timing of the FFT computation and the page IO If
necessary write and WriteFromCache methods can be implemented in the ArrayDevice class analogously
5 Computation of the Fourier Transform
In this section we describe the computation of a large 64 TB data-intensive Fourier transform on a small
8 nodes 96 cores cluster We describe our prototype implementation of the process framework and analyze
the performance of the Fourier transform computation
Software Implementation of Processes
The Fourier transform application was developed by completing the process-oriented code which is sketched
in sections 2 and 4 translating it into C with MPI and linking it against an auxiliary library of tools
for implementation of basic process functionality We describe the procedure for ArrayDevice processes In
order to instantiate ArrayDevice processes we created an ArrayDevice server class and a C program
le ArrayDevice processcpp containing the following code
 start the server and d i s c o n n e c t from parent process
int main  int argc  char  argv 
M P I  I n i t  argc   argv  
M P I  C o m m parent 
M P I  C o m m  g e t  p a r e n t  parent  
A r r a y D e v i c e  s e r v e r  s 
new A r r a y D e v i c e  s e r v e r  
M P I  C o m m  d i s c o n n e c t  parent  
M P I  F i n a l i z e  
In addition we implemented the function
void L a u n c h P r o c e s s 
const string  p r o c e s s  f i l e  n a m e 
const string  m a c hi n e a d dr e ss 
string  s e r v e r  a d d r e s s
LaunchProcess uses MPI Comm spawn to create an MPI process by running the executable process file name
on the remote machine specied by machine address The launched process starts a server which uses
MPI Open port to open a port and return the port address in the server address output parameter The
server process then disconnects from the launching process
In order to implement remote method execution we also created an ArrayDevice client class The
ArrayDevice client and the ArrayDevice server classes are automatically constructed from the
ArrayDevice class For the purposes of this example we use a simple name mangling scheme to rst generate
the following ArrayDevice interface class
class A r r a y D e v i c e  i n t e r f a c e
public 
virtual void A r r a y D e v i c e  c r e a t e 
 c o n s t r u c t o r
string filename 
sizet numberofpages 
sizet p a g e s i z e
virtual void A r r a y D e v i c e  d e s t r o y  
 d e s t r u c t o r
virtual void A r r a y D e v i c e  w r i t e  Page  p  sizet p a g e  i n d e x  
virtual void A r r a y D e v i c e  R e a d I n t o C a c h e 
int n 
sizet  p a g e  i n d e x
 c o m m a n d s used in client - server p r o t o c o l
enum A r r a y D e v i c e  c o m m a n d
c r e a t e  C M D  0 
d e s t r o y  C M D  1 
w r i t e  C M D  2 
R e a d I n t o C a c h e  C M D  3
The ArrayDevice interface class contains meta-information obtained from the ArrayDevice class Both
the ArrayDevice client and the ArrayDevice server classes are derived from ArrayDevice interface
ArrayDevice client is also derived from a general ProcessClient class and similarly ArrayDevice server
is derived from ProcessServer Jointly the ArrayDevice client ArrayDevice server pair implement
remote procedure calls RPC for ArrayDevice The client-server communications protocol for ArrayDevice
uses the ArrayDevice interface class and the general-purpose functionality implemented in ProcessClient
and ProcessServer The client-server implementation uses a simple object serialization library to send
method parameters and results over the network This serialization library is also used to implement a
rudimentary le-based object persistence mechanism Information is sent over the network using an MPI-
based communications software layer
The translation procedure we implemented for the processes of the Fourier transform application can be
extended and incorporated into a C compiler It converts several hundred lines of process-oriented code
into a CMPI application for computing Fourier transform which is approximately 15000 lines long A
very small subset of MPI functions is used The following is the almost complete list
 MPI Send MPI Recv  for inter-process communication
 MPI barrier  for process synchronization
 MPI Open port MPI Close port MPI Comm accept  to implement client-server functionality
 MPI Comm spawn MPI Info create MPI Info set  to spawn processes on specied target ma-
chines
 MPI Comm connect MPI Comm disconnect MPI Comm get parent  to manage process connections
Both MPICH and Intel MPI were used in our computation and with both implementations we encoun-
tered problems with some MPI functions The most signicant bugs were found in MPI Comm spawn and
MPI Comm disconnect
Fourier Transform Computation
We used the process prototype implementation to carry out a computation of the Fourier transform of a
16K3-point array of complex double precision numbers We use the notation 1K  1024 16K  16384 
128  128 The total size of the array is 64 TB The computations were carried out on a cluster of 8 nodes
interconnected with a 10 Gbsec Ethernet similar to the cluster depicted in Figure 1 Each computational
node has an Intel Rcid13 Xeon Rcid13 X5650 12 core CPU with hyperthreading rated at 267GHz and 24 attached
1 TB hard drives The hard drives are manufactured by Samsung model SpinPoint F3 HD103SJ with
manufacturer listed latency of 414 ms and average seek time of 89 ms Benchamark hard drive read and
write throughput is reported at over 100 MBsec
The 16 K3-point array was broken up into 1283 ArrayPage pages of 128  128  128 points each The
resulting page size is 32 MB The choice of the page size is constrained by the latency and seek time of the
systems hard drives the smaller the page size the lower the overall disk IO throughput We measured a
typical page readwrite time in the range of 025-035 sec
We used 4 of the 8 available nodes to store the Array object creating 24 ArrayDevice processes on each
node one process for each available hard drive Because ArrayDevice processes are primarily dedicated to
disk IO it is possible to run 24 processes on a 12-core node The process framework makes it easy to shift
computation closer to data by extending the ArrayDevice class and in such case relatively more powerful
CPUs may be needed to run the server processes
We used the other 4 nodes to run 16 processes of the Fourier transform application 4 processes per node
Each of the 16 Fourier transform processes was assigned an array slab of 8  128  128 pages The process
computes the transform of its slab line by line in 8  128  1024 iterations Although the 16 processes are
independent of each other they compete among themselves for service from 96 page servers
The wall clock time for a single iteration of ComputeTransform see section 43 generally ranged from 68
to 78 seconds with the average of approximately 73 seconds The total speed of data processing including
reading computing and writing the data has been therefore close to 1 GBsec
In the next section we
analyze the performance in detail and indicate a number of ways to substantially improve it
53 Performance analysis
The computation of the Fourier transform was completed in the course of several long over 10 hours
continuous runs Our implementation of persistence mechanism for processes made it possible to stop and
restart the computation multiple times The primary reason for long runs was to test the stability and
robustness of our implementation We instrumented the code to measure the utilization of the systems
components the network the hard drives and the processors Because the results did not vary substantially
over the course of the computation we present a detailed analysis of a typical iteration of ComputeTransform
The synchronization of the processes at the end of each iteration is not strictly necessary but we found
that it did not signicantly aect performance and made the code easier to analyze On the other hand we
found that introducing additional barriers within the iteration would slow down the computation signicantly
We now present detailed measurements of the component phases of the iteration
At the beginning of every iteration each process reads a page line consisting of 128 pages which are evenly
distributed among the 96 servers by the circulant map see section 41 Except for the rst iteration the
required pages have already been read from the hard drive and placed in the memory of the corresponding
servers Each server has either 21 or 22 pages to send to the clients In total 64 GB of data is sent from
the servers to the clients Accordingly the combined size of the server caches in each of the server nodes is
slightly more than 16 GB We timed the parallel reading of page lines by the 16 client processes in a typical
time step
15 Array  read 11520 -11647 x 2432 -2559 x 0 -16383 4 GB  173638 sec  235894 MB  sec
12 Array  read 9216 -9343 x 2432 -2559 x 0 -16383 4 GB  186695 sec  219395 MB  sec
0 Array  read 0 -127 x 2432 -2559 x 0 -16383 4 GB  216104 sec  189538 MB  sec
1 Array  read 768 -895 x 2432 -2559 x 0 -16383 4 GB  222821 sec  183824 MB  sec
11 Array  read 8448 -8575 x 2432 -2559 x 0 -16383 4 GB  224682 sec  182302 MB  sec
8 Array  read 6144 -6271 x 2432 -2559 x 0 -16383 4 GB  227035 sec  180412 MB  sec
14 Array  read 10752 -10879 x 2432 -2559 x 0 -16383 4 GB  229213 sec  178698 MB  sec
5 Array  read 3840 -3967 x 2432 -2559 x 0 -16383 4 GB  271039 sec  151122 MB  sec
6 Array  read 4608 -4735 x 2432 -2559 x 0 -16383 4 GB  270914 sec  151192 MB  sec
2 Array  read 1536 -1663 x 2432 -2559 x 0 -16383 4 GB  272604 sec  150255 MB  sec
4 Array  read 3072 -3199 x 2432 -2559 x 0 -16383 4 GB  281986 sec  145256 MB  sec
3 Array  read 2304 -2431 x 2432 -2559 x 0 -16383 4 GB  283351 sec  144556 MB  sec
7 Array  read 5376 -5503 x 2432 -2559 x 0 -16383 4 GB  289623 sec  141425 MB  sec
10 Array  read 7680 -7807 x 2432 -2559 x 0 -16383 4 GB  29027 sec  14111 MB  sec
9 Array  read 6912 -7039 x 2432 -2559 x 0 -16383 4 GB  295628 sec  138553 MB  sec
13 Array  read 9984 -10111 x 2432 -2559 x 0 -16383 4 GB  301746 sec  135743 MB  sec
The rst number in each row is the client process id followed by the description of the domain domain
size the time it took to read it and the corresponding throughput The fastest process completes the
execution of the Arrayread method including sending the command and the parameter to page servers
in approximately 175 seconds the slowest  in about 30 seconds The faster processes proceed to start
the execution of ArrayReadIntoServerCache immediately after completion of Arrayread The typical
aggregate throughput during the parallel execution of Arrayread is therefore signicantly larger than 2
GBsec The maximal possible throughput to the four nodes computing the transform is approximately 4
GBsec
In the ArrayReadIntoServerCache phase of the computation 16 clients send small messages to 96
page servers with instructions to read a total of 16  128 pages These commands are queued for execution
in the servers so that the clients do not wait for the completion of the execution The time measurements of
the parallel execution of start ArrayReadIntoServerCache by the 16 client processes in a typical time
step were
14 Array  R e a d I n t o S e r v e r C a c h e 10752 -10879 x 2560 -2687 x 0 -16383 936148 sec
13 Array  R e a d I n t o S e r v e r C a c h e 9984 -10111 x 2560 -2687 x 0 -16383 210771 sec
10 Array  R e a d I n t o S e r v e r C a c h e 7680 -7807 x 2560 -2687 x 0 -16383 325717 sec
12 Array  R e a d I n t o S e r v e r C a c h e 9216 -9343 x 2560 -2687 x 0 -16383 95796 sec
8 Array  R e a d I n t o S e r v e r C a c h e 6144 -6271 x 2560 -2687 x 0 -16383 981484 sec
11 Array  R e a d I n t o S e r v e r C a c h e 8448 -8575 x 2560 -2687 x 0 -16383 136131 sec
9 Array  R e a d I n t o S e r v e r C a c h e 6912 -7039 x 2560 -2687 x 0 -16383 267452 sec
5 Array  R e a d I n t o S e r v e r C a c h e 3840 -3967 x 2560 -2687 x 0 -16383 516807 sec
2 Array  R e a d I n t o S e r v e r C a c h e 1536 -1663 x 2560 -2687 x 0 -16383 50223 sec
4 Array  R e a d I n t o S e r v e r C a c h e 3072 -3199 x 2560 -2687 x 0 -16383 408604 sec
0 Array  R e a d I n t o S e r v e r C a c h e 0 -127 x 2560 -2687 x 0 -16383 106741 sec
3 Array  R e a d I n t o S e r v e r C a c h e 2304 -2431 x 2560 -2687 x 0 -16383 394899 sec
1 Array  R e a d I n t o S e r v e r C a c h e 768 -895 x 2560 -2687 x 0 -16383 998827 sec
6 Array  R e a d I n t o S e r v e r C a c h e 4608 -4735 x 2560 -2687 x 0 -16383 516787 sec
15 Array  R e a d I n t o S e r v e r C a c h e 11520 -11647 x 2560 -2687 x 0 -16383 149185 sec
7 Array  R e a d I n t o S e r v e r C a c h e 5376 -5503 x 2560 -2687 x 0 -16383 327696 sec
These results are signicantly worse than expected Our implementation of processes repeatedly establishes
and breaks client-server connections We found the MPI Comm connect function to be very fast but with
increasing number of client-server connections it sporadically performed hundreds of times slower than usual
An implementation of caching connections is likely to reduce the total time for this phase of the computation
to about 3 seconds
We timed the execution of the FFTW function call by every processor
553611 sec
586336 sec
14 fftw 10752 -10879 x 2432 -2559 x 0 -16383
0 fftw 0 -127 x 2432 -2559 x 0 -16383
6 fftw 4608 -4735 x 2432 -2559 x 0 -16383
4 fftw 3072 -3199 x 2432 -2559 x 0 -16383
8 fftw 6144 -6271 x 2432 -2559 x 0 -16383
12 fftw 9216 -9343 x 2432 -2559 x 0 -16383
10 fftw 7680 -7807 x 2432 -2559 x 0 -16383
2 fftw 1536 -1663 x 2432 -2559 x 0 -16383
11 fftw 8448 -8575 x 2432 -2559 x 0 -16383
7 fftw 5376 -5503 x 2432 -2559 x 0 -16383
15 fftw 11520 -11647 x 2432 -2559 x 0 -16383
3 fftw 2304 -2431 x 2432 -2559 x 0 -16383
9 fftw 6912 -7039 x 2432 -2559 x 0 -16383
13 fftw 9984 -10111 x 2432 -2559 x 0 -16383
5 fftw 3840 -3967 x 2432 -2559 x 0 -16383
1 fftw 768 -895 x 2432 -2559 x 0 -16383
591158 sec
599485 sec
601364 sec
603876 sec
606033 sec
607081 sec
638444 sec
638421 sec
642032 sec
642975 sec
14001 sec
145631 sec
145582 sec
14878 sec
The last 4 processes 9135 and 1 ran on the same node Throughout our computation these 4 processes
executed the FFTW library function call signicantly slower than the processes running on other nodes
Additional investigation of the conguration of this node is needed to speed up the computation
The reading of the pages on the server side is done concurrently with the FFTW computation We
include the measurements for a few of the 96 servers
34 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  559864 sec  125745 MB  sec
25 A r r a y D e v i c e  R e a d I n t o C a c h e  21 pages  672 MB  5614 sec  119701 MB  sec
57 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  558319 sec  126093 MB  sec
52 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  559719 sec  125777 MB  sec
40 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  561367 sec  125408 MB  sec
73 A r r a y D e v i c e  R e a d I n t o C a c h e  21 pages  672 MB  557943 sec  120442 MB  sec
21 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  564487 sec  124715 MB  sec
33 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  563134 sec  125015 MB  sec
89 A r r a y D e v i c e  R e a d I n t o C a c h e  21 pages  672 MB  556771 sec  120696 MB  sec
46 A r r a y D e v i c e  R e a d I n t o C a c h e  22 pages  704 MB  562876 sec  125072 MB  sec
The reading throughput is close to the maximal throughput for this type of hard drives For every server the
page reading commands are scheduled before the page writing commands of the last phase of the iteration
The writing of the pages will therefore start only after the completion of the page read commands The
clients write pages in parallel with the typical timing as follows
8 Array  write 6144 -6271 x 2432 -2559 x 0 -16383 4 GB  223279 sec  183448 MB  sec
4 Array  write 3072 -3199 x 2432 -2559 x 0 -16383 4 GB  22784 sec  179776 MB  sec
11 Array  write 8448 -8575 x 2432 -2559 x 0 -16383 4 GB  227165 sec  18031 MB  sec
12 Array  write 9216 -9343 x 2432 -2559 x 0 -16383 4 GB  23083 sec  177447 MB  sec
15 Array  write 11520 -11647 x 2432 -2559 x 0 -16383 4 GB  230278 sec  177872 MB  sec
7 Array  write 5376 -5503 x 2432 -2559 x 0 -16383 4 GB  232437 sec  17622 MB  sec
0 Array  write 0 -127 x 2432 -2559 x 0 -16383 4 GB  237938 sec  172145 MB  sec
14 Array  write 10752 -10879 x 2432 -2559 x 0 -16383 4 GB  245307 sec  166975 MB  sec
10 Array  write 7680 -7807 x 2432 -2559 x 0 -16383 4 GB  242152 sec  16915 MB  sec
6 Array  write 4608 -4735 x 2432 -2559 x 0 -16383 4 GB  246648 sec  166066 MB  sec
2 Array  write 1536 -1663 x 2432 -2559 x 0 -16383 4 GB  247291 sec  165635 MB  sec
3 Array  write 2304 -2431 x 2432 -2559 x 0 -16383 4 GB  253921 sec  16131 MB  sec
9 Array  write 6912 -7039 x 2432 -2559 x 0 -16383 4 GB  218906 sec  187113 MB  sec
13 Array  write 9984 -10111 x 2432 -2559 x 0 -16383 4 GB  220268 sec  185956 MB  sec
1 Array  write 768 -895 x 2432 -2559 x 0 -16383 4 GB  217936 sec  187945 MB  sec
5 Array  write 3840 -3967 x 2432 -2559 x 0 -16383 4 GB  222789 sec  183851 MB  sec
The results for page writing are fairly uniform with the total time for each process between 22 and 255
seconds The aggregate througfhput for this phase is therefore over 25 GBsec We did not implement
explicit caching for writing pages The implementation of Arraywrite is analogous to the implementation
of ArrayReadIntoServerCache see section 43
void Array  write  Domain  domain 
for every page in domain
start write page to the appropriate server
There is a limited caching eect as a result of the start command having transmitted the page to the
server the client disconnects and proceeds to transmit pages to other servers while the server starts writing
the page to disk only after the client has disconnected We found that writing pages to hard drive with the
O DIRECT ag is about twice as fast as reading presumably because of buering The typical throughput
measured was between 230 and 240 MBsec
22 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 6 9 5 8 sec  233648 MB  sec
78 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 5 6 1 5 sec  235962 MB  sec
73 A r r a y D e v i c e  write  1 page  32 MB  013452 sec  237883 MB  sec
64 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 6 4 8 4 sec  23446 MB  sec
8 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 7 0 4 2 sec  233505 MB  sec
54 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 6 5 3 3 sec  234376 MB  sec
87 A r r a y D e v i c e  write  1 page  32 MB  0  1 3 6 2 9 3 sec  234788 MB  sec
The conclusion of our performance analysis is that the presented computation could be sped up by 25
or more but greater benets can be derived from a more balanced hardware conguration The aggregate
throughput of the 24 hard drives of a cluster node is about 3 GBsec about 3 times the capacity of the
incoming network connection Furthermore the FFTW computation takes only 10-20 of the total iteration
time It appears that a 24-fold increase in the network capacity of the present cluster is likely to result
in a more balanced system with better hardware utilization and a total runtime of under 20 seconds per
iteration
6 Discussion and Conclusions
In this paper we introduced process-oriented programming as a natural extension of object-oriented pro-
gramming for parallel computing We implemented a prototype of the process framework and carried out
a data-intensive computation We have shown that a complex and ecient application can be built using
only a few hundred lines of process-oriented code which is equivalent to many thousands of lines of object-
oriented code with MPI The process-oriented code in this paper is an extension of C but processes can
be introduced into any object-oriented language The syntax extension is minimal In C for example it
amounts to adding a parameter to the new operator and introducing the keyword start Combined with the
fact that creating a process can be thought of as simply placing an object on a remote machine it suggests
that a lot of existing code can be easily modied to run in parallel Potentially the most important impact of
the process-oriented extension of languages such as C and Python is a widespread adoption of parallel
programming as application developers realize the ability to easily create processes instead of using thread
libraries and to place dierent objects on dierent CPU cores
The process-oriented programming model is based on a simple hardware abstraction the computer
consists of a collection of processors interconnected by a network where each processor is capable of running
multiple processes The run-time system is responsible for mapping the abstract model onto a concrete
hardware system and must provide the programmer with system functions describing the state of the
hardware The hardware abstraction of the process-oriented model makes it possible to create portable
parallel applications and applications that run in the cloud
Processes are accessible by remote pointers Syntactically executing a method on a remote process
is not dierent from method execution on an object Any class of an object-oriented language can be
interpreted as a process but even more importantly in the process-oriented framework only processes that
are class instances are allowed We argue that object-based parallelism is a high level abstraction which is
naturally suitable for reasoning about parallelism Although shared memory and message passing can be
realized in a process-oriented language these are lower implementation-level models Process inheritance
is a powerful aspect of object-based parallelism as it enables the denition of new processes in terms of
previously dened processes In combination with process pointers it gives the programmer the exibility
to adapt the computation to the hardware by adding simple methods to class denitions see the comments
at the end of section 41 about the computation of array page transpose
A process-oriented program like a typical sequential program starts with a single main process The
main process may launch new processes on remote machines as easily as it can create objects In contrast
with MPI processes are explicitly managed by the programmer Process launching is part of the program
and is not determined by the runtime command line parameters Using process pointers and language data
structures the programmer can form groups of processes assign process ids and perform tasks that in MPI
would require using communicators
Processes exchange information by executing remote methods rather than via shared memory or message
passing Wed like to use the analogy that writing programs with message passing today is like writing
programs with GOTOs fty years ago it is easy to write intractable code And just like GOTO statements
are used in assembly languages message passing is a low-level language construct underlying remote method
execution in process-oriented programming
We introduced the start keyword to enable parallel execution of remote methods In order to use the
keyword the programmer must decide whether there is a need to wait for the remote task to complete before
proceeding with the computation In general this decision is easy and intuitive but keeping track of task
dependencies is not Each process executes only one method at a time and remote method execution requests
are queued The programmer must keep in mind the state of the execution queue for every process This is
possible only for very simple scenarios We used barriers to synchronize processes Barrier synchronization
helps the programmer to keep track of the execution queues of the processes but it may reduce the parallelism
of the computation
We view a large data object as a collection of persistent processes For a large data object a negligible
amount of additional storage space is needed to store serialized processes alongside the data Process
persistence is needed to enable stopping and restarting a computation and to make a data set accessible to
several simultaneous applications It is also needed to develop basic mechanisms for fault tolerance In this
paper we showed that the process-oriented view of a large data object is very powerful using only a small
amount of code the programmer can copy and reformat very large data objects and even carry out complex
operations such as the Fourier transform Yet software users and application developers tend to see a data
set as consisting of just data and being independent of a specic programming language We stop short
of suggesting a solution for this problem but in this context it is worth recalling the CORBA standard 8
The introduction of process-oriented programming was motivated by our research in data-intensive com-
puting The data-intensive Fourier transform computation was carried out on a small cluster of 8 nodes
96 cores Our measurements indicate that Petascale data-intensive computations can be eciently carried
out on a larger cluster with more nodes and signicantly increased network bandwidth Such a cluster can
serve as a general-purpose data-intensive computer whose operating system and applications are developed
as process-oriented programs
The introduction of process-oriented programming in this paper is far from complete It is merely the
rst step of an extensive research program We tested a substantial subset of the process framework in a
prototype implementation A full-edged implementation must include a compiler and a run-time system
that substantially expand the basic prototype
7 Acknowledgements
I am grateful to J J Bunn for discussions that signicantly improved the presentation of the material
References
1 MPI A Message-Passing Interface Standard version 30 httpwwwmpi-forumorg
2 OpenMP 40 Specications httpopenmporgwpopenmp-specifications
3 J Diaz C Munoz-Caro and A Nino A survey of parallel programming models and tools in the multi
and many-core era Parallel and Distributed Systems IEEE Transactions on 23813691386 2012
4 M Frigo and S G Johnson The design and implementation of tw3 Proceedings of the IEEE
932216231 2005
5 J Gallagher N Potter T Sgouros S Hankin and G Flierl The data access protocol-DAP 20 2004
6 E Givelberg Object-oriented parallel programming April 2014 arXiv14044666 csPL
7 E Givelberg A Szalay K Kanov and R Burns An architecture for a data-intensive computer In
Proceedings of the rst international workshop on Network-aware data management pages 5764 ACM
8 M Henning The rise and fall of CORBA Queue 452834 June 2006
9 M Herlihy and N Shavit The Art of Multiprocessor Programming Revised Reprint Elsevier 2012
10 E A Lee The problem with threads Computer 3953342 2006
11 T G Mattson B A Sanders and B L Massingill Patterns for parallel programming Pearson
Education 2004
