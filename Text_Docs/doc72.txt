A Note on Zipf s Law Natural Languages and Noncoding DNA
Regions
Partha Niyogi and Robert C Berwick
MIT Center for Biological and Computational Learning
Cambridge MA 
March  
Abstract
In Phys Rev Letters    Dec  Mantegna et al conclude on the
basis of Zipf rank frequency data that noncoding DNA sequence regions are
more like natural languages than coding regions We argue on the contrary
that an empirical cid12t to Zipf s law cannot be used as a criterion for similarity
to natural languages Although DNA is a presumably an organized system
of signs in Mandelbrots  sense an observation of statistical features
of the sort presented in the Mantegna et al paper does not shed light on
the similarity between DNAs grammar and natural language grammars
just as the observation of exact Zipf-like behavior cannot distinguish between
the underlying processes of tossing an M sided die or a cid12nite-state branching
process
In Phys Review Letters    Dec  Mantegna et al extend the Zipf approach
to analyzing linguistic texts to the statistical study of DNA base pair sequences and cid12nd
that the noncoding regions are more similar to natural languages than the coding sequences
p  Specicid12cally the authors analyze codingnoncoding DNA sequences and conclude
that noncoding regions show a more Zipf-like behavior than coding regions Asserting that
A remarkable feature of languages is Zipf s law p  they further conclude that
noncoding regions are more similar to natural languages than coding regions p 
The averages for each category support the observation that cid24 is consistently
larger for the noncoding sequences suggesting that the noncoding sequences
bear more resemblance to a natural language than the coding sequences
Their result has received popular notice in both Science  p   Nov  and
Scienticid12c American  March 
In this note we would like to argue that the Mantegna et al conclusion is rather far-
fetched Noncoding DNA sequences do not show much similarity to natural languages
Rather as far as one can judge from the evidence of the Mantegna et al paper all one can
sayif their statistical analysis is not in question which it may well beis that noncoding
DNA sequences and natural languages combine discrete symbols to form strings that obey
Zipf s law But this is of course what we knew from the outset In particular
cid15 Any number of random processes outputting discrete symbols can display Zipf-like
behavior without bearing any resemblance to the special generative processes currently
believed to govern sentence formation word sequences in natural languages In this
sense Zipf s law is not peculiar to natural languages at all and therefore cannot be
used as a strong test for whether DNA or anything else for that matter has something
in common with natural languages Indeed exactly this same point was made at
length over  years ago by Mandelbrot  in his familiar discussion of Zipf s law
Further because statistical and grammatical structures seem uncorrelated
in the cid12rst approximation one might expect to encounter laws which are
independent of the grammar of the language under consideration Hence
from the viewpoint of signicid12cance and also of the mathematical method
there would be an enormous dicid11erence between on the one hand  the col-
lection of data that are unlikely to exhibit any regularity other than the
approximate stability of the relative frequencies when dicid11erent samples are
compared ie data leading to statistical laws like Zipf s law our comments
pnrcb and on the other hand  the study of laws that are valid for natural
discourse the discovery of such laws being the goal of linguistics pnrcb but
not for other organized systems of signs p 
As is also familiar and as we show by examples below it is quite easy to generate Zipf-like
distributions from very simple generative processes that are quite unlike natural languages
eg tossing an M -sided die and particular very simple cid12nite-state branching processes
short although DNA is a presumably an organized system of signs in Mandelbrots sense
an observation of statistical features of the sort presented in the Mantegna et al paper does
not shed light on the similarity between DNAs grammar and natural language grammars
just as the observation of exact Zipf-like behavior cannot distinguish between the underlying
processes of tossing an M sided die or a cid12nite-state branching process An empirical cid12t to
Zipf s law cannot be used as a criterion for similarity to natural languages
cid15 Zipf s law is given by f r  C where f is the frequency of any word and r is its rank
with words arranged from most frequent to least frequent In other words lnf  
K cid0 cid24 lnr with cid24   The authors cid12nd that cid24 is  for coding regions and 
for noncoding regions and  for natural languages Without further statistical tests
it is not unreasonable to conclude that both coding and noncoding DNA sequences
are more alike to each other than either is to natural languages and that Zipf s law is
violated  What is plainly required are the usual signicid12cance tests addressing precisely
this question eg the null hypothesis that coding cid24 is the same as natural language
Indeed as N Chomsky points out pc if we take a collection of English sentences and decid12ne
words by taking the strings starting with say e and ending with e then the resulting
more random collection of words shows a better cid12t to Zipf s lawprecisely because there are
no interfering ecid11ects from the more organized features of natural language words On this view
the closer cid12t of noncoding sequences to a Zipf distribution actually means that noncoding DNA
sequences are more random and more unlike natural languages than coding sequencesexactly
the opposite conclusion that Mantegna maintain
cid24  Since the variances are clearly available the authors or others should be possible
to carry the required tests on the original data
cid15 As a minor point in fact the two measures used in the paperZipf behavior and
Shannon entropyare exactly correlated Therefore it is not surprising that given Zipf-
like behavior for noncoding sequences one would also observe that noncoding regions
have lower entropy than coding regions In ecid11ect there is just one not two similar
statistical properties p  that natural languages and noncoding sequences share
if they share it at all namely Zipf-like behavior or lower entropy
For a cid12nite number of words entropy is largest for a uniform distribution over word
frequencies The more skewed the word frequencies the lower the entropy For coding
regions with cid24   the word frequencies fall ocid11 more slowly with rank than for
noncoding regions cid24   Consequently coding regions will have have higher
entropy and lower redundancy than noncoding regions Having carried out a Zipf
analysis and obtained cid24  one does not need to compute a separate entropy test Yet
the authors do so as they recognize implicitly in the caption of cid12gure  of their paper
Putting aside these and other possibly grave statistical fallacies in the remainder of
this note we exhibit two random processes one an M -sided die the other a cid12nite-state
grammar that are very dicid11erent from each other yet yield exact Zipf distributions We then
review some of the many properties of natural languages not shared by these two processes
Consequently even if we accept the results of the Mantegna et al paper the inference from
Zipf-behavior to a similarity with natural languages cannot be justicid12ed As mentioned
these points have been discussed more than thirty years ago by Mandelbrot  and we
conclude with some historical remarks that underscore his results along with related more
recent work that has also examined Zipf-behavior in DNA sequences
I ZIPFS LAW AND RANDOM PROCESS SOME EXAMPLES
Zipf s Law and Random Processes
To begin let us consider two very dicid11erent simple random processes that both generate
Zipf distributions an M -sided die and a cid12nite-state grammar
Let us cid12rst recall Zipf s law itself Suppose there are M words in a system These
words might be generated in various combinations according to some underlying process
giving rise to a corpus of sentences or more generally word sequences Since there are only
a M words each word would occur multiple times in a large potentially incid12nite corpus
One can then rank these words from most frequent to least frequent Let the frequency of
the ith word be f
is proportional to
 the generative process is said to obey Zipf s
Example  An M -sided die
Let the sequence of words be generated by throwing a biased M sided die In particular
let the die be such that the probability of the ith side appearing on top is given by
P robi 
Now consider the following process
 Toss the biased die
 If the die shows j output word w
 Repeat 
Clearly this process generates a sequence of words where the cid12rst word is twice as likely
as the second three times as likely as the third and so on The process thus follows Zipf s
law exactly
Example  Finite-State Grammars
Next we consider a random process generating sentences in a completely dicid11erent
fashion from example  but still obeying Zipf s law Rather than deal with the case of
M words directly we provide some intuition in the form of an example where M  
Suppose there are four words w
 and w
 Sentences word sequences are produced
by combining words in some fashion according to a grammar Let us assume that the
generative process is as follows
FIGURES
FIG  A tree diagram representation of a cid12nite state grammar
 Start at the root node of the annotated tree of cid12g 
 At each node choose to go down any of the connected branches leading to a daughter
node with equal probability Output the word w
if the branch is associated with the
number i If the branch is associated with e output nothing empty string
 On reaching a leaf node stop
The reader will recognize that this is a cid12nite-state grammar Every path starting from
the root node gives rise to a sentence There are  dicid11erent paths corresponding to 
dicid11erent leaves giving rise to  possible sentence types Since the paths are all equally
likely each of these sentences occurs with equal likelihood
However due to the way in which the tree is constructed many paths yield the same
sentence For example the two paths highlighted in the cid12gure yield the same sentence
 The reader can check that such a grammar generates eight dicid11erent sentences with
the associated probabilities in table I
TABLES
Sentence
TABLE I Sentences generated by the cid12nite state grammar of cid12g  along with the probability
with which they are generated
If a corpus of sentences is generated with the probabilities shown in the table then it
can easily be shown that the word w
occurs twice as often as w
 three times as often as w
and four times as often as w
 In other words if we plot word frequencies then they would
follow Zipf s law
In general if there are M words then one could construct a similar tree Such a tree
would have M  leaves each leaf giving rise to a sentence The branches could be numbered
as done in the case where M   so that all the M  dicid11erent permutations of M words
can be generated Now as in the specicid12c M   case we replace some of the numbers by
e equivalent to outputting an empty string for that branch Let us now argue that this
replacement can be carried out and yields a grammar that generates a Zipf distribution
We cid12rst make the following observations to describe what M -tree looks like before any
such replacements have been made There are M branches at level  Each of these branches
bears a label from  to M  and no two branches bears the same label There are M M cid0 
branches at level  There are an equal number of branches bearing each label from  to
M  Consequently M cid0  of the branches at level  are labelled i for every i from  to M 
Similarly there M M cid0 M cid0  branches at level  with M cid0 M cid0  being labelled i
for every i from  to M  As mentioned before there are M  dicid11erent leaves each giving rise
to a dicid11erent sentence assuming no label were replaced by e Each sentence is M words
long a permutation of the M words with no repeated word
Next consider how we replace the labels by empty strings e Consider all the branches
labelled j Each time such a branch is traversed the grammar outputs the word w
 Suppose
Note that the probability of occurrence of each word is inversely proportional to its rank In a
cid12nite corpus the frequency of occurrence need not be exactly equal to the probability However
the convergence of frequencies to their underlying expectations make it more and more likely that
frequency-rank behavior will follow Zipf s law as the number of sentences in the corpus increases
with convergence in the limit as the corpus size goes to incid12nity
we chose to replace some of the j labels by e leaving only a
branches at level  still labelled
branches at level  and so on We can then prove the following two theorems given here
without proof 
Theorem  Suppose a
branches at level  are stil l label led and the remaining branches
are label led e Similarly suppose a
are label led at level  a
label led at level  and so on
Then a fraction f of the total number of paths through the tree yields a sentence containing
the word w
 where f is given by
    
M M cid0 
M M cid0 M cid0 
Clearly  cid20 a
cid20   cid20 a
cid20 M cid0  and in general  cid20 a
cid20
 Given these
M cid0i
M cid0
constraints on the a
s we can also prove the following
Theorem  Any fraction that can be represented as
where i is an integer between  and
M  can be obtained by an appropriate setting for the a
s under the constraints of Theorem 
A consequence of these theorems taken together is that one can generate sentences in
such a way that in a corpus the word w
can be made to occur in only a fraction f 
of the sentences In particular by choosing k appropriately we can make the j th word w
occur with frequency j in the text thus following Zipf s law exactly
II GENERAL REMARKS AND HISTORY
A Some Observations on the Structure of Natural Languages
It is well known that natural languages possess many other special properties that are
not tested by the Zipf-law behavior In particular while cid12nite-state grammars obey Zipf s
law it has long been known that they do not capture most of the striking properties of
natural languages
 Finite-state grammars by algebraic decid12nition cannot express hierarchical relationships
the acknowledged hallmark of natural languages Recall that cid12nite-state grammars are
algebraically associative concatenative systems see eg Harrison  that is if
L is a cid12nite-state grammar then a b c  cid6
 a cid1 bc  L if f ab cid1 c  L where cid1 is
cid3
the concatenation operator Such a system cannot even express the fact that one and
the same linear string of words such as the deep blue sky can have at least two
structural hierarchical bracketings the deep blue sky and the deep blue sky
In other words cid12nite-state grammars can express only linear precedence relations not
hierarchical relations
This demonstrates a failure of what Chomsky  called
strong generative capacity
 Finite-state grammars unlike natural language grammars cannot generate arbitrarily
deep center-embedded languages see Chomsky   and many other conven-
tional sources
 Under the currently best working assumptions natural language grammars contain
very specicid12c constraint statements with proprietary theoretical vocabularies unlikely
to be duplicated in DNA grammar eg one component so-called trace theory is
stated in terms of hierarchical structural sentence properties and noun phrases both
not shared by DNA as far as it is known
B Previous work on Zipf s Law and on DNA word frequencies
Both Zipf s law and its application to DNA sequences have a long history We mention
only a few of the relevant points here In the s as summarized in Mandelbrot 
We should point out that some researchers eg Searls  maintain the contrary position
and argue that natural language and DNA grammars share at least some generative processes A
discussion of this point is beyond the scope of this note
both Mandelbrot Simon  and Miller and Newman  among others explored the
nature of the word-frequency relationship embodied in Zipf s law In particular Mandelbrot
showed how Markovian models of discourse subsets of cid12nite-state models can give rise to
Zipf-like behavior Mandelbrot is careful to note the well-known inadequacy of such cid12nite-
state models to describe linguistic rules For example he writes p  the cid12nite-state
model appears as rather shocking because of the well known existence of some long-range
incid13uences in discourse such as those studied by grammar He advocates ways out of this
dicid14culty while acknowledging that the degree of validity of the cid12nite state model decreases
as the wealth of grammars increases Mandelbrot also uses various information-theoretical
arguments to suggest that Zipf s law is not peculiar to language but extends to any coding
scheme with a cid12nite number of symbolsand therefore can tell us relatively little about
any coding scheme like DNA
As it turns out there have also been many word-frequency analyses of DNA sequences
As Pevzner et al
 point out Mathematical models of the generation of genetic
texts appeared simultaneously with the cid12rst sequencing of sic pnrcb DNA Pevzner et
al  actually address the key question of variance and signicid12cant dicid11erences explicitly
proposing formulae for the variance of number of word occurrences in texts making it
possible to assess the signicid12cance of deviations from expected statistical characteristics
One can therefore carry out the signicid12cance tests suggested earlier in this note
III CONCLUSIONS
We have argued that an observation of Zipf-like behavior provides very little information
about the nature of the underlying process generating such frequency data This is simply
because the underlying generative processes could be as diverse as M -sided dies simple
cid12nite-state grammars DNA sequences and natural languages
Inferring that noncoding
DNA sequence grammars are like natural language grammars solely on the basis of Zipf-
behavior is at best premature and indeed at worst is likely to be completely misleading and
false
ACKNOWLEDGMENTS
This note describes research done at the Center for Biological and Computational Learn-
ing and the Articid12cial Intelligence Laboratory of the Massachusetts Institute of Technology
Support for the Center is provided in part by a grant from the National Science Foundation
under contract ASC Robert C Berwick was also supported by a Vinton-Hayes
Fellowship
This note has been improved by discussion with Morris Halle Federico Girosi Benoit
Mandelbrot and Noam Chomsky All remaining errors are ours
REFERENCES
cid3
Chomsky N  Three models for the description of language IRE Transactions on
Information Theory  IT-
Chomsky N  Know ledge of Language  New York Praeger Publishers
Harrison M  An Introduction to Automata and Formal Language Theory  Reading
MA Addison-Wesley
Mantegna Phys Review Letters   pp 
Miller G and Newman E  Tests of a statistical explanation of the rank-frequency
relation for words in written English American Journal of Psychology   
Mandelbrot B  Word frequencies and Markovian models of discourse In Structure
of Language and its Mathematical Aspects  Proceedings of the Symposia on Applied Math-
ematics v  American Mathematical Society 
Pevzner PA Borodovsky MY and Mironov AA  Linguistics of nucleotide se-
quences I The signicid12cance of deviation from mean statistical characteristics and pre-
diction of the frequency of occurrence of words J Biomolecular Structural Dynamics 

Searls D  The computational linguistics of biological sequences In L Hunter ed
Articid12cial Intel ligence and Molecular Biology  AAAI Press 
Simon H  On a class of skew probability distributions Biometrika   
Zipf G  Human Behavior and the Principle of Least Ecid11ort  Reading MA Addison-
Wesley
