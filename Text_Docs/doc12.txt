U T
F II
C  D H
Language Segmentation
Author
David A
Supervisors
Prof Dr Caroline S
Dr Sven N
August 18 2015
Erklrung zur Masterarbeit
Hiermit erklre ich dass ich die Masterarbeit selbststndig verfasst und keine ande-
ren als die angegebenen ellen und Hilfsmiel benutzt und die aus fremden ellen
direkt oder indirekt bernommenen Gedanken als solche kenntlich gemacht habe
Die Arbeit habe ich bisher keinem anderen Prfungsamt in gleicher oder vergleich-
barer Form vorgelegt Sie wurde bisher nicht verentlicht
Unterschri
Abstract
Language segmentation consists in nding the boundaries where one lan-
guage ends and another language begins in a text wrien in more than one lan-
guage is is important for all natural language processing tasks
e problem can be solved by training language models on language data
However in the case of low- or no-resource languages this is problematic
therefore investigate whether unsupervised methods perform beer than super-
vised methods when it is dicult or impossible to train supervised approaches
A special focus is given to dicult texts ie texts that are rather short one
sentence containing abbreviations low-resource languages and non-standard
language
I compare three approaches supervised n-gram language models unsuper-
vised clustering and weakly supervised n-gram language model induction I de-
vised the weakly supervised approach in order to deal with dicult text specif-
ically In order to test the approach I compiled a small corpus of dierent text
types ranging from one-sentence texts to texts of about 300 words
e weakly supervised language model induction approach works well on
short and dicult texts outperforming the clustering algorithm and reaching
scores in the vicinity of the supervised approach e results look promising
but there is room for improvement and a more thorough investigation should be
undertaken
Anowledgements
My thanks go to professor Caroline Sporleder for sharing her knowledge with me for
her inspiring ideas and for agreeing to supervise my Bachelors and Masters esis
despite her busy schedule It was also thanks to the topic she suggested for my Bach-
elors esis that I met Jrgen Knauth and later was able to get a research assistant
position at the SeNeReKo project collaborating closely with Jrgen
Which brings me to the next person on the list I would like to thank Jrgen Knauth
for the wonderful collaboration for his patience for his contagious enthusiasm and
all the interesting conversations in passing that always lasted longer than intended
Finally I would like to thank all the people that volunteered to proofread my thesis
and all the people that helped me during the writing of this thesis Unfortunately I
cannot list everyone You know who you are
I would like to thank Stephan Faber for his insightful comments when I couldnt
see the wood for the trees for his patience and optimism for pushing me to go further
and to persevere
I would also like to thank Julian Vaudroz for accompanying me throughout the
degree program We both didnt know what we were in for when we started but we
persevered and it paid o It wouldnt have been the same without you
List of Figures
Out-of-place metric 
Simple text illustration 
Initial model creation 
Initial model evaluation 
Model update
Evaluation 
New model creation 
Multiple model evaluation 
Updating relevant model
10 Multiple model evaluation 2 
11 New model creation 2 
14 Merging most similar models 
15 Word-Model assignment
Clustering preprocessor 
17 WEKA Cluster visualization 
22 Alternating language structure 
Problematic text sample 
Finding the most similar models 
ELKI Cluster visualization 
Language model Distribution 1 
Language Model Distribution 2 
Language model Distribution 3 
List of Tables
Training data Size 
Unambiguous encoding distances 
Simplied encoding distances 
N-Gram language model results Latin script 
N-Gram language model results Mixed script
N-Gram language model results Pali data 
N-Gram language model results Twier data 
Textcat results Latin script 
Textcat results Mixed script
Textcat results Pali data 
Textcat results Twier data 
Clustering results Latin script
Clustering results Mixed script
Clustering results Pali data 
Clustering results Twier data 
Induction results Latin script 
Induction results Mixed script 
Induction results Pali data 
Induction results Twier data 
Twier 3 Textcat versus Gold clustering 
Twier 4 Textcat versus Gold clustering 
List of Algorithms
N-gram numerical encoding 
Model induction 
Initial model creation 
Max model and max score 
Model merger
Distributional Similarity Calculation 
Contents
Introduction
2 Related work
21 N-Grams and rank order statistics 
22 N-Grams and maximum likelihood estimator 
23 Trigrams and short words
24 N-Grams and clustering 
26 Clustering and speech 
27 Monolingual training data 
Predictive sux trees 
Inclusion detection 
3 eory
Supervised language model 
311 N-Gram models 
Formal denition 
Smoothing 
32 Unsupervised clustering 
33 Weakly supervised language model induction 
4 Experimental setup
Preprocessing 
41 Data 
Implementation 
Training phase 
Supervised language model 
423 Application of the approach 
Textcat and language segmentation 
43 Unsupervised clustering 
432 Dening features 
433 Mapping features to a common scale 
434 e problem of unambiguous encoding 
435 e clusterer
44 Weakly supervised language model induction 
Evaluating clusterings 
441 Distributional similarity 
Evaluating results 
Estimating the parameters 
5 Results
51 N-Gram language model
52 Textcat
53 Clustering 
Language model induction 
6 Discussion
61 N-Gram language models 
62 Textcat
63 Clustering 
Language model induction 
Scores 
7 Conclusion
8 Appendix
81 Development data 
Latin script data 
Latin script data 
812 Mixed script data 
Twier data 
Pali dictionary data 
Twier data 
Pali dictionary data 
831 N-Gram Language Models 
Textcat 
Clustering 
Language Model Induction 
822 Mixed script data 
82 Test data 
83 Results 
Introduction
Language segmentation and identication are important for all natural language pro-
cessing operations that are language-specic such as taggers parsers or machine
translation Jain and Bhat 2014 Zubiaga et al 2014 Indeed using traditional mono-
lingual natural language processing components on mixed language data leads to mis-
erable results Jain and Bhat 2014 Even if the results are not terrible language identi-
cation and segmentation can improve the overall results For example by identifying
foreign language inclusions in an otherwise monolingual text parser accuracy can be
increased Alex et al 2007
One important point that has to be borne in mind is the dierence between lan-
guage identication and language segmentation Language identication is concerned
with recognizing the language at hand It is possible to use language identication
for language segmentation
Indeed by identifying the languages in a text the seg-
mentation is implicitly obtained Language segmentation on the other hand is only
concerned with identifying language boundaries No claims about the languages in-
volved are made
Aer giving an overview over related work and dierent approaches that can be
taken for language segmentation I will present the theory behind supervised methods
as well as unsupervised methods Finally I will introduce a weakly supervised method
for language segmentation that I developed
Aer the theoretical part I will present experiments done with the dierent ap-
proaches comparing their eectiveness on the task of language segmentation on dif-
ferent text types A special focus will be given to dicult text types such as short texts
texts containing under-resourced languages or texts containing a lot of abbreviations
or other non-standard features
A big advantage of unsupervised methods is language independence
If the ap-
proach used does not rely on language-specic details the approach is more exible
as no language resources have to be adapted for the method to work on other lan-
guages ese advantages might be especially useful for under-resourced languages
When there is no or insucient data available to train a supervised language model
an unsupervised approach might yield beer results
Another advantage is that unsupervised methods do not require prior training
ey are not dependent on training data and thus cannot be skewed by the data In-
deed supervised approaches that are trained on data are qualitatively tied to their
training data dierent training data will in all probability yield dierent models
is thesis aims at answering the question whether unsupervised language seg-
mentation approaches work beer on dicult text types than supervised language
approaches
2 Related work
21 N-Grams and rank order statistics
Cavnar and Trenkle 1994 use an n-gram language model for language identication
purposes eir program Textcat is intended to classify documents by language e
system calculates n-grams for 1 6 n 6 5 from training data and orders the n-grams
according to inverse frequency ie from the most frequent n-grams to the most infre-
quent n-grams e numerical frequency data is then discarded and only inherently
present
During training the program calculates an n-gram prole consisting of these n-
gram lists for each category ie language to classify
New data is classied by rst calculating the n-gram prole and then comparing
the prole to existing proles e category with the lowest dierence score is taken
as the category for the document
e score they use for classication is called out-of-place metric For each n-gram
in the document n-gram prole the corresponding n-gram in the category prole is
looked up and the absolute dierence of ranks is taken as score e sum is calculated
over all n-grams More formally the out-of-place metric moop is calculated as
jrxi d cid0 rxi cj
With n the number of n-grams in the document prole xi the i-th n-gram rxi d
the rank of the i-th n-gram in the document prole rxi c the rank of the i-th n-gram
in the category prole
Figure 1 illustrates the out-of-place metric
Category prole
Document prole
Most frequent
Least frequent
no match  max
Figure 1 Out-of-place metric
In gure 1 the document prole has ER as most frequent n-gram at rank 1 fol-
lowed by ING at rank 2 etc e category prole does not contain the n-gram ER in
that case an arbitrary xed maximum value is assigned e category prole contains
the n-gram ING at rank 2 the same rank as in the document prole the dierence is
0 e category prole contains the n-gram AT at rank 1 while in the document pro-
le it occurs at rank 3 e absolute dierence is 2 e out-of-place metric consists
of the sum of all scores thus calculated
Cavnar and Trenkle 1994 collected 3713 Usenet texts with a cultural theme in
dierent languages ey ltered out non-monolingual texts and texts that had no
useful content for language classication In the end they had 3478 articles ranging
from a single line of text to 50 KB of text
eir results indicated that length had no signicant impact on the classication
contrary to what they thought Also they found that training the system with 400
n-grams yielded the best result with a precision of 998
ey also showed that their approach could be used for subject classication of
texts in the same language with reasonable precision is nding indicates that lan-
guage and domain are linked to a certain degree
22 N-Grams and maximum likelihood estimator
Dunning 1994 also uses an n-gram language model for language identication pur-
poses e program calculates n-grams and their frequencies from the training data
and estimates the probability P of a given string using the Maximum Likelihood Esti-
mator MLE with Laplace add-one smoothingMore formally
P wijw1     wicid01 
Cw1     wi  1
Cw1     wicid01  jV j
with Cw1     Ci the number of times the n-gram w1     wi occurred
Cw1     Cicid01 the number of times the ncid0 1-gram w1     wicid01 occurred and jV j
the size of the vocabulary
For a string S the string is decomposed into n-grams and the log probability lk is
calculated as
w1wk2S
Cw1     wk log P wkjw1     wkcid01
where k is the order of the n-gram k  n used
In order to test the system Dunning 1994 uses a specially constructed test cor-
pus from a bilingual parallel translated English-Spanish corpus containing English and
Spanish texts with 10 texts varying from 1000 to 50000 bytes for the training set and
100 texts varying from 10 to 500 bytes for the test set
e results indicate that bigram models perform beer for shorter strings and less
training data while trigram models work beer for larger strings and more training
Dunning 1994 criticizes Cavnar and Trenkle 1994 for saying that their system
would be insensitive to the length of the string to be classied as the shortest text they
classied was about 50 words e system implemented by Dunning 1994 can classify
strings of 10 characters in length moderately well while strings of 50 characters or
more are classied very well Accuracies given vary from 92 for 20 bytes of training
data to 999 for 500 bytes of text
23 Trigrams and short words
Grefenstee 1995 compares trigrams versus short words for language identication
Short words are oen function words that are typical for and highly frequent in a given
language
e trigram language guesser was trained on one million characters of text in 10
languages Danish Dutch English French German Italian Norwegian Portuguese
Spanish and Swedish From the same texts all words with 5 or less characters were
counted for the short-word-strategy
e results indicate that the trigram approach works beer for small text fragments
of up to 15 words while for any text longer than 15 words both methods work equally
well with reported accuracies of up to 100 in the 11-15 word range
24 N-Grams and clustering
Gao et al 2001 present a system that augments n-gram language models with clus-
tering techniques ey cluster words by similarity and use these clusters in order to
overcome the data sparsity problem
In traditional cluster-based n-gram models the probability P wi is dened as the
product of the probability of a word given a cluster ci and the probability of the cluster
ci given the preceding clusters For a trigram model the probability P wi of a word
wi is calculated as
P wijwicid02wicid01  P wijci cid2 P cijcicid02cicid01
e probability of a word given a cluster is calculated as
P wijci 
with Cwi the count of the word wi and Cci the count of the cluster ci
e probability of a cluster given the preceding clusters is calculated using the
Maximum Likelihood Estimator
P cijcicid02cicid01 
Ccicid02cicid01ci
Ccicid02cicid01
Gao et al 2001 derive from this three ways of using clusters to augment language
models predictive clustering 7 conditional clustering 8 and combined clustering
P wijwicid02wicid01  P cijwicid02wicid01 cid2 P wijwicid02wicid01ci
P wijwicid02wicid01  P wijcicid02cicid01
P wijwicid02wicid01  P cijcicid02cicid01 cid2 P wijcicid02cicid01ci
Similarly Dreyfuss et al 2007 use clustering to cluster words by their context in
order to improve trigram language models In addition to Gao et al 2001 they also
use information about the subject-verb and verb-object relations of the sentence
ey show that their model using clustering subject-verb information verb-object
information and the Porter stemmer outperforms a traditional trigram model
Carter 1994 clusters training sentences ie the corpus into subcorpora of similar
sentences and calculates separate language model parameters for each subcorpus in
order to capture contextual information
In contrast to other works Carter 1994
clusters sentences instead of single words compare Pereira et al 1993 and Ney et al
1994 Carter 1994 shows that the subdivision into smaller clusters increases the
accuracy of bigram language models but not trigram models
Inclusion detection
Beatrice Alex cf Alex 2005 2006 2007 Alex et al 2007 Alex and Onysko 2010
addresses the problem of English inclusions in mainly non-English texts For the lan-
guage pair German-English inclusions are detected using a German and an English
lexicon as rst resource If a word is found only in the English lexicon it is tagged
as unambiguously English
If the word is found in neither lexicon a web search is
conducted restricting the search options to either German or English and counting
the number of results If the German search yields more results the word is tagged as
German otherwise as English inclusion If a word is found in both lexicons a post-
processing module resolves the ambiguity
Alex is mainly concerned with the improvement of parsing results by inclusion
detection For example in Alex et al 2007 they report an increase in F-Score of 43
by using inclusion detection when parsing a German text with a parser trained on the
TIGER corpus Brants et al 2002
26 Clustering and spee
In the area of clustering and spoken language identication Yin et al 2007 present a
hierarchical clusterer for spoken language ey cluster 10 languages1 using prosodic
features and Mel Frequency Cepstral Coecients MFCC MFCC vectors are a way of
representing acoustic signals Logan et al 2000 e signal is rst divided into smaller
frames each frame is passed through the discrete Fourier transform and only the log-
arithm of the amplitude spectrum is retained Logan et al 2000 e spectrum is then
projected onto the Mel frequency scale a scale that maps actual pitch to perceived
pitch as apparently the human auditory system does not perceive pitch in a linear
manner Logan et al 2000 Finally a discrete cosine transform is applied to the
spectrum to get the MFCC representations of the original signal Logan et al 2000
Yin et al 2007 show that their hierarchical clusterer outperforms traditional Acous-
As spoken language will not be further investigated in this thesis I will not dive
tic Gaussian Mixture Model systems
deeper into the maer at this point
27 Monolingual training data
Yamaguchi and Tanaka-Ishii 2012 King and Abney 2013 and Lui et al 2014 use
monolingual training data in order to train a system capable of recognizing the lan-
guages in a multilingual text
Yamaguchi and Tanaka-Ishii 2012 use a dynamic programming approach to seg-
ment a text by language eir test data contains fragments of 40 to 160 characters and
achieves F-scores of 094 on the relatively closed data set of the Universal Declara-
tion of Human Rights2 and 084 on the more open Wikipedia data set However the
approach is computationally intensive not to say prohibitive while Yamaguchi and
Tanaka-Ishii 2012 self-report a processing time of 1 second for an input of 1000 char-
acters Lui et al 2014 found that with 44 languages the approach by Yamaguchi and
Tanaka-Ishii 2012 takes almost 24 hours to complete the computation on a 16 core
workstation
King and Abney 2013 use weakly supervised methods to label the languages of
words ey consider the task as sequence labeling task ey have limited them-
selves to bilingual documents with a single language boundary and the task consists
1e authors do not explicitly list the languages clustered except for two-leer abbreviations which
seem to correspond to ISO 639-1 e languages under investigation could have been Vietnamese Ger-
man Farsi French Japanese Spanish Korean English Tamil and ma though it is impossible to tell
2httpwwwunorgendocumentsudhr
in discriminating between English and non-English text ey found that a Condi-
tional Random Field model augmented with Generalized Expectation criteria worked
best yielding accuracies of 88 with as lile as 10 words used for training
Lui et al 2014 consider the task as multi-label classication task ey represent
a document as an n-gram distribution of byte sequences in a bag-of-words manner
ey report F-scores of 0957 and 0959 ey note that similar languages will pose
problems when trying to identify a language and solve this problem by identifying a
set of languages that most probably are correct instead of a single language
One problem that these approaches all have is that they need to know the languages
that will occur in the test data King and Abney 2013 Lui et al 2014
28 Predictive sux trees
Seldin et al 2001 propose a system for automatic unsupervised language segmenta-
tion and protein sequence segmentation eir system uses Variable Memory Markov
VMM sources an alternative to Hidden Markov Models HMM implemented as Pre-
dictive Sux Trees PST
Whereas HMMs require substantial amounts of training data and a deep under-
standing of the problem in order to restrict the model architecture VMMs are simpler
and less expressive than HMMs but have been shown to solve many applications
with notable success Begleiter et al 2004
In contrast to n-gram models that es-
timate the probability of w as P wjN  with N the context typically the n previous
words VMMs can vary N in function of the available context Begleiter et al 2004
us they can capture both small and large order dependencies depending on the
training data Begleiter et al 2004
ere is no single VMM algorithm but rather a family of related algorithms One
of these algorithms is called Predictive Sux Tree PST Ron et al 1996 A PST is
a tree over an alphabet cid6 with each node either having 0 leaf nodes or jcid6j children
non-terminal nodes Ron et al 1996 Each node is labeled with the result of the walk
from that node up to the root Ron et al 1996 Each edge is labeled by a symbol s 2 cid6
and the probability for the next symbol being s Ron et al 1996
By modifying the Predictive Sux Tree PST algorithm using the Minimum De-
scription Length MDL principle Seldin et al 2001 end up with a non-parametric
self-regulating algorithm e MDL principle avoids overing of the model by favor-
ing low complexity over goodness-of-t Grnwald 2007
ey embed the algorithm in a deterministic annealing DA procedure to rene
the results Finally they use the Blahut-Arimoto algorithm a rate-distortion function
until convergence of the system
For the language segmentation task they use 150000 leers of text 30000 from
each of the following languages English German French Italian transliterated Rus-
sian ey used continuous language fragments of approximately 100 leers yielding a
synthetic multilingual text that switches language approximately every two sentences
One important point that they note is that too short segments do not enable reliable
discrimination between dierent models erefore they disallow switching models
aer every word
ey report very good results on the language segmentation task and on the pro-
tein segmentation task Aer 2000-3000 iterations of the Blahut-Arimoto algorithm
the correct number of languages is identied and the segmentation is accurate up to a
few leers
3 eory
Supervised language model
311 N-Gram models
Among supervised language models n-gram models are very popular Gao et al 2001
An n-gram is a slice from the original string Cavnar and Trenkle 1994 ese slices
can be contiguous or not Non-contiguous n-grams are also called skip-grams Guthrie
et al 2006 In skip-grams an additional parameter k indicates the maximum distance
that is allowed between units In this parlance contiguous n-grams can be regarded
as 0-skip-n-grams Guthrie et al 2006
e following example demonstrates the dierence between traditional n-grams
and skip-grams Given the following sentence
i s a sample sentence 
We can construct for example the following word k-skip-n-grams
0-skip-2-grams is is is a a sample sample sentence
2-skip-2-grams is is is a is sample is a is sample is sentence a sample a
sentence sample sentence
0-skip-3-gramsis is a is a sample a sample sentence
2-skip-3-gramsis is a is is sample is is sentence is a sample is a sen-
tence is sample sentence is a sample is a sentence is sample sentence a sample
sentence
e results for 2-skip-2-grams does not include the skip-gram is sentence as
the distance in words between these two words is 3 higher than the allowed k of 2 As
can be seen from this example the number of skip-grams is more than two times higher
than the number of contiguous n-grams and this trend continues the more skips are
allowed Guthrie et al 2006 Skip-grams unlike n-grams do not incur the problem
of data sparseness with an increase of n
Instead of using words as unit for n-gram decompositions we can also choose char-
acters Each word is then decomposed into sequences of n characters For example
the word
can be decomposed into the 2-grams mo de el Oen the word to decompose
is padded with start and end tags in order to improve the model Cavnar and Tren-
kle 1994 If we pad the word with w and w the 2-gram decomposition yields
wm mo de el l w e use of paddings allows the model to capture details about
character distribution with regard to the start and end of words Cavnar and Trenkle
1994 For example in English the leer y occurs more oen at the end of words than
at the beginning of words while the leer w occurs mainly at the beginning of words
Taylor 2015 A non-padding model cannot capture this distinction while a padding
model can
One advantage of n-gram models is that the decomposition of a string into smaller
units reduces the impact of typing errors Cavnar and Trenkle 1994 Indeed a typ-
ing error only aects a limited number of units Cavnar and Trenkle 1994 Due to
this property n-gram models have been shown to be able to deal well with noisy text
Cavnar and Trenkle 1994
312 Formal denition
Traditional n-gram language models predict the next word wi given the previous words
w1     wicid01 is prediction uses the conditional probability P wijw1     wicid01 In-
stead of using the entire history w1     wicid01 the probability is approximated by using
only the n previous words wicid0n1     wicid01
P wijw1     wicid01  P wijwicid0n1     wicid01
e probability can be estimated using the Maximum Likelihood Estimation MLE
P wijwicid0n1     wicid01 
Cwicid0n1     wi
Cwicid0n1     wicid01
Where Cwicid0n1     wi represents the number of times the n-gram sequence
wicid0n1     wi occurred in the training corpus and Cwicid0n1     wicid01 represents the
number of times the n cid0 1-gram sequence wicid0n1     wicid01 was seen in the training
corpus
Smoothing
e problem with MLE is that sequences not seen during training will have a prob-
ability of zero
In order to avoid this problem dierent smoothing techniques can
be used Chen and Goodman 1996 e simplest smoothing technique is additive
Laplace smoothing Chen and Goodman 1996 Let V be the vocabulary size ie the
total number of unique words in the test corpus e smoothed probability PLaplace
becomes
PLaplacewijwicid0n1     wicid01 
Cwicid0n1     wi  cid21
Cwicid0n1     wicid01  cid21V
With cid21 the smoothing factor If we choose cid21  1 we speak of add one smoothing
Jurafsky and Martin 2000 In practice cid21  1 is oen chosen Manning and Schtze
An important estimation is the Good-Turing estimation Chen and Goodman 1996
While not directly a smoothing method it estimates the frequency of a given observa-
tion with
cid3
 c  1
where c is the number of times the observation was made Nc is the number of times
the frequency c was observed and Nc1 the frequency of the frequency c  1 us
cid3 Chen and Goodman
instead of using the actual count c the count is taken to be c
Another way to avoid assigning probabilities of zero to unseen sequences is by
using back-o models ere are linear and non-linear back-o models In non-linear
back-o models if the original n-gram probability falls below a certain threshold value
the probability is estimated by the next lowest n-gram model Katzs back-o model
Katz 1987 for instance calculates probability Pbo using the formula
dwicid0n1wi
cid11wicid0n1wicid01Pbowijwicid0n2     wicid01 otherwise
Cwicid0n1wi
Cwicid0n1wicid01
if Cwicid0n1     wi  k
With d and cid11 as smoothing parameters e parameter k is oen chosen k  0
is means that if the probability given a high-order n-gram model is zero we back
o to the next lowest model For tri-gram models the formula becomes
Pbowijwicid02 wicid01 
8P wijwicid02 wicid01
cid111P wijwicid01
cid112P wi
if Cwicid02 wicid01  0
if Cwicid02 wicid01  0 and Cwicid01 wi  0
otherwise
In contrast linear back-o models use an interpolated probability estimate by com-
bining multiple probability estimates and weighting each estimate e probability PLI
for a tri-gram model is
PLIwijwicid02 wicid01  cid213P wijwicid02 wicid01  cid212P wijwicid01  cid211P wi
cid21i  1
32 Unsupervised clustering
Clustering consists in the grouping of objects based on their mutual similarity Bie-
mann 2006 Objects to be clustered are typically represented as feature vectors Bie-
mann 2006 from the original objects a feature representation is calculated and used
for further processing
Clustering can be partitional or hierarchical Yin et al 2007 Partitional clustering
divides the initial objects into separate groups in one step whereas hierarchical clus-
tering builds a hierarchy of objects by rst grouping the most similar objects together
and then clustering the next level hierarchy with regard to the existing clusters Yin
et al 2007
e clustering algorithm uses a distance metric to measure the distance between the
feature vectors of objects Biemann 2006 e distance metric denes the similarity
of objects based on the feature space in which the objects are represented Jain et al
1999 ere are dierent metrics available A frequently chosen metric is the cosine
similarity that calculates the distance between two vectors ie the angle between them
Biemann 2006
In order for a clustering algorithm to work features that represent the object to be
clustered have to be dened Jain et al 1999 Features can be quantitative eg word
length or qualitative eg word starts with a capital leer Jain et al 1999
Most clustering algorithms eg k-means need the number of clusters to generate
Jain et al 1999 e question how to best choose this key number has been addressed
in-depth by Dubes 1987
Clustering can be so or hard When hard-clustering an object can belong to one
class only while in so-clustering an object can belong to one or more classes some-
times with dierent probabilities Jain et al 1999
33 Weakly supervised language model induction
e main idea behind language model induction is that by inducing language models
from the text itself the models are highly specialized but the approach is generally
more exible since genre or text specic issues do not arise
is approach is similar in character to the work by Seldin et al 2001 in that the
text itself is used as data set However the realization diers greatly Whereas Seldin
et al 2001 use predictive sux trees I use n-gram language models
e intuition is to learn the language models from the text itself in an iterative
manner Suppose we have a document as follows where wi represents the word at
position i in the text Suppose the text contains two languages marked in red and
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 2 Simple text illustration
We take the rst word and create a language model m1 from that word
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 3 Initial model creation
We then evaluate the second word using the rst language model If the language
model score is high enough we update the language model with the second word
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 4 Initial model evaluation
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 5 Model update
If the score is below a certain threshold the existing language model does not model
the word well enough and a new model is created
evaluate
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 6 Evaluation
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 7 New model creation
When there is more than one language model each word is evaluated by every
language model and the highest scoring model is updated or a new model is created
if no language model models the word well enough
evaluate
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 8 Multiple model evaluation
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 9 Updating relevant model
evaluate
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 10 Multiple model evaluation 2
m1 m2 m3
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 11 New model creation 2
e last example shows that it is not necessarily the case that exactly one language
model is created per language it oen is the case that many language models are
created for one language
At the beginning the models are not very reliable as they only have a few words
as basis but the more text is analyzed the more reliable the models become
However the approach is problematic in that the text structure itself inuences
the language models created If the text starts with a foreign language inclusion as
illustrated in gure 12 the initial model might be too frail to recognize the following
words as being a dierent language updating the rst model with the second and third
word and so on us the approach would fail at recognizing the foreign language
inclusion
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 12 Problematic text sample
If we were to start from the end of the text and work towards the beginning the
probability of having a relatively robust language model for the blue language would
be high and so it would theoretically be easier to recognize the rst word as not being
blue
erefore one induction step involves one forward generation and one backwards
generation is yields two sets the set of models from the forward generation F 
ff1 f2     fng and the set from the backwards generation B  fb1 b2     bmg
en from the two sets of models the most similar models are selected For this
every model from F is compared to every model from B as gure 13 shows e most
similar models are then merged as illustrated in gure 14 Indeed if both the forward
and backwards generation yielded a similar language model it is probable that the
model is correct
Even so both forward and backwards generation can not guarantee ideal results
there is the option to run the generation from a random position is random induc-
tion picks a random position in the text and runs one induction step from that position
meaning one forward and one backwards generation Finally the most similar models
are merged as for the general generation
Figure 13 Finding the most similar models
Merged model
Figure 14 Merging most similar models
is only yields one probable language model therefore the induction is repeated
with the dierence that all probable models are taken into consideration as well For
each word if a probable model models the word well enough no new model is created
otherwise a new model is created
At the end of the induction loop the set of probable models P is examined As long
as there are two models that have a similarity score below a certain threshold the two
most similar models are merged
Finally aer the language models have been induced another pass is made over
the text and each word is assigned to the language model which yields the highest
score for that word resulting in a word-to-model assignment as illustrated in gure
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 15 Word-Model assignment
I have made the approach parametric with parameters being
 Induction iterations Number of induction iterations
 Random iterations Number of random iterations
 ForwardBackwards threshold reshold for forwardbackwards merging
 Silver threshold reshold for P model merging
ese parameters can be adapted in the hope that some parameter congurations
will work beer on certain data sets than other congurations Since the approach
has parameters that have to be learned from a development set the approach is said to
be weakly supervised the development set is not used to train any language specics
only for the estimation of the parameters of the approach
4 Experimental setup
In this chapter I present experiments done using the approaches delineated in the pre-
vious section in order to nd out whether there are approaches that work beer on
certain types of text
e central hypothesis is that unsupervised language segmentation approaches are
more successful on dicult data Dicult data is data for which there is not enough
data to train a language model or data which contains a lot of non-standard language
such as abbreviations
First I present the data used to test the language segmentation systems and elabo-
rate on the dierent aspects that had to be considered for the data compilation
I then present two supervised language segmentation experiments using n-gram
language models and Textcat
For unsupervised language segmentation I will rst present experiments using
clustering algorithms before presenting experiments using language model induction
41 Data
In order to test the dierent language segmentation approaches I compiled dierent
sets of test data As I want to focus on short texts most texts from the test corpus are
rather small sometimes consisting of only one sentence However in order to test the
general applicability of the approach the test corpus also contains larger text samples
e test corpus can be subdivided into dierent sub-corpora
 Latin-based Texts consisting of languages using Latin-based scripts such as
German English Finnish or Italian
 Mixed script Texts consisting of languages using Latin-based scripts and lan-
guages using non-Latin-based scripts
 Twier data Short texts taken from Twier
 Pali dictionary data Unstructured texts containing many dierent language in-
clusions such as Vedic Sanskrit Sanskrit Indogermanic reconstructions Old Bul-
garian Lithuanian Greek Latin Old Irish many abbreviations and references
to text passages
As every outcome has to be manually checked the test corpus is rather small Every
category consists of ve texts Each texts consists of two or three languages with the
exception of the Pali dictionary data that oen contains inclusions from many dierent
languages in the etymological explanations
For each text I also created a gold standard version with the expected clusters In
some cases it is not clear how to cluster certain objects In that case I use a clustering
that makes sense to me but this need not mean that it is the correct or only possible
clustering
For the parameter estimation of the language model induction approach I also
compiled a set of development data All texts can be found in the appendix under 81
and 82
Supervised language model
Implementation
For the supervised language segmentation method I implemented an n-gram language
model as described by Dunning 1994 e n-gram language model is implemented
as a character trigram model with non-linear back-o to bigram and unigram models
e conditional probability P is calculated using the formula
P wijwicid02 wicid01 
cid111
cid112
cid113
cid114
Cwicid02wicid01wi
Cwicid02wicid01
Cwicid01wi
Cwicid01
V W X
if Cwicid02 wicid01 wi  0
if Cwicid01 wi  0
if Cwi  0
otherwise
8
P w 
with cid111  07 cid112  02 cid113  009 cid114  001 V the number of unigrams W the
number of bigrams and X the number of trigrams
Each word is padded by two dierent start symbols and two dierent end symbols
e joint probability for a word w of length n is calculated as
j log P wijwicid02 wicid01j
In the denominator I use the log probability instead of the probability to increase
numerical stability
Indeed multiplying very small numbers can lead to the result
being approximated as zero by the computer when the numbers become too small to
be represented as normalized number Goldberg 1991 Using the sum of logarithms
avoids this problem and is less computationally expensive Brgisser et al 1997
As the logarithm of a number approaching zero tends to innity rare observations
get a higher score than frequent observations As such the denominator can be seen
as a scale of rarity with a higher score corresponding to a rarer word By taking the
inverse of this scale we get a score corresponding to the commonness cid25 frequency
of a word
422 Training phase
First models are trained on training data in the relevant languages I have not included
the languages from the Pali dictionary data as there are too many dierent languages
Language
Amharic
Chinese
English
Finnish
Italian
Russian
Spanish
Turkish
Ukrainian
Size in MB
Table 1 Training data Size
and there are typically only small inclusions of dierent languages in a dictionary en-
try as such it would not have made sense to train a language model just to recognize
a single word Another reason for not using the Pali dictionary data languages is that
sometimes it is not possible to nd data for a language eg Old Bulgarian or recon-
structed Indogermanic In some cases it would have been conceivable to train models
on similar languages but again the eort of training a model is disproportionately
high compared to the uncertain result of recognizing a single inclusion Instead an
additional catch-all language model is used to capture words that do not seem to belong
to a trained model
e training data consists of Wikipedia dumps from the months June and July 2015
a dump is a copy of the whole encyclopedia for a given language Due to the dierence
in size of the Wikipedia of the dierent languages I choose the full dump for languages
with less than 3 GB of compressed data and limited the amount of data to maximally
3 GB of compressed data
e Wikipedia data was processed using the Wikipedia Extractor3 version 28 in
order to extract the textual content from the article pages Indeed the Wikipedia pages
are wrien using the MediaWiki Markup Language4 While this markup is useful for
meta-data annotation and cross-referencing the encoded information is superuous
for language model training and has to be removed before training a model on the
data Table 1 shows the size of the training data per language aer text extraction
3httpmedialabdiunipiitwikiWikipediaExtractor
4httpswwwmediawikiorgwikiHelpFormatting
As the test data only contains transliterated Amharic text the Wikipedia data writ-
ten in the Geez script had to be transliterated e text was transliterated according
to the EAE transliteration scheme by the Encyclopaedia Aethiopica
As the test data contains transliterated Greek the Greek data was used once as-is
and once transliterated according to the ELOT Hellenic Organization for Standardiza-
tion transliteration scheme for Modern monotonic Greek
It should be borne in mind that the training data inuences the quality and accuracy
of the model Furthermore a model might work well on certain text types and less well
on other text types It is not possible to train a perfect universal model
423 Application of the approa
In the second step an input text is segmented into words en each word is evaluated
by each language model and the model with the highest score is assigned as the words
language model
e approach taken consists in classifying words as either belonging to a trained
language model or to the additional catch-all model other which simply means that
the word could not be assigned to a trained model class
424 Textcat and language segmentation
I also tested how well Textcat is suited to the task of language segmentation e
approach is similar to the n-gram approach with the exception that I do not train any
models and rely on Textcats classier for language prediction
In the rst step an input text is segmented into words en each word is passed
to Textcat and the guess made by Textcat is taken as the words language
43 Unsupervised clustering
In order to test the eciency of clustering algorithms on the task of language segmen-
tation I looked at various algorithms readily available through WEKA a collection of
machine learning algorithms for data mining tasks by the University of Waikato in
New Zealand Hall et al 2009 and the Environment for Developing KDD-Applications
Supported by Index-Structures ELKI an open source data mining soware  with
an emphasis on unsupervised methods in cluster analysis and outlier detection by
the Ludwig-Maximilians-Universitt Mnchen Achtert et al 2013 I also looked at
JavaML a collection of machine learning and data mining algorithms Abeel et al
2009 in order to integrate clusterers into my own code framework JavaML oers dif-
ferent clustering algorithms and also oers access to WEKAs clustering algorithms In
contrast to WEKA and ELKI which can be used in stand-alone mode JavaML is meant
to be integrated into bigger programs and provides an application programming inter-
face API that allows the provided algorithms to be accessed in a programmatic way
ie from inside a program
431 Preprocessing
However in order for the clustering algorithms to work the document to segment has
to be preprocessed in a number of ways as shown in gure 16
Figure 16 Clustering preprocessor
Read input
Tokenize input
Has token
Normalize
Remove tags
is empty
Extract features
First of all the document has to be read in by the program is step is straightfor-
e document then has to be tokenized Tokenization is not trivial and depends on
the denition of a word For this task I have used a whitespace tokenizer that denes
a word as a continuous sequence of character literals separated by one or more whites-
pace characters While it can be objected that for scripts that dont use whitespace to
separate words such as Chinese tokenization fails this is not too big a concern In-
deed if a continuous block of Chinese characters is treated as one word it is likely to
be clustered separately due to the dierent in word length and the dierent charac-
ter set If however a document contains two scripts that do not separate words by
whitespace the approach totally fails It is beyond the scope of this thesis and possi-
bly of any thesis to implement a universal tokenizer that works regardless of language
without prior knowledge about the languages at hand
Each token is then normalized Normalization of a non-Latin-based input eg Ara-
bic or Cyrillic script returns the input without modication Otherwise the following
modications are made if applicable
 remove leading and trailing whitespace
 remove punctuation
 remove control characters
Control characters are dened as the set
Punctuation is dened as the set
  cid0
e token is then stripped of XML-like tags if applicable e following example
illustrates this step Let us assume we have the following token
lemma go goes  word
word i d 1
e token is replaced by the text content of the node thus the resulting token is goes
If aer all these modications the token corresponds to the empty string we con-
tinue with the next token Otherwise the token is passed on to the feature extraction
module e algorithm terminates when all tokens have been consumed
432 Dening features
e nal step consists in dening features by which to cluster and implementing fea-
ture extractors that build the feature vectors from the input Since the features are to
be language independent using features such as occurs in an English lexicon cannot
be used e following features were devised
1 word length the length of the word in characters
2 X tail bigrams bigrams calculated from the end of the word
3 Y tail trigrams trigrams calculated from the end of the word
4 X rst bigrams bigrams calculcated from the beginning of the word
5 Y rst trigrams trigrams calculated from the beginning of the word
6 latin basic is the word latin basic
7 latin extended is the word latin extended
8 capitalized is the word capitalized
9 contains non-word does the word contain a non-word
10 is non-word is the word a non-word
11 number of latin leers number of latin leers
12 number of non-latin leers number of non-latin leers
13 vowel ratio number of vowels divided by the word length
14 basic latin leer ratio number of latin leers divided by the word length
15 max consonant cluster the longest consonant cluster size in characters
16 is digit is the word a digit
17 is ideographic is the word ideographic
18 directionality what directionality does rst character of the word have
19 is BMP codepoint does the word contain non-BMP characters
20 general type what is the general type of the rst character of the word
e last two features are based on the Java Character class is class provides
methods to check for specic implementation-based properties of characters
While most features are rather self-explanatory a few require further explanation
For the n-grams the number of n-grams is restricted so as to keep the resulting vec-
tors the same size is is important because the clustering algorithm considers one
data column as one feature and having vectors of dierent length would disrupt this
precondition Implementing the comparison of vectors of dierent lengths or rather
or vectors containing vectors as features would have been possible but rather time-
consuming If a word is too short to generate the required number of n-grams only
the possible n-grams are generated and all other positions lled with 0
e latin features check whether the word consists only of the basic latin leers
A-Z and a-z basic while the extended feature also covers leers derived from the
latin leers eg    
Non-words are dened as anything not consisting of leers such as punctuation
marks or digits
Directionality indicates which direction a character should be wrien While the
actual list is much more exhaustive this property basically indicates whether the char-
acter is wrien from le to right or from right to le 5
BMP stands for Basic Multilingual Plane and refers to an encoding unit known as
plane which consists of 216  65536 codepoints ie encoding slots for characters
e Unicode Consortium 2014 e BMP is the rst plane covering the codepoints
U0000 to UFFFF e Unicode Consortium 2014 While it is not important to un-
derstand the technical details fully it is interesting to note that most characters are
covered by the BMP including Chinese Japanese and Korean characters e Unicode
Consortium 2014 e next plane called Supplementary Multilingual Plane or Plane
1 contains historic scripts such as Egyptian hieroglyphs and cuneiform scripts but also
musical notation game symbols and various other scripts and symbols e Unicode
Consortium 2014 ere are 17 planes in total e Unicode Consortium 2014
e last feature in the list General Type is also an implementation-related property
Type can be for example5 ENDPUNCTUATION LETTERNUMBER or
MATHSYMBOL ese constants are represented as numbers internally which are
taken as feature for the clustering algorithm
433 Mapping features to a common scale
As JavaML requires numerical features all features were mapped to numerical scales
 Binary features were mapped to 0 false and 1 true
 Ternary features were mapped to 0 false 1 true and 99 not applicable
 Numerical features were represented as themselves either as whole numbers
eg word length or as oating point numbers eg vowel ratio
 Java specic features 1820 take the underlying numerical value as feature
 N-grams were encoded numerically using algorithm 1
5e full
hpdocsoraclecomjavase7docsapijavalangCharacterhtml
can be found under
the documentation of
the Java Character
sum   0
for character in word do
value  code-point of character
sum   sum  value
Algorithm 1 N-gram numerical encoding
1 function word
8 end function
end for
return sum
While algorithm 1 does not encode n-grams in an unambiguous way en and ne
are both encoded as 211 it provides a suciently good encoding
434 e problem of unambiguous encoding
I have tried using unambiguous encodings e main problem with unambiguous en-
coding is that the notion of distance is distorted e idea behind the unambiguous
encoding is that each word ie string of characters is encoded numerically so that
no two words are represented as the same number Besides the encoding of each sep-
arate character the position of the character inside the string also has to be encoded
A possible encoding e for a string w1w2w3 could be
ew1w2w3  nw1  x cid3 nw2  y cid3 nw3
with wi the character of the string at position i nwi the numerical encoding of
the character wi and x and y parameters If jAj is the alphabet size of the alphabet A
in which the word is encoded the following constraints must be true for the encoding
to be unambiguous
x cid21 jAj
y cid21 jAj2
If we take for example the English alphabet with 26 lowercase and 26 uppercase
leers not counting punctuation digits and other characters it has to be true that
x cid21 52 and y cid21 2704 e problem is that we cannot know in advance what size
the alphabet will be If we have English and German texts the size can be estimated
around 60 However if we have English Russian and Arabic text the size drastically
increases We could choose any two very big numbers but if we want to guarantee
our encoding to be unambiguous we run the risk of ending up with numbers too big
to be represented eciently
In this encoding scheme distance is skewed changes to the rst character result
man and nan have a distance of 1 because m and n have a
in linear distance
distance of 1 man and lan have a distance of 2 etc Changes to the second character
are multiplied by x man and men have a distance of x cid3 distancea e  4 cid3 x
Changes to the third character are scaled by y For any suciently big x and y the
distances are too skewed to be used for automatic cluster analysis Let us consider the
following example with only two characters for simplicity For this example let us
assume x  1373
Table 2 Unambiguous encoding distances
It should be apparent from table 2 that the notion of distance is distorted
comparison table 3 shows the encoding achieved with algorithm 1
Table 3 Simplied encoding distances
While this encoding is not unambiguous it is considered suciently good for our
purposes
435 e clusterer
Most clustering algorithms such as k-means need to be passed the number of clusters to
generate As we want to work as exibly as possible I ignored all algorithms that need
the number of clusters before clustering In contrast the x-means algorithm Pelleg
and Moore 2000 estimates the number of clusters to generate itself is algorithm
has been chosen to perform the language clustering tasks
While WEKA and ELKI oer a graphical user interface and various graphical rep-
resentations of the results the output is not easily interpretable Indeed we can get a
visualization of a clustering operation as shown in gures 17 WEKA and 18 ELKI
However all data points have to be manually checked by either clicking each point
in order to get additional information about that data point WEKA or by hovering
over the data points aer having selected the Object Label Tooltip option ELKI Fig-
ure 18 shows the information for the lowest orange rectangle data point in the ELKI
visualization
Figure 17 WEKA Cluster visualization
Figure 18 ELKI Cluster visualization
erefore I have decided to embed the x-means clustering algorithm into a custom
framework Originally part of the WEKA algorithms the x-means algorithm has been
integrated into a Java program via the JavaML library e framework takes an input
le constructs the aforementioned feature vectors from the input performs normal-
ization passes the calculated feature vectors to the clustering algorithm and displays
the results in a text-based easily interpretable manner
Preliminary analyses have shown that the rst clustering result oen is not dis-
criminating enough Hence I perform a rst clustering analysis followed by a second
clustering analysis on the clusters obtained from the rst analysis
436 Evaluating clusterings
e clustering results are evaluated using four common similarity measures used in
evaluating the accuracy of clustering algorithms ese methods are based on counting
pairs Wagner and Wagner 2007
Let us consider the clustering C  fC1     Ckg C is a set of non-empty disjoint
 fC1     Clg We
clusters C1     Ck Let us consider the reference clustering C
dene the following sets
 S11 set of pairs that are in the same cluster in C and C
 S00 set of pairs that are in dierent clusters in C and C
 S10 set of pairs that are in the same cluster in C and in dierent clusters in C
 S01 set of pairs that are in dierent clusters in C and in the same cluster in C
Let nij  jSijj with i j 2 f0 1g be the size of a given set Sij
e Rand Index is dened as
n11  n00
n11  n10  n01  n00
e Rand Index measures the accuracy of the clustering given a reference partition
Wagner and Wagner 2007 However it is criticized for being highly dependent on
the number of clusters Wagner and Wagner 2007
e Jaccard Index measures the similarity of sets It is similar to the Rand Index
but it disregards S00 the set of pairs that are clustered into dierent clusters in C and
 Wagner and Wagner 2007 It is calculated as
e Fowlkes-Mallows Index measures precision It is calculated as
n11  n10n11  n01
n11  n10  n01
e Fowlkes-Mallows Index has the undesired property of yielding high values
when the number of clusters is small Wagner and Wagner 2007
Finally I will indicate the F-Score According to Manning et al 2008 in the context
of clustering evaluation the Fcid12 score is dened as
F cid12 
cid122  1 cid3 P cid3 R
cid122P  R
with precision P and recall R dened as
n11  n10
n11  n01
By varying cid12 it is possible to give more weight to either precision cid12  0 or recall
cid12  1 Manning et al 2008 As I value recall higher than precision I will indicate F1
cid12  1 and F5 cid12  5 scores Indeed I want to penalize the algorithm for clustering
together pairs that are separate in the gold standard while not penalizing the algorithm
for spliing pairs that are together in the gold standard
All measures of similarity fall between 0 1 with 0 being most dissimilar and 1 be-
ing identical As there is no ultimate measure and all measures of similarity have their
drawbacks Wagner and Wagner 2007 all measures will be indicated in the results
section
44 Weakly supervised language model induction
e language model induction approach works in two stages In the rst stage n-gram
language models are induced from the text In the second stage the text is mapped to
the induced models e algorithm for the language model induction is as follows
Algorithm 2 Model induction
1 IM
2 for word in words do
12 end for
modelAndScore   MSword
score   modelAndScorescore
if score  threshold then
model   Mword
modelsaddmodel
maxM odel   modelAndScoremodel
maxM odelupdateword
First of all an initial language model is created For each word the maximum model
and maximum score is calculated ese values correspond to the language model that
yielded the highest probability for the word in question and the associated probability
If the score falls below a threshold t ie none of the existing language models model
the word well enough a new language model is created on the basis of the word and
added to the list of language models Otherwise the top scoring language model is
updated with the word in question
As the text structure itself inuences the quality of the induced models the lan-
guage model induction is run i times i 6 1 with one iteration consisting of two
induction steps once forward and once backward and j times from a random position
j 6 0 e initial model creation thus either picks the rst word of the text as shown
in algorithm 3 line 2 or the last word of the text or a random word
Algorithm 3 Initial model creation
1 function IM
5 end function
word   wordsf irst
model   createM odelword
modelsaddmodel
maxScore   0
maxM odel   none
for model in models do
Algorithm 4 Max model and max score
1 function MSword
12 end function
score   modelprobabilityword
if score  maxScore then
maxScore   score
maxM odel   model
end for
return maxM odel maxScore
Algorithm 4 returns both the max model and the max score wrapped as a custom
object e individual values can then be read as necessary
Aer the models have been induced the most similar models are merged based
on distributional similarity Distributional similarity is calculated as explained below
is merging step only merges one model from the forward induction group with one
model from the backward induction group e resulting model is added to the set of
probable silver models
Merging is performed according to algorithm 5 e merging algorithm only re-
tains the common set of unigrams from both models and all resulting bi- and trigrams
excluding any bi- and trigrams that contain character that occur only in one of the
models e values for the resulting language model are calculated according to one
of four dierent merge modes
e merge modes are
 f u1 is the frequency of u1
 or u2 since both are equal
 frequency of b in model1
 or 0 if it does not exist
for unigram u2 in model2unigrams do
end for
merged   
for unigram u1 in model1unigrams do
if u1  u2 then
v1   f u1
v2   f u2
value   modev1 v2
unigram   u1
merged   unigram value
exclude   u1
exclude   u2
Algorithm 5 Model merger
1 function model1model2 mode
35 end function
v1   f b model1 or 0
v2   f b model2 or 0
value   modev1 v2
merged   b value
v1   f t model1 or 0
v2   f t model2 or 0
value   modev1 v2
merged   t value
end for
return merged
end for
for all bigrams b in model1 and model2 do
if not exclude contains any char in b then
end for
for all trigrams t in model1 and model2 do
if not exclude contains any char in t then
 MAX use the maximum value maxv1 v2
 MIN use the minimum value minv1 v2
 MEAN use the mean value  v1v2
 ADD use the sum of the values v1  v2
If the random iteration count j  0 a random word is chosen and the induction
is run once forward and once backward starting from this position en the most
similar models from each set are merged and added to the set of probable models
It should be noted that seing the parameter j  0 will make the algorithm non-
deterministic
e model induction is then repeated while the iteration count i has not been
reached or until no more models are induced with the dierence that for each word
each probable model is rst consulted If any of the probable models yields a score
higher than the threshold value t it is assumed that the word is already well repre-
sented by one of the probable models and no models are induced for this word If the
score falls below the threshold value t induction is run as described
At the end of the induction loop all probable models are checked against each other
While there are two models that have a similarity below the silver threshold value s
the two models are merged and added to the set of very probable gold models
If the set of probable models is not empty aer this merging step all remaining
probable models are added to the set of very probable models
In the second stage the text is segmented according to the induced gold models
For each word the language model with the highest probability for the word is chosen
as that words hypothetical language model
441 Distributional similarity
Suppose we have three models with the distributions of leers as shown in gures 19
20 and 216 Similarity could be calculated based on the occurrence of unigramsleers
alone ie if model1 contains the leer a and model2 also contains the leer a their
similarity increases by 1
However if we calculate similarity in such a way all three models are equally simi-
lar to each other as each of the leers occurs at least once in each model Yet it should
be clear that models 1 and 2 are very similar to each other while model 3 is dissimilar
erefore in order to include the distribution of leers in the similarity measure
similarity is calculated as shown in algorithm 6
6e gures shown are used for illustration purposes only and do not necessarily reect real language
models
Figure 19 Language model Distribution 1
Figure 20 Language Model Distribution 2
Figure 21 Language model Distribution 3
 Initialize dierence to 1 to avoid division by zero
 unigram occurs in both models
 Normalize value by model size
for unigram u2 in model2unigrams do
similarity   0
dierence   1
for unigram u1 in model1unigrams do
Algorithm 6 Distributional Similarity Calculation
1 function model1model2
17 end function
if u1  u2 then
v1   f u1
v2   f u2
q   jv1cid0v2j
similarity   similarity 2 cid0 q
dierence   dierence 1
model1size
model2size
end for
end for
return similarity
dierence
with f c returning the frequency of the character c e number 2 in 2cid0q in line
10 can be explained as follows q expresses the dissimilarity of the models with regard
to a unigram distribution with 0 6 q 6 1 hence 1 cid0 q expresses the similarity To
this we add 1 as we increase similarity by 1 due to the match we augment the simple
increase of 1 by the similarity of the distribution
442 Evaluating results
e results of this approach can be interpreted as clusters where each language model
represents one cluster core and all words assigned to that model making up that cluster
Evaluation will hence be analogous to the evaluation of the clustering approach
443 Estimating the parameters
As the language model induction can be controlled by parameters we have to nd a
combination of parameters that works well for our task e parameters i j and merge
mode have been estimated on the development set e development set contains
similar documents to those in the test set e development set can be found in the
appendix
It has been found that the parameter combination i  4 j  2 ADD yields good
results across the development set Hence these values have been used for the test set
evaluation
5 Results
Baseline indicates the measurement where all words have been thrown into one clus-
ter measured against the gold standard For Baseline 2 every word has been put into
its own cluster and this clustering is evaluated against the gold standard e column
F1 stands for the F1 score and the F5 column stands for the F5 score
If any of the runs yields a higher score than any of the baseline values the max-
imum score is indicated in bold
If a eld contains na this means that the value
could not be calculated for whatever reason most oen a division by zero would have
occurred
51 N-Gram language model
Jaccard
Fowlkes-
Mallows
GermanEnglish
Baseline
Baseline 2
GermanFinnishTurkish
Baseline
Baseline 2
EnglishFren
Baseline
Baseline 2
EnglishTransliterated Greek
Baseline
Baseline 2
ItalianGerman
Baseline
Baseline 2
Table 4 N-Gram language model results Latin script
Jaccard
Fowlkes-
Mallows
GreekRussian
Baseline
Baseline 2
EnglishGreek
Baseline
Baseline 2
EnglishSpanishArabic
Baseline
Baseline 2
EnglishChinese
Baseline
Baseline 2
UkrainianRussian
Baseline
Baseline 2
Table 5 N-Gram language model results Mixed script
Jaccard
Fowlkes-
Mallows
Baseline
Baseline 2
Baseline
Baseline 2
Baseline
Baseline 2
Baseline
Baseline 2
Baseline
Baseline 2
Table 6 N-Gram language model results Pali data
Twitter 1
Baseline
Baseline 2
Twitter 2
Baseline
Baseline 2
Twitter 3
Baseline
Baseline 2
Twitter 4
Baseline
Baseline 2
Twitter 5
Baseline
Baseline 2
Jaccard
Fowlkes-
Mallows
Table 7 N-Gram language model results Twier data
52 Textcat
Jaccard
Fowlkes-
Mallows
GermanEnglish
Baseline
Baseline 2
Textcat
GermanFinnishTurkish
Baseline
Baseline 2
Textcat
EnglishFren
Baseline
Baseline 2
Textcat
EnglishTransliterated Greek
Baseline
Baseline 2
Textcat
ItalianGerman
Baseline
Baseline 2
Textcat
Table 8 Textcat results Latin script
Jaccard
Fowlkes-
Mallows
GreekRussian
Baseline
Baseline 2
Textcat
EnglishGreek
Baseline
Baseline 2
Textcat
EnglishSpanishArabic
Baseline
Baseline 2
Textcat
EnglishChinese
Baseline
Baseline 2
Textcat
UkrainianRussian
Baseline
Baseline 2
Textcat
Table 9 Textcat results Mixed script
Jaccard
Fowlkes-
Mallows
Baseline
Baseline 2
Textcat
Baseline
Baseline 2
Textcat
Baseline
Baseline 2
Textcat
Baseline
Baseline 2
Textcat
Baseline
Baseline 2
Textcat
Table 10 Textcat results Pali data
Jaccard
Fowlkes-
Mallows
Twitter 1
Baseline
Baseline 2
Textcat
Twitter 2
Baseline
Baseline 2
Textcat
Twitter 3
Baseline
Baseline 2
Textcat
Twitter 4
Baseline
Baseline 2
Textcat
Twitter 5
Baseline
Baseline 2
Textcat
Table 11 Textcat results Twier data
53 Clustering
e rst run indicates the value aer one clustering step and the second run indicates
the value aer applying the clustering algorithm to the results of the rst run
Jaccard
Fowlkes-
Mallows
GermanEnglish
Baseline
Baseline 2
First run
GermanFinnishTurkish
Baseline
Baseline 2
First run
EnglishFren
Baseline
Baseline 2
First run
EnglishTransliterated Greek
Baseline
Baseline 2
First run
ItalianGerman
Baseline
Baseline 2
First run
Table 12 Clustering results Latin script
Jaccard
Fowlkes-
Mallows
GreekRussian
Baseline
Baseline 2
First run
EnglishGreek
Baseline
Baseline 2
First run
EnglishSpanishArabic
Baseline
Baseline 2
First run
EnglishChinese
Baseline
Baseline 2
First run
UkrainianRussian
Baseline
Baseline 2
First run
Table 13 Clustering results Mixed script
Jaccard
Fowlkes-
Mallows
Baseline
Baseline 2
First run
Baseline
Baseline 2
First run
Baseline
Baseline 2
First run
Baseline
Baseline 2
First run
Baseline
Baseline 2
First run
Table 14 Clustering results Pali data
Jaccard
Fowlkes-
Mallows
Twitter 1
Baseline
Baseline 2
First run
Twitter 2
Baseline
Baseline 2
First run
Twitter 3
Baseline
Baseline 2
First run
Twitter 4
Baseline
Baseline 2
First run
Twitter 5
Baseline
Baseline 2
First run
Table 15 Clustering results Twier data
54 Language model induction
In addition to highlighting results that outperform the baseline values the following
tables have been color coded Results that outperform the clustering algorithm are
indicated in red and results that outperform both the clustering algorithm and the n-
gram language model are indicated in blue7
Jaccard
Fowlkes-
Mallows
GermanEnglish
Baseline
Baseline 2
Inducted
GermanFinnishTurkish
Baseline
Baseline 2
Inducted
EnglishFren
Baseline
Baseline 2
Inducted
EnglishTransliterated Greek
Baseline
Baseline 2
Inducted
ItalianGerman
Baseline
Baseline 2
Inducted
Table 16 Induction results Latin script
7Results that outperform only the n-gram language model would have been indicated in green but
there is no score that outperforms only the n-gram language model
Jaccard
Fowlkes-
Mallows
GreekRussian
Baseline
Baseline 2
Inducted
EnglishGreek
Baseline
Baseline 2
Inducted
EnglishSpanishArabic
Baseline
Baseline 2
Inducted
EnglishChinese
Baseline
Baseline 2
Inducted
UkrainianRussian
Baseline
Baseline 2
Inducted
Table 17 Induction results Mixed script
Jaccard
Fowlkes-
Mallows
Baseline
Baseline 2
Inducted
Baseline
Baseline 2
Inducted
Baseline
Baseline 2
Inducted
Baseline
Baseline 2
Inducted
Baseline
Baseline 2
Inducted
Table 18 Induction results Pali data
Jaccard
Fowlkes-
Mallows
Twitter 1
Baseline
Baseline 2
Inducted
Twitter 2
Baseline
Baseline 2
Inducted
Twitter 3
Baseline
Baseline 2
Inducted
Twitter 4
Baseline
Baseline 2
Inducted
Twitter 5
Baseline
Baseline 2
Inducted
Table 19 Induction results Twier data
6 Discussion
e work by Seldin et al 2001 is similar to the work presented here ey propose
an unsupervised language and protein sequence segmentation approach that yields
accurate segmentations While their work looks promising it also has its drawbacks
eir method requires longer monolingual text fragments and a sizable amount of text
Furthermore they disallow switching language models aer each word is presump-
tion will fail to detect single-word inclusions and structures as shown in gure 22
where the language alternates aer each word
Figure 22 Alternating language structure
While this structure looks very articial such a structure is found for instance
in the h Pali dictionary text in the passage Pacati Ved pacati Igd peq Av
pac- In this case red corresponds to Pali blue to abbreviations in English and
green to reconstructed Indo-european
61 N-Gram language models
e trained n-gram language model approach works well on the Latin script data man-
aging to single out the German inclusion from the EnglishGerman text even though
it is classied as other instead of German
For GermanFinnishTurkish EnglishFrench EnglishTransliterated Greek and
ItalianGerman the separation of the main languages involved is good although there
appear to be some problems when words contain non-word characters such as quotes
or parentheses
Some puzzling misclassications happen in the EnglishTransliterated Greek case
agpe is considered English and ros is considered Transliterated Amharic
In the ItalianGerman text the Italian language leads to a rather important Spanish
cluster due to the relatedness of the two Romance languages
On the mixed script data set the results are more diverse GreekRussian En-
glishSpanishArabic and UkrainianRussian are segmented well with English Span-
ishArabic having Spanish split into Spanish French and Italian due to the relatedness
of the languages
In contrast the segmentation of EnglishGreek did not work well at all Of the two
Greek words  and   was considered French and  was considered
Russian It must be noted though that these words bear polytonic diacritics whereas
the model was trained on monotonic Greek
Also the segmentation of EnglishChinese did not work well is is probably
due to the way the model was trained Chinese script is wrien without whitespace
characters between words and the correct segmentation of a text wrien in Chinese
requires in-depth knowledge of the language Some words are wrien with only one
character but others are composed of two or more characters with the meaning oen
being non-compositional the meaning of a two-character word is dierent from the
sum of the meaning of the two characters Sometimes more than one segmentation
would be possible and the context decides on which segmentation is correct In other
cases more than one segmentation might be correct is problem occurs with all
scripts that are wrien without whitespace
As with the simplied assumption in the tokenization of whitespace-scripts where
I consider a word to be a character sequence delineated by whitespace I have treated
each character as a word Adapting the method to Chinese and similar scripts would
have been possible but would have introduced the need for large amounts of external
linguistic knowledge Indeed every possible non-whitespace-script would have to be
considered and each of the tokenizers would be language dependent ie a tokenizer
for Chinese would not work on Korean or Japanese
e supervised approach did not work well on the Pali dictionary data While
English words could be isolated somewhat successfully the rest of the data proved
dicult to segment As an example let us look at the rst Pali text e English
cluster contains almost only English words but not all the other cluster contains
mainly marked up words and the rest is seemingly haphazardly distributed among
the other models
Pali 1 abbha
 AR  134 289
 DE Miln imber dark Miln
 EL  abbha
 EN water mountain of free used or like referred also A is cloudy
clouds later a froth 1 summit thundering by mass Pv Oir obscure scum
that water thick As from It is at as the in clouds things also
 ES dense f sense expl rajo
 FI 239 rain Lat Vin perhaps SnA
 FR cloud Dh adj point cloud Dhs A rain VvA DhsA list
 IT dark  ambha 3 1 317 J sunshine cp abhra Vedic megho
 PL 487  S 295 br moon 249
 RU 348 53
 TR viz ambu Vv
 TrAM 687 PvA sama 101 nl cp 64 nt 581 m Sn 1064
 TrEL  Gr Sk Idg to pabbata nt
 UK 12 273 617 348 250 251 382
 other b savilpa b b mua b smallcaps vi smallcaps
mahiy smallcaps iv smallcaps cloud b Rhu b b abbh
b b abbha superscript 9 superscript marajo b abbhmua
valhaka smallcaps i smallcaps b abbhmaa b valhakasikhara
superscript s superscript smallcaps ii smallcaps b dh- storm
cloud b ka b thundercloud at afros at b paala b
atombrosat nlamegha superscript1superscript m bhro dull
acchdesi mahikb b ghana b
On the Twier data the supervised approach achieved passable results While
the numbers look great the actual segmentations do not For Twier 1 too many
clusters were generated for Twier 2 and 3 the recognition of French words worked
somewhat also recognizing English words as French and French words as English
For Twier 4 the Polish inclusion was isolated but recognized as other together
with strawberries e recognition of transliterated Amharic worked satisfactorily
yielding naw to the Polish model
As the number of language models increases so does the risk of misclassication
As can be seen we already have quite some misclassication with only 15 language
models For example in our data the English preposition to is oen erroneously clas-
sied as transliterated Greek e Greek particle  to can be either the neuter sin-
gular accusative or nominative denite article the the masculine singular accusative
or nominative denite article the or the 3rd person neuter singular nominativeac-
cusative weak pronoun it and as such is rather frequent in the language is is
especially problematic with the transliterated Greek language model which tends to
misclassify the English preposition to as transliterated Greek
A quick corpus study using the Corpus of Modern Greek8 and the Corpus of Con-
temporary American English9 reveals that the frequency per million words for the
Greek particle  is 22666 while the English preposition to has a frequency per mil-
lion words of 25193 eir relative frequencies are very close together and it might
8httpweb-corporanetGreekCorpus
9httpcorpusbyueducoca
just have happened that the training data used in this work contained more Greek tos
than English tos leading to this misclassication
Other reasons for misclassication include relatedness of the modeled languages
as in the case of Germanic or Romance language families Also the text types used
for training and the text types used for testing play an important role as well as the
amount of training data
For n-gram language models the quality of the model is dependent on the texts
used for training and the texts used in evaluation It is probable that a dierent training
set would have yielded dierent results is is also the problem with the supervised
approach it is necessary to have language data for training and the trained models
reect the training data to some extent
62 Textcat
Textcat works well on monolingual texts However it fails on multilingual texts and
does not work well on short fragments of text such as single words Many of the words
are tagged as unknown and if a language has been identied the language guess oen
is not correct Hence Textcat cannot be used for language segmentation purposes
Indeed Textcat fails to exceed the baseline values except for two cases Twier
3 and Twier 4 yield beer values than the baseline values However upon closer
inspection it is clear that the numerical index values do not give a reliable picture of
the quality of the clustering
Indeed while the clustering of Twier 3 is not nonsensical it is not very good
failing to extract the French insertion breuvages e Rand Index also only shows a
slightly beer value than the baseline values It seems that the outstanding score for
Twier 4 is achieved because both the clustering by Textcat and the gold standard
have the same number of clusters
Tables 20 and 21 show the clusterings side by side Clearly Textcat performed
poorly despite the high numerical index values A closer inspection of all the Textcat
results shows that Textcat performs poorly at the task of language segmentation oen
a word cannot be assigned a language and thus is added to the cluster of unknown
language words For the words where a language has been identied it most oen
is not the correct language While language identication is not necessary for the
task of language segmentation it helps to understand why Textcat failed at the task of
language segmentation
Cluster 1
Cluster 2
Cluster 3
Textcat
bilingualism
Food and breuvages in Ed-
monton are ready
to go
just waiting for the fans
FWWC2015
Gold standard
breuvages
FWWC2015 bilingualism
Food and in Edmonton are
ready to go just waiting for
the fans
Table 20 Twier 3 Textcat versus Gold clustering
Textcat
strawberries
Cluster 1
Cluster 2 my dad comes back from
poland with two crates of
ubrwka and adidas jack-
ets omg
Gold standard
ubrwka
my dad comes back from
poland with
crates
of strawberries and adidas
jackets omg
Table 21 Twier 4 Textcat versus Gold clustering
63 Clustering
e clustering results are more dicult to interpret Oen the rst distinction made
seems to be based on case ie words that begin with a capital leer versus words that
are all lowercase leers e second run on the mixed script English  Greek data
shows that the rst cluster from the rst run has been separated into a cluster with
words that begin with a capital leer and two clusters with words that dont begin
with a capital leer
EnglishGreek First run First cluster
 intimate without Although Aquinas Christians Corinthians Socrates Sym-
posium Testament Whether aection ancient another appreciation aspires
araction araction becomes benevolence biblical brotherly chapter char-
ity children children contemplation content continues contributes deni-
tion described existence explained express feeling feelings nding further
holding initially inspired knowledge marriage necessary non-corporeal pas-
sage passion philosophers physical platonic rened relationships returned
self-benet sensually spiritual subject suggesting through throughout tran-
scendence unconditional understanding without youthful
EnglishGreek Second run Splitting of rst cluster
 aection ancient another aspires becomes biblical chapter charity chil-
dren children content denition feeling feelings nding holding marriage
necessary passage passion platonic rened returned subject through with-
 Although Aquinas Christians Corinthians Socrates Symposium Testament
Whether
 intimate appreciation araction araction benevolence brotherly contem-
plation continues contributes described existence explained express further
initially inspired knowledge non-corporeal philosophers physical relation-
ships self-benet sensually spiritual suggesting throughout transcendence
unconditional understanding youthful
Indeed the results
Another important distinction seems to be the length of words
oen show clusters that clearly are based on the length of the contained words e
rst run on the latin script German  Italian data shows that short words have been
singled out into the rst cluster
ItalianGerman First run First cluster
 il E So a ad da di e es ha i il in la le lo ma ne se si un va zu
e clustering works well when the scripts involved are dissimilar as in the case
of the EnglishChinese text where the Chinese characters were isolated aer the rst
run and also the EnglishSpanishArabic example where the Arabic part was com-
pletely isolated in the rst run
e closer the scripts become the less well clear cut the results are For Greek
Russian the results are acceptable with one mixed cluster However the number of
clusters is too high for the number of languages involved and the separation is only
achieved aer two consecutive clusterings
e clustering of closer scripts such as UkrainianRussian does not work well e
clusters with the exception of the cluster containing the datum 913 are all impure
consisting of Ukrainian and Russian words e second run also fails at improving the
clustering
Finally clustering of latin based scripts does not perform well unless diacritics are
involved and the diacritics form the most salient distinction Word containing leers
with diacritics are then generally separated from words containing no diacritics as in
the GermanFinnish-Turkish example e rst run generates a cluster for numbers
two clusters with diacritics and one cluster without diacritics
Probably for this reason the clustering of Transliterated GreekEnglish and Greek
In both cases the rst run managed to separate
English worked surprisingly well
the transliterated Greek parts from the English words However unaccented Greek
words such as Agape erotas or eros were clustered with English
EnglishTransliterated Greek First run Transliterated Greek cluster
 agpe phila storg ros
EnglishGreek First run Greek cluster
   Agpe agp ros rs 
e problem is that when there are other salient distinguishing features besides
diacritics the result is less good as can be seen on the Pali data
Pali abhijjhitar Second run
 abhijjhita abhijjhtar covets function med one who itar itar tar
 T A M
  l v
 smallcaps i smallcaps smallcaps v smallcaps ag fr in
 265 287
 n
In some cases the clustering fails at the task of language segmentation as in the
case of the various EnglishFrench texts and the EnglishGerman example with the
German inclusion We can thus say that the surface structure or morphology or in
other words the basis from which we can extract features is not sucient to deduce
relevant information about language
When there are more than two languages that are to be separated the cluster-
ing also does not work well Indeed the most dissimilar objects are separated rst
In the case of EnglishSpanishArabic the Arabic part is separated rst as well as
words with diacritics while English and Spanish words without diacritics are thrown
together Subsequent runs show no improvement of the clustering concerning the
separation of English and Spanish
In the case of GermanFinnishTurkish the clustering algorithm seems to cluster
out Turkish rst followed by Finnish e results are however much less clear-cut
than for EnglishSpanishArabic
64 Language model induction
e language model induction does not seem to work very well on the Latin script data
ere are almost only impure clusters containing more than one language However
the approach consistently outperforms the clustering approach when we look at the
F5 score For the EnglishFrench data set the clustering approach even outperforms
the n-gram language model approach Indeed the French words are relatively well
separated from the English text with the exception of sucr which is still thrown
together with English words
Latin script EnglishFren
 both so in English although their is is the opposite of rough or is
the opposite of sweet only for wines otherwise is
 mou  mou but
 doux
 Doux rugueux Doux
 while
 hard used
 translate as meaning very dierent coarse can also mean almostsucr
In contrast the approach works well on the mixed script data Indeed we achieve
a good separation of the languages by script However when there are also Latin
based scripts we encounter the same problems as mentioned above with rather modest
results For example for the EnglishGreek text the approach separates out the Greek
character words but it fails to separate transliterated Greek and English Also for the
EnglishSpanishArabic text Arabic is separated out but English and Spanish are not
separated well
One interesting observation can be made in the case of the EnglishChinese text
e Chinese characters have been isolated but the Pinyin transcription is thrown to-
gether with the Chinese characters Based on the prior observations this is rather
unexpected is raises the question of whether Pinyin ought to be clustered out or
clustered together with English or Chinese
Again the language model induction approach outperforms the clustering approach
and also the n-gram language model approach in the case of the EnglishGreek text
On the larger Pali dictionary entries the language model induction approach yields
acceptable results On the shorter Pali dictionary entries the language model induction
approach yields good results
e quite low performance must be blamed on the data Indeed the Pali dictionary
data contain various problematic characters such as commadot and whitespace as
one character On such characters whitespace tokenization fails yielding big chunks
of nonsense tokens For example the fourth Pali dictionary entry was split into ve
chunks while it might not be displayed as such all commata and all dots are in fact
not followed by whitespace the whitespace is part of the character10 hence whitespace
tokenization fails
Pali ghan Chunks
 Ghanf
 abstrfrghatighan
qv
 Pug19CppariPage
 253
Furthermore the data contains markup abbreviations references typing mistakes
and signs such as - that are dicult to assign to a language
On the Twier data the language model induction approach works rather well
For example on the rst text separation is not perfect with the Greek cluster still
containing some English words
Twitter 1 EnglishGreek
 BUSINESS EXCELLENCE
      Internet of  
 ings IT
For the third and fourth text the approach manages to single out the other-language
inclusions but not exclusively Both times there is one additional item in the cluster
the relevant clusters are marked in red
10e comma has the Unicode codepoint UFF0C FULLWIDTH COMMA and the dot has the Uni-
code codepoint UFF0E FULLWIDTH FULL STOP
Twitter 3 FrenEnglish
 FWWC2015
 breuvages go
 Food Edmonton to for the
 in waiting bilingualism
 and are ready just fans
Twitter 4 EnglishPolish
 comes from with two crates of strawberries jackets omg
 my dad poland and adidas
 back ubrwka
e approach exceeded expectations on the second and h Twier text On the
second text the French cluster does not only contain the French words Demain and
par but also the French way of notating time 18h
Twitter 2 FrenEnglish
 Keynote e collective of science-publish or perish it all that counts
 Demain 18h par
 dhiha6 David
 dhiparis dynamics is
On the h text an almost perfect result was achieved with only one additional
subdivision of the English cluster
Twitter 5 Transliterated AmharicEnglish
 coee
 bread is our
 Buna dabo naw
It seems that the language model approach does not work very well on longer texts
especially on longer texts in Latin-based scripts with the chosen parameter set still
the approach outperforms the clustering approach and achieves scores in the vicinity
of the scores achieved with the supervised trained n-gram language model approach
On mixed script texts the approach consistently outperforms the clustering approach
and we also reach scores in the vicinity of the scores achieved with the supervised
trained n-gram language model approach
Moreover on short texts the approach works rather well We succeed in outper-
forming the supervised trained n-gram language model approach on a number of texts
and we achieve scores close to the scores achieved with the supervised trained n-gram
language model approach
Although the language model induction approach tends to generate too many clus-
ters it also generally succeeds at separating the languages involved
Of the scores I used for evaluation purposes it seems that a combination of a high
Rand Index and a high F5 score indicate a good language segmentation A high F5
score alone is not signicant For example the clustering algorithm achieves an F5
score of 07215 on Twier 3 is score looks good but the Rand Index score is at
04571 and the segmentation is not good
Twitter 3 Cluster analysis
 Edmonton Food
 go in to
 and are breuvages fans for just ready the waiting
Similarly a high Rand Index score alone is not signicant For example the clus-
tering algorithm achieves a Rand Index score of 06738 on the Pali 2 text but the F5
score is at 03825 and the clustering is not good
Pali 2 Cluster analysis
 abhijjhita abhijjhtar covets function med one who itar itar tar
 T smallcapsismallcaps smallcapsvsmallcaps  A M ag fr in
 265 287
 n
7 Conclusion
In this thesis I have asked the question of whether unsupervised approaches to lan-
guage segmentation perform beer on short and dicult texts than supervised ap-
proaches by overcoming some of the diculties associated with supervised approaches
such as the need for enough and adequate11 training data the language-specicity of
the language model or the inexibility of trained language models when it comes to
spelling variation and abbreviations unless the training data also contained spelling
variation and abbreviations
I have given an overview over related work presenting supervised approaches that
have been used in monolingual language identication and the amelioration of such
approaches through unsupervised approaches such as clustering
Unfortunately the body of literature covering the topic of language segmentation
is sparse e work by Yin et al 2007 and the work by Seldin et al 2001 are closest
in topic to this thesis However Yin et al 2007 concern themselves with spoken
language with requires a dierent approach than dealing with wrien language As I
concentrated on wrien language their work was not conducive to this thesis
In contrast Seldin et al 2001 present a work that looks promising ey present
a system that nds language borders in a text with great accuracy using unsuper-
vised algorithms However they restrict their algorithm in such a way that switching
language models aer each word is disallowed us they are unable to detect single-
word inclusions and cannot handle situations where the language switches every word
as has been shown to occur in the test data used in section 4
Another major drawback of the approach is that it also needs longer fragments of
monolingual text and an overall longer text Hence their approach would not work
well on short texts if at all
Next I have presented the theoretical foundations of a supervised n-gram language
model approach and an unsupervised clustering approach Finally I have introduced a
weakly supervised n-gram language model inducing approach devised by myself All
of these approaches can be used for language segmentation In order to test how well
the dierent approaches perform on dierent text types I have performed experiments
I have rst compiled a small corpus
of texts ranging from longer texts with clearly separated languages to one-sentence
Twier messages containing foreign language inclusions I have also included a set
of dictionary entries from the Pali dictionary by the Pali Text Society Indeed these
entries contain a lot of dierent languages and abbreviations and unfortunately are
not consistently formaed
Section 4 presents the experiments made
I have then presented my implementations of the supervised and weakly super-
11e question of what is to be considered enough or adequate is another point of contention the
data always inuences the resulting models
vised approaches and the choice of the unsupervised clustering algorithms en I
have presented the results of their application to the data
It can be said that the supervised approach works reasonably well e drawbacks
are that the approach needs training data to train the models on e problems of the
training data and its inuence on the models have been raised more than once
e supervised approach failed for non-whitespace scripts e models would have
to be adapted for non-whitespace scripts introducing more complexity Also the
training and test texts would have to be split in meaningful ways introducing the
need for a vast array of language-specic text spliers should the approach work on
a wide range of languages
e unsupervised approach generally succeeded in separating languages by script
when dierent scripts were involved Other than that it seems that the chosen mor-
phological features or possibly morphological features in general are insucient for
the algorithm to separate languages eectively
e weakly supervised approach worked well on short texts and on dicult short
texts but less well on long texts while still outperforming the clustering approach
on long texts e approach consistently outperforms the clustering approach and
reaches scores in the vicinity of the scores achieved by the supervised approach even
surpassing the supervised approach in some cases ese results are promising but
more thorough investigations have to be undertaken
In conclusion it can be said that some unsupervised or weakly supervised ap-
proaches can perform beer on the task of language segmentation on dicult and
short texts e presented weakly supervised approach does not only outperform the
unsupervised clustering approach it also achieves scores comparable to the scores
achieved with the supervised approach
Future work could concentrate on the reduction of the number of generated clus-
ters ideally geing down to one cluster per language it would also be thinkable to
prevent overly frequent language model switching by taking a words context into
account Finally the parameters could conceivably be adapted automatically With
an increased interest in the area of multilingual text processing lately the emergence
and evolution of the texts themselves will inuence the direction of the work in that
direction
Il est venu le temps des cathdrales
le monde est entr
dans un nouveau millnaire
Lhomme a voulu monter vers les toiles
crire son histoire
dans le verre ou dans la pierre
 Gringoire
References
Abeel T de Peer Y V and Saeys Y 2009 Java-ML A Machine Learning Library
Journal of Machine Learning Research pages 931934
Achtert E Kriegel H Schubert E and Zimek A 2013
Interactive data mining
with 3D-parallel-coordinate-trees In Proceedings of the ACM SIGMOD International
Conference on Management of Data SIGMOD 2013 New York NY USA June 22-27
2013 pages 10091012
Alex B 2005 An unsupervised system for identifying English inclusions in German
text In Proceedings of the 43rd Annual Meeting of the Association for Computational
Linguistics ACL 2005 Student Research Workshop pages 133138 Association for
Computational Linguistics
Alex B 2006 Integrating language knowledge resources to extend the English inclu-
sion classier to a new language In Proceedings of the 5th International Conference
on Language Resources and Evaluation LREC European Language Resources Asso-
ciation
Alex B 2007 Automatic detection of English inclusions in mixed-lingual data with an
application to parsing PhD thesis University of Edinburgh
Alex B Dubey A and Keller F 2007 Using Foreign Inclusion Detection to Improve
Parsing Performance In EMNLP-CoNLL pages 151160
Alex B and Onysko A 2010 Zum Erkennen von Anglizismen im Deutschen der
Vergleich von einer automatisierten mit einer manuellen Erhebung In Scherer C
and Holler A editors Strategien der Integration und Isolation nicht-nativer Einheiten
und Strukturen pages 223239 de Gruyter
Begleiter R El-Yaniv R and Yona G 2004 On prediction using variable order
Markov models Journal of Articial Intelligence Research pages 385421
Biemann C 2006 Chinese whispers an ecient graph clustering algorithm and
its application to natural language processing problems In Proceedings of the rst
workshop on graph based methods for natural language processing pages 7380 As-
sociation for Computational Linguistics
Brants S Dipper S Hansen S Lezius W and Smith G 2002 e TIGER treebank
In Proceedings of the workshop on treebanks and linguistic theories volume 168
Brgisser P Clausen M and Shokrollahi M A 1997 Algebraic complexity theory
volume 315 Springer
Carter D 1994
Improving language models by clustering training sentences
Proceedings of the fourth conference on Applied natural language processing pages
5964 Association for Computational Linguistics
Cavnar W B and Trenkle J M 1994 N-gram-based text categorization
In Pro-
ceedings of SDAIR-94 3rd Annual Symposium on Document Analysis and Information
Retrieval pages 161175
Chen S F and Goodman J 1996 An empirical study of smoothing techniques for
language modeling In Proceedings of the 34th annual meeting on Association for Com-
putational Linguistics pages 310318 Association for Computational Linguistics
Dreyfuss E Goodfellow I and Baumstarck P 2007 Clustering Methods for Improv-
ing Language Models
Dubes R C 1987 How many clusters are best-an experiment Paern Recognition
206645663
Dunning T 1994 Statistical Identication of Language Computing Research Labo-
ratory New Mexico State University
Gale W and Sampson G 1995 Good-turing smoothing without tears Journal of
antitative Linguistics 23217237
Gao J Goodman J Miao J et al 2001 e use of clustering techniques for language
International Journal of Computational
modelingapplication to Asian languages
Linguistics and Chinese Language Processing 612760
Goldberg D 1991 What every computer scientist should know about oating-point
arithmetic ACM Computing Surveys CSUR 231548
Goodman J and Gao J 2000 Language model size reduction by pruning and clus-
tering In INTERSPEECH pages 110113
Goodman J T 2001 A bit of progress in language modeling Computer Speech and
Language 154403434
Grefenstee G 1995 Comparing two language identication schemes In Proceedings
of the 3rd International conference on Statistical Analysis of Textual Data JADT 1995
Grnwald P D 2007 e minimum description length principle MIT press
Guthrie D Allison B Liu W Guthrie L and Wilks Y 2006 A closer look at
skip-gram modelling In Proceedings of the 5th international Conference on Language
Resources and Evaluation LREC-2006 pages 14
Hall M Frank E Holmes G Pfahringer B Reutemann P and Wien I H 2009
e WEKA Data Mining Soware An Update SIGKDD Explorations 11
Jain A K Murty M N and Flynn P J 1999 Data clustering a review ACM
computing surveys CSUR 313264323
Jain N and Bhat R A 2014 Language Identication in Code-Switching Scenario In
Proceedings of the Conference on Empirical Methods on Natural Language Processing
pages 8793
Jurafsky D and Martin J H 2000 Speech and language processing An Introduction
to Natural Language Processing Computational Linguistics and Speech Recognition
Pearson Education India 2nd edition
Katz S 1987 Estimation of probabilities from sparse data for the language model
component of a speech recognizer Acoustics Speech and Signal Processing IEEE
Transactions on 353400401
King B and Abney S P 2013 Labeling the Languages of Words in Mixed-Language
Documents using Weakly Supervised Methods In Proceedings of the Conference of
the North American Chapter of the Association for Computational Linguistics  Human
Language Technologies pages 11101119
Liu H and Cong J 2013 Language clustering with word co-occurrence networks
based on parallel texts Chinese Science Bulletin 581011391144
Logan B et al 2000 Mel frequency cepstral coecients for music modeling
Proceedings of the 1st International Symposium on Music Information Retrieval ISMIR
Lui M Lau J H and Baldwin T 2014 Automatic detection and language identi-
cation of multilingual documents Transactions of the Association for Computational
Linguistics 22740
Manning C D Raghavan P and Schtze H 2008 Introduction to information re-
trieval volume 1 Cambridge University Press
Manning C D and Schtze H 1999 Foundations of statistical natural language pro-
cessing MIT press
Marsland S 2003 Novelty detection in learning systems Neural computing surveys
32157195
Mendizabal I Carandell J and Horowitz D 2014 TweetSafa Tweet language
identication TweetLID  SEPLN
Mikolov T Chen K Corrado G and Dean J 2013 Ecient estimation of word
In Proceedings of the International Conference on
representations in vector space
Learning Representations ICLR 2013
Ney H Essen U and Kneser R 1994 On structuring probabilistic dependences in
stochastic language modelling Computer Speech  Language 81138
Pelleg D and Moore A W 2000 X-means Extending K-means with Ecient Es-
timation of the Number of Clusters In Proceedings of the Seventeenth International
Conference on Machine Learning ICML 2000 pages 727734
Pereira F Tishby N and Lee L 1993 Distributional clustering of english words In
Proceedings of the 31st annual meeting on Association for Computational Linguistics
pages 183190 Association for Computational Linguistics
Porta J 2014 Twier Language Identication using Rational Kernels and its potential
application to Sociolinguistics TweetLID  SEPLN
Ravi S Vassilivitskii S and Rastogi V 2014 Parallel Algorithms for Unsupervised
Tagging Transactions of the Association for Computational Linguistics 2105118
Ron D Singer Y and Tishby N 1996 e power of amnesia Learning probabilistic
automata with variable memory length Machine learning 252-3117149
Schlkopf B Williamson R C Smola A J Shawe-Taylor J and Pla J C 1999
In Advances in Neural Information
Support vector method for novelty detection
Processing Systems NIPS volume 12 pages 582588
Seldin Y Bejerano G and Tishby N 2001 Unsupervised sequence segmentation
by a mixture of switching variable memory Markov sources In Proceedings of the
Seventeenth International Conference on Machine Learning ICML pages 513520
Solorio T Blair E Maharjan S Bethard S Diab M Gohneim M Hawwari A Al-
Ghamdi F Hirschberg J Chang A et al 2014 Overview for the First Shared Task
on Language Identication in Code-Switched Data In Proceedings of the Conference
on Empirical Methods on Natural Language Processing pages 6272
Taylor D 2015 Graphing the distribution of English leers towards the be-
httpwwwprooffreadercom201405
ginning middle or end of words
graphing-distribution-of-englishhtml
e Unicode Consortium 2014 e Unicode Standard httpunicodeorg
standardstandardhtml Online accessed 21-July-2015
Uszkoreit J and Brants T 2008 Distributed word clustering for large scale class-
based language modeling in machine translation In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguistics pages 755762
Wagner S and Wagner D 2007 Comparing clusterings an overview Universitt
Karlsruhe Fakultt fr Informatik Karlsruhe
Yamaguchi H and Tanaka-Ishii K 2012 Text segmentation by language using mini-
mum description length In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics pages 969978 Association for Computational Lin-
guistics
Yin B Ambikairajah E and Chen F 2007 Hierarchical language identication
based on automatic language clustering In INTERSPEECH pages 178181
Yuan L 2006 Language model based on word clustering In Proceedings of the 20th
Pacic Asia Conference on Language Information and Computation pages 394397
Zubiaga A San Vicente I Gamallo P Pichel J R Alegria I Aranberri N Ezeiza
A and Fresno V 2014 Overview of TweetLID Tweet language identication at
SEPLN 2014 TweetLID  SEPLN
8 Appendix
81 Development data
811 Latin script data
Karl Marx anses som en af de re klassiske sociologer Marx er epokegrende for den
historiske videnskab Og Marx spillede en vigtig rolle for den samtidige og eerfl-
gende arbejderbevgelse
1891 nach einer Tuberkuloseerkrankung Hopes ernete das Ehepaar ein mod-
ernes Lungensanatorium in Nordrach im Schwarzwald das sie bis 1893 gemeinsam
hrten 1895 wurde die Ehe geschieden
Sources
hpsdawikipediaorgwikiKarlMarx
hpsdewikipediaorgwikiHopeBridgesAdamsLehmann
812 Mixed script data
Capitalism is an economic system and a mode of production in which trade industries
and the means of production are largely or entirely privately owned Private rms and
proprietorships usually operate in order to generate prot but may operate as private
nonprot organizations
                        
                        
                          
                         
                      
    
hpsenwikipediaorgwikiCapitalism
hpsfawikipediaorgwiki
Sources
813 Twitter data
Twitter 1 Fallo ergo sum On being wrong
Source
Roland Hieber danielbohrer Fallo ergo sum On being wrong 26 July 2015
1647 Tweet
Twitter 2 Music for Airports  le piano en libre-accs dans laroport Charles-de-
Gaulles
Source
Yannick Rochat yrochat Music for Airports  le piano en libre-accs dans laroport
Charles-de-Gaulles 26 July 2015 1812 Tweet
814 Pali dictionary data
All entries have been taken from the Pali Text Societys Pali-English dictionary T W
Rhys Davids William Stede editors e Pali Text Societys PaliEnglish dictionary
Chipstead Pali Text Society 19215 8 parts 738 pp
Hambho Hambhoindeclhabho a particle expressing surprise or haughti-
ness JI184494See also ambhoPage 729
Ussada Ussadamost likely to ud  syadsee ussannathis word is beset with
dicultiesthe phrase sa-ussada is applied in all kinds of meaningsevidently the
result of an original application  meaning having become obliteratedsa is taken
as sapta sevenas well as sava beingussada as prominenceprotuberance
fulnessarrogancee meanings may be tabulated as follows1prominencecp
Skutsedhaused in characterisation of the Nirayasas projectingprominent
hellsussadaniray but see also below 4JI174IV3422 pallaka
vlcaturasswith four cornersV266 adjprominent A13 tej-
ussadehi ariyamaggadhammehior as below 4 2protuberancebumpswelling
JIV188also in phrase saussada having 7 protuberancesa qualication of the
Mahpurisa DIII151 vizon both handsfeetshouldersand on his back
 3rubbing inanointingointmentadjanointed with -in candan JIII
139IV601267Vv 537DhAI28VvA237 4a crowd adjfull
of -in phrase saussada crowded with human beingsDI87 cpDAI
245aneka-saa-samkiabut in same sense BSksapt-otsada Divy 620621Pv
IV18 of Niraya  full of beingsexpldby saehi ussanna uparpari nicita PvA
221 5qualicationcharacteristicmarkaributein catussada having the
four qualications of a good villageJIV309 vizplenty of peoplecorn
wood and water Ce phrase is evidently shaped aer DI87 under 4As
preponderant qualitycharacteristicwe nd ussada used at Vism103 cfAsl
267in combnslobhdosmohalobh etcquoted from theUssadakiana
and similarly at VvA19 in Dhammaplas denition of manussalobhdhi alobh
dhi sahitassa manassa ussannatya manussvizsa manussa-jtik tesu lobh
- dayo alobhdayo ca ussad 6metaphself-elevationarroganceconceit
haughtiness VinI3Sn515624 an  tah-ussada-abhvena SnA 467783
expldby Nd1 72 under formula saussadaieshowing 7 bad qualitiesvizrga
dosamoha etc855 See also ussdanaussdeti etcPage 157
82 Test data
821 Latin script data
English - German e German word Nabelschau means navel-gazing or staring
at your navel But in this case it doesnt refer to anyone elses belly buon  just your
Source
Glass Nicole 2015 German Missions in the United States - Word of the Week
Germanyinfo
English - Fren doux mou  both translate as so in English although their mean-
ing is very dierent Doux is the opposite of rough or coarse rugueux while mou
is the opposite of hard Doux can also mean sweet but almost only for wines oth-
erwise sucr is used
Source
Maciamo 2015 French words and nuances that dont exist in English Eupedia
English - Transliterated Greek e Greek language distinguishes at least four dif-
ferent ways as to how the word love is used Ancient Greek has four distinct words for
love agpe ros phila and storg However as with other languages it has been his-
torically dicult to separate the meanings of these words when used outside of their
respective contexts Nonetheless the senses in which these words were generally used
are as follows
Source
hpsenwikipediaorgwikiGreekwordsforlove
Italian - German Milano ne custodisce lesempio pi struggente quel Cenacolo
che il vinciano aresc con amore cura e rivoluzionaria psicologia il Giuda non vie-
ne privato dellaureola ma si condanna da solo con la consapevolezza del peccato
cominci subito ad autodistruggersi con un cancro che solo un lunghissimo restauro
ha di recente arginato
Kaum eine Woche vergeht in der es keine neue Studie Umfrage oder Warnung
zum ema Fachkremangel in Deutschland gibt
Certo lo faceva per denire le idee ma anche perch consapevole che le intuizioni
sono periture che la vita stessa va caurata in qualche modo
Dabei mehren sich letzter Zeit auch Stimmen die Entwarnung geben So kam
jngst eine Studie des Stierverbands r die Deutsche Wissenscha zu dem Ergebnis
dass ein allgemeiner Fachkremangel in den MINT-Berufen eher nicht mehr drohe
Come anche i riccioli del Baista richiamano il movimento delle acque moto che
poi Leonardo studier pi approfonditamente a Venezia nelle ricerche sui bacini in
chiave di difesa anti-Turchi E si vada alla bellissima Annunciazione con un occhio
aento alle ali dellangelo la delicatezza delle punte allins che cosa sono se non
il barbaglio di un sogno che lo ossessionava da anni ovvero quello di volare
Ist das seit Jahren angemahnte Szenario vom drohenden Fachkremangel bei In-
genieuren und Naturwissenschalern also nur ein Mythos
Source
Stalinski Sandra 2015 Ingenieure Mythos Fachkremangel tagesschaude
Scorranese Roberta 2015 Nelle grandi opere il racconto soerto della natura mor-
tale Archiviostoricocorriereit
German - Finnish - Turkish Der Sommer ist die wrmste der vier Jahreszeiten in der
gemigten und arktischen Klimazone Je nachdem ob er gerade auf der Nord- oder
Sdhalbkugel herrscht spricht man vom Nord- oder Sdsommer Der Nordsommer
ndet gleichzeitig mit dem Sdwinter sta
Kes eli suvi on vuodenaika kevn ja syksyn vliss Kes on vuodenajoista lm-
pimin koska maapallo on silloin kallistunut niin e aurinko steilee maan pinnalle
jyrkemmss kulmassa kuin muina vuodenaikoina Pohjoisella pallonpuoliskolla kes-
kuukausiksi lasketaan tavallisesti kes- hein- ja elokuu etelisell pallonpuoliskolla
joulu- tammi- ja helmikuu
Yaz en scak mevsimdir Kuzey Yarm Krede en uzun gnler yazda gerekleir
Dnya sy depo eii iin en scak gnler genellikle yaklak iki ay sonra ortaya
kar Scak gnler Kuzey Yarm Krede 21 Haziran ile 22 Eyll arasnda Gney Yarm
Krede ise 22 Aralk ile 21 Mart arasndadr
Source
hpswikipediaorgwikiKes
hpsdewikipediaorgwikiSommer
hpstrwikipediaorgwikiYaz
822 Mixed script data
Greek - Russian         
         -
         
      15    
          -
         -
        
       
         
         
        
         
  
Source
hpselwikipediaorgwiki
hpsruwikipediaorgwiki
English - Greek - Transliterated Greek Agpe  agp means love esp
brotherly love charity the love of God for man and of man for God Agape is used
in the biblical passage known as the love chapter 1 Corinthians 13 and is described
there and throughout the New Testament as brotherly love aection good will love
and benevolence Whether the love given is returned or not the person continues to
love even without any self-benet Agape is also used in ancient texts to denote feel-
ings for ones children and the feelings for a spouse and it was also used to refer to
a love feast It can also be described as the feeling of being content or holding one in
high regard Agape is used by Christians to express the unconditional love of God for
his children is type of love was further explained by omas Aquinas as to will
the good of another
ros  rs means love mostly of the sexual passion e Modern Greek
word erotas means intimate love It can also apply to dating relationships as well as
marriage Plato rened his own denition Although eros is initially felt for a person
with contemplation it becomes an appreciation of the beauty within that person or
even becomes appreciation of beauty itself Plato does not talk of physical araction as
a necessary part of love hence the use of the word platonic to mean without physical
araction
In the Symposium the most famous ancient work on the subject Plato has Socrates
argue that eros helps the soul recall knowledge of beauty and contributes to an under-
standing of spiritual truth the ideal Form of youthful beauty that leads us humans
to feel erotic desire  thus suggesting that even that sensually based love aspires to
the non-corporeal spiritual plane of existence that is nding its truth just like nd-
ing any truth leads to transcendence Lovers and philosophers are all inspired to seek
truth through the means of eros
Source
hpsenwikipediaorgwikiGreekwordsforlove
English - Spanish - Arabic A black ribbon is a symbol of remembrance or mourn-
ing Wearing or displaying a black ribbon has been used for POWMIA remembrance
mourning tragedies or as a political statement
El crespn negro o lazo negro es un smbolo utilizado por personas estados so-
ciedades y organizaciones representando un sentimiento poltico-social en seal de
                      
            
Source
hpseswikipediaorgtitleLazonegro
hpsenwikipediaorgwikiBlackribbon
hpsarwikipediaorgwiki
English - Chinese - Pinyin e Chinese word for crisis simplied Chinese 
 traditional Chinese  pinyin wij is frequently invoked in Western
motivational speaking because the word is composed of two Chinese characters that
can represent danger and opportunity Some linguists have criticized this usage
because the component pronounced j simplied Chinese  traditional Chinese
 has other meanings besides opportunity In Chinese tradition certain numbers
are believed by some to be auspicious  or inauspicious  based on the
Chinese word that the number name sounds similar to e numbers 0 6 8 and 9 are
believed to have auspicious meanings because their names sound similar to words
that have positive meanings
Source
hpsenwikipediaorgwindexphptitleChinesewordforcrisis
Ukrainian - Russian       
        
      913 
         
        

Source
hpsukwikipediaorgwiki
Surgut-safariru 2015  - Safari Tour
823 Twitter data
Tweet 1 Greek  English      Internet of ings 
 BUSINESS IT EXCELLENCE
Source
GaloTyri      Internet of ings  
BUSINESS IT EXCELLENCE 19 June 2015 1206 Tweet
Tweet 2 English  Fren Demain dhiha6 Keynote 18h dhiparis e collective
dynamics of science-publish or perish is it all that counts par David chavalarias
Source
Claudine Moulin ClaudineMoulin Demain dhiha6 Keynote 18h dhiparis e
collective dynamics of science-publish or perish is it all that counts par David
chavalarias 10 June 2015 1735 Tweet
Tweet 3 English  Fren Food and breuvages in Edmonton are ready to go just
waiting for the fans FWWC2015 bilingualism
Source
HBS HBSTweets Food and breuvages in Edmonton are ready to go just waiting
for the fans FWWC2015 bilingualism 6 June 2015 2329 Tweet
Tweet 4 English  Polish my dad comes back from poland with two crates of
strawberries ubrwka and adidas jackets omg
Source
katarzyne wifeyriddim my dad comes back from poland with two crates of
strawberries ubrwka and adidas jackets omg 8 June 2015 0849 Tweet
Tweet 5 Transliterated Amharic  English Buna dabo naw coee is our bread
Source
eCodeswitcher Buna dabo naw coee is our bread 9 June 2015 0212 Tweet
824 Pali dictionary data
All entries have been taken from the Pali Text Societys Pali-English dictionary T W
Rhys Davids William Stede editors e Pali Text Societys PaliEnglish dictionary
Chipstead Pali Text Society 19215 8 parts 738 pp
nt Vedic abhra nt  later Sk abhra m dark cloud Idg m bhro cp Gr
atafronnsat scum froth Lat imber rain also Sk ambha water Gr
atombrosat rain Oir ambu water A dense  dark cloud a cloudy mass A
smallcapsiismallcaps 53  Vin smallcapsiismallcaps 295  Miln 273 in
list of to things that obscure moon  sunshine viz babbha mahikb mahiy
A bdh- marajob megho Miln bRhub  is list is referred to at SnA
487  VvA 134 S smallcapsismallcaps 101 sama pabbata a mountain like a
thundercloud J smallcapsvismallcaps 581 abbha rajo acchdesi Pv
smallcapsivsmallcaps 3 superscript9superscript nl  nlamegha PvA
251 As f babbhb at Dhs 617  DhsA 317 used in sense of adj dull DhsA
expl superscriptssuperscript by valhaka perhaps also in babbhmaab
 br bkab the point or summit of a stormcloud  1 1064 J
smallcapsvismallcaps 249 250 Vv 1 superscript1superscript 
valhakasikhara VvA 12 bghanab a mass of clouds a thick cloud It 64 Sn
348 cp SnA 348 bpaalab a mass of clouds DhsA 239 bmuab free
from clouds Sn 687 also as abbhmua Dh 382 bsavilpab thundering S
smallcapsivsmallcaps 289
n ag fr abhijjhita in med function one who covets M
abhijjhitar
smallcapsismallcaps 287 T abhijjhtar v l itar  A
smallcapsvsmallcaps 265 T itar v l tar
ajja Ajja Ajj advVedic adya  adya  dya being base of demonstr
pron see a3and dy an old Loc of dyaus see divathus on this day
to-daynow Sn75153158970998Dh326JI279III425 read bahuta
ajjnot with KernToev s v as foodPvI117  idni PvA59PvA6
23Mhvs 1564 - Freq in phrase ajjatagge  ajjato  aggeor ajja-tagge
see agga3from this day onwardhenceforth VinI18DI85DAI235
kla advthis morning JVI180divasa the present day Mhvs 3223
Page 10
ghan Ghanfabstrfrghatighan qvPug19Cp
pariPage 253
pacati PacatiVedpacatiIdgpeqAvpac-Obulgpeka to fryroast
Lithkep bakeGrpssw cookppwn ripe to cookboilroast VinIV264
gtorment in purgatory trsand intrsNiraye pacitv aer roasting in NS
II225PvA1014 pprpacanto tormentingGenpacato Caus
pcayatoDI52 expld at DAI159where read pacato for paccatoby pare
daena pentassa pppakka qv- Causpacpeti  pceti qv
 Passpaccati to be roasted or tormented qvPage 382
83 Results
831 N-Gram Language Models
For the n-gram language model approach the identied language is indicated in
parentheses e language abbreviations are
Abbreviation
Language
English
Spanish
Finnish
Italian
Russian
Ukrainian
Turkish
Transliterated Amharic
Transliterated Greek
Chinese
Data Latin script German  English
 EN own belly refer buon But it or your at in staring anyone doesnt
elses word this
 FI 
 FR case just means navel
 TrAM e
 TrEL to German
 other Nabelschau navel-gazing
Data Latin script German  Finnish  Turkish
 DE ob oder Sommer und Nord- arktischen der Der dem gemigten mit
er Sdsommer spricht Jahreszeiten Sdwinter herrscht wrmste vom die
sta nachdem auf
 EN ist Nordsommer Mart in
 ES en depo
 FI joulu- kevn suvi on eli vuodenajoista syksyn koska kes- kuin Po-
hjoisella man helmikuu tammi- lmpimin hein- niin maapallo maan pin-
nalle Kes steilee tavallisesti vuodenaika kallistunut lasketaan muina eii
jyrkemmss elokuu vliss e etelisell silloin ja kulmassa
 FR vier Je
 PL aurinko
 RU 22 21
 TR yaklak ortaya genellikle Eyll Scak kar Yaz sonra arasnda Kuzey
Gney Aralk gerade sy gerekleir Krede gnler iin ndet mevsimdir
arasndadr Haziran iki yazda uzun ise ay scak ile Yarm Dnya
 TrAM Der
 other Klimazone gleichzeitigkeskuukausiksi vuodenaikoina pallonpuolis-
kollaSdhalbkugel
Data Latin script English  French
 EL coarse
 EN but both for while wines almost sweet of although only is rough
used or as meaning the in translate hard their English also dierent
 ES can
 FI mean
 FR opposite Doux doux sucr 
 RU so
 TrEL mou
 other otherwise rugueux
Data Latin script English  Transliterated Greek
 EN for meanings least used been distinct love of were are when agpe
these how and Greek word used outside ways dierent other follows
words respective generally However is with it at as historically the in
which their
 ES has separate
 FR language senses Ancient languages dicult four
 IT contexts
 TrAM ros e love
 TrEL to storg phila
 other Nonetheless distinguishes
Data Latin script Italian  German
 DE drohe geben allgemeiner Studie jngst r Ergebnis keine kam dro-
henden oder und letzter neue Mythos Deutschland Ist sich der vergeht
studier Dabei Studie den dem auch Entwarnung dass nur eher nicht gibt
Umfrage Woche eine Kaum Jahren bei mehren Stimmen Deutsche das zum
mehr angemahnte ein Zeit ein So vom zu die seit Warnung Wissenscha
 EL aresc
 EN moto aento a in ad also
 ES custodisce cura subito Certo Giuda lo del difesa con denire restauro
se modo la arginato recente vada movimento Leonardo Szenario quel
cominci
 FI va si Baista ema
 FR lesempio non des acque perch un es le sui condanna
 IT solo faceva caurata chiave peccato periture il delicatezza cancro pri-
vato bellissima anni bacini ovvero delle sogno di barbaglio ma qualche e
amore ricerche Come per richiamano ne intuizioni punte occhio struggente
nelle vita riccioli solo che volare sono alla alle anche Cenacolo quello
cosa ali viene il psicologia vinciano Venezia
 PL i
 TR ha pi da
 TrAM Milano E
 TrEL poi idee stessa
 other MINT-Berufen Fachkremangel dellangelo consapevole anti-Turchi
Annunciazione lunghissimo consapevolezza ossessionava dellaureola appro-
fonditamente autodistruggersi rivoluzionaria Stierverbands allins Natur-
wissenschalern Ingenieuren
Data Mixed script Greek  Russian
 EL          
        
15         
    
 RU       
      
        -
        -
       
        -
       
 TrAM 
 UK        
 other    -
   
Data Mixed script English  Greek
 DE Symposium Modern being felt
 EL Form
 EN sensually platonic for holding existence rened its explained arac-
tion of even are spiritual given refer Agape beauty or araction like
without not further will own love knowledge will ones most use ex-
press is another e leads truth suggesting dating relationships in-
spired love mostly hence denition regard appreciation a ideal us helps
seek Agpe plane recall feeling within returned chapter based described
apply physical Although good by used love God children his any char-
ity Socrates be work throughout and that Greek even word agp love
known biblical feelings does famous In subject becomes one understand-
ing children love through beauty well It was initially feast nding itself
13 all without feel with is it thus New as the brotherly in is an there
God youthful necessary high Lovers also Whether
 ES person Aquinas esp continues has omas truth can erotic sexual
 FI on  man mean
 FR  spouse not ancient marriage soul person content Christians
Testament ros just part type passage means humans passion aspires con-
templation contributes argue aection
 IT texts 1 intimate Plato to
 RU 
 TR talk
 TrAM rs love
 TrEL erotas denote eros to eros
 other non-corporeal Corinthians self-benet benevolence unconditional
philosophers transcendence
Data Mixed script English  Spanish  Arabic
 AR


  
   





 
 



 

 

 




 EN for used been displaying of ribbon black or mourning statement
tragedies is political a Wearing as mourning
 ES por has crespn sociedades personas sentimiento representando esta-
dos de El seal lazo smbolo en utilizado y
 FR remembrance remembrance un es
 IT negro duelo POWMIA
 TrAM
 TrEL symbol o
 other poltico-social organizaciones
Data Mixed script English  Chinese
 DE  Chinese Western
 EL 
 EN Some for meanings by of are 8 positive speaking be composed or
meanings tradition number and that sound linguists word some this other
In have invoked criticized 6 because e believed words numbers sounds
frequently is pronounced besides traditional the in represent two motiva-
tional usage their based
 ES  has  can Chinese crisis similar
 FI on
 FR  component danger characters  certain j
 PL pinyin
 RU 0 9 wij
 TrEL to to names name
 other inauspicious opportunity simplied auspicious
Data Mixed script Ukrainian  Russian
 RU         9
13        

 TrAM 
 UK        -
       
      
 other  
Data Pali abbha
 AR  134 289
 DE Miln imber dark Miln
 EL  abbha
 EN water mountain of free used or like referred also A is cloudy
clouds later a froth 1 summit thundering by mass Pv Oir obscure scum
that water thick As from It is at as the in clouds things also
 ES dense f sense expl rajo
 FI 239 rain Lat Vin perhaps SnA
 FR cloud Dh adj point cloud Dhs A rain VvA DhsA list
 IT dark  ambha 3 1 317 J sunshine cp abhra Vedic megho
 PL 487  S 295 br moon 249
 RU 348 53
 TR viz ambu Vv
 TrAM 687 PvA sama 101 nl cp 64 nt 581 m Sn 1064
 TrEL  Gr Sk Idg to pabbata nt
 UK 12 273 617 348 250 251 382
 other b savilpa b b mua b smallcaps vi smallcaps
mahiy smallcaps iv smallcaps cloud b Rhu b b abbh
b b abbha superscript 9 superscript marajo b abbhmua
valhaka smallcaps i smallcaps b abbhmaa b valhakasikhara
superscript s superscript smallcaps ii smallcaps b dh- storm
cloud b ka b thundercloud atafrosat bpaalab
atombrosat nlamegha superscript1superscript m bhro dull
acchdesi mahikb b ghana b
Data Pali abhijjhitar
 DE v
 EN A one in who covets med function
 IT ag M fr
 PL 287 
 RU 265
 TrAM l n
 TrEL T
 other smallcaps v smallcaps abhijjhtar abhijjhita tar smallcaps
i smallcaps itar itar
Data Pali ajja
 DE see v being Ajj
 EN of or and not present Freq day this on from adya with as the
morning in day an
 ES bahuta
 FI 3223 ajjato
 FR Loc dyaus 1564 dy pron
 IT Vedic Mhvs  divasa
 PL   demonstr s
 RU III425 agge
 TR old adya 10 idni
 TrAM -
 TrEL phrase base
 UK a3
 other onwardhenceforth ajj DAI235 adv JI279 DI85
ajja-taggesee Sn75153158970998 JVI180 PvA623 kla
divathus PvA59 agga3 KernToev PvI117 Dh326 ajjatagge
read Page VinI18 dya Ajja to-daynow food
Data Pali ghan
 ES 253
 other abstrfrghatighan Pug19CppariPage qv
Ghanf
Data Pali pacati
 EL 382
 EN for aer roasting read roasted be or at tormented in
 FR pare DI52
 IT  pacato purgatory
 TrAM pceti ripe
 TrEL to daena
 other bakeGrpsswCauspcayatoqvPage DAI159where
Causpacpeti intrsNiraye pacitv Passpaccatitrsand tormenting
Genpacato pentassa gtorment cookppwn PacatiVedpacati
IdgpeqAvpac- paccatoby pprpacanto cookboilroast fry
roastLithkep qv expld VinIV264 Obulgpeka pp
pakka qv- NSII225PvA1014
Data Twier 1 GreekEnglish
 DE Internet
 EL      
 EN of IT ings
 ES BUSINESS
 TrAM 
 other EXCELLENCE
Data Twier 2 FrenchEnglish
 EN David e is it perish or collective Demain counts that of dynam-
ics all
 FI 18h
 FR par Keynote
 other dhiha6 dhiparis science-publish
Data Twier 3 FrenchEnglish
 EN for Food waiting the in ready and are
 ES go
 FI Edmonton
 FR just breuvages fans
 TrEL to
 other bilingualism FWWC2015
Data Twier 4 EnglishPolish
 EN with back from comes crates and poland two of jackets
 ES dad adidas
 TrAM my
 TrEL omg
 other ubrwka strawberries
Data Twier 5 Transliterated AmharicEnglish
 EN is bread
 FR our
 IT coee
 PL naw
 TrAM Buna dabo
832 Textcat
For Textcat the identied language is indicated in parentheses As Textcat returns
unknown for many words I merely indicate the non-unknown categories to save
space and write rest to indicate that all other words of the text have been classied as
unknown e language abbreviations are
Abbreviation
Language
English
Spanish
Finnish
Hungarian
Indonesian
Italian
Lithuanian
Latvian
Portuguese
Russian
Chinese
Data Latin script German  English
 HU navel-gazing
 ZH Nabelschau
 unknown rest
Data Latin script German  Finnish  Turkish
 DA Sdsommer genellikle
 DE Jahreszeiten arktischen
 FI vuodenajoista kallistunut tavallisesti
 ZH gemigten Klimazone Sdhalbkugel Nordsommer gleichzeitig vuoden-
aika jyrkemmss vuodenaikoina Pohjoisella pallonpuoliskolla keskuukausik-
si etelisell mevsimdir gerekleir arasndadr
 unknown rest
Data Latin script English  French
 HU dierent
 ZH rugueuxotherwise
 unknown rest
Data Latin script English  Transliterated Greek
 EN historically respective
 LT languages
 ZH distinguishes Nonetheless
 unknown rest
Data Latin script Italian  German
 DE allgemeiner angemahnte
 ES delicatezza
 HU bellissima
 IT dellaureola consapevole richiamano anti-Turchi ossessionava
 NL Ingenieuren
 PT approfonditamente
 ZH custodisce struggente rivoluzionaria psicologia consapevolezza auto-
distruggersi lunghissimo Fachkremangel Deutschland intuizioni Entwar-
nung Stierverbands Wissenscha MINT-Berufen Annunciazione dellan-
gelo Naturwissenschalern
 unknown rest
Data Mixed script Greek  Russian
 EL  
 RU     
    
   
 TH 
 ZH    

 unknown rest
Data Mixed script English  Greek
 DA denition understanding
 EN aection unconditional suggesting
 FR relationships contemplation appreciation araction araction transcen-
dence
 HU benevolence self-benet
 IT non-corporeal
 PT contributes
 ZH Corinthians throughout Christians Symposium existence philosophers
 unknown rest
Data Mixed script English  Spanish  Arabic
 ES sociedades organizaciones sentimiento poltico-social
 FR remembrance remembrance statement
 ID displaying
 PT representando
 unknown rest
Data Mixed script English  Chinese
 EN traditional motivational pronounced tradition
 FR characters
 ZH simplied frequently opportunity criticized auspicious inauspicious
 unknown rest
Data Mixed script Ukrainian  Russian
 RU   
 TH 
 ZH    
 unknown rest
Data Pali abbha
 DA stormcloud thundering
 HU marajob nlamegha valhakasikhara
 ZH
at afroat at ombros at smallcaps ii smallcaps mahikb
b Rhu b smallcaps i smallcaps thundercloud smallcaps vi
smallcaps acchdesi smallcaps iv smallcaps superscript 9 su-
perscript b abbh b superscript s superscript valhaka b
abbhmaa b b ka b superscript 1 superscript b ghana
b b paala b b mua b abbhmua b savilpa b
 unknown rest
Data Pali abhijjhitar
 ZH abhijjhita smallcaps i smallcaps abhijjhtar smallcaps v
smallcaps
 unknown rest
Data Pali ajja
 ZH divathus to-daynow Sn75153158970998 KernToev
ajja-taggesee onwardhenceforth
 unknown rest
Data Pali ghan
 ZH Ghanf abstrfrghatihan Pug19CppariPage
 unknown rest
Data Pali pacati
 ZH gtorment PacatiVedpacatiIdgpeqAvpac- Obulgpeka
fryroastLithkep bakeGrpssw cookppwn cookboilroast Vin
IV264 intrsNiraye NSII225PvA1014 pprpacanto
tormentingGenpacatoCauspcayato DAI159where paccatoby
pentassa qv- Causpacpeti Passpaccati qvPage
 unknown rest
Data Twier 1 GreekEnglish
 ZH  EXCELLENCE
 unknown rest
Data Twier 2 FrenchEnglish
 IT collective
 ZH science-publish
 unknown rest
Data Twier 3 FrenchEnglish
 ZH bilingualism
 unknown rest
Data Twier 4 EnglishPolish
 LV strawberries
 unknown rest
Data Twier 5 Transliterated AmharicEnglish
 unknown rest
833 Clustering
Clustering the dierent data sets produced the following clusters e second run
uses the clusters from the rst run and possibly subdivides each cluster into two or
more clusters
Data Latin script German  English
First run
 navel-gazing doesnt elses
 staring But German Nabelschau anyone belly buon case just means
navel own refer this word your
 at in it or to
  e
Second run
 doesnt elses
 navel-gazing
 staring But German Nabelschau belly case means navel refer this
 anyone buon just own word your
 it or to
 at in
  e
Data Latin script German  Finnish  Turkish
First run
 Dnya Gney Krede Sdhalbkugel Sdsommer Sdwinter Scak arasnda
gemigten gnler iin keskuukausiksi lmpimin steilee scak wrmste
kar Der
 Aralk Eyll Kes Yarm arasndadr etelisell eii e gerekleir hein-
 jyrkemmss kes- kevn vliss yaklak sy
 21 22
 Der Haziran Jahreszeiten Je Klimazone Kuzey Mart Nord- Nordsommer
Pohjoisella Sommer Yaz arktischen auf aurinko ay dem depo der die eli
elokuu en er ndet genellikle gerade gleichzeitig helmikuu herrscht iki
ile in ise ist ja joulu- kallistunut koska kuin kulmassa lasketaan maan
maapallo man mevsimdir mit muina nachdem niin ob oder on ortaya pal-
lonpuoliskolla pinnalle silloin sonra spricht sta suvi syksyn tammi- taval-
lisesti und uzun vier vom vuodenaika vuodenaikoina vuodenajoista yazda
Second run
 Sdhalbkugel Sdsommer Sdwinter arasnda gemigten keskuukausiksi
lmpimin steilee wrmste
 Dnya Gney Krede Scak gnler iin scak kar Der
 arasndadr etelisell eii e gerekleir hein- jyrkemmss kes-
kevn vliss yaklak sy
 Aralk Eyll Yarm
 Kes
 Der Haziran Jahreszeiten Klimazone Kuzey Mart Nord- Nordsommer Po-
hjoisella Sommer Yaz
 arktischen auf aurinko dem depo der die eli elokuu ndet genellikle gerade
gleichzeitig helmikuu herrscht iki ile ise ist joulu- kallistunut koska kuin
kulmassa lasketaan maan maapallo man mevsimdir mit muina nachdem
niin oder ortaya pallonpuoliskolla pinnalle silloin sonra spricht sta suvi
syksyn tammi- tavallisesti und uzun vier vom vuodenaika vuodenaikoina
vuodenajoista yazda
 Je ay en er in ja ob on
Data Latin script English  French
First run
 coarse hard rough so otherwise rugueux Doux English almost
also although both but can dierent doux for mean meaning mou only
opposite sucr sweet the their translate used very while wines
 is or
 as in of
Second run
 Doux English
 coarse otherwise rugueux almost although dierent meaning opposite
translate
 hard rough so also both but can doux for mean mou only sucr
sweet the their used very while wines
Data Latin script English  Transliterated Greek
First run
 e
 agpe phila storg ros
 Ancient However Nonetheless contexts dierent dicult distinct distin-
guishes follows generally historically language languages meanings outside
respective senses separate which words
 Greek and are as at been for four has how in is it least love love of
other the their these to used used ways were when with word
Second run
 e
 phila storg
 agpe ros
 Ancient However Nonetheless contexts dierent dicult distinct distin-
guishes follows generally historically meanings respective
 words
 language languages outside senses separate which
 and are as at been for four has how in is it least love love of other the
their these to used used ways were when with word
 Greek
Data Latin script German  Italian
First run
 il E So a ad da di e es ha i il in la le lo ma ne se si un va zu
 ein  Annunciazione Baista Cenacolo Certo Come Dabei Deutsche Deutsch-
land Entwarnung Ergebnis Giuda Ingenieuren Ist Jahren Kaum Leonardo
MINT-Berufen Mythos Naturwissenschalern Stierverbands Stimmen Stu-
die Studie Szenario ema Umfrage Venezia Warnung Wissenscha Woche
Zeit acque ali alla alle allgemeiner also amore anche angemahnte anni
anti-Turchi approfonditamente arginato aento auch autodistruggersi baci-
ni barbaglio bei bellissima cancro caurata che chiave con condanna consa-
pevole consapevolezza cosa cura custodisce das dass denire del delicatezza
delle dem den der des die difesa drohe drohenden eher ein eine faceva
geben gibt idee intuizioni kam keine letzter lunghissimo mehr mehren
modo moto movimento nelle neue nicht non nur occhio oder ossessiona-
va ovvero peccato per periture poi privato psicologia punte qualche quel
quello recente restauro riccioli ricerche richiamano rivoluzionaria seit sich
sogno solo solo sono stessa struggente subito sui und vada vergeht viene
vinciano vita volare vom zum
 allins dellangelo dellaureola lesempio Milano
 Fachkremangel aresc cominci r jngst perch pi studier
Second run
 a e i
 il ad da di es ha il in la le lo ma ne se si un va zu
 Annunciazione Baista Cenacolo Certo Come Dabei Deutsche Deutschland
Entwarnung Ergebnis Giuda Ingenieuren Ist Jahren Kaum Leonardo MINT-
Berufen Mythos Naturwissenschalern Stierverbands Stimmen Studie Stu-
die Szenario ema Umfrage Venezia Warnung Wissenscha Woche Zeit
 ein acque ali alla alle allgemeiner also amore anche angemahnte an-
ni anti-Turchi approfonditamente arginato aento auch autodistruggersi
bacini barbaglio bei bellissima cancro caurata che chiave con condanna
consapevole consapevolezza cosa cura custodisce das dass denire del de-
licatezza delle dem den der des die difesa drohe drohenden eher ein
lunghissimo
mehr mehren modo moto movimento nelle neue nicht non nur oc-
chio oder ossessionava ovvero peccato per periture poi privato psicologia
idee intuizioni kam keine letzter
faceva geben gibt
punte qualche quel quello recente restauro riccioli ricerche richiamano ri-
voluzionaria seit sich sogno solo solo sono stessa struggente subito sui
und vada vergeht viene vinciano vita volare vom zum
 allins dellangelo dellaureola lesempio Milano
 Fachkremangel
 aresc cominci jngst perch studier
 r
 pi
Data Mixed script Greek  Russian
First run
 15  
        
         -
      
         -
        -
        
         
      
      
     -
      -
        
    
    
Second run
 15
 
    
   
  
        -
     
         -
       
        
   
      
     -
     -
     
   
           -
        
     
Data Mixed script English  Greek
First run
 intimate without Although Aquinas Christians Corinthians Socrates Sym-
posium Testament Whether aection ancient another appreciation aspires
araction araction becomes benevolence biblical brotherly chapter char-
ity children children contemplation content continues contributes deni-
tion described existence explained express feeling feelings nding further
holding initially inspired knowledge marriage necessary non-corporeal pas-
sage passion philosophers physical platonic rened relationships returned
self-benet sensually spiritual subject suggesting through throughout tran-
scendence unconditional understanding without youthful
   Agpe agp ros rs 
 Form erotas love love love even Agape Greek Lovers Modern Plato
is omas also apply argue based beauty beauty being dating denote
desire does eros eros erotic even famous feast feel felt given good helps
hence high humans ideal itself just known leads like love love love mean
means most mostly ones part person person plane recall refer regard seek
sexual soul spouse talk texts that there thus truth truth type used well
will will with within word work
 to 1 13 God God In It New e a all an and any are as be by can esp
for has his in is is it its man not not of on one or own the to us use
Second run
 aection ancient another aspires becomes biblical chapter charity chil-
dren children content denition feeling feelings nding holding marriage
necessary passage passion platonic rened returned subject through with-
 Although Aquinas Christians Corinthians Socrates Symposium Testament
Whether
 intimate appreciation araction araction benevolence brotherly contem-
plation continues contributes described existence explained express further
initially inspired knowledge non-corporeal philosophers physical relation-
ships self-benet sensually spiritual suggesting throughout transcendence
unconditional understanding youthful
 Agpe agp ros rs
  
 
 erotas beauty beauty dating denote desire erotic famous humans itself
mostly person person recall regard sexual spouse within
 Form Agape Greek Lovers Modern Plato is omas based being feast
hence ideal leads means plane refer there
 apply felt helps high just known most part talk texts that thus truth truth
type well will will with word work
 love love love even also argue does eros eros even feel given good
like love love love mean ones seek soul used
 1 13 In It
 to a an as be by in is is it of on or to us
 God God New e all and any esp its own the
 are can for has his man not not one use was
Data Mixed script English  Spanish  Arabic
First run
 El POWMIA Wearing a as been black de displaying duelo en es estados
for has is lazo mourning mourning negro o of or organizaciones personas
political por remembrance remembrance representando ribbon sentimiento
sociedades statement symbol tragedies un used utilizado y
 crespn poltico-social seal smbolo
                
                
Second run
 a o y
 El as de en es is of or un
 Wearing been black displaying duelo estados for has lazo mourning mourn-
ing negro organizaciones personas political por remembrance remembrance
representando ribbon sentimiento sociedades statement symbol tragedies
used utilizado
 POWMIA
 poltico-social smbolo
 crespn seal
                  
         
    
Data Mixed script English  Chinese
First run
 crisis danger opportunity simplied Chinese Chinese Western aus-
picious because believed besides certain characters component composed
criticized frequently inauspicious invoked linguists meanings meanings mo-
tivational number numbers pinyin positive pronounced represent similar
sounds speaking tradition traditional wij
      
 0 6 8 9
 In Some e and are based be by can for has have in is j name names
of on or other some sound that the their this to to two usage word words
Second run
 Chinese Chinese
 Western
 crisis danger opportunity simplied auspicious because believed be-
sides certain characters component composed criticized frequently inaus-
picious invoked linguists meanings meanings motivational number num-
bers pinyin positive pronounced represent similar sounds speaking tradi-
tion traditional wij
  
  
  
 6 8 9
 Some e and are based can for has have name names other some sound
that the their this two usage word words
 In be by in is of on or to to
 j
Data Mixed script Ukrainian  Russian
First run
 913
       -
    
        -
        
    
             
Second run
 913
     
      
       
   
      

   
   
     
  
Data Pali abbha
First run
 also cp dense megho used sama 1 1 101 1064 12 134 239 249
250 251 273 289 295 3 317 348 348 382 487 53 581 617 64 687 at
afros at at ombros at smallcaps i smallcaps smallcaps
ii smallcaps smallcaps iv smallcaps smallcaps vi smallcaps
superscript 1 superscript superscript 9 superscript superscript
s superscript A A As Dh Dhs DhsA Gr Idg It J Lat Miln Miln
Oir Pv PvA S Sk Sn SnA  is Vin Vv VvA Vedic a abhra adj also
ambha ambu as at by cloud cloud cloud clouds clouds cloudy cp dark
expl f free from froth imber in is later like list m marajob mass
moon mountain nt obscure of or pabbata perhaps point rain rain rajo
referred scum sense stormcloud summit sunshine that the thick things
thundercloud thundering to viz water water
   bghanab bmuab br  dark dull
 abbha mahiy nl b savilpa b b Rhu b b abbh b
b abbhmaa b abbhmua acchdesi mahik b nlamegha val-
haka valhaka sikhara
 m bhrocite bkab bpaalab babbha bdh- nt
Second run
 cp Dhs DhsA Idg Lat Miln Miln Oir PvA SnA is Vin VvA Vedic as
at by cp in is nt of or to
 also dense megho used sama at afros at at ombros at
smallcaps ii smallcaps smallcaps iv smallcaps smallcaps vi
smallcaps abhra adj also ambha ambu cloud cloud cloud clouds clouds
cloudy dark expl free from froth imber later like list marajo b mass
moon mountain obscure pabbata perhaps point rain rain rajo referred
scum sense storm cloud summit sunshine that the thick things thunder
cloud thundering viz water water
 1 1 101 1064 12 134 239 249 250 251 273 289 295 3 317 348 348
382 487 53 581 617 64 687 superscript 1 superscript superscript 9
superscript
 smallcaps i smallcaps superscript s superscript A A As Dh Gr
It J Pv S Sk Sn  Vv a f m
 b ghana b b mua b br dark dull
   
 abbha mahiy nl b Rhu b b abbh b nlamegha
 b savilpa b b abbhmaa b abbhmua acchdesi mahik
b valhaka valhakasikhara
 m bhro b ka b b paala b b abbha b dh-
 nt
Data Pali abhijjhitar
First run
 abhijjhita abhijjhtar covets function med one who itar itar tar
 T smallcaps i smallcaps smallcaps v smallcaps  A M ag fr
in l v
 265 287
 n
Second run
 abhijjhita abhijjhtar covets function med one who itar itar tar
 T A M
  l v
 smallcaps i smallcaps smallcaps v smallcaps ag fr in
 265 287
 n
Data Pali ajja
First run
 divasa Freq Loc Vedic adya ajjatagge ajjato an and as base being day
demonstr dyaus from in morning not of old or phrase present pron the
this with
   Mhvs s v
 kla 10 1564 3223 Ajj DI85 DAI235 Dh326 III425 JI
279 JVI180 KernToev PvI117 PvA59 PvA623 Sn75153
158970998 VinI18 a3 adya agga3 agge ajja-taggesee
ajj bahuta day divathus dy dya idni onwardhenceforth
to-daynow food on - Ajja  Page adv read see
Second run
 an as in of or
 Freq Loc Vedic
 divasa adya ajjatagge ajjato and base being day demonstr dyaus from
morning not old phrase present pron the this with
 Mhvs
 s v
on - Ajja  Page adv read see
 kla 10 1564 3223 Ajj DI85 DAI235 Dh326 III425
JI279 JVI180 KernToev PvI117 PvA623 Sn75153158
970998 VinI18 a3 agga3 ajja-taggesee ajj bahuta day
 divathus dy idni onwardhenceforth to-daynow
 PvA59 adya agge dya food
Data Pali ghan
First run
 253 Pug19CppariPage abstrfrghatighan Ghanf
 qv
Second run
 253 Pug19CppariPage abstrfrghatighan Ghanf
 qv
Data Pali pacati
First run
 382 Causpacpeti DAI159where Obulgpeka Passpaccati Vin
IV264 bakeGrpssw cookboilroast cookppwn daena g
torment fryroastLithkep intrsNiraye paccatoby pprpacanto
pppakka pentassa tormentingGenpacato
 DI52 NSII225PvA1014 PacatiVedpacatiIdgpeq
Avpac- Causpcayato expld qv qv- q
vPage trsand
 aer at be for in or pacato pare purgatory read ripe roasted roasting to
tormented
  pacitv pceti
Second run
 Causpacpeti DAI159where Obulgpeka Passpaccati VinIV264
 bakeGrpssw cookboilroast cookppwn daena gtorment fry
roastLithkep intrsNiraye paccatoby pprpacanto pentassa
tormentingGenpacato
 382 pppakka
 DI52 NSII225PvA1014 qv
 PacatiVedpacatiIdgpeqAvpac-Causpcayatoexpld
qv- qvPage trsand
 for pacato pare read ripe
 aer purgatory roasted roasting tormented
 or to
 at be in
 pacitv pceti
Data Twier 1 GreekEnglish
First run
       
 BUSINESS EXCELLENCE IT Internet ings of
Second run
 
      
 IT of
 Internet ings
 BUSINESS EXCELLENCE
Data Twier 2 FrenchEnglish
First run
 e 18h dhiparis David Demain Keynote all collective counts dynam-
ics par perish science-publish that
 is it of or
Second run
 e dhiparis David Demain Keynote all collective counts dynamics
par perish science-publish that
 18h
 is it or
Data Twier 3 FrenchEnglish
First run
 Edmonton Food
 go in to
 and are breuvages fans for just ready the waiting
Second run
 Edmonton Food
 go in
 for just
 and are breuvages fans ready the waiting
Data Twier 4 EnglishPolish
First run
 ubrwka my
 adidas and back comes crates dad from jackets of omg poland strawberries
two with
Second run
 ubrwka my
 adidas comes dad of
 and back crates from jackets omg poland strawberries two with
Data Twier 5 Transliterated AmharicEnglish
First run
 Buna
 coee bread dabo is naw our
Second run
 Buna
 our
 coee bread dabo is naw
834 Language Model Induction
For all language model induction tasks the threshold value t has been set t  002
and the silver threshold value s has been set s  01 e other parameters have been
set to maximum iteration count i  4 maximum random iteration count j  2
and merge mode ADD
Data Latin script GermanEnglish
 e German word Nabelschau means or staring at your But in this it
doesnt refer to anyone elses buon just your own
 
 navel-gazing navel case belly
Data Latin script GermanFinnishTurkish
 die in und Klimazone Je ob auf Sdhalbkugel vom eli on vuodenaika ja on
vuodenajoista koska maapallo on silloin kallistunut aurinko maan pinnalle
kulmassa muina vuodenaikoina Pohjoisella pallonpuoliskolla lasketaan ta-
vallisesti ja elokuu etelisell pallonpuoliskolla joulu- ja helmikuu en s-
cak en yazda Dnya depo en scak yaklak ay sonra ortaya Scak Haziran
Eyll ise Aralk arasndadr
 Der ist wrmste der vier Jahreszeiten der arktischen nachdem er der Nord-
oder herrscht spricht Nord- oder Der ndet mit Sdwinter sta suvi lm-
pimin niin e steilee hein- Yaz mevsimdir Krede Krede 21 22 ara-
snda Krede 22 21 Mart
 gemigten gerade gleichzeitig kuin Kuzey uzun gnler gerekleir eii
iin gnler genellikle iki gnler Kuzey ile ile
 Sommer man Sdsommer Nordsommer dem Kes kevn syksyn vliss
Kes jyrkemmss keskuukausiksi kes- tammi- Yarm sy kar Yarm
Gney Yarm
Data Latin script EnglishFrench
 both so in English although their is is the opposite of rough or is
the opposite of sweet only for wines otherwise is
 mou  mou but
 doux
 Doux rugueux Doux
 while
 hard used
 translate as meaning very dierent coarse can also mean almostsucr
Data Latin script EnglishTransliterated Greek
 at least ways as to is has phila and storg as has historically dicult to
which generally as
 e language distinguishes dierent the Ancient distinct with languages it
been separate the meanings these used outside their respective the senses
in these used
 Greek how word Greek agpe ros However other when were are
 four love used four words for love of words of contexts Nonetheless
words follows
Data Latin script ItalianGerman
 aresc privato Studie denire peritureStierverbands Wissenschastudier
difesa ovvero Szenario Naturwissenschalern
 dellaureola da del di der zum modo dem den drohe Come vom
 custodisce quel es oder per le idee stessa des dass delle E se Ist das seit
 pi Cenacolo vinciano rivoluzionaria Giuda condanna con peccato comin-
ci con cancro faceva intuizioni vita va Dabei Ergebnis in i riccioli poi
pi bacini in Annunciazione con ali la cosa barbaglio anni bei
 ne struggente che amore e non viene ma consapevolezza ad che ha re-
cente Kaum eine Woche vergeht keine neue Umfrage Warnung ema
Fachkremangel Deutschland Certo ma anche consapevole che qualche
mehren letzter Zeit Stimmen Entwarnung geben kam jngst eine Deutsche
ein allgemeiner Fachkremangel eher mehr anche Baista che Leonardo
approfonditamente a Venezia nelle vada alla aento alle dellangelo deli-
catezza punte che non che volare Jahren angemahnte drohenden Fachkre-
mangel Ingenieuren ein
 Milano lesempio psicologia il subito autodistruggersi solo lunghissimo
So il movimento moto sui si bellissima occhio allins sono sogno lo
ossessionava quello und also Mythos
 un r MINT-Berufen
 cura restauro arginato gibt perch caurata sich auch zu nicht richia-
mano acque ricerche chiave anti-Turchi nur
Data Mixed script GreekRussian
          
       
        
      15   
         
     -
          -
      
         
       
        
        -
        
     
Data Mixed script EnglishGreek
 is biblical is will is without self-benet is feelings feelings it be feeling
being high is by his is by will mostly sexual intimate well rened his
denition is initially felt with it beauty within beauty itself use with-
out helps soul beauty spiritual youthful beauty feel suggesting sensually
spiritual nding its like nding all seek
  
 Agpe love brotherly love love of God for of for in known love 1 13
throughout New brotherly love aection good love love given or not per-
son continues love even in for ones for spouse refer love of content or
holding one in unconditional love of God for of love to good of ros
love of e Modern Greek word love own Although eros for person
contemplation becomes of person or even becomes of not of of love of
word mean In Symposium work on subject eros knowledge of of Form
of erotic  even love non-corporeal of is Lovers philosophers through of
 agp means esp charity the man and man God Agape used the pas-
sage as the chapter Corinthians and described there and the Testament
as and benevolence Whether the returned the to any Agape also used
ancient texts to denote children and the a and was also used to to a
feast It can also described as the regard Agape used Christians to express
the children type was further explained omas Aquinas as the another
rs means the passion erotas means It can also apply to dating re-
lationships as as marriage Plato a an appreciation the that appreciation
Plato does talk physical araction as a necessary part hence the the pla-
tonic to physical araction the the most famous ancient the Plato has
Socrates argue that the recall and contributes to an understanding truth
the ideal that leads us humans to desire thus that that based aspires to
the plane existence that truth just any truth leads to transcendence and
are inspired to truth the means eros
Data Mixed script EnglishSpanishArabic
                  
               
 
 ribbon symbol mourning ribbon mourning El un y un en
 black is a of remembrance or Wearing or displaying a black has been used
for remembrance tragedies or as a political statement crespn negro o lazo
negro es smbolo utilizado por personas estados sociedades organizaciones
representando sentimiento poltico-social seal de duelo
 A POWMIA
Data Mixed script EnglishChinese
 e Chinese simplied traditional Chinese invoked motivational speaking
because the composed characters that represent linguists have criticized
this usage because the component simplied Chinese traditional Chinese
has other besides Chinese certain some be based the Chinese that the e
numbers believed have because their similar words that have positive
 
 Western can and Some meanings In are number name and are meanings
names meanings
 0 6 8 9
 crisis is auspicious inauspicious sounds sound
 for pinyin frequently in word of two danger opportunity pronounced
tradition by or on word to to
   wij j   
Data Mixed script UkrainianRussian
         -

  913   
  
   
 
       
         -
  
      
Data Pali abbha
 nt nt Sk dark Idg cp Gr Lat Sk water Gr water dark at SnA S
at It Sn cp SnA Sn S
  A A  J 251 1 1064 249 250 12 64 348 382
 viz 134 101 581 f 289
 53 295 273 487 3 617 317 348 239 687
 cloud also cloud cloudy smallcaps ii smallcaps  list is smallcaps
i smallcaps sama smallcaps vi smallcaps abbha smallcaps iv
smallcaps nl As Dhs DhsA used  clouds cloud also as
 m adj
 abhra mahiy VvA acchdesi Pv PvA dull valhaka Vv valhaka
sikhara
 atafrosat froth of superscript 9 superscript superscript s su-
perscript superscript 1 superscript
 later scum rain ambha rain a Miln megho Miln nlamegha sense expl
 Dh
 m bhro atombrosat ambu mass to obscure moon babbha mahik
b bdh- marajob bRhub pabbata rajo babbhb by
perhaps babbhmaab br bkab or summit stormcloud
bghanab bpaalab mass bmuab from abbhmua b
savilpab
 Vedic imber Oir dense Vin in things that sunshine is referred moun-
tain like thundercloud the point thick free thundering
Data Pali abhijjhitar
 smallcapsismallcaps v l smallcapsvsmallcaps
 abhijjhita abhijjhtar itar itar tar
 n ag fr med M 287 T  A 265
 in function one who covets
Data Pali ajja
 Ajja Ajj adv base a3 divathus Dh326 ajj v PvA59
PvA623 phrase ajjatagge ajjato agge ajja-taggesee agga3adv
 the 3223 Page
 - kla
 Vedic   being see see read as  Mhvs   Mhvs
 of of on food
 and an old not
 adya adya dya dy dyaus day to-daynow bahuta with day
divasa day
 demonstr pron Loc this KernToev s Freq or from this onwardhence-
forth this morning present
 Sn75153158970998 JI279 III425 PvI117 idni 1564 in
VinI18 DI85 DAI235 JVI180 10
Data Pali ghan
 Pug19CppariPage
 Ghanf abstrfrghatighan
 253qv
Data Pali pacati
 VinIV264 NSII225PvA1014 DI52
 DAI159where 382
 at 
 cookppwn cookboilroast
 PacatiVedpacatiIdgpeqAvpac- Obulgpeka to fryroast
Lithkep ripe to gtorment purgatorytrsand pacitv aer roasting
pprpacanto tormentingGenpacatoCauspcayato read pacato for
paccatoby pare pppakka Causpacpeti pceti Passpaccati to roasted
or tormented
 bakeGrpssw intrsNiraye expld daena pentassa qv
- qv be qvPage
Normalized data
 pacati peka pssw ppwn pacitv ppr pacanto Gen pacato Caus pcay-
ato pacato paccato pare pentassa pp pakka Caus pacpeti Pass paccati
 peq bake
 pac- 264 52  382
 1014 159  - 
 fry Niraye I I by
 Av Obulg Gr trs D DA qv qv qv
 Ved to roast kep cook ripe to cook roast torment purgatory and aer
roasting tormenting expld at where read for daena pceti to be roasted
or tormented Page
 Pacati Idg Lith boil VinIV g in intrs in NSII225PvA
Data Twier 1 GreekEnglish
 BUSINESS EXCELLENCE
      Internet of  
 ings IT
Data Twier 2 FrenchEnglish
 Keynote e collective of science-publish or perish it all that counts
 Demain 18h par
 dhiha6 David
 dhiparis dynamics is
Data Twier 3 FrenchEnglish
 FWWC2015
 breuvages go
 Food Edmonton to for the
 in waiting bilingualism
 and are ready just fans
Data Twier 4 EnglishPolish
 comes from with two crates of strawberries jackets omg
 my dad poland and adidas
 back ubrwka
Data Twier 5 Transliterated AmharicEnglish
 coee
 bread is our
 Buna dabo naw
U T
F II
C  D H
Language Segmentation
Author
David A
Supervisors
Prof Dr Caroline S
Dr Sven N
August 18 2015
Erklrung zur Masterarbeit
Hiermit erklre ich dass ich die Masterarbeit selbststndig verfasst und keine ande-
ren als die angegebenen ellen und Hilfsmiel benutzt und die aus fremden ellen
direkt oder indirekt bernommenen Gedanken als solche kenntlich gemacht habe
Die Arbeit habe ich bisher keinem anderen Prfungsamt in gleicher oder vergleich-
barer Form vorgelegt Sie wurde bisher nicht verentlicht
Unterschri
Abstract
Language segmentation consists in nding the boundaries where one lan-
guage ends and another language begins in a text wrien in more than one lan-
guage is is important for all natural language processing tasks
e problem can be solved by training language models on language data
However in the case of low- or no-resource languages this is problematic
therefore investigate whether unsupervised methods perform beer than super-
vised methods when it is dicult or impossible to train supervised approaches
A special focus is given to dicult texts ie texts that are rather short one
sentence containing abbreviations low-resource languages and non-standard
language
I compare three approaches supervised n-gram language models unsuper-
vised clustering and weakly supervised n-gram language model induction I de-
vised the weakly supervised approach in order to deal with dicult text specif-
ically In order to test the approach I compiled a small corpus of dierent text
types ranging from one-sentence texts to texts of about 300 words
e weakly supervised language model induction approach works well on
short and dicult texts outperforming the clustering algorithm and reaching
scores in the vicinity of the supervised approach e results look promising
but there is room for improvement and a more thorough investigation should be
undertaken
Anowledgements
My thanks go to professor Caroline Sporleder for sharing her knowledge with me for
her inspiring ideas and for agreeing to supervise my Bachelors and Masters esis
despite her busy schedule It was also thanks to the topic she suggested for my Bach-
elors esis that I met Jrgen Knauth and later was able to get a research assistant
position at the SeNeReKo project collaborating closely with Jrgen
Which brings me to the next person on the list I would like to thank Jrgen Knauth
for the wonderful collaboration for his patience for his contagious enthusiasm and
all the interesting conversations in passing that always lasted longer than intended
Finally I would like to thank all the people that volunteered to proofread my thesis
and all the people that helped me during the writing of this thesis Unfortunately I
cannot list everyone You know who you are
I would like to thank Stephan Faber for his insightful comments when I couldnt
see the wood for the trees for his patience and optimism for pushing me to go further
and to persevere
I would also like to thank Julian Vaudroz for accompanying me throughout the
degree program We both didnt know what we were in for when we started but we
persevered and it paid o It wouldnt have been the same without you
List of Figures
Out-of-place metric 
Simple text illustration 
Initial model creation 
Initial model evaluation 
Model update
Evaluation 
New model creation 
Multiple model evaluation 
Updating relevant model
10 Multiple model evaluation 2 
11 New model creation 2 
14 Merging most similar models 
15 Word-Model assignment
Clustering preprocessor 
17 WEKA Cluster visualization 
22 Alternating language structure 
Problematic text sample 
Finding the most similar models 
ELKI Cluster visualization 
Language model Distribution 1 
Language Model Distribution 2 
Language model Distribution 3 
List of Tables
Training data Size 
Unambiguous encoding distances 
Simplied encoding distances 
N-Gram language model results Latin script 
N-Gram language model results Mixed script
N-Gram language model results Pali data 
N-Gram language model results Twier data 
Textcat results Latin script 
Textcat results Mixed script
Textcat results Pali data 
Textcat results Twier data 
Clustering results Latin script
Clustering results Mixed script
Clustering results Pali data 
Clustering results Twier data 
Induction results Latin script 
Induction results Mixed script 
Induction results Pali data 
Induction results Twier data 
Twier 3 Textcat versus Gold clustering 
Twier 4 Textcat versus Gold clustering 
List of Algorithms
N-gram numerical encoding 
Model induction 
Initial model creation 
Max model and max score 
Model merger
Distributional Similarity Calculation 
Contents
Introduction
2 Related work
21 N-Grams and rank order statistics 
22 N-Grams and maximum likelihood estimator 
23 Trigrams and short words
24 N-Grams and clustering 
26 Clustering and speech 
27 Monolingual training data 
Predictive sux trees 
Inclusion detection 
3 eory
Supervised language model 
311 N-Gram models 
Formal denition 
Smoothing 
32 Unsupervised clustering 
33 Weakly supervised language model induction 
4 Experimental setup
Preprocessing 
41 Data 
Implementation 
Training phase 
Supervised language model 
423 Application of the approach 
Textcat and language segmentation 
43 Unsupervised clustering 
432 Dening features 
433 Mapping features to a common scale 
434 e problem of unambiguous encoding 
435 e clusterer
44 Weakly supervised language model induction 
Evaluating clusterings 
441 Distributional similarity 
Evaluating results 
Estimating the parameters 
5 Results
51 N-Gram language model
52 Textcat
53 Clustering 
Language model induction 
6 Discussion
61 N-Gram language models 
62 Textcat
63 Clustering 
Language model induction 
Scores 
7 Conclusion
8 Appendix
81 Development data 
82 Test data 
Latin script data 
Latin script data 
812 Mixed script data 
Twier data 
Pali dictionary data 
Twier data 
Pali dictionary data 
831 N-Gram Language Models 
Textcat 
Clustering 
Language Model Induction 
822 Mixed script data 
83 Results 
Introduction
Language segmentation and identication are important for all natural language pro-
cessing operations that are language-specic such as taggers parsers or machine
translation Jain and Bhat 2014 Zubiaga et al 2014 Indeed using traditional mono-
lingual natural language processing components on mixed language data leads to mis-
erable results Jain and Bhat 2014 Even if the results are not terrible language identi-
cation and segmentation can improve the overall results For example by identifying
foreign language inclusions in an otherwise monolingual text parser accuracy can be
increased Alex et al 2007
One important point that has to be borne in mind is the dierence between lan-
guage identication and language segmentation Language identication is concerned
with recognizing the language at hand It is possible to use language identication
for language segmentation
Indeed by identifying the languages in a text the seg-
mentation is implicitly obtained Language segmentation on the other hand is only
concerned with identifying language boundaries No claims about the languages in-
volved are made
Aer giving an overview over related work and dierent approaches that can be
taken for language segmentation I will present the theory behind supervised methods
as well as unsupervised methods Finally I will introduce a weakly supervised method
for language segmentation that I developed
Aer the theoretical part I will present experiments done with the dierent ap-
proaches comparing their eectiveness on the task of language segmentation on dif-
ferent text types A special focus will be given to dicult text types such as short texts
texts containing under-resourced languages or texts containing a lot of abbreviations
or other non-standard features
A big advantage of unsupervised methods is language independence
If the ap-
proach used does not rely on language-specic details the approach is more exible
as no language resources have to be adapted for the method to work on other lan-
guages ese advantages might be especially useful for under-resourced languages
When there is no or insucient data available to train a supervised language model
an unsupervised approach might yield beer results
Another advantage is that unsupervised methods do not require prior training
ey are not dependent on training data and thus cannot be skewed by the data In-
deed supervised approaches that are trained on data are qualitatively tied to their
training data dierent training data will in all probability yield dierent models
is thesis aims at answering the question whether unsupervised language seg-
mentation approaches work beer on dicult text types than supervised language
approaches
2 Related work
21 N-Grams and rank order statistics
Cavnar and Trenkle 1994 use an n-gram language model for language identication
purposes eir program Textcat is intended to classify documents by language e
system calculates n-grams for 1 6 n 6 5 from training data and orders the n-grams
according to inverse frequency ie from the most frequent n-grams to the most infre-
quent n-grams e numerical frequency data is then discarded and only inherently
present
During training the program calculates an n-gram prole consisting of these n-
gram lists for each category ie language to classify
New data is classied by rst calculating the n-gram prole and then comparing
the prole to existing proles e category with the lowest dierence score is taken
as the category for the document
e score they use for classication is called out-of-place metric For each n-gram
in the document n-gram prole the corresponding n-gram in the category prole is
looked up and the absolute dierence of ranks is taken as score e sum is calculated
over all n-grams More formally the out-of-place metric moop is calculated as
jrxi d cid0 rxi cj
With n the number of n-grams in the document prole xi the i-th n-gram rxi d
the rank of the i-th n-gram in the document prole rxi c the rank of the i-th n-gram
in the category prole
Figure 1 illustrates the out-of-place metric
Category prole
Document prole
Most frequent
Least frequent
no match  max
Figure 1 Out-of-place metric
In gure 1 the document prole has ER as most frequent n-gram at rank 1 fol-
lowed by ING at rank 2 etc e category prole does not contain the n-gram ER in
that case an arbitrary xed maximum value is assigned e category prole contains
the n-gram ING at rank 2 the same rank as in the document prole the dierence is
0 e category prole contains the n-gram AT at rank 1 while in the document pro-
le it occurs at rank 3 e absolute dierence is 2 e out-of-place metric consists
of the sum of all scores thus calculated
Cavnar and Trenkle 1994 collected 3713 Usenet texts with a cultural theme in
dierent languages ey ltered out non-monolingual texts and texts that had no
useful content for language classication In the end they had 3478 articles ranging
from a single line of text to 50 KB of text
eir results indicated that length had no signicant impact on the classication
contrary to what they thought Also they found that training the system with 400
n-grams yielded the best result with a precision of 998
ey also showed that their approach could be used for subject classication of
texts in the same language with reasonable precision is nding indicates that lan-
guage and domain are linked to a certain degree
22 N-Grams and maximum likelihood estimator
Dunning 1994 also uses an n-gram language model for language identication pur-
poses e program calculates n-grams and their frequencies from the training data
and estimates the probability P of a given string using the Maximum Likelihood Esti-
mator MLE with Laplace add-one smoothingMore formally
P wijw1     wicid01 
Cw1     wi  1
Cw1     wicid01  jV j
with Cw1     Ci the number of times the n-gram w1     wi occurred
Cw1     Cicid01 the number of times the ncid0 1-gram w1     wicid01 occurred and jV j
the size of the vocabulary
For a string S the string is decomposed into n-grams and the log probability lk is
calculated as
w1wk2S
Cw1     wk log P wkjw1     wkcid01
where k is the order of the n-gram k  n used
In order to test the system Dunning 1994 uses a specially constructed test cor-
pus from a bilingual parallel translated English-Spanish corpus containing English and
Spanish texts with 10 texts varying from 1000 to 50000 bytes for the training set and
100 texts varying from 10 to 500 bytes for the test set
e results indicate that bigram models perform beer for shorter strings and less
training data while trigram models work beer for larger strings and more training
Dunning 1994 criticizes Cavnar and Trenkle 1994 for saying that their system
would be insensitive to the length of the string to be classied as the shortest text they
classied was about 50 words e system implemented by Dunning 1994 can classify
strings of 10 characters in length moderately well while strings of 50 characters or
more are classied very well Accuracies given vary from 92 for 20 bytes of training
data to 999 for 500 bytes of text
23 Trigrams and short words
Grefenstee 1995 compares trigrams versus short words for language identication
Short words are oen function words that are typical for and highly frequent in a given
language
e trigram language guesser was trained on one million characters of text in 10
languages Danish Dutch English French German Italian Norwegian Portuguese
Spanish and Swedish From the same texts all words with 5 or less characters were
counted for the short-word-strategy
e results indicate that the trigram approach works beer for small text fragments
of up to 15 words while for any text longer than 15 words both methods work equally
well with reported accuracies of up to 100 in the 11-15 word range
24 N-Grams and clustering
Gao et al 2001 present a system that augments n-gram language models with clus-
tering techniques ey cluster words by similarity and use these clusters in order to
overcome the data sparsity problem
In traditional cluster-based n-gram models the probability P wi is dened as the
product of the probability of a word given a cluster ci and the probability of the cluster
ci given the preceding clusters For a trigram model the probability P wi of a word
wi is calculated as
P wijwicid02wicid01  P wijci cid2 P cijcicid02cicid01
e probability of a word given a cluster is calculated as
P wijci 
with Cwi the count of the word wi and Cci the count of the cluster ci
e probability of a cluster given the preceding clusters is calculated using the
Maximum Likelihood Estimator
P cijcicid02cicid01 
Ccicid02cicid01ci
Ccicid02cicid01
Gao et al 2001 derive from this three ways of using clusters to augment language
models predictive clustering 7 conditional clustering 8 and combined clustering
P wijwicid02wicid01  P cijwicid02wicid01 cid2 P wijwicid02wicid01ci
P wijwicid02wicid01  P wijcicid02cicid01
P wijwicid02wicid01  P cijcicid02cicid01 cid2 P wijcicid02cicid01ci
Similarly Dreyfuss et al 2007 use clustering to cluster words by their context in
order to improve trigram language models In addition to Gao et al 2001 they also
use information about the subject-verb and verb-object relations of the sentence
ey show that their model using clustering subject-verb information verb-object
information and the Porter stemmer outperforms a traditional trigram model
Carter 1994 clusters training sentences ie the corpus into subcorpora of similar
sentences and calculates separate language model parameters for each subcorpus in
order to capture contextual information
In contrast to other works Carter 1994
clusters sentences instead of single words compare Pereira et al 1993 and Ney et al
1994 Carter 1994 shows that the subdivision into smaller clusters increases the
accuracy of bigram language models but not trigram models
Inclusion detection
Beatrice Alex cf Alex 2005 2006 2007 Alex et al 2007 Alex and Onysko 2010
addresses the problem of English inclusions in mainly non-English texts For the lan-
guage pair German-English inclusions are detected using a German and an English
lexicon as rst resource If a word is found only in the English lexicon it is tagged
as unambiguously English
If the word is found in neither lexicon a web search is
conducted restricting the search options to either German or English and counting
the number of results If the German search yields more results the word is tagged as
German otherwise as English inclusion If a word is found in both lexicons a post-
processing module resolves the ambiguity
Alex is mainly concerned with the improvement of parsing results by inclusion
detection For example in Alex et al 2007 they report an increase in F-Score of 43
by using inclusion detection when parsing a German text with a parser trained on the
TIGER corpus Brants et al 2002
26 Clustering and spee
In the area of clustering and spoken language identication Yin et al 2007 present a
hierarchical clusterer for spoken language ey cluster 10 languages1 using prosodic
features and Mel Frequency Cepstral Coecients MFCC MFCC vectors are a way of
representing acoustic signals Logan et al 2000 e signal is rst divided into smaller
frames each frame is passed through the discrete Fourier transform and only the log-
arithm of the amplitude spectrum is retained Logan et al 2000 e spectrum is then
projected onto the Mel frequency scale a scale that maps actual pitch to perceived
pitch as apparently the human auditory system does not perceive pitch in a linear
manner Logan et al 2000 Finally a discrete cosine transform is applied to the
spectrum to get the MFCC representations of the original signal Logan et al 2000
Yin et al 2007 show that their hierarchical clusterer outperforms traditional Acous-
As spoken language will not be further investigated in this thesis I will not dive
tic Gaussian Mixture Model systems
deeper into the maer at this point
27 Monolingual training data
Yamaguchi and Tanaka-Ishii 2012 King and Abney 2013 and Lui et al 2014 use
monolingual training data in order to train a system capable of recognizing the lan-
guages in a multilingual text
Yamaguchi and Tanaka-Ishii 2012 use a dynamic programming approach to seg-
ment a text by language eir test data contains fragments of 40 to 160 characters and
achieves F-scores of 094 on the relatively closed data set of the Universal Declara-
tion of Human Rights2 and 084 on the more open Wikipedia data set However the
approach is computationally intensive not to say prohibitive while Yamaguchi and
Tanaka-Ishii 2012 self-report a processing time of 1 second for an input of 1000 char-
acters Lui et al 2014 found that with 44 languages the approach by Yamaguchi and
Tanaka-Ishii 2012 takes almost 24 hours to complete the computation on a 16 core
workstation
King and Abney 2013 use weakly supervised methods to label the languages of
words ey consider the task as sequence labeling task ey have limited them-
selves to bilingual documents with a single language boundary and the task consists
1e authors do not explicitly list the languages clustered except for two-leer abbreviations which
seem to correspond to ISO 639-1 e languages under investigation could have been Vietnamese Ger-
man Farsi French Japanese Spanish Korean English Tamil and ma though it is impossible to tell
2httpwwwunorgendocumentsudhr
in discriminating between English and non-English text ey found that a Condi-
tional Random Field model augmented with Generalized Expectation criteria worked
best yielding accuracies of 88 with as lile as 10 words used for training
Lui et al 2014 consider the task as multi-label classication task ey represent
a document as an n-gram distribution of byte sequences in a bag-of-words manner
ey report F-scores of 0957 and 0959 ey note that similar languages will pose
problems when trying to identify a language and solve this problem by identifying a
set of languages that most probably are correct instead of a single language
One problem that these approaches all have is that they need to know the languages
that will occur in the test data King and Abney 2013 Lui et al 2014
28 Predictive sux trees
Seldin et al 2001 propose a system for automatic unsupervised language segmenta-
tion and protein sequence segmentation eir system uses Variable Memory Markov
VMM sources an alternative to Hidden Markov Models HMM implemented as Pre-
dictive Sux Trees PST
Whereas HMMs require substantial amounts of training data and a deep under-
standing of the problem in order to restrict the model architecture VMMs are simpler
and less expressive than HMMs but have been shown to solve many applications
with notable success Begleiter et al 2004
In contrast to n-gram models that es-
timate the probability of w as P wjN  with N the context typically the n previous
words VMMs can vary N in function of the available context Begleiter et al 2004
us they can capture both small and large order dependencies depending on the
training data Begleiter et al 2004
ere is no single VMM algorithm but rather a family of related algorithms One
of these algorithms is called Predictive Sux Tree PST Ron et al 1996 A PST is
a tree over an alphabet cid6 with each node either having 0 leaf nodes or jcid6j children
non-terminal nodes Ron et al 1996 Each node is labeled with the result of the walk
from that node up to the root Ron et al 1996 Each edge is labeled by a symbol s 2 cid6
and the probability for the next symbol being s Ron et al 1996
By modifying the Predictive Sux Tree PST algorithm using the Minimum De-
scription Length MDL principle Seldin et al 2001 end up with a non-parametric
self-regulating algorithm e MDL principle avoids overing of the model by favor-
ing low complexity over goodness-of-t Grnwald 2007
ey embed the algorithm in a deterministic annealing DA procedure to rene
the results Finally they use the Blahut-Arimoto algorithm a rate-distortion function
until convergence of the system
For the language segmentation task they use 150000 leers of text 30000 from
each of the following languages English German French Italian transliterated Rus-
sian ey used continuous language fragments of approximately 100 leers yielding a
synthetic multilingual text that switches language approximately every two sentences
One important point that they note is that too short segments do not enable reliable
discrimination between dierent models erefore they disallow switching models
aer every word
ey report very good results on the language segmentation task and on the pro-
tein segmentation task Aer 2000-3000 iterations of the Blahut-Arimoto algorithm
the correct number of languages is identied and the segmentation is accurate up to a
few leers
3 eory
Supervised language model
311 N-Gram models
Among supervised language models n-gram models are very popular Gao et al 2001
An n-gram is a slice from the original string Cavnar and Trenkle 1994 ese slices
can be contiguous or not Non-contiguous n-grams are also called skip-grams Guthrie
et al 2006 In skip-grams an additional parameter k indicates the maximum distance
that is allowed between units In this parlance contiguous n-grams can be regarded
as 0-skip-n-grams Guthrie et al 2006
e following example demonstrates the dierence between traditional n-grams
and skip-grams Given the following sentence
i s a sample sentence 
We can construct for example the following word k-skip-n-grams
0-skip-2-grams is is is a a sample sample sentence
2-skip-2-grams is is is a is sample is a is sample is sentence a sample a
sentence sample sentence
0-skip-3-gramsis is a is a sample a sample sentence
2-skip-3-gramsis is a is is sample is is sentence is a sample is a sen-
tence is sample sentence is a sample is a sentence is sample sentence a sample
sentence
e results for 2-skip-2-grams does not include the skip-gram is sentence as
the distance in words between these two words is 3 higher than the allowed k of 2 As
can be seen from this example the number of skip-grams is more than two times higher
than the number of contiguous n-grams and this trend continues the more skips are
allowed Guthrie et al 2006 Skip-grams unlike n-grams do not incur the problem
of data sparseness with an increase of n
Instead of using words as unit for n-gram decompositions we can also choose char-
acters Each word is then decomposed into sequences of n characters For example
the word
can be decomposed into the 2-grams mo de el Oen the word to decompose
is padded with start and end tags in order to improve the model Cavnar and Tren-
kle 1994 If we pad the word with w and w the 2-gram decomposition yields
wm mo de el l w e use of paddings allows the model to capture details about
character distribution with regard to the start and end of words Cavnar and Trenkle
1994 For example in English the leer y occurs more oen at the end of words than
at the beginning of words while the leer w occurs mainly at the beginning of words
Taylor 2015 A non-padding model cannot capture this distinction while a padding
model can
One advantage of n-gram models is that the decomposition of a string into smaller
units reduces the impact of typing errors Cavnar and Trenkle 1994 Indeed a typ-
ing error only aects a limited number of units Cavnar and Trenkle 1994 Due to
this property n-gram models have been shown to be able to deal well with noisy text
Cavnar and Trenkle 1994
312 Formal denition
Traditional n-gram language models predict the next word wi given the previous words
w1     wicid01 is prediction uses the conditional probability P wijw1     wicid01 In-
stead of using the entire history w1     wicid01 the probability is approximated by using
only the n previous words wicid0n1     wicid01
P wijw1     wicid01  P wijwicid0n1     wicid01
e probability can be estimated using the Maximum Likelihood Estimation MLE
P wijwicid0n1     wicid01 
Cwicid0n1     wi
Cwicid0n1     wicid01
Where Cwicid0n1     wi represents the number of times the n-gram sequence
wicid0n1     wi occurred in the training corpus and Cwicid0n1     wicid01 represents the
number of times the n cid0 1-gram sequence wicid0n1     wicid01 was seen in the training
corpus
Smoothing
e problem with MLE is that sequences not seen during training will have a prob-
ability of zero
In order to avoid this problem dierent smoothing techniques can
be used Chen and Goodman 1996 e simplest smoothing technique is additive
Laplace smoothing Chen and Goodman 1996 Let V be the vocabulary size ie the
total number of unique words in the test corpus e smoothed probability PLaplace
becomes
PLaplacewijwicid0n1     wicid01 
Cwicid0n1     wi  cid21
Cwicid0n1     wicid01  cid21V
With cid21 the smoothing factor If we choose cid21  1 we speak of add one smoothing
Jurafsky and Martin 2000 In practice cid21  1 is oen chosen Manning and Schtze
An important estimation is the Good-Turing estimation Chen and Goodman 1996
While not directly a smoothing method it estimates the frequency of a given observa-
tion with
cid3
 c  1
where c is the number of times the observation was made Nc is the number of times
the frequency c was observed and Nc1 the frequency of the frequency c  1 us
cid3 Chen and Goodman
instead of using the actual count c the count is taken to be c
Another way to avoid assigning probabilities of zero to unseen sequences is by
using back-o models ere are linear and non-linear back-o models In non-linear
back-o models if the original n-gram probability falls below a certain threshold value
the probability is estimated by the next lowest n-gram model Katzs back-o model
Katz 1987 for instance calculates probability Pbo using the formula
dwicid0n1wi
cid11wicid0n1wicid01Pbowijwicid0n2     wicid01 otherwise
Cwicid0n1wi
Cwicid0n1wicid01
if Cwicid0n1     wi  k
With d and cid11 as smoothing parameters e parameter k is oen chosen k  0
is means that if the probability given a high-order n-gram model is zero we back
o to the next lowest model For tri-gram models the formula becomes
Pbowijwicid02 wicid01 
8P wijwicid02 wicid01
cid111P wijwicid01
cid112P wi
if Cwicid02 wicid01  0
if Cwicid02 wicid01  0 and Cwicid01 wi  0
otherwise
In contrast linear back-o models use an interpolated probability estimate by com-
bining multiple probability estimates and weighting each estimate e probability PLI
for a tri-gram model is
PLIwijwicid02 wicid01  cid213P wijwicid02 wicid01  cid212P wijwicid01  cid211P wi
cid21i  1
32 Unsupervised clustering
Clustering consists in the grouping of objects based on their mutual similarity Bie-
mann 2006 Objects to be clustered are typically represented as feature vectors Bie-
mann 2006 from the original objects a feature representation is calculated and used
for further processing
Clustering can be partitional or hierarchical Yin et al 2007 Partitional clustering
divides the initial objects into separate groups in one step whereas hierarchical clus-
tering builds a hierarchy of objects by rst grouping the most similar objects together
and then clustering the next level hierarchy with regard to the existing clusters Yin
et al 2007
e clustering algorithm uses a distance metric to measure the distance between the
feature vectors of objects Biemann 2006 e distance metric denes the similarity
of objects based on the feature space in which the objects are represented Jain et al
1999 ere are dierent metrics available A frequently chosen metric is the cosine
similarity that calculates the distance between two vectors ie the angle between them
Biemann 2006
In order for a clustering algorithm to work features that represent the object to be
clustered have to be dened Jain et al 1999 Features can be quantitative eg word
length or qualitative eg word starts with a capital leer Jain et al 1999
Most clustering algorithms eg k-means need the number of clusters to generate
Jain et al 1999 e question how to best choose this key number has been addressed
in-depth by Dubes 1987
Clustering can be so or hard When hard-clustering an object can belong to one
class only while in so-clustering an object can belong to one or more classes some-
times with dierent probabilities Jain et al 1999
33 Weakly supervised language model induction
e main idea behind language model induction is that by inducing language models
from the text itself the models are highly specialized but the approach is generally
more exible since genre or text specic issues do not arise
is approach is similar in character to the work by Seldin et al 2001 in that the
text itself is used as data set However the realization diers greatly Whereas Seldin
et al 2001 use predictive sux trees I use n-gram language models
e intuition is to learn the language models from the text itself in an iterative
manner Suppose we have a document as follows where wi represents the word at
position i in the text Suppose the text contains two languages marked in red and
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 2 Simple text illustration
We take the rst word and create a language model m1 from that word
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 3 Initial model creation
We then evaluate the second word using the rst language model If the language
model score is high enough we update the language model with the second word
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 4 Initial model evaluation
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 5 Model update
If the score is below a certain threshold the existing language model does not model
the word well enough and a new model is created
evaluate
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 6 Evaluation
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 7 New model creation
When there is more than one language model each word is evaluated by every
language model and the highest scoring model is updated or a new model is created
if no language model models the word well enough
evaluate
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 8 Multiple model evaluation
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 9 Updating relevant model
evaluate
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 10 Multiple model evaluation 2
m1 m2 m3
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 11 New model creation 2
e last example shows that it is not necessarily the case that exactly one language
model is created per language it oen is the case that many language models are
created for one language
At the beginning the models are not very reliable as they only have a few words
as basis but the more text is analyzed the more reliable the models become
However the approach is problematic in that the text structure itself inuences
the language models created If the text starts with a foreign language inclusion as
illustrated in gure 12 the initial model might be too frail to recognize the following
words as being a dierent language updating the rst model with the second and third
word and so on us the approach would fail at recognizing the foreign language
inclusion
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 12 Problematic text sample
If we were to start from the end of the text and work towards the beginning the
probability of having a relatively robust language model for the blue language would
be high and so it would theoretically be easier to recognize the rst word as not being
blue
erefore one induction step involves one forward generation and one backwards
generation is yields two sets the set of models from the forward generation F 
ff1 f2     fng and the set from the backwards generation B  fb1 b2     bmg
en from the two sets of models the most similar models are selected For this
every model from F is compared to every model from B as gure 13 shows e most
similar models are then merged as illustrated in gure 14 Indeed if both the forward
and backwards generation yielded a similar language model it is probable that the
model is correct
Even so both forward and backwards generation can not guarantee ideal results
there is the option to run the generation from a random position is random induc-
tion picks a random position in the text and runs one induction step from that position
meaning one forward and one backwards generation Finally the most similar models
are merged as for the general generation
Figure 13 Finding the most similar models
Merged model
Figure 14 Merging most similar models
is only yields one probable language model therefore the induction is repeated
with the dierence that all probable models are taken into consideration as well For
each word if a probable model models the word well enough no new model is created
otherwise a new model is created
At the end of the induction loop the set of probable models P is examined As long
as there are two models that have a similarity score below a certain threshold the two
most similar models are merged
Finally aer the language models have been induced another pass is made over
the text and each word is assigned to the language model which yields the highest
score for that word resulting in a word-to-model assignment as illustrated in gure
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 
Figure 15 Word-Model assignment
I have made the approach parametric with parameters being
 Induction iterations Number of induction iterations
 Random iterations Number of random iterations
 ForwardBackwards threshold reshold for forwardbackwards merging
 Silver threshold reshold for P model merging
ese parameters can be adapted in the hope that some parameter congurations
will work beer on certain data sets than other congurations Since the approach
has parameters that have to be learned from a development set the approach is said to
be weakly supervised the development set is not used to train any language specics
only for the estimation of the parameters of the approach
4 Experimental setup
In this chapter I present experiments done using the approaches delineated in the pre-
vious section in order to nd out whether there are approaches that work beer on
certain types of text
e central hypothesis is that unsupervised language segmentation approaches are
more successful on dicult data Dicult data is data for which there is not enough
data to train a language model or data which contains a lot of non-standard language
such as abbreviations
First I present the data used to test the language segmentation systems and elabo-
rate on the dierent aspects that had to be considered for the data compilation
I then present two supervised language segmentation experiments using n-gram
language models and Textcat
For unsupervised language segmentation I will rst present experiments using
clustering algorithms before presenting experiments using language model induction
41 Data
In order to test the dierent language segmentation approaches I compiled dierent
sets of test data As I want to focus on short texts most texts from the test corpus are
rather small sometimes consisting of only one sentence However in order to test the
general applicability of the approach the test corpus also contains larger text samples
e test corpus can be subdivided into dierent sub-corpora
 Latin-based Texts consisting of languages using Latin-based scripts such as
German English Finnish or Italian
 Mixed script Texts consisting of languages using Latin-based scripts and lan-
guages using non-Latin-based scripts
 Twier data Short texts taken from Twier
 Pali dictionary data Unstructured texts containing many dierent language in-
clusions such as Vedic Sanskrit Sanskrit Indogermanic reconstructions Old Bul-
garian Lithuanian Greek Latin Old Irish many abbreviations and references
to text passages
As every outcome has to be manually checked the test corpus is rather small Every
category consists of ve texts Each texts consists of two or three languages with the
exception of the Pali dictionary data that oen contains inclusions from many dierent
languages in the etymological explanations
For each text I also created a gold standard version with the expected clusters In
some cases it is not clear how to cluster certain objects In that case I use a clustering
that makes sense to me but this need not mean that it is the correct or only possible
clustering
For the parameter estimation of the language model induction approach I also
compiled a set of development data All texts can be found in the appendix under 81
and 82
Supervised language model
Implementation
For the supervised language segmentation method I implemented an n-gram language
model as described by Dunning 1994 e n-gram language model is implemented
as a character trigram model with non-linear back-o to bigram and unigram models
e conditional probability P is calculated using the formula
P wijwicid02 wicid01 
cid111
cid112
cid113
cid114
Cwicid02wicid01wi
Cwicid02wicid01
Cwicid01wi
Cwicid01
V W X
if Cwicid02 wicid01 wi  0
if Cwicid01 wi  0
if Cwi  0
otherwise
8
P w 
with cid111  07 cid112  02 cid113  009 cid114  001 V the number of unigrams W the
number of bigrams and X the number of trigrams
Each word is padded by two dierent start symbols and two dierent end symbols
e joint probability for a word w of length n is calculated as
j log P wijwicid02 wicid01j
In the denominator I use the log probability instead of the probability to increase
numerical stability
Indeed multiplying very small numbers can lead to the result
being approximated as zero by the computer when the numbers become too small to
be represented as normalized number Goldberg 1991 Using the sum of logarithms
avoids this problem and is less computationally expensive Brgisser et al 1997
As the logarithm of a number approaching zero tends to innity rare observations
get a higher score than frequent observations As such the denominator can be seen
as a scale of rarity with a higher score corresponding to a rarer word By taking the
inverse of this scale we get a score corresponding to the commonness cid25 frequency
of a word
422 Training phase
First models are trained on training data in the relevant languages I have not included
the languages from the Pali dictionary data as there are too many dierent languages
Language
Amharic
Chinese
English
Finnish
Italian
Russian
Spanish
Turkish
Ukrainian
Size in MB
Table 1 Training data Size
and there are typically only small inclusions of dierent languages in a dictionary en-
try as such it would not have made sense to train a language model just to recognize
a single word Another reason for not using the Pali dictionary data languages is that
sometimes it is not possible to nd data for a language eg Old Bulgarian or recon-
structed Indogermanic In some cases it would have been conceivable to train models
on similar languages but again the eort of training a model is disproportionately
high compared to the uncertain result of recognizing a single inclusion Instead an
additional catch-all language model is used to capture words that do not seem to belong
to a trained model
e training data consists of Wikipedia dumps from the months June and July 2015
a dump is a copy of the whole encyclopedia for a given language Due to the dierence
in size of the Wikipedia of the dierent languages I choose the full dump for languages
with less than 3 GB of compressed data and limited the amount of data to maximally
3 GB of compressed data
e Wikipedia data was processed using the Wikipedia Extractor3 version 28 in
order to extract the textual content from the article pages Indeed the Wikipedia pages
are wrien using the MediaWiki Markup Language4 While this markup is useful for
meta-data annotation and cross-referencing the encoded information is superuous
for language model training and has to be removed before training a model on the
data Table 1 shows the size of the training data per language aer text extraction
3httpmedialabdiunipiitwikiWikipediaExtractor
4httpswwwmediawikiorgwikiHelpFormatting
As the test data only contains transliterated Amharic text the Wikipedia data writ-
ten in the Geez script had to be transliterated e text was transliterated according
to the EAE transliteration scheme by the Encyclopaedia Aethiopica
As the test data contains transliterated Greek the Greek data was used once as-is
and once transliterated according to the ELOT Hellenic Organization for Standardiza-
tion transliteration scheme for Modern monotonic Greek
It should be borne in mind that the training data inuences the quality and accuracy
of the model Furthermore a model might work well on certain text types and less well
on other text types It is not possible to train a perfect universal model
423 Application of the approa
In the second step an input text is segmented into words en each word is evaluated
by each language model and the model with the highest score is assigned as the words
language model
e approach taken consists in classifying words as either belonging to a trained
language model or to the additional catch-all model other which simply means that
the word could not be assigned to a trained model class
424 Textcat and language segmentation
I also tested how well Textcat is suited to the task of language segmentation e
approach is similar to the n-gram approach with the exception that I do not train any
models and rely on Textcats classier for language prediction
In the rst step an input text is segmented into words en each word is passed
to Textcat and the guess made by Textcat is taken as the words language
43 Unsupervised clustering
In order to test the eciency of clustering algorithms on the task of language segmen-
tation I looked at various algorithms readily available through WEKA a collection of
machine learning algorithms for data mining tasks by the University of Waikato in
New Zealand Hall et al 2009 and the Environment for Developing KDD-Applications
Supported by Index-Structures ELKI an open source data mining soware  with
an emphasis on unsupervised methods in cluster analysis and outlier detection by
the Ludwig-Maximilians-Universitt Mnchen Achtert et al 2013 I also looked at
JavaML a collection of machine learning and data mining algorithms Abeel et al
2009 in order to integrate clusterers into my own code framework JavaML oers dif-
ferent clustering algorithms and also oers access to WEKAs clustering algorithms In
contrast to WEKA and ELKI which can be used in stand-alone mode JavaML is meant
to be integrated into bigger programs and provides an application programming inter-
face API that allows the provided algorithms to be accessed in a programmatic way
ie from inside a program
431 Preprocessing
However in order for the clustering algorithms to work the document to segment has
to be preprocessed in a number of ways as shown in gure 16
Figure 16 Clustering preprocessor
Read input
Tokenize input
Has token
Normalize
Remove tags
is empty
Extract features
First of all the document has to be read in by the program is step is straightfor-
e document then has to be tokenized Tokenization is not trivial and depends on
the denition of a word For this task I have used a whitespace tokenizer that denes
a word as a continuous sequence of character literals separated by one or more whites-
pace characters While it can be objected that for scripts that dont use whitespace to
separate words such as Chinese tokenization fails this is not too big a concern In-
deed if a continuous block of Chinese characters is treated as one word it is likely to
be clustered separately due to the dierent in word length and the dierent charac-
ter set If however a document contains two scripts that do not separate words by
whitespace the approach totally fails It is beyond the scope of this thesis and possi-
bly of any thesis to implement a universal tokenizer that works regardless of language
without prior knowledge about the languages at hand
Each token is then normalized Normalization of a non-Latin-based input eg Ara-
bic or Cyrillic script returns the input without modication Otherwise the following
modications are made if applicable
 remove leading and trailing whitespace
 remove punctuation
 remove control characters
Control characters are dened as the set
Punctuation is dened as the set
  cid0
e token is then stripped of XML-like tags if applicable e following example
illustrates this step Let us assume we have the following token
lemma go goes  word
word i d 1
e token is replaced by the text content of the node thus the resulting token is goes
If aer all these modications the token corresponds to the empty string we con-
tinue with the next token Otherwise the token is passed on to the feature extraction
module e algorithm terminates when all tokens have been consumed
432 Dening features
e nal step consists in dening features by which to cluster and implementing fea-
ture extractors that build the feature vectors from the input Since the features are to
be language independent using features such as occurs in an English lexicon cannot
be used e following features were devised
1 word length the length of the word in characters
2 X tail bigrams bigrams calculated from the end of the word
3 Y tail trigrams trigrams calculated from the end of the word
4 X rst bigrams bigrams calculcated from the beginning of the word
5 Y rst trigrams trigrams calculated from the beginning of the word
6 latin basic is the word latin basic
7 latin extended is the word latin extended
8 capitalized is the word capitalized
9 contains non-word does the word contain a non-word
10 is non-word is the word a non-word
11 number of latin leers number of latin leers
12 number of non-latin leers number of non-latin leers
13 vowel ratio number of vowels divided by the word length
14 basic latin leer ratio number of latin leers divided by the word length
15 max consonant cluster the longest consonant cluster size in characters
16 is digit is the word a digit
17 is ideographic is the word ideographic
18 directionality what directionality does rst character of the word have
19 is BMP codepoint does the word contain non-BMP characters
20 general type what is the general type of the rst character of the word
e last two features are based on the Java Character class is class provides
methods to check for specic implementation-based properties of characters
While most features are rather self-explanatory a few require further explanation
For the n-grams the number of n-grams is restricted so as to keep the resulting vec-
tors the same size is is important because the clustering algorithm considers one
data column as one feature and having vectors of dierent length would disrupt this
precondition Implementing the comparison of vectors of dierent lengths or rather
or vectors containing vectors as features would have been possible but rather time-
consuming If a word is too short to generate the required number of n-grams only
the possible n-grams are generated and all other positions lled with 0
e latin features check whether the word consists only of the basic latin leers
A-Z and a-z basic while the extended feature also covers leers derived from the
latin leers eg    
Non-words are dened as anything not consisting of leers such as punctuation
marks or digits
Directionality indicates which direction a character should be wrien While the
actual list is much more exhaustive this property basically indicates whether the char-
acter is wrien from le to right or from right to le 5
BMP stands for Basic Multilingual Plane and refers to an encoding unit known as
plane which consists of 216  65536 codepoints ie encoding slots for characters
e Unicode Consortium 2014 e BMP is the rst plane covering the codepoints
U0000 to UFFFF e Unicode Consortium 2014 While it is not important to un-
derstand the technical details fully it is interesting to note that most characters are
covered by the BMP including Chinese Japanese and Korean characters e Unicode
Consortium 2014 e next plane called Supplementary Multilingual Plane or Plane
1 contains historic scripts such as Egyptian hieroglyphs and cuneiform scripts but also
musical notation game symbols and various other scripts and symbols e Unicode
Consortium 2014 ere are 17 planes in total e Unicode Consortium 2014
e last feature in the list General Type is also an implementation-related property
Type can be for example5 ENDPUNCTUATION LETTERNUMBER or
MATHSYMBOL ese constants are represented as numbers internally which are
taken as feature for the clustering algorithm
433 Mapping features to a common scale
As JavaML requires numerical features all features were mapped to numerical scales
 Binary features were mapped to 0 false and 1 true
 Ternary features were mapped to 0 false 1 true and 99 not applicable
 Numerical features were represented as themselves either as whole numbers
eg word length or as oating point numbers eg vowel ratio
 Java specic features 1820 take the underlying numerical value as feature
 N-grams were encoded numerically using algorithm 1
5e full
hpdocsoraclecomjavase7docsapijavalangCharacterhtml
can be found under
the documentation of
the Java Character
sum   0
for character in word do
value  code-point of character
sum   sum  value
Algorithm 1 N-gram numerical encoding
1 function word
8 end function
end for
return sum
While algorithm 1 does not encode n-grams in an unambiguous way en and ne
are both encoded as 211 it provides a suciently good encoding
434 e problem of unambiguous encoding
I have tried using unambiguous encodings e main problem with unambiguous en-
coding is that the notion of distance is distorted e idea behind the unambiguous
encoding is that each word ie string of characters is encoded numerically so that
no two words are represented as the same number Besides the encoding of each sep-
arate character the position of the character inside the string also has to be encoded
A possible encoding e for a string w1w2w3 could be
ew1w2w3  nw1  x cid3 nw2  y cid3 nw3
with wi the character of the string at position i nwi the numerical encoding of
the character wi and x and y parameters If jAj is the alphabet size of the alphabet A
in which the word is encoded the following constraints must be true for the encoding
to be unambiguous
x cid21 jAj
y cid21 jAj2
If we take for example the English alphabet with 26 lowercase and 26 uppercase
leers not counting punctuation digits and other characters it has to be true that
x cid21 52 and y cid21 2704 e problem is that we cannot know in advance what size
the alphabet will be If we have English and German texts the size can be estimated
around 60 However if we have English Russian and Arabic text the size drastically
increases We could choose any two very big numbers but if we want to guarantee
our encoding to be unambiguous we run the risk of ending up with numbers too big
to be represented eciently
In this encoding scheme distance is skewed changes to the rst character result
man and nan have a distance of 1 because m and n have a
in linear distance
distance of 1 man and lan have a distance of 2 etc Changes to the second character
are multiplied by x man and men have a distance of x cid3 distancea e  4 cid3 x
Changes to the third character are scaled by y For any suciently big x and y the
distances are too skewed to be used for automatic cluster analysis Let us consider the
following example with only two characters for simplicity For this example let us
assume x  1373
Table 2 Unambiguous encoding distances
It should be apparent from table 2 that the notion of distance is distorted
comparison table 3 shows the encoding achieved with algorithm 1
Table 3 Simplied encoding distances
While this encoding is not unambiguous it is considered suciently good for our
purposes
435 e clusterer
Most clustering algorithms such as k-means need to be passed the number of clusters to
generate As we want to work as exibly as possible I ignored all algorithms that need
the number of clusters before clustering In contrast the x-means algorithm Pelleg
and Moore 2000 estimates the number of clusters to generate itself is algorithm
has been chosen to perform the language clustering tasks
While WEKA and ELKI oer a graphical user interface and various graphical rep-
resentations of the results the output is not easily interpretable Indeed we can get a
visualization of a clustering operation as shown in gures 17 WEKA and 18 ELKI
However all data points have to be manually checked by either clicking each point
in order to get additional information about that data point WEKA or by hovering
over the data points aer having selected the Object Label Tooltip option ELKI Fig-
ure 18 shows the information for the lowest orange rectangle data point in the ELKI
visualization
Figure 17 WEKA Cluster visualization
Figure 18 ELKI Cluster visualization
erefore I have decided to embed the x-means clustering algorithm into a custom
framework Originally part of the WEKA algorithms the x-means algorithm has been
integrated into a Java program via the JavaML library e framework takes an input
le constructs the aforementioned feature vectors from the input performs normal-
ization passes the calculated feature vectors to the clustering algorithm and displays
the results in a text-based easily interpretable manner
Preliminary analyses have shown that the rst clustering result oen is not dis-
criminating enough Hence I perform a rst clustering analysis followed by a second
clustering analysis on the clusters obtained from the rst analysis
436 Evaluating clusterings
e clustering results are evaluated using four common similarity measures used in
evaluating the accuracy of clustering algorithms ese methods are based on counting
pairs Wagner and Wagner 2007
Let us consider the clustering C  fC1     Ckg C is a set of non-empty disjoint
 fC1     Clg We
clusters C1     Ck Let us consider the reference clustering C
dene the following sets
 S11 set of pairs that are in the same cluster in C and C
 S00 set of pairs that are in dierent clusters in C and C
 S10 set of pairs that are in the same cluster in C and in dierent clusters in C
 S01 set of pairs that are in dierent clusters in C and in the same cluster in C
Let nij  jSijj with i j 2 f0 1g be the size of a given set Sij
e Rand Index is dened as
n11  n00
n11  n10  n01  n00
e Rand Index measures the accuracy of the clustering given a reference partition
Wagner and Wagner 2007 However it is criticized for being highly dependent on
the number of clusters Wagner and Wagner 2007
e Jaccard Index measures the similarity of sets It is similar to the Rand Index
but it disregards S00 the set of pairs that are clustered into dierent clusters in C and
 Wagner and Wagner 2007 It is calculated as
e Fowlkes-Mallows Index measures precision It is calculated as
n11  n10n11  n01
n11  n10  n01
e Fowlkes-Mallows Index has the undesired property of yielding high values
when the number of clusters is small Wagner and Wagner 2007
Finally I will indicate the F-Score According to Manning et al 2008 in the context
of clustering evaluation the Fcid12 score is dened as
F cid12 
cid122  1 cid3 P cid3 R
cid122P  R
with precision P and recall R dened as
n11  n10
n11  n01
By varying cid12 it is possible to give more weight to either precision cid12  0 or recall
cid12  1 Manning et al 2008 As I value recall higher than precision I will indicate F1
cid12  1 and F5 cid12  5 scores Indeed I want to penalize the algorithm for clustering
together pairs that are separate in the gold standard while not penalizing the algorithm
for spliing pairs that are together in the gold standard
All measures of similarity fall between 0 1 with 0 being most dissimilar and 1 be-
ing identical As there is no ultimate measure and all measures of similarity have their
drawbacks Wagner and Wagner 2007 all measures will be indicated in the results
section
44 Weakly supervised language model induction
e language model induction approach works in two stages In the rst stage n-gram
language models are induced from the text In the second stage the text is mapped to
the induced models e algorithm for the language model induction is as follows
Algorithm 2 Model induction
1 IM
2 for word in words do
12 end for
modelAndScore   MSword
score   modelAndScorescore
if score  threshold then
model   Mword
modelsaddmodel
maxM odel   modelAndScoremodel
maxM odelupdateword
First of all an initial language model is created For each word the maximum model
and maximum score is calculated ese values correspond to the language model that
yielded the highest probability for the word in question and the associated probability
If the score falls below a threshold t ie none of the existing language models model
the word well enough a new language model is created on the basis of the word and
added to the list of language models Otherwise the top scoring language model is
updated with the word in question
As the text structure itself inuences the quality of the induced models the lan-
guage model induction is run i times i 6 1 with one iteration consisting of two
induction steps once forward and once backward and j times from a random position
j 6 0 e initial model creation thus either picks the rst word of the text as shown
in algorithm 3 line 2 or the last word of the text or a random word
Algorithm 3 Initial model creation
1 function IM
5 end function
word   wordsf irst
model   createM odelword
modelsaddmodel
maxScore   0
maxM odel   none
for model in models do
Algorithm 4 Max model and max score
1 function MSword
12 end function
score   modelprobabilityword
if score  maxScore then
maxScore   score
maxM odel   model
end for
return maxM odel maxScore
Algorithm 4 returns both the max model and the max score wrapped as a custom
object e individual values can then be read as necessary
Aer the models have been induced the most similar models are merged based
on distributional similarity Distributional similarity is calculated as explained below
is merging step only merges one model from the forward induction group with one
model from the backward induction group e resulting model is added to the set of
probable silver models
Merging is performed according to algorithm 5 e merging algorithm only re-
tains the common set of unigrams from both models and all resulting bi- and trigrams
excluding any bi- and trigrams that contain character that occur only in one of the
models e values for the resulting language model are calculated according to one
of four dierent merge modes
e merge modes are
 f u1 is the frequency of u1
 or u2 since both are equal
 frequency of b in model1
 or 0 if it does not exist
for unigram u2 in model2unigrams do
end for
merged   
for unigram u1 in model1unigrams do
if u1  u2 then
v1   f u1
v2   f u2
value   modev1 v2
unigram   u1
merged   unigram value
exclude   u1
exclude   u2
Algorithm 5 Model merger
1 function model1model2 mode
35 end function
v1   f b model1 or 0
v2   f b model2 or 0
value   modev1 v2
merged   b value
v1   f t model1 or 0
v2   f t model2 or 0
value   modev1 v2
merged   t value
end for
return merged
end for
for all bigrams b in model1 and model2 do
if not exclude contains any char in b then
end for
for all trigrams t in model1 and model2 do
if not exclude contains any char in t then
 MAX use the maximum value maxv1 v2
 MIN use the minimum value minv1 v2
 MEAN use the mean value  v1v2
 ADD use the sum of the values v1  v2
If the random iteration count j  0 a random word is chosen and the induction
is run once forward and once backward starting from this position en the most
similar models from each set are merged and added to the set of probable models
It should be noted that seing the parameter j  0 will make the algorithm non-
deterministic
e model induction is then repeated while the iteration count i has not been
reached or until no more models are induced with the dierence that for each word
each probable model is rst consulted If any of the probable models yields a score
higher than the threshold value t it is assumed that the word is already well repre-
sented by one of the probable models and no models are induced for this word If the
score falls below the threshold value t induction is run as described
At the end of the induction loop all probable models are checked against each other
While there are two models that have a similarity below the silver threshold value s
the two models are merged and added to the set of very probable gold models
If the set of probable models is not empty aer this merging step all remaining
probable models are added to the set of very probable models
In the second stage the text is segmented according to the induced gold models
For each word the language model with the highest probability for the word is chosen
as that words hypothetical language model
441 Distributional similarity
Suppose we have three models with the distributions of leers as shown in gures 19
20 and 216 Similarity could be calculated based on the occurrence of unigramsleers
alone ie if model1 contains the leer a and model2 also contains the leer a their
similarity increases by 1
However if we calculate similarity in such a way all three models are equally simi-
lar to each other as each of the leers occurs at least once in each model Yet it should
be clear that models 1 and 2 are very similar to each other while model 3 is dissimilar
erefore in order to include the distribution of leers in the similarity measure
similarity is calculated as shown in algorithm 6
6e gures shown are used for illustration purposes only and do not necessarily reect real language
models
Figure 19 Language model Distribution 1
Figure 20 Language Model Distribution 2
Figure 21 Language model Distribution 3
 Initialize dierence to 1 to avoid division by zero
 unigram occurs in both models
 Normalize value by model size
for unigram u2 in model2unigrams do
similarity   0
dierence   1
for unigram u1 in model1unigrams do
Algorithm 6 Distributional Similarity Calculation
1 function model1model2
17 end function
if u1  u2 then
v1   f u1
v2   f u2
q   jv1cid0v2j
similarity   similarity 2 cid0 q
dierence   dierence 1
model1size
model2size
end for
end for
return similarity
dierence
with f c returning the frequency of the character c e number 2 in 2cid0q in line
10 can be explained as follows q expresses the dissimilarity of the models with regard
to a unigram distribution with 0 6 q 6 1 hence 1 cid0 q expresses the similarity To
this we add 1 as we increase similarity by 1 due to the match we augment the simple
increase of 1 by the similarity of the distribution
442 Evaluating results
e results of this approach can be interpreted as clusters where each language model
represents one cluster core and all words assigned to that model making up that cluster
Evaluation will hence be analogous to the evaluation of the clustering approach
443 Estimating the parameters
As the language model induction can be controlled by parameters we have to nd a
combination of parameters that works well for our task e parameters i j and merge
mode have been estimated on the development set e development set contains
similar documents to those in the test set e development set can be found in the
appendix
It has been found that the parameter combination i  4 j  2 ADD yields good
results across the development set Hence these values have been used for the test set
evaluation
5 Results
Baseline indicates the measurement where all words have been thrown into one clus-
ter measured against the gold standard For Baseline 2 every word has been put into
its own cluster and this clustering is evaluated against the gold standard e column
F1 stands for the F1 score and the F5 column stands for the F5 score
If any of the runs yields a higher score than any of the baseline values the max-
imum score is indicated in bold
If a eld contains na this means that the value
could not be calculated for whatever reason most oen a division by zero would have
occurred
51 N-Gram language model
Jaccard
Fowlkes-
Mallows
GermanEnglish
Baseline
Baseline 2
GermanFinnishTurkish
Baseline
Baseline 2
EnglishFren
Baseline
Baseline 2
EnglishTransliterated Greek
Baseline
Baseline 2
ItalianGerman
Baseline
Baseline 2
Table 4 N-Gram language model results Latin script
Jaccard
Fowlkes-
Mallows
GreekRussian
Baseline
Baseline 2
EnglishGreek
Baseline
Baseline 2
EnglishSpanishArabic
Baseline
Baseline 2
EnglishChinese
Baseline
Baseline 2
UkrainianRussian
Baseline
Baseline 2
Table 5 N-Gram language model results Mixed script
Jaccard
Fowlkes-
Mallows
Baseline
Baseline 2
Baseline
Baseline 2
Baseline
Baseline 2
Baseline
Baseline 2
Baseline
Baseline 2
Table 6 N-Gram language model results Pali data
Twitter 1
Baseline
Baseline 2
Twitter 2
Baseline
Baseline 2
Twitter 3
Baseline
Baseline 2
Twitter 4
Baseline
Baseline 2
Twitter 5
Baseline
Baseline 2
Jaccard
Fowlkes-
Mallows
Table 7 N-Gram language model results Twier data
52 Textcat
Jaccard
Fowlkes-
Mallows
GermanEnglish
Baseline
Baseline 2
Textcat
GermanFinnishTurkish
Baseline
Baseline 2
Textcat
EnglishFren
Baseline
Baseline 2
Textcat
EnglishTransliterated Greek
Baseline
Baseline 2
Textcat
ItalianGerman
Baseline
Baseline 2
Textcat
Table 8 Textcat results Latin script
Jaccard
Fowlkes-
Mallows
GreekRussian
Baseline
Baseline 2
Textcat
EnglishGreek
Baseline
Baseline 2
Textcat
EnglishSpanishArabic
Baseline
Baseline 2
Textcat
EnglishChinese
Baseline
Baseline 2
Textcat
UkrainianRussian
Baseline
Baseline 2
Textcat
Table 9 Textcat results Mixed script
Jaccard
Fowlkes-
Mallows
Baseline
Baseline 2
Textcat
Baseline
Baseline 2
Textcat
Baseline
Baseline 2
Textcat
Baseline
Baseline 2
Textcat
Baseline
Baseline 2
Textcat
Table 10 Textcat results Pali data
Jaccard
Fowlkes-
Mallows
Twitter 1
Baseline
Baseline 2
Textcat
Twitter 2
Baseline
Baseline 2
Textcat
Twitter 3
Baseline
Baseline 2
Textcat
Twitter 4
Baseline
Baseline 2
Textcat
Twitter 5
Baseline
Baseline 2
Textcat
Table 11 Textcat results Twier data
53 Clustering
e rst run indicates the value aer one clustering step and the second run indicates
the value aer applying the clustering algorithm to the results of the rst run
Jaccard
Fowlkes-
Mallows
GermanEnglish
Baseline
Baseline 2
First run
GermanFinnishTurkish
Baseline
Baseline 2
First run
EnglishFren
Baseline
Baseline 2
First run
EnglishTransliterated Greek
Baseline
Baseline 2
First run
ItalianGerman
Baseline
Baseline 2
First run
Table 12 Clustering results Latin script
Jaccard
Fowlkes-
Mallows
GreekRussian
Baseline
Baseline 2
First run
EnglishGreek
Baseline
Baseline 2
First run
EnglishSpanishArabic
Baseline
Baseline 2
First run
EnglishChinese
Baseline
Baseline 2
First run
UkrainianRussian
Baseline
Baseline 2
First run
Table 13 Clustering results Mixed script
Jaccard
Fowlkes-
Mallows
Baseline
Baseline 2
First run
Baseline
Baseline 2
First run
Baseline
Baseline 2
First run
Baseline
Baseline 2
First run
Baseline
Baseline 2
First run
Table 14 Clustering results Pali data
Jaccard
Fowlkes-
Mallows
Twitter 1
Baseline
Baseline 2
First run
Twitter 2
Baseline
Baseline 2
First run
Twitter 3
Baseline
Baseline 2
First run
Twitter 4
Baseline
Baseline 2
First run
Twitter 5
Baseline
Baseline 2
First run
Table 15 Clustering results Twier data
54 Language model induction
In addition to highlighting results that outperform the baseline values the following
tables have been color coded Results that outperform the clustering algorithm are
indicated in red and results that outperform both the clustering algorithm and the n-
gram language model are indicated in blue7
Jaccard
Fowlkes-
Mallows
GermanEnglish
Baseline
Baseline 2
Inducted
GermanFinnishTurkish
Baseline
Baseline 2
Inducted
EnglishFren
Baseline
Baseline 2
Inducted
EnglishTransliterated Greek
Baseline
Baseline 2
Inducted
ItalianGerman
Baseline
Baseline 2
Inducted
Table 16 Induction results Latin script
7Results that outperform only the n-gram language model would have been indicated in green but
there is no score that outperforms only the n-gram language model
Jaccard
Fowlkes-
Mallows
GreekRussian
Baseline
Baseline 2
Inducted
EnglishGreek
Baseline
Baseline 2
Inducted
EnglishSpanishArabic
Baseline
Baseline 2
Inducted
EnglishChinese
Baseline
Baseline 2
Inducted
UkrainianRussian
Baseline
Baseline 2
Inducted
Table 17 Induction results Mixed script
Jaccard
Fowlkes-
Mallows
Baseline
Baseline 2
Inducted
Baseline
Baseline 2
Inducted
Baseline
Baseline 2
Inducted
Baseline
Baseline 2
Inducted
Baseline
Baseline 2
Inducted
Table 18 Induction results Pali data
Jaccard
Fowlkes-
Mallows
Twitter 1
Baseline
Baseline 2
Inducted
Twitter 2
Baseline
Baseline 2
Inducted
Twitter 3
Baseline
Baseline 2
Inducted
Twitter 4
Baseline
Baseline 2
Inducted
Twitter 5
Baseline
Baseline 2
Inducted
Table 19 Induction results Twier data
6 Discussion
e work by Seldin et al 2001 is similar to the work presented here ey propose
an unsupervised language and protein sequence segmentation approach that yields
accurate segmentations While their work looks promising it also has its drawbacks
eir method requires longer monolingual text fragments and a sizable amount of text
Furthermore they disallow switching language models aer each word is presump-
tion will fail to detect single-word inclusions and structures as shown in gure 22
where the language alternates aer each word
Figure 22 Alternating language structure
While this structure looks very articial such a structure is found for instance
in the h Pali dictionary text in the passage Pacati Ved pacati Igd peq Av
pac- In this case red corresponds to Pali blue to abbreviations in English and
green to reconstructed Indo-european
61 N-Gram language models
e trained n-gram language model approach works well on the Latin script data man-
aging to single out the German inclusion from the EnglishGerman text even though
it is classied as other instead of German
For GermanFinnishTurkish EnglishFrench EnglishTransliterated Greek and
ItalianGerman the separation of the main languages involved is good although there
appear to be some problems when words contain non-word characters such as quotes
or parentheses
Some puzzling misclassications happen in the EnglishTransliterated Greek case
agpe is considered English and ros is considered Transliterated Amharic
In the ItalianGerman text the Italian language leads to a rather important Spanish
cluster due to the relatedness of the two Romance languages
On the mixed script data set the results are more diverse GreekRussian En-
glishSpanishArabic and UkrainianRussian are segmented well with English Span-
ishArabic having Spanish split into Spanish French and Italian due to the relatedness
of the languages
In contrast the segmentation of EnglishGreek did not work well at all Of the two
Greek words  and   was considered French and  was considered
Russian It must be noted though that these words bear polytonic diacritics whereas
the model was trained on monotonic Greek
Also the segmentation of EnglishChinese did not work well is is probably
due to the way the model was trained Chinese script is wrien without whitespace
characters between words and the correct segmentation of a text wrien in Chinese
requires in-depth knowledge of the language Some words are wrien with only one
character but others are composed of two or more characters with the meaning oen
being non-compositional the meaning of a two-character word is dierent from the
sum of the meaning of the two characters Sometimes more than one segmentation
would be possible and the context decides on which segmentation is correct In other
cases more than one segmentation might be correct is problem occurs with all
scripts that are wrien without whitespace
As with the simplied assumption in the tokenization of whitespace-scripts where
I consider a word to be a character sequence delineated by whitespace I have treated
each character as a word Adapting the method to Chinese and similar scripts would
have been possible but would have introduced the need for large amounts of external
linguistic knowledge Indeed every possible non-whitespace-script would have to be
considered and each of the tokenizers would be language dependent ie a tokenizer
for Chinese would not work on Korean or Japanese
e supervised approach did not work well on the Pali dictionary data While
English words could be isolated somewhat successfully the rest of the data proved
dicult to segment As an example let us look at the rst Pali text e English
cluster contains almost only English words but not all the other cluster contains
mainly marked up words and the rest is seemingly haphazardly distributed among
the other models
Pali 1 abbha
 AR  134 289
 DE Miln imber dark Miln
 EL  abbha
 EN water mountain of free used or like referred also A is cloudy
clouds later a froth 1 summit thundering by mass Pv Oir obscure scum
that water thick As from It is at as the in clouds things also
 ES dense f sense expl rajo
 FI 239 rain Lat Vin perhaps SnA
 FR cloud Dh adj point cloud Dhs A rain VvA DhsA list
 IT dark  ambha 3 1 317 J sunshine cp abhra Vedic megho
 PL 487  S 295 br moon 249
 RU 348 53
 TR viz ambu Vv
 TrAM 687 PvA sama 101 nl cp 64 nt 581 m Sn 1064
 TrEL  Gr Sk Idg to pabbata nt
 UK 12 273 617 348 250 251 382
 other b savilpa b b mua b smallcaps vi smallcaps
mahiy smallcaps iv smallcaps cloud b Rhu b b abbh
b b abbha superscript 9 superscript marajo b abbhmua
valhaka smallcaps i smallcaps b abbhmaa b valhakasikhara
superscript s superscript smallcaps ii smallcaps b dh- storm
cloud b ka b thundercloud at afros at b paala b
atombrosat nlamegha superscript1superscript m bhro dull
acchdesi mahikb b ghana b
On the Twier data the supervised approach achieved passable results While
the numbers look great the actual segmentations do not For Twier 1 too many
clusters were generated for Twier 2 and 3 the recognition of French words worked
somewhat also recognizing English words as French and French words as English
For Twier 4 the Polish inclusion was isolated but recognized as other together
with strawberries e recognition of transliterated Amharic worked satisfactorily
yielding naw to the Polish model
As the number of language models increases so does the risk of misclassication
As can be seen we already have quite some misclassication with only 15 language
models For example in our data the English preposition to is oen erroneously clas-
sied as transliterated Greek e Greek particle  to can be either the neuter sin-
gular accusative or nominative denite article the the masculine singular accusative
or nominative denite article the or the 3rd person neuter singular nominativeac-
cusative weak pronoun it and as such is rather frequent in the language is is
especially problematic with the transliterated Greek language model which tends to
misclassify the English preposition to as transliterated Greek
A quick corpus study using the Corpus of Modern Greek8 and the Corpus of Con-
temporary American English9 reveals that the frequency per million words for the
Greek particle  is 22666 while the English preposition to has a frequency per mil-
lion words of 25193 eir relative frequencies are very close together and it might
8httpweb-corporanetGreekCorpus
9httpcorpusbyueducoca
just have happened that the training data used in this work contained more Greek tos
than English tos leading to this misclassication
Other reasons for misclassication include relatedness of the modeled languages
as in the case of Germanic or Romance language families Also the text types used
for training and the text types used for testing play an important role as well as the
amount of training data
For n-gram language models the quality of the model is dependent on the texts
used for training and the texts used in evaluation It is probable that a dierent training
set would have yielded dierent results is is also the problem with the supervised
approach it is necessary to have language data for training and the trained models
reect the training data to some extent
62 Textcat
Textcat works well on monolingual texts However it fails on multilingual texts and
does not work well on short fragments of text such as single words Many of the words
are tagged as unknown and if a language has been identied the language guess oen
is not correct Hence Textcat cannot be used for language segmentation purposes
Indeed Textcat fails to exceed the baseline values except for two cases Twier
3 and Twier 4 yield beer values than the baseline values However upon closer
inspection it is clear that the numerical index values do not give a reliable picture of
the quality of the clustering
Indeed while the clustering of Twier 3 is not nonsensical it is not very good
failing to extract the French insertion breuvages e Rand Index also only shows a
slightly beer value than the baseline values It seems that the outstanding score for
Twier 4 is achieved because both the clustering by Textcat and the gold standard
have the same number of clusters
Tables 20 and 21 show the clusterings side by side Clearly Textcat performed
poorly despite the high numerical index values A closer inspection of all the Textcat
results shows that Textcat performs poorly at the task of language segmentation oen
a word cannot be assigned a language and thus is added to the cluster of unknown
language words For the words where a language has been identied it most oen
is not the correct language While language identication is not necessary for the
task of language segmentation it helps to understand why Textcat failed at the task of
language segmentation
Cluster 1
Cluster 2
Cluster 3
Textcat
bilingualism
Food and breuvages in Ed-
monton are ready
to go
just waiting for the fans
FWWC2015
Gold standard
breuvages
FWWC2015 bilingualism
Food and in Edmonton are
ready to go just waiting for
the fans
Table 20 Twier 3 Textcat versus Gold clustering
Textcat
strawberries
Cluster 1
Cluster 2 my dad comes back from
poland with two crates of
ubrwka and adidas jack-
ets omg
Gold standard
ubrwka
my dad comes back from
poland with
crates
of strawberries and adidas
jackets omg
Table 21 Twier 4 Textcat versus Gold clustering
63 Clustering
e clustering results are more dicult to interpret Oen the rst distinction made
seems to be based on case ie words that begin with a capital leer versus words that
are all lowercase leers e second run on the mixed script English  Greek data
shows that the rst cluster from the rst run has been separated into a cluster with
words that begin with a capital leer and two clusters with words that dont begin
with a capital leer
EnglishGreek First run First cluster
 intimate without Although Aquinas Christians Corinthians Socrates Sym-
posium Testament Whether aection ancient another appreciation aspires
araction araction becomes benevolence biblical brotherly chapter char-
ity children children contemplation content continues contributes deni-
tion described existence explained express feeling feelings nding further
holding initially inspired knowledge marriage necessary non-corporeal pas-
sage passion philosophers physical platonic rened relationships returned
self-benet sensually spiritual subject suggesting through throughout tran-
scendence unconditional understanding without youthful
EnglishGreek Second run Splitting of rst cluster
 aection ancient another aspires becomes biblical chapter charity chil-
dren children content denition feeling feelings nding holding marriage
necessary passage passion platonic rened returned subject through with-
 Although Aquinas Christians Corinthians Socrates Symposium Testament
Whether
 intimate appreciation araction araction benevolence brotherly contem-
plation continues contributes described existence explained express further
initially inspired knowledge non-corporeal philosophers physical relation-
ships self-benet sensually spiritual suggesting throughout transcendence
unconditional understanding youthful
Indeed the results
Another important distinction seems to be the length of words
oen show clusters that clearly are based on the length of the contained words e
rst run on the latin script German  Italian data shows that short words have been
singled out into the rst cluster
ItalianGerman First run First cluster
 il E So a ad da di e es ha i il in la le lo ma ne se si un va zu
e clustering works well when the scripts involved are dissimilar as in the case
of the EnglishChinese text where the Chinese characters were isolated aer the rst
run and also the EnglishSpanishArabic example where the Arabic part was com-
pletely isolated in the rst run
e closer the scripts become the less well clear cut the results are For Greek
Russian the results are acceptable with one mixed cluster However the number of
clusters is too high for the number of languages involved and the separation is only
achieved aer two consecutive clusterings
e clustering of closer scripts such as UkrainianRussian does not work well e
clusters with the exception of the cluster containing the datum 913 are all impure
consisting of Ukrainian and Russian words e second run also fails at improving the
clustering
Finally clustering of latin based scripts does not perform well unless diacritics are
involved and the diacritics form the most salient distinction Word containing leers
with diacritics are then generally separated from words containing no diacritics as in
the GermanFinnish-Turkish example e rst run generates a cluster for numbers
two clusters with diacritics and one cluster without diacritics
Probably for this reason the clustering of Transliterated GreekEnglish and Greek
In both cases the rst run managed to separate
English worked surprisingly well
the transliterated Greek parts from the English words However unaccented Greek
words such as Agape erotas or eros were clustered with English
EnglishTransliterated Greek First run Transliterated Greek cluster
 agpe phila storg ros
EnglishGreek First run Greek cluster
   Agpe agp ros rs 
e problem is that when there are other salient distinguishing features besides
diacritics the result is less good as can be seen on the Pali data
Pali abhijjhitar Second run
 abhijjhita abhijjhtar covets function med one who itar itar tar
 T A M
  l v
 smallcaps i smallcaps smallcaps v smallcaps ag fr in
 265 287
 n
In some cases the clustering fails at the task of language segmentation as in the
case of the various EnglishFrench texts and the EnglishGerman example with the
German inclusion We can thus say that the surface structure or morphology or in
other words the basis from which we can extract features is not sucient to deduce
relevant information about language
When there are more than two languages that are to be separated the cluster-
ing also does not work well Indeed the most dissimilar objects are separated rst
In the case of EnglishSpanishArabic the Arabic part is separated rst as well as
words with diacritics while English and Spanish words without diacritics are thrown
together Subsequent runs show no improvement of the clustering concerning the
separation of English and Spanish
In the case of GermanFinnishTurkish the clustering algorithm seems to cluster
out Turkish rst followed by Finnish e results are however much less clear-cut
than for EnglishSpanishArabic
64 Language model induction
e language model induction does not seem to work very well on the Latin script data
ere are almost only impure clusters containing more than one language However
the approach consistently outperforms the clustering approach when we look at the
F5 score For the EnglishFrench data set the clustering approach even outperforms
the n-gram language model approach Indeed the French words are relatively well
separated from the English text with the exception of sucr which is still thrown
together with English words
Latin script EnglishFren
 both so in English although their is is the opposite of rough or is
the opposite of sweet only for wines otherwise is
 mou  mou but
 doux
 Doux rugueux Doux
 while
 hard used
 translate as meaning very dierent coarse can also mean almostsucr
In contrast the approach works well on the mixed script data Indeed we achieve
a good separation of the languages by script However when there are also Latin
based scripts we encounter the same problems as mentioned above with rather modest
results For example for the EnglishGreek text the approach separates out the Greek
character words but it fails to separate transliterated Greek and English Also for the
EnglishSpanishArabic text Arabic is separated out but English and Spanish are not
separated well
One interesting observation can be made in the case of the EnglishChinese text
e Chinese characters have been isolated but the Pinyin transcription is thrown to-
gether with the Chinese characters Based on the prior observations this is rather
unexpected is raises the question of whether Pinyin ought to be clustered out or
clustered together with English or Chinese
Again the language model induction approach outperforms the clustering approach
and also the n-gram language model approach in the case of the EnglishGreek text
On the larger Pali dictionary entries the language model induction approach yields
acceptable results On the shorter Pali dictionary entries the language model induction
approach yields good results
e quite low performance must be blamed on the data Indeed the Pali dictionary
data contain various problematic characters such as commadot and whitespace as
one character On such characters whitespace tokenization fails yielding big chunks
of nonsense tokens For example the fourth Pali dictionary entry was split into ve
chunks while it might not be displayed as such all commata and all dots are in fact
not followed by whitespace the whitespace is part of the character10 hence whitespace
tokenization fails
Pali ghan Chunks
 Ghanf
 abstrfrghatighan
qv
 Pug19CppariPage
 253
Furthermore the data contains markup abbreviations references typing mistakes
and signs such as - that are dicult to assign to a language
On the Twier data the language model induction approach works rather well
For example on the rst text separation is not perfect with the Greek cluster still
containing some English words
Twitter 1 EnglishGreek
 BUSINESS EXCELLENCE
      Internet of  
 ings IT
For the third and fourth text the approach manages to single out the other-language
inclusions but not exclusively Both times there is one additional item in the cluster
the relevant clusters are marked in red
10e comma has the Unicode codepoint UFF0C FULLWIDTH COMMA and the dot has the Uni-
code codepoint UFF0E FULLWIDTH FULL STOP
Twitter 3 FrenEnglish
 FWWC2015
 breuvages go
 Food Edmonton to for the
 in waiting bilingualism
 and are ready just fans
Twitter 4 EnglishPolish
 comes from with two crates of strawberries jackets omg
 my dad poland and adidas
 back ubrwka
e approach exceeded expectations on the second and h Twier text On the
second text the French cluster does not only contain the French words Demain and
par but also the French way of notating time 18h
Twitter 2 FrenEnglish
 Keynote e collective of science-publish or perish it all that counts
 Demain 18h par
 dhiha6 David
 dhiparis dynamics is
On the h text an almost perfect result was achieved with only one additional
subdivision of the English cluster
Twitter 5 Transliterated AmharicEnglish
 coee
 bread is our
 Buna dabo naw
It seems that the language model approach does not work very well on longer texts
especially on longer texts in Latin-based scripts with the chosen parameter set still
the approach outperforms the clustering approach and achieves scores in the vicinity
of the scores achieved with the supervised trained n-gram language model approach
On mixed script texts the approach consistently outperforms the clustering approach
and we also reach scores in the vicinity of the scores achieved with the supervised
trained n-gram language model approach
Moreover on short texts the approach works rather well We succeed in outper-
forming the supervised trained n-gram language model approach on a number of texts
and we achieve scores close to the scores achieved with the supervised trained n-gram
language model approach
Although the language model induction approach tends to generate too many clus-
ters it also generally succeeds at separating the languages involved
Of the scores I used for evaluation purposes it seems that a combination of a high
Rand Index and a high F5 score indicate a good language segmentation A high F5
score alone is not signicant For example the clustering algorithm achieves an F5
score of 07215 on Twier 3 is score looks good but the Rand Index score is at
04571 and the segmentation is not good
Twitter 3 Cluster analysis
 Edmonton Food
 go in to
 and are breuvages fans for just ready the waiting
Similarly a high Rand Index score alone is not signicant For example the clus-
tering algorithm achieves a Rand Index score of 06738 on the Pali 2 text but the F5
score is at 03825 and the clustering is not good
Pali 2 Cluster analysis
 abhijjhita abhijjhtar covets function med one who itar itar tar
 T smallcapsismallcaps smallcapsvsmallcaps  A M ag fr in
 265 287
 n
7 Conclusion
In this thesis I have asked the question of whether unsupervised approaches to lan-
guage segmentation perform beer on short and dicult texts than supervised ap-
proaches by overcoming some of the diculties associated with supervised approaches
such as the need for enough and adequate11 training data the language-specicity of
the language model or the inexibility of trained language models when it comes to
spelling variation and abbreviations unless the training data also contained spelling
variation and abbreviations
I have given an overview over related work presenting supervised approaches that
have been used in monolingual language identication and the amelioration of such
approaches through unsupervised approaches such as clustering
Unfortunately the body of literature covering the topic of language segmentation
is sparse e work by Yin et al 2007 and the work by Seldin et al 2001 are closest
in topic to this thesis However Yin et al 2007 concern themselves with spoken
language with requires a dierent approach than dealing with wrien language As I
concentrated on wrien language their work was not conducive to this thesis
In contrast Seldin et al 2001 present a work that looks promising ey present
a system that nds language borders in a text with great accuracy using unsuper-
vised algorithms However they restrict their algorithm in such a way that switching
language models aer each word is disallowed us they are unable to detect single-
word inclusions and cannot handle situations where the language switches every word
as has been shown to occur in the test data used in section 4
Another major drawback of the approach is that it also needs longer fragments of
monolingual text and an overall longer text Hence their approach would not work
well on short texts if at all
Next I have presented the theoretical foundations of a supervised n-gram language
model approach and an unsupervised clustering approach Finally I have introduced a
weakly supervised n-gram language model inducing approach devised by myself All
of these approaches can be used for language segmentation In order to test how well
the dierent approaches perform on dierent text types I have performed experiments
I have rst compiled a small corpus
of texts ranging from longer texts with clearly separated languages to one-sentence
Twier messages containing foreign language inclusions I have also included a set
of dictionary entries from the Pali dictionary by the Pali Text Society Indeed these
entries contain a lot of dierent languages and abbreviations and unfortunately are
not consistently formaed
Section 4 presents the experiments made
I have then presented my implementations of the supervised and weakly super-
11e question of what is to be considered enough or adequate is another point of contention the
data always inuences the resulting models
vised approaches and the choice of the unsupervised clustering algorithms en I
have presented the results of their application to the data
It can be said that the supervised approach works reasonably well e drawbacks
are that the approach needs training data to train the models on e problems of the
training data and its inuence on the models have been raised more than once
e supervised approach failed for non-whitespace scripts e models would have
to be adapted for non-whitespace scripts introducing more complexity Also the
training and test texts would have to be split in meaningful ways introducing the
need for a vast array of language-specic text spliers should the approach work on
a wide range of languages
e unsupervised approach generally succeeded in separating languages by script
when dierent scripts were involved Other than that it seems that the chosen mor-
phological features or possibly morphological features in general are insucient for
the algorithm to separate languages eectively
e weakly supervised approach worked well on short texts and on dicult short
texts but less well on long texts while still outperforming the clustering approach
on long texts e approach consistently outperforms the clustering approach and
reaches scores in the vicinity of the scores achieved by the supervised approach even
surpassing the supervised approach in some cases ese results are promising but
more thorough investigations have to be undertaken
In conclusion it can be said that some unsupervised or weakly supervised ap-
proaches can perform beer on the task of language segmentation on dicult and
short texts e presented weakly supervised approach does not only outperform the
unsupervised clustering approach it also achieves scores comparable to the scores
achieved with the supervised approach
Future work could concentrate on the reduction of the number of generated clus-
ters ideally geing down to one cluster per language it would also be thinkable to
prevent overly frequent language model switching by taking a words context into
account Finally the parameters could conceivably be adapted automatically With
an increased interest in the area of multilingual text processing lately the emergence
and evolution of the texts themselves will inuence the direction of the work in that
direction
Il est venu le temps des cathdrales
le monde est entr
dans un nouveau millnaire
Lhomme a voulu monter vers les toiles
crire son histoire
dans le verre ou dans la pierre
 Gringoire
References
Abeel T de Peer Y V and Saeys Y 2009 Java-ML A Machine Learning Library
Journal of Machine Learning Research pages 931934
Achtert E Kriegel H Schubert E and Zimek A 2013
Interactive data mining
with 3D-parallel-coordinate-trees In Proceedings of the ACM SIGMOD International
Conference on Management of Data SIGMOD 2013 New York NY USA June 22-27
2013 pages 10091012
Alex B 2005 An unsupervised system for identifying English inclusions in German
text In Proceedings of the 43rd Annual Meeting of the Association for Computational
Linguistics ACL 2005 Student Research Workshop pages 133138 Association for
Computational Linguistics
Alex B 2006 Integrating language knowledge resources to extend the English inclu-
sion classier to a new language In Proceedings of the 5th International Conference
on Language Resources and Evaluation LREC European Language Resources Asso-
ciation
Alex B 2007 Automatic detection of English inclusions in mixed-lingual data with an
application to parsing PhD thesis University of Edinburgh
Alex B Dubey A and Keller F 2007 Using Foreign Inclusion Detection to Improve
Parsing Performance In EMNLP-CoNLL pages 151160
Alex B and Onysko A 2010 Zum Erkennen von Anglizismen im Deutschen der
Vergleich von einer automatisierten mit einer manuellen Erhebung In Scherer C
and Holler A editors Strategien der Integration und Isolation nicht-nativer Einheiten
und Strukturen pages 223239 de Gruyter
Begleiter R El-Yaniv R and Yona G 2004 On prediction using variable order
Markov models Journal of Articial Intelligence Research pages 385421
Biemann C 2006 Chinese whispers an ecient graph clustering algorithm and
its application to natural language processing problems In Proceedings of the rst
workshop on graph based methods for natural language processing pages 7380 As-
sociation for Computational Linguistics
Brants S Dipper S Hansen S Lezius W and Smith G 2002 e TIGER treebank
In Proceedings of the workshop on treebanks and linguistic theories volume 168
Brgisser P Clausen M and Shokrollahi M A 1997 Algebraic complexity theory
volume 315 Springer
Carter D 1994
Improving language models by clustering training sentences
Proceedings of the fourth conference on Applied natural language processing pages
5964 Association for Computational Linguistics
Cavnar W B and Trenkle J M 1994 N-gram-based text categorization
In Pro-
ceedings of SDAIR-94 3rd Annual Symposium on Document Analysis and Information
Retrieval pages 161175
Chen S F and Goodman J 1996 An empirical study of smoothing techniques for
language modeling In Proceedings of the 34th annual meeting on Association for Com-
putational Linguistics pages 310318 Association for Computational Linguistics
Dreyfuss E Goodfellow I and Baumstarck P 2007 Clustering Methods for Improv-
ing Language Models
Dubes R C 1987 How many clusters are best-an experiment Paern Recognition
206645663
Dunning T 1994 Statistical Identication of Language Computing Research Labo-
ratory New Mexico State University
Gale W and Sampson G 1995 Good-turing smoothing without tears Journal of
antitative Linguistics 23217237
Gao J Goodman J Miao J et al 2001 e use of clustering techniques for language
International Journal of Computational
modelingapplication to Asian languages
Linguistics and Chinese Language Processing 612760
Goldberg D 1991 What every computer scientist should know about oating-point
arithmetic ACM Computing Surveys CSUR 231548
Goodman J and Gao J 2000 Language model size reduction by pruning and clus-
tering In INTERSPEECH pages 110113
Goodman J T 2001 A bit of progress in language modeling Computer Speech and
Language 154403434
Grefenstee G 1995 Comparing two language identication schemes In Proceedings
of the 3rd International conference on Statistical Analysis of Textual Data JADT 1995
Grnwald P D 2007 e minimum description length principle MIT press
Guthrie D Allison B Liu W Guthrie L and Wilks Y 2006 A closer look at
skip-gram modelling In Proceedings of the 5th international Conference on Language
Resources and Evaluation LREC-2006 pages 14
Hall M Frank E Holmes G Pfahringer B Reutemann P and Wien I H 2009
e WEKA Data Mining Soware An Update SIGKDD Explorations 11
Jain A K Murty M N and Flynn P J 1999 Data clustering a review ACM
computing surveys CSUR 313264323
Jain N and Bhat R A 2014 Language Identication in Code-Switching Scenario In
Proceedings of the Conference on Empirical Methods on Natural Language Processing
pages 8793
Jurafsky D and Martin J H 2000 Speech and language processing An Introduction
to Natural Language Processing Computational Linguistics and Speech Recognition
Pearson Education India 2nd edition
Katz S 1987 Estimation of probabilities from sparse data for the language model
component of a speech recognizer Acoustics Speech and Signal Processing IEEE
Transactions on 353400401
King B and Abney S P 2013 Labeling the Languages of Words in Mixed-Language
Documents using Weakly Supervised Methods In Proceedings of the Conference of
the North American Chapter of the Association for Computational Linguistics  Human
Language Technologies pages 11101119
Liu H and Cong J 2013 Language clustering with word co-occurrence networks
based on parallel texts Chinese Science Bulletin 581011391144
Logan B et al 2000 Mel frequency cepstral coecients for music modeling
Proceedings of the 1st International Symposium on Music Information Retrieval ISMIR
Lui M Lau J H and Baldwin T 2014 Automatic detection and language identi-
cation of multilingual documents Transactions of the Association for Computational
Linguistics 22740
Manning C D Raghavan P and Schtze H 2008 Introduction to information re-
trieval volume 1 Cambridge University Press
Manning C D and Schtze H 1999 Foundations of statistical natural language pro-
cessing MIT press
Marsland S 2003 Novelty detection in learning systems Neural computing surveys
32157195
Mendizabal I Carandell J and Horowitz D 2014 TweetSafa Tweet language
identication TweetLID  SEPLN
Mikolov T Chen K Corrado G and Dean J 2013 Ecient estimation of word
In Proceedings of the International Conference on
representations in vector space
Learning Representations ICLR 2013
Ney H Essen U and Kneser R 1994 On structuring probabilistic dependences in
stochastic language modelling Computer Speech  Language 81138
Pelleg D and Moore A W 2000 X-means Extending K-means with Ecient Es-
timation of the Number of Clusters In Proceedings of the Seventeenth International
Conference on Machine Learning ICML 2000 pages 727734
Pereira F Tishby N and Lee L 1993 Distributional clustering of english words In
Proceedings of the 31st annual meeting on Association for Computational Linguistics
pages 183190 Association for Computational Linguistics
Porta J 2014 Twier Language Identication using Rational Kernels and its potential
application to Sociolinguistics TweetLID  SEPLN
Ravi S Vassilivitskii S and Rastogi V 2014 Parallel Algorithms for Unsupervised
Tagging Transactions of the Association for Computational Linguistics 2105118
Ron D Singer Y and Tishby N 1996 e power of amnesia Learning probabilistic
automata with variable memory length Machine learning 252-3117149
Schlkopf B Williamson R C Smola A J Shawe-Taylor J and Pla J C 1999
In Advances in Neural Information
Support vector method for novelty detection
Processing Systems NIPS volume 12 pages 582588
Seldin Y Bejerano G and Tishby N 2001 Unsupervised sequence segmentation
by a mixture of switching variable memory Markov sources In Proceedings of the
Seventeenth International Conference on Machine Learning ICML pages 513520
Solorio T Blair E Maharjan S Bethard S Diab M Gohneim M Hawwari A Al-
Ghamdi F Hirschberg J Chang A et al 2014 Overview for the First Shared Task
on Language Identication in Code-Switched Data In Proceedings of the Conference
on Empirical Methods on Natural Language Processing pages 6272
Taylor D 2015 Graphing the distribution of English leers towards the be-
httpwwwprooffreadercom201405
ginning middle or end of words
graphing-distribution-of-englishhtml
e Unicode Consortium 2014 e Unicode Standard httpunicodeorg
standardstandardhtml Online accessed 21-July-2015
Uszkoreit J and Brants T 2008 Distributed word clustering for large scale class-
based language modeling in machine translation In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguistics pages 755762
Wagner S and Wagner D 2007 Comparing clusterings an overview Universitt
Karlsruhe Fakultt fr Informatik Karlsruhe
Yamaguchi H and Tanaka-Ishii K 2012 Text segmentation by language using mini-
mum description length In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics pages 969978 Association for Computational Lin-
guistics
Yin B Ambikairajah E and Chen F 2007 Hierarchical language identication
based on automatic language clustering In INTERSPEECH pages 178181
Yuan L 2006 Language model based on word clustering In Proceedings of the 20th
Pacic Asia Conference on Language Information and Computation pages 394397
Zubiaga A San Vicente I Gamallo P Pichel J R Alegria I Aranberri N Ezeiza
A and Fresno V 2014 Overview of TweetLID Tweet language identication at
SEPLN 2014 TweetLID  SEPLN
8 Appendix
81 Development data
811 Latin script data
Karl Marx anses som en af de re klassiske sociologer Marx er epokegrende for den
historiske videnskab Og Marx spillede en vigtig rolle for den samtidige og eerfl-
gende arbejderbevgelse
1891 nach einer Tuberkuloseerkrankung Hopes ernete das Ehepaar ein mod-
ernes Lungensanatorium in Nordrach im Schwarzwald das sie bis 1893 gemeinsam
hrten 1895 wurde die Ehe geschieden
Sources
hpsdawikipediaorgwikiKarlMarx
hpsdewikipediaorgwikiHopeBridgesAdamsLehmann
812 Mixed script data
Capitalism is an economic system and a mode of production in which trade industries
and the means of production are largely or entirely privately owned Private rms and
proprietorships usually operate in order to generate prot but may operate as private
nonprot organizations
                        
                        
                          
                         
                      
    
hpsenwikipediaorgwikiCapitalism
hpsfawikipediaorgwiki
Sources
813 Twitter data
Twitter 1 Fallo ergo sum On being wrong
Source
Roland Hieber danielbohrer Fallo ergo sum On being wrong 26 July 2015
1647 Tweet
Twitter 2 Music for Airports  le piano en libre-accs dans laroport Charles-de-
Gaulles
Source
Yannick Rochat yrochat Music for Airports  le piano en libre-accs dans laroport
Charles-de-Gaulles 26 July 2015 1812 Tweet
814 Pali dictionary data
All entries have been taken from the Pali Text Societys Pali-English dictionary T W
Rhys Davids William Stede editors e Pali Text Societys PaliEnglish dictionary
Chipstead Pali Text Society 19215 8 parts 738 pp
Hambho Hambhoindeclhabho a particle expressing surprise or haughti-
ness JI184494See also ambhoPage 729
Ussada Ussadamost likely to ud  syadsee ussannathis word is beset with
dicultiesthe phrase sa-ussada is applied in all kinds of meaningsevidently the
result of an original application  meaning having become obliteratedsa is taken
as sapta sevenas well as sava beingussada as prominenceprotuberance
fulnessarrogancee meanings may be tabulated as follows1prominencecp
Skutsedhaused in characterisation of the Nirayasas projectingprominent
hellsussadaniray but see also below 4JI174IV3422 pallaka
vlcaturasswith four cornersV266 adjprominent A13 tej-
ussadehi ariyamaggadhammehior as below 4 2protuberancebumpswelling
JIV188also in phrase saussada having 7 protuberancesa qualication of the
Mahpurisa DIII151 vizon both handsfeetshouldersand on his back
 3rubbing inanointingointmentadjanointed with -in candan JIII
139IV601267Vv 537DhAI28VvA237 4a crowd adjfull
of -in phrase saussada crowded with human beingsDI87 cpDAI
245aneka-saa-samkiabut in same sense BSksapt-otsada Divy 620621Pv
IV18 of Niraya  full of beingsexpldby saehi ussanna uparpari nicita PvA
221 5qualicationcharacteristicmarkaributein catussada having the
four qualications of a good villageJIV309 vizplenty of peoplecorn
wood and water Ce phrase is evidently shaped aer DI87 under 4As
preponderant qualitycharacteristicwe nd ussada used at Vism103 cfAsl
267in combnslobhdosmohalobh etcquoted from theUssadakiana
and similarly at VvA19 in Dhammaplas denition of manussalobhdhi alobh
dhi sahitassa manassa ussannatya manussvizsa manussa-jtik tesu lobh
- dayo alobhdayo ca ussad 6metaphself-elevationarroganceconceit
haughtiness VinI3Sn515624 an  tah-ussada-abhvena SnA 467783
expldby Nd1 72 under formula saussadaieshowing 7 bad qualitiesvizrga
dosamoha etc855 See also ussdanaussdeti etcPage 157
82 Test data
821 Latin script data
English - German e German word Nabelschau means navel-gazing or staring
at your navel But in this case it doesnt refer to anyone elses belly buon  just your
Source
Glass Nicole 2015 German Missions in the United States - Word of the Week
Germanyinfo
English - Fren doux mou  both translate as so in English although their mean-
ing is very dierent Doux is the opposite of rough or coarse rugueux while mou
is the opposite of hard Doux can also mean sweet but almost only for wines oth-
erwise sucr is used
Source
Maciamo 2015 French words and nuances that dont exist in English Eupedia
English - Transliterated Greek e Greek language distinguishes at least four dif-
ferent ways as to how the word love is used Ancient Greek has four distinct words for
love agpe ros phila and storg However as with other languages it has been his-
torically dicult to separate the meanings of these words when used outside of their
respective contexts Nonetheless the senses in which these words were generally used
are as follows
Source
hpsenwikipediaorgwikiGreekwordsforlove
Italian - German Milano ne custodisce lesempio pi struggente quel Cenacolo
che il vinciano aresc con amore cura e rivoluzionaria psicologia il Giuda non vie-
ne privato dellaureola ma si condanna da solo con la consapevolezza del peccato
cominci subito ad autodistruggersi con un cancro che solo un lunghissimo restauro
ha di recente arginato
Kaum eine Woche vergeht in der es keine neue Studie Umfrage oder Warnung
zum ema Fachkremangel in Deutschland gibt
Certo lo faceva per denire le idee ma anche perch consapevole che le intuizioni
sono periture che la vita stessa va caurata in qualche modo
Dabei mehren sich letzter Zeit auch Stimmen die Entwarnung geben So kam
jngst eine Studie des Stierverbands r die Deutsche Wissenscha zu dem Ergebnis
dass ein allgemeiner Fachkremangel in den MINT-Berufen eher nicht mehr drohe
Come anche i riccioli del Baista richiamano il movimento delle acque moto che
poi Leonardo studier pi approfonditamente a Venezia nelle ricerche sui bacini in
chiave di difesa anti-Turchi E si vada alla bellissima Annunciazione con un occhio
aento alle ali dellangelo la delicatezza delle punte allins che cosa sono se non
il barbaglio di un sogno che lo ossessionava da anni ovvero quello di volare
Ist das seit Jahren angemahnte Szenario vom drohenden Fachkremangel bei In-
genieuren und Naturwissenschalern also nur ein Mythos
Source
Stalinski Sandra 2015 Ingenieure Mythos Fachkremangel tagesschaude
Scorranese Roberta 2015 Nelle grandi opere il racconto soerto della natura mor-
tale Archiviostoricocorriereit
German - Finnish - Turkish Der Sommer ist die wrmste der vier Jahreszeiten in der
gemigten und arktischen Klimazone Je nachdem ob er gerade auf der Nord- oder
Sdhalbkugel herrscht spricht man vom Nord- oder Sdsommer Der Nordsommer
ndet gleichzeitig mit dem Sdwinter sta
Kes eli suvi on vuodenaika kevn ja syksyn vliss Kes on vuodenajoista lm-
pimin koska maapallo on silloin kallistunut niin e aurinko steilee maan pinnalle
jyrkemmss kulmassa kuin muina vuodenaikoina Pohjoisella pallonpuoliskolla kes-
kuukausiksi lasketaan tavallisesti kes- hein- ja elokuu etelisell pallonpuoliskolla
joulu- tammi- ja helmikuu
Yaz en scak mevsimdir Kuzey Yarm Krede en uzun gnler yazda gerekleir
Dnya sy depo eii iin en scak gnler genellikle yaklak iki ay sonra ortaya
kar Scak gnler Kuzey Yarm Krede 21 Haziran ile 22 Eyll arasnda Gney Yarm
Krede ise 22 Aralk ile 21 Mart arasndadr
Source
hpswikipediaorgwikiKes
hpsdewikipediaorgwikiSommer
hpstrwikipediaorgwikiYaz
822 Mixed script data
Greek - Russian         
         -
         
      15    
          -
         -
        
       
         
         
        
         
  
Source
hpselwikipediaorgwiki
hpsruwikipediaorgwiki
English - Greek - Transliterated Greek Agpe  agp means love esp
brotherly love charity the love of God for man and of man for God Agape is used
in the biblical passage known as the love chapter 1 Corinthians 13 and is described
there and throughout the New Testament as brotherly love aection good will love
and benevolence Whether the love given is returned or not the person continues to
love even without any self-benet Agape is also used in ancient texts to denote feel-
ings for ones children and the feelings for a spouse and it was also used to refer to
a love feast It can also be described as the feeling of being content or holding one in
high regard Agape is used by Christians to express the unconditional love of God for
his children is type of love was further explained by omas Aquinas as to will
the good of another
ros  rs means love mostly of the sexual passion e Modern Greek
word erotas means intimate love It can also apply to dating relationships as well as
marriage Plato rened his own denition Although eros is initially felt for a person
with contemplation it becomes an appreciation of the beauty within that person or
even becomes appreciation of beauty itself Plato does not talk of physical araction as
a necessary part of love hence the use of the word platonic to mean without physical
araction
In the Symposium the most famous ancient work on the subject Plato has Socrates
argue that eros helps the soul recall knowledge of beauty and contributes to an under-
standing of spiritual truth the ideal Form of youthful beauty that leads us humans
to feel erotic desire  thus suggesting that even that sensually based love aspires to
the non-corporeal spiritual plane of existence that is nding its truth just like nd-
ing any truth leads to transcendence Lovers and philosophers are all inspired to seek
truth through the means of eros
Source
hpsenwikipediaorgwikiGreekwordsforlove
English - Spanish - Arabic A black ribbon is a symbol of remembrance or mourn-
ing Wearing or displaying a black ribbon has been used for POWMIA remembrance
mourning tragedies or as a political statement
El crespn negro o lazo negro es un smbolo utilizado por personas estados so-
ciedades y organizaciones representando un sentimiento poltico-social en seal de
                      
            
Source
hpseswikipediaorgtitleLazonegro
hpsenwikipediaorgwikiBlackribbon
hpsarwikipediaorgwiki
English - Chinese - Pinyin e Chinese word for crisis simplied Chinese 
 traditional Chinese  pinyin wij is frequently invoked in Western
motivational speaking because the word is composed of two Chinese characters that
can represent danger and opportunity Some linguists have criticized this usage
because the component pronounced j simplied Chinese  traditional Chinese
 has other meanings besides opportunity In Chinese tradition certain numbers
are believed by some to be auspicious  or inauspicious  based on the
Chinese word that the number name sounds similar to e numbers 0 6 8 and 9 are
believed to have auspicious meanings because their names sound similar to words
that have positive meanings
Source
hpsenwikipediaorgwindexphptitleChinesewordforcrisis
Ukrainian - Russian       
        
      913 
         
        

Source
hpsukwikipediaorgwiki
Surgut-safariru 2015  - Safari Tour
823 Twitter data
Tweet 1 Greek  English      Internet of ings 
 BUSINESS IT EXCELLENCE
Source
GaloTyri      Internet of ings  
BUSINESS IT EXCELLENCE 19 June 2015 1206 Tweet
Tweet 2 English  Fren Demain dhiha6 Keynote 18h dhiparis e collective
dynamics of science-publish or perish is it all that counts par David chavalarias
Source
Claudine Moulin ClaudineMoulin Demain dhiha6 Keynote 18h dhiparis e
collective dynamics of science-publish or perish is it all that counts par David
chavalarias 10 June 2015 1735 Tweet
Tweet 3 English  Fren Food and breuvages in Edmonton are ready to go just
waiting for the fans FWWC2015 bilingualism
Source
HBS HBSTweets Food and breuvages in Edmonton are ready to go just waiting
for the fans FWWC2015 bilingualism 6 June 2015 2329 Tweet
Tweet 4 English  Polish my dad comes back from poland with two crates of
strawberries ubrwka and adidas jackets omg
Source
katarzyne wifeyriddim my dad comes back from poland with two crates of
strawberries ubrwka and adidas jackets omg 8 June 2015 0849 Tweet
Tweet 5 Transliterated Amharic  English Buna dabo naw coee is our bread
Source
eCodeswitcher Buna dabo naw coee is our bread 9 June 2015 0212 Tweet
824 Pali dictionary data
All entries have been taken from the Pali Text Societys Pali-English dictionary T W
Rhys Davids William Stede editors e Pali Text Societys PaliEnglish dictionary
Chipstead Pali Text Society 19215 8 parts 738 pp
nt Vedic abhra nt  later Sk abhra m dark cloud Idg m bhro cp Gr
atafronnsat scum froth Lat imber rain also Sk ambha water Gr
atombrosat rain Oir ambu water A dense  dark cloud a cloudy mass A
smallcapsiismallcaps 53  Vin smallcapsiismallcaps 295  Miln 273 in
list of to things that obscure moon  sunshine viz babbha mahikb mahiy
A bdh- marajob megho Miln bRhub  is list is referred to at SnA
487  VvA 134 S smallcapsismallcaps 101 sama pabbata a mountain like a
thundercloud J smallcapsvismallcaps 581 abbha rajo acchdesi Pv
smallcapsivsmallcaps 3 superscript9superscript nl  nlamegha PvA
251 As f babbhb at Dhs 617  DhsA 317 used in sense of adj dull DhsA
expl superscriptssuperscript by valhaka perhaps also in babbhmaab
 br bkab the point or summit of a stormcloud  1 1064 J
smallcapsvismallcaps 249 250 Vv 1 superscript1superscript 
valhakasikhara VvA 12 bghanab a mass of clouds a thick cloud It 64 Sn
348 cp SnA 348 bpaalab a mass of clouds DhsA 239 bmuab free
from clouds Sn 687 also as abbhmua Dh 382 bsavilpab thundering S
smallcapsivsmallcaps 289
n ag fr abhijjhita in med function one who covets M
abhijjhitar
smallcapsismallcaps 287 T abhijjhtar v l itar  A
smallcapsvsmallcaps 265 T itar v l tar
ajja Ajja Ajj advVedic adya  adya  dya being base of demonstr
pron see a3and dy an old Loc of dyaus see divathus on this day
to-daynow Sn75153158970998Dh326JI279III425 read bahuta
ajjnot with KernToev s v as foodPvI117  idni PvA59PvA6
23Mhvs 1564 - Freq in phrase ajjatagge  ajjato  aggeor ajja-tagge
see agga3from this day onwardhenceforth VinI18DI85DAI235
kla advthis morning JVI180divasa the present day Mhvs 3223
Page 10
ghan Ghanfabstrfrghatighan qvPug19Cp
pariPage 253
pacati PacatiVedpacatiIdgpeqAvpac-Obulgpeka to fryroast
Lithkep bakeGrpssw cookppwn ripe to cookboilroast VinIV264
gtorment in purgatory trsand intrsNiraye pacitv aer roasting in NS
II225PvA1014 pprpacanto tormentingGenpacato Caus
pcayatoDI52 expld at DAI159where read pacato for paccatoby pare
daena pentassa pppakka qv- Causpacpeti  pceti qv
 Passpaccati to be roasted or tormented qvPage 382
83 Results
831 N-Gram Language Models
For the n-gram language model approach the identied language is indicated in
parentheses e language abbreviations are
Abbreviation
Language
English
Spanish
Finnish
Italian
Russian
Ukrainian
Turkish
Transliterated Amharic
Transliterated Greek
Chinese
Data Latin script German  English
 EN own belly refer buon But it or your at in staring anyone doesnt
elses word this
 FI 
 FR case just means navel
 TrAM e
 TrEL to German
 other Nabelschau navel-gazing
Data Latin script German  Finnish  Turkish
 DE ob oder Sommer und Nord- arktischen der Der dem gemigten mit
er Sdsommer spricht Jahreszeiten Sdwinter herrscht wrmste vom die
sta nachdem auf
 EN ist Nordsommer Mart in
 ES en depo
 FI joulu- kevn suvi on eli vuodenajoista syksyn koska kes- kuin Po-
hjoisella man helmikuu tammi- lmpimin hein- niin maapallo maan pin-
nalle Kes steilee tavallisesti vuodenaika kallistunut lasketaan muina eii
jyrkemmss elokuu vliss e etelisell silloin ja kulmassa
 FR vier Je
 PL aurinko
 RU 22 21
 TR yaklak ortaya genellikle Eyll Scak kar Yaz sonra arasnda Kuzey
Gney Aralk gerade sy gerekleir Krede gnler iin ndet mevsimdir
arasndadr Haziran iki yazda uzun ise ay scak ile Yarm Dnya
 TrAM Der
 other Klimazone gleichzeitigkeskuukausiksi vuodenaikoina pallonpuolis-
kollaSdhalbkugel
Data Latin script English  French
 EL coarse
 EN but both for while wines almost sweet of although only is rough
used or as meaning the in translate hard their English also dierent
 ES can
 FI mean
 FR opposite Doux doux sucr 
 RU so
 TrEL mou
 other otherwise rugueux
Data Latin script English  Transliterated Greek
 EN for meanings least used been distinct love of were are when agpe
these how and Greek word used outside ways dierent other follows
words respective generally However is with it at as historically the in
which their
 ES has separate
 FR language senses Ancient languages dicult four
 IT contexts
 TrAM ros e love
 TrEL to storg phila
 other Nonetheless distinguishes
Data Latin script Italian  German
 DE drohe geben allgemeiner Studie jngst r Ergebnis keine kam dro-
henden oder und letzter neue Mythos Deutschland Ist sich der vergeht
studier Dabei Studie den dem auch Entwarnung dass nur eher nicht gibt
Umfrage Woche eine Kaum Jahren bei mehren Stimmen Deutsche das zum
mehr angemahnte ein Zeit ein So vom zu die seit Warnung Wissenscha
 EL aresc
 EN moto aento a in ad also
 ES custodisce cura subito Certo Giuda lo del difesa con denire restauro
se modo la arginato recente vada movimento Leonardo Szenario quel
cominci
 FI va si Baista ema
 FR lesempio non des acque perch un es le sui condanna
 IT solo faceva caurata chiave peccato periture il delicatezza cancro pri-
vato bellissima anni bacini ovvero delle sogno di barbaglio ma qualche e
amore ricerche Come per richiamano ne intuizioni punte occhio struggente
nelle vita riccioli solo che volare sono alla alle anche Cenacolo quello
cosa ali viene il psicologia vinciano Venezia
 PL i
 TR ha pi da
 TrAM Milano E
 TrEL poi idee stessa
 other MINT-Berufen Fachkremangel dellangelo consapevole anti-Turchi
Annunciazione lunghissimo consapevolezza ossessionava dellaureola appro-
fonditamente autodistruggersi rivoluzionaria Stierverbands allins Natur-
wissenschalern Ingenieuren
Data Mixed script Greek  Russian
 EL          
        
15         
    
 RU       
      
        -
        -
       
        -
       
 TrAM 
 UK        
 other    -
   
Data Mixed script English  Greek
 DE Symposium Modern being felt
 EL Form
 EN sensually platonic for holding existence rened its explained arac-
tion of even are spiritual given refer Agape beauty or araction like
without not further will own love knowledge will ones most use ex-
press is another e leads truth suggesting dating relationships in-
spired love mostly hence denition regard appreciation a ideal us helps
seek Agpe plane recall feeling within returned chapter based described
apply physical Although good by used love God children his any char-
ity Socrates be work throughout and that Greek even word agp love
known biblical feelings does famous In subject becomes one understand-
ing children love through beauty well It was initially feast nding itself
13 all without feel with is it thus New as the brotherly in is an there
God youthful necessary high Lovers also Whether
 ES person Aquinas esp continues has omas truth can erotic sexual
 FI on  man mean
 FR  spouse not ancient marriage soul person content Christians
Testament ros just part type passage means humans passion aspires con-
templation contributes argue aection
 IT texts 1 intimate Plato to
 RU 
 TR talk
 TrAM rs love
 TrEL erotas denote eros to eros
 other non-corporeal Corinthians self-benet benevolence unconditional
philosophers transcendence
Data Mixed script English  Spanish  Arabic
 AR


  
   





 
 



 

 

 




 EN for used been displaying of ribbon black or mourning statement
tragedies is political a Wearing as mourning
 ES por has crespn sociedades personas sentimiento representando esta-
dos de El seal lazo smbolo en utilizado y
 FR remembrance remembrance un es
 IT negro duelo POWMIA
 TrAM
 TrEL symbol o
 other poltico-social organizaciones
Data Mixed script English  Chinese
 DE  Chinese Western
 EL 
 EN Some for meanings by of are 8 positive speaking be composed or
meanings tradition number and that sound linguists word some this other
In have invoked criticized 6 because e believed words numbers sounds
frequently is pronounced besides traditional the in represent two motiva-
tional usage their based
 ES  has  can Chinese crisis similar
 FI on
 FR  component danger characters  certain j
 PL pinyin
 RU 0 9 wij
 TrEL to to names name
 other inauspicious opportunity simplied auspicious
Data Mixed script Ukrainian  Russian
 RU         9
13        

 TrAM 
 UK        -
       
      
 other  
Data Pali abbha
 AR  134 289
 DE Miln imber dark Miln
 EL  abbha
 EN water mountain of free used or like referred also A is cloudy
clouds later a froth 1 summit thundering by mass Pv Oir obscure scum
that water thick As from It is at as the in clouds things also
 ES dense f sense expl rajo
 FI 239 rain Lat Vin perhaps SnA
 FR cloud Dh adj point cloud Dhs A rain VvA DhsA list
 IT dark  ambha 3 1 317 J sunshine cp abhra Vedic megho
 PL 487  S 295 br moon 249
 RU 348 53
 TR viz ambu Vv
 TrAM 687 PvA sama 101 nl cp 64 nt 581 m Sn 1064
 TrEL  Gr Sk Idg to pabbata nt
 UK 12 273 617 348 250 251 382
 other b savilpa b b mua b smallcaps vi smallcaps
mahiy smallcaps iv smallcaps cloud b Rhu b b abbh
b b abbha superscript 9 superscript marajo b abbhmua
valhaka smallcaps i smallcaps b abbhmaa b valhakasikhara
superscript s superscript smallcaps ii smallcaps b dh- storm
cloud b ka b thundercloud atafrosat bpaalab
atombrosat nlamegha superscript1superscript m bhro dull
acchdesi mahikb b ghana b
Data Pali abhijjhitar
 DE v
 EN A one in who covets med function
 IT ag M fr
 PL 287 
 RU 265
 TrAM l n
 TrEL T
 other smallcaps v smallcaps abhijjhtar abhijjhita tar smallcaps
i smallcaps itar itar
Data Pali ajja
 DE see v being Ajj
 EN of or and not present Freq day this on from adya with as the
morning in day an
 ES bahuta
 FI 3223 ajjato
 FR Loc dyaus 1564 dy pron
 IT Vedic Mhvs  divasa
 PL   demonstr s
 RU III425 agge
 TR old adya 10 idni
 TrAM -
 TrEL phrase base
 UK a3
 other onwardhenceforth ajj DAI235 adv JI279 DI85
ajja-taggesee Sn75153158970998 JVI180 PvA623 kla
divathus PvA59 agga3 KernToev PvI117 Dh326 ajjatagge
read Page VinI18 dya Ajja to-daynow food
Data Pali ghan
 ES 253
 other abstrfrghatighan Pug19CppariPage qv
Ghanf
Data Pali pacati
 EL 382
 EN for aer roasting read roasted be or at tormented in
 FR pare DI52
 IT  pacato purgatory
 TrAM pceti ripe
 TrEL to daena
 other bakeGrpsswCauspcayatoqvPage DAI159where
Causpacpeti intrsNiraye pacitv Passpaccatitrsand tormenting
Genpacato pentassa gtorment cookppwn PacatiVedpacati
IdgpeqAvpac- paccatoby pprpacanto cookboilroast fry
roastLithkep qv expld VinIV264 Obulgpeka pp
pakka qv- NSII225PvA1014
Data Twier 1 GreekEnglish
 DE Internet
 EL      
 EN of IT ings
 ES BUSINESS
 TrAM 
 other EXCELLENCE
Data Twier 2 FrenchEnglish
 EN David e is it perish or collective Demain counts that of dynam-
ics all
 FI 18h
 FR par Keynote
 other dhiha6 dhiparis science-publish
Data Twier 3 FrenchEnglish
 EN for Food waiting the in ready and are
 ES go
 FI Edmonton
 FR just breuvages fans
 TrEL to
 other bilingualism FWWC2015
Data Twier 4 EnglishPolish
 EN with back from comes crates and poland two of jackets
 ES dad adidas
 TrAM my
 TrEL omg
 other ubrwka strawberries
Data Twier 5 Transliterated AmharicEnglish
 EN is bread
 FR our
 IT coee
 PL naw
 TrAM Buna dabo
832 Textcat
For Textcat the identied language is indicated in parentheses As Textcat returns
unknown for many words I merely indicate the non-unknown categories to save
space and write rest to indicate that all other words of the text have been classied as
unknown e language abbreviations are
Abbreviation
Language
English
Spanish
Finnish
Hungarian
Indonesian
Italian
Lithuanian
Latvian
Portuguese
Russian
Chinese
Data Latin script German  English
 HU navel-gazing
 ZH Nabelschau
 unknown rest
Data Latin script German  Finnish  Turkish
 DA Sdsommer genellikle
 DE Jahreszeiten arktischen
 FI vuodenajoista kallistunut tavallisesti
 ZH gemigten Klimazone Sdhalbkugel Nordsommer gleichzeitig vuoden-
aika jyrkemmss vuodenaikoina Pohjoisella pallonpuoliskolla keskuukausik-
si etelisell mevsimdir gerekleir arasndadr
 unknown rest
Data Latin script English  French
 HU dierent
 ZH rugueuxotherwise
 unknown rest
Data Latin script English  Transliterated Greek
 EN historically respective
 LT languages
 ZH distinguishes Nonetheless
 unknown rest
Data Latin script Italian  German
 DE allgemeiner angemahnte
 ES delicatezza
 HU bellissima
 IT dellaureola consapevole richiamano anti-Turchi ossessionava
 NL Ingenieuren
 PT approfonditamente
 ZH custodisce struggente rivoluzionaria psicologia consapevolezza auto-
distruggersi lunghissimo Fachkremangel Deutschland intuizioni Entwar-
nung Stierverbands Wissenscha MINT-Berufen Annunciazione dellan-
gelo Naturwissenschalern
 unknown rest
Data Mixed script Greek  Russian
 EL  
 RU     
    
   
 TH 
 ZH    

 unknown rest
Data Mixed script English  Greek
 DA denition understanding
 EN aection unconditional suggesting
 FR relationships contemplation appreciation araction araction transcen-
dence
 HU benevolence self-benet
 IT non-corporeal
 PT contributes
 ZH Corinthians throughout Christians Symposium existence philosophers
 unknown rest
Data Mixed script English  Spanish  Arabic
 ES sociedades organizaciones sentimiento poltico-social
 FR remembrance remembrance statement
 ID displaying
 PT representando
 unknown rest
Data Mixed script English  Chinese
 EN traditional motivational pronounced tradition
 FR characters
 ZH simplied frequently opportunity criticized auspicious inauspicious
 unknown rest
Data Mixed script Ukrainian  Russian
 RU   
 TH 
 ZH    
 unknown rest
Data Pali abbha
 DA stormcloud thundering
 HU marajob nlamegha valhakasikhara
 ZH
at afroat at ombros at smallcaps ii smallcaps mahikb
b Rhu b smallcaps i smallcaps thundercloud smallcaps vi
smallcaps acchdesi smallcaps iv smallcaps superscript 9 su-
perscript b abbh b superscript s superscript valhaka b
abbhmaa b b ka b superscript 1 superscript b ghana
b b paala b b mua b abbhmua b savilpa b
 unknown rest
Data Pali abhijjhitar
 ZH abhijjhita smallcaps i smallcaps abhijjhtar smallcaps v
smallcaps
 unknown rest
Data Pali ajja
 ZH divathus to-daynow Sn75153158970998 KernToev
ajja-taggesee onwardhenceforth
 unknown rest
Data Pali ghan
 ZH Ghanf abstrfrghatihan Pug19CppariPage
 unknown rest
Data Pali pacati
 ZH gtorment PacatiVedpacatiIdgpeqAvpac- Obulgpeka
fryroastLithkep bakeGrpssw cookppwn cookboilroast Vin
IV264 intrsNiraye NSII225PvA1014 pprpacanto
tormentingGenpacatoCauspcayato DAI159where paccatoby
pentassa qv- Causpacpeti Passpaccati qvPage
 unknown rest
Data Twier 1 GreekEnglish
 ZH  EXCELLENCE
 unknown rest
Data Twier 2 FrenchEnglish
 IT collective
 ZH science-publish
 unknown rest
Data Twier 3 FrenchEnglish
 ZH bilingualism
 unknown rest
Data Twier 4 EnglishPolish
 LV strawberries
 unknown rest
Data Twier 5 Transliterated AmharicEnglish
 unknown rest
833 Clustering
Clustering the dierent data sets produced the following clusters e second run
uses the clusters from the rst run and possibly subdivides each cluster into two or
more clusters
Data Latin script German  English
First run
 navel-gazing doesnt elses
 staring But German Nabelschau anyone belly buon case just means
navel own refer this word your
 at in it or to
  e
Second run
 doesnt elses
 navel-gazing
 staring But German Nabelschau belly case means navel refer this
 anyone buon just own word your
 it or to
 at in
  e
Data Latin script German  Finnish  Turkish
First run
 Dnya Gney Krede Sdhalbkugel Sdsommer Sdwinter Scak arasnda
gemigten gnler iin keskuukausiksi lmpimin steilee scak wrmste
kar Der
 Aralk Eyll Kes Yarm arasndadr etelisell eii e gerekleir hein-
 jyrkemmss kes- kevn vliss yaklak sy
 21 22
 Der Haziran Jahreszeiten Je Klimazone Kuzey Mart Nord- Nordsommer
Pohjoisella Sommer Yaz arktischen auf aurinko ay dem depo der die eli
elokuu en er ndet genellikle gerade gleichzeitig helmikuu herrscht iki
ile in ise ist ja joulu- kallistunut koska kuin kulmassa lasketaan maan
maapallo man mevsimdir mit muina nachdem niin ob oder on ortaya pal-
lonpuoliskolla pinnalle silloin sonra spricht sta suvi syksyn tammi- taval-
lisesti und uzun vier vom vuodenaika vuodenaikoina vuodenajoista yazda
Second run
 Sdhalbkugel Sdsommer Sdwinter arasnda gemigten keskuukausiksi
lmpimin steilee wrmste
 Dnya Gney Krede Scak gnler iin scak kar Der
 arasndadr etelisell eii e gerekleir hein- jyrkemmss kes-
kevn vliss yaklak sy
 Aralk Eyll Yarm
 Kes
 Der Haziran Jahreszeiten Klimazone Kuzey Mart Nord- Nordsommer Po-
hjoisella Sommer Yaz
 arktischen auf aurinko dem depo der die eli elokuu ndet genellikle gerade
gleichzeitig helmikuu herrscht iki ile ise ist joulu- kallistunut koska kuin
kulmassa lasketaan maan maapallo man mevsimdir mit muina nachdem
niin oder ortaya pallonpuoliskolla pinnalle silloin sonra spricht sta suvi
syksyn tammi- tavallisesti und uzun vier vom vuodenaika vuodenaikoina
vuodenajoista yazda
 Je ay en er in ja ob on
Data Latin script English  French
First run
 coarse hard rough so otherwise rugueux Doux English almost
also although both but can dierent doux for mean meaning mou only
opposite sucr sweet the their translate used very while wines
 is or
 as in of
Second run
 Doux English
 coarse otherwise rugueux almost although dierent meaning opposite
translate
 hard rough so also both but can doux for mean mou only sucr
sweet the their used very while wines
Data Latin script English  Transliterated Greek
First run
 e
 agpe phila storg ros
 Ancient However Nonetheless contexts dierent dicult distinct distin-
guishes follows generally historically language languages meanings outside
respective senses separate which words
 Greek and are as at been for four has how in is it least love love of
other the their these to used used ways were when with word
Second run
 e
 phila storg
 agpe ros
 Ancient However Nonetheless contexts dierent dicult distinct distin-
guishes follows generally historically meanings respective
 words
 language languages outside senses separate which
 and are as at been for four has how in is it least love love of other the
their these to used used ways were when with word
 Greek
Data Latin script German  Italian
First run
 il E So a ad da di e es ha i il in la le lo ma ne se si un va zu
 ein  Annunciazione Baista Cenacolo Certo Come Dabei Deutsche Deutsch-
land Entwarnung Ergebnis Giuda Ingenieuren Ist Jahren Kaum Leonardo
MINT-Berufen Mythos Naturwissenschalern Stierverbands Stimmen Stu-
die Studie Szenario ema Umfrage Venezia Warnung Wissenscha Woche
Zeit acque ali alla alle allgemeiner also amore anche angemahnte anni
anti-Turchi approfonditamente arginato aento auch autodistruggersi baci-
ni barbaglio bei bellissima cancro caurata che chiave con condanna consa-
pevole consapevolezza cosa cura custodisce das dass denire del delicatezza
delle dem den der des die difesa drohe drohenden eher ein eine faceva
geben gibt idee intuizioni kam keine letzter lunghissimo mehr mehren
modo moto movimento nelle neue nicht non nur occhio oder ossessiona-
va ovvero peccato per periture poi privato psicologia punte qualche quel
quello recente restauro riccioli ricerche richiamano rivoluzionaria seit sich
sogno solo solo sono stessa struggente subito sui und vada vergeht viene
vinciano vita volare vom zum
 allins dellangelo dellaureola lesempio Milano
 Fachkremangel aresc cominci r jngst perch pi studier
Second run
 a e i
 il ad da di es ha il in la le lo ma ne se si un va zu
 Annunciazione Baista Cenacolo Certo Come Dabei Deutsche Deutschland
Entwarnung Ergebnis Giuda Ingenieuren Ist Jahren Kaum Leonardo MINT-
Berufen Mythos Naturwissenschalern Stierverbands Stimmen Studie Stu-
die Szenario ema Umfrage Venezia Warnung Wissenscha Woche Zeit
 ein acque ali alla alle allgemeiner also amore anche angemahnte an-
ni anti-Turchi approfonditamente arginato aento auch autodistruggersi
bacini barbaglio bei bellissima cancro caurata che chiave con condanna
consapevole consapevolezza cosa cura custodisce das dass denire del de-
licatezza delle dem den der des die difesa drohe drohenden eher ein
lunghissimo
mehr mehren modo moto movimento nelle neue nicht non nur oc-
chio oder ossessionava ovvero peccato per periture poi privato psicologia
idee intuizioni kam keine letzter
faceva geben gibt
punte qualche quel quello recente restauro riccioli ricerche richiamano ri-
voluzionaria seit sich sogno solo solo sono stessa struggente subito sui
und vada vergeht viene vinciano vita volare vom zum
 allins dellangelo dellaureola lesempio Milano
 Fachkremangel
 aresc cominci jngst perch studier
 r
 pi
Data Mixed script Greek  Russian
First run
 15  
        
         -
      
         -
        -
        
         
      
      
     -
      -
        
    
    
Second run
 15
 
    
   
  
        -
     
         -
       
        
   
      
     -
     -
     
   
           -
        
     
Data Mixed script English  Greek
First run
 intimate without Although Aquinas Christians Corinthians Socrates Sym-
posium Testament Whether aection ancient another appreciation aspires
araction araction becomes benevolence biblical brotherly chapter char-
ity children children contemplation content continues contributes deni-
tion described existence explained express feeling feelings nding further
holding initially inspired knowledge marriage necessary non-corporeal pas-
sage passion philosophers physical platonic rened relationships returned
self-benet sensually spiritual subject suggesting through throughout tran-
scendence unconditional understanding without youthful
   Agpe agp ros rs 
 Form erotas love love love even Agape Greek Lovers Modern Plato
is omas also apply argue based beauty beauty being dating denote
desire does eros eros erotic even famous feast feel felt given good helps
hence high humans ideal itself just known leads like love love love mean
means most mostly ones part person person plane recall refer regard seek
sexual soul spouse talk texts that there thus truth truth type used well
will will with within word work
 to 1 13 God God In It New e a all an and any are as be by can esp
for has his in is is it its man not not of on one or own the to us use
Second run
 aection ancient another aspires becomes biblical chapter charity chil-
dren children content denition feeling feelings nding holding marriage
necessary passage passion platonic rened returned subject through with-
 Although Aquinas Christians Corinthians Socrates Symposium Testament
Whether
 intimate appreciation araction araction benevolence brotherly contem-
plation continues contributes described existence explained express further
initially inspired knowledge non-corporeal philosophers physical relation-
ships self-benet sensually spiritual suggesting throughout transcendence
unconditional understanding youthful
 Agpe agp ros rs
  
 
 erotas beauty beauty dating denote desire erotic famous humans itself
mostly person person recall regard sexual spouse within
 Form Agape Greek Lovers Modern Plato is omas based being feast
hence ideal leads means plane refer there
 apply felt helps high just known most part talk texts that thus truth truth
type well will will with word work
 love love love even also argue does eros eros even feel given good
like love love love mean ones seek soul used
 1 13 In It
 to a an as be by in is is it of on or to us
 God God New e all and any esp its own the
 are can for has his man not not one use was
Data Mixed script English  Spanish  Arabic
First run
 El POWMIA Wearing a as been black de displaying duelo en es estados
for has is lazo mourning mourning negro o of or organizaciones personas
political por remembrance remembrance representando ribbon sentimiento
sociedades statement symbol tragedies un used utilizado y
 crespn poltico-social seal smbolo
                
                
Second run
 a o y
 El as de en es is of or un
 Wearing been black displaying duelo estados for has lazo mourning mourn-
ing negro organizaciones personas political por remembrance remembrance
representando ribbon sentimiento sociedades statement symbol tragedies
used utilizado
 POWMIA
 poltico-social smbolo
 crespn seal
                  
         
    
Data Mixed script English  Chinese
First run
 crisis danger opportunity simplied Chinese Chinese Western aus-
picious because believed besides certain characters component composed
criticized frequently inauspicious invoked linguists meanings meanings mo-
tivational number numbers pinyin positive pronounced represent similar
sounds speaking tradition traditional wij
      
 0 6 8 9
 In Some e and are based be by can for has have in is j name names
of on or other some sound that the their this to to two usage word words
Second run
 Chinese Chinese
 Western
 crisis danger opportunity simplied auspicious because believed be-
sides certain characters component composed criticized frequently inaus-
picious invoked linguists meanings meanings motivational number num-
bers pinyin positive pronounced represent similar sounds speaking tradi-
tion traditional wij
  
  
  
 6 8 9
 Some e and are based can for has have name names other some sound
that the their this two usage word words
 In be by in is of on or to to
 j
Data Mixed script Ukrainian  Russian
First run
 913
       -
    
        -
        
    
             
Second run
 913
     
      
       
   
      

   
   
     
  
Data Pali abbha
First run
 also cp dense megho used sama 1 1 101 1064 12 134 239 249
250 251 273 289 295 3 317 348 348 382 487 53 581 617 64 687 at
afros at at ombros at smallcaps i smallcaps smallcaps
ii smallcaps smallcaps iv smallcaps smallcaps vi smallcaps
superscript 1 superscript superscript 9 superscript superscript
s superscript A A As Dh Dhs DhsA Gr Idg It J Lat Miln Miln
Oir Pv PvA S Sk Sn SnA  is Vin Vv VvA Vedic a abhra adj also
ambha ambu as at by cloud cloud cloud clouds clouds cloudy cp dark
expl f free from froth imber in is later like list m marajob mass
moon mountain nt obscure of or pabbata perhaps point rain rain rajo
referred scum sense stormcloud summit sunshine that the thick things
thundercloud thundering to viz water water
   bghanab bmuab br  dark dull
 abbha mahiy nl b savilpa b b Rhu b b abbh b
b abbhmaa b abbhmua acchdesi mahik b nlamegha val-
haka valhaka sikhara
 m bhrocite bkab bpaalab babbha bdh- nt
Second run
 cp Dhs DhsA Idg Lat Miln Miln Oir PvA SnA is Vin VvA Vedic as
at by cp in is nt of or to
 also dense megho used sama at afros at at ombros at
smallcaps ii smallcaps smallcaps iv smallcaps smallcaps vi
smallcaps abhra adj also ambha ambu cloud cloud cloud clouds clouds
cloudy dark expl free from froth imber later like list marajo b mass
moon mountain obscure pabbata perhaps point rain rain rajo referred
scum sense storm cloud summit sunshine that the thick things thunder
cloud thundering viz water water
 1 1 101 1064 12 134 239 249 250 251 273 289 295 3 317 348 348
382 487 53 581 617 64 687 superscript 1 superscript superscript 9
superscript
 smallcaps i smallcaps superscript s superscript A A As Dh Gr
It J Pv S Sk Sn  Vv a f m
 b ghana b b mua b br dark dull
   
 abbha mahiy nl b Rhu b b abbh b nlamegha
 b savilpa b b abbhmaa b abbhmua acchdesi mahik
b valhaka valhakasikhara
 m bhro b ka b b paala b b abbha b dh-
 nt
Data Pali abhijjhitar
First run
 abhijjhita abhijjhtar covets function med one who itar itar tar
 T smallcaps i smallcaps smallcaps v smallcaps  A M ag fr
in l v
 265 287
 n
Second run
 abhijjhita abhijjhtar covets function med one who itar itar tar
 T A M
  l v
 smallcaps i smallcaps smallcaps v smallcaps ag fr in
 265 287
 n
Data Pali ajja
First run
 divasa Freq Loc Vedic adya ajjatagge ajjato an and as base being day
demonstr dyaus from in morning not of old or phrase present pron the
this with
   Mhvs s v
 kla 10 1564 3223 Ajj DI85 DAI235 Dh326 III425 JI
279 JVI180 KernToev PvI117 PvA59 PvA623 Sn75153
158970998 VinI18 a3 adya agga3 agge ajja-taggesee
ajj bahuta day divathus dy dya idni onwardhenceforth
to-daynow food on - Ajja  Page adv read see
Second run
 an as in of or
 Freq Loc Vedic
 divasa adya ajjatagge ajjato and base being day demonstr dyaus from
morning not old phrase present pron the this with
 Mhvs
 s v
on - Ajja  Page adv read see
 kla 10 1564 3223 Ajj DI85 DAI235 Dh326 III425
JI279 JVI180 KernToev PvI117 PvA623 Sn75153158
970998 VinI18 a3 agga3 ajja-taggesee ajj bahuta day
 divathus dy idni onwardhenceforth to-daynow
 PvA59 adya agge dya food
Data Pali ghan
First run
 253 Pug19CppariPage abstrfrghatighan Ghanf
 qv
Second run
 253 Pug19CppariPage abstrfrghatighan Ghanf
 qv
Data Pali pacati
First run
 382 Causpacpeti DAI159where Obulgpeka Passpaccati Vin
IV264 bakeGrpssw cookboilroast cookppwn daena g
torment fryroastLithkep intrsNiraye paccatoby pprpacanto
pppakka pentassa tormentingGenpacato
 DI52 NSII225PvA1014 PacatiVedpacatiIdgpeq
Avpac- Causpcayato expld qv qv- q
vPage trsand
 aer at be for in or pacato pare purgatory read ripe roasted roasting to
tormented
  pacitv pceti
Second run
 Causpacpeti DAI159where Obulgpeka Passpaccati VinIV264
 bakeGrpssw cookboilroast cookppwn daena gtorment fry
roastLithkep intrsNiraye paccatoby pprpacanto pentassa
tormentingGenpacato
 382 pppakka
 DI52 NSII225PvA1014 qv
 PacatiVedpacatiIdgpeqAvpac-Causpcayatoexpld
qv- qvPage trsand
 for pacato pare read ripe
 aer purgatory roasted roasting tormented
 or to
 at be in
 pacitv pceti
Data Twier 1 GreekEnglish
First run
       
 BUSINESS EXCELLENCE IT Internet ings of
Second run
 
      
 IT of
 Internet ings
 BUSINESS EXCELLENCE
Data Twier 2 FrenchEnglish
First run
 e 18h dhiparis David Demain Keynote all collective counts dynam-
ics par perish science-publish that
 is it of or
Second run
 e dhiparis David Demain Keynote all collective counts dynamics
par perish science-publish that
 18h
 is it or
Data Twier 3 FrenchEnglish
First run
 Edmonton Food
 go in to
 and are breuvages fans for just ready the waiting
Second run
 Edmonton Food
 go in
 for just
 and are breuvages fans ready the waiting
Data Twier 4 EnglishPolish
First run
 ubrwka my
 adidas and back comes crates dad from jackets of omg poland strawberries
two with
Second run
 ubrwka my
 adidas comes dad of
 and back crates from jackets omg poland strawberries two with
Data Twier 5 Transliterated AmharicEnglish
First run
 Buna
 coee bread dabo is naw our
Second run
 Buna
 our
 coee bread dabo is naw
834 Language Model Induction
For all language model induction tasks the threshold value t has been set t  002
and the silver threshold value s has been set s  01 e other parameters have been
set to maximum iteration count i  4 maximum random iteration count j  2
and merge mode ADD
Data Latin script GermanEnglish
 e German word Nabelschau means or staring at your But in this it
doesnt refer to anyone elses buon just your own
 
 navel-gazing navel case belly
Data Latin script GermanFinnishTurkish
 die in und Klimazone Je ob auf Sdhalbkugel vom eli on vuodenaika ja on
vuodenajoista koska maapallo on silloin kallistunut aurinko maan pinnalle
kulmassa muina vuodenaikoina Pohjoisella pallonpuoliskolla lasketaan ta-
vallisesti ja elokuu etelisell pallonpuoliskolla joulu- ja helmikuu en s-
cak en yazda Dnya depo en scak yaklak ay sonra ortaya Scak Haziran
Eyll ise Aralk arasndadr
 Der ist wrmste der vier Jahreszeiten der arktischen nachdem er der Nord-
oder herrscht spricht Nord- oder Der ndet mit Sdwinter sta suvi lm-
pimin niin e steilee hein- Yaz mevsimdir Krede Krede 21 22 ara-
snda Krede 22 21 Mart
 gemigten gerade gleichzeitig kuin Kuzey uzun gnler gerekleir eii
iin gnler genellikle iki gnler Kuzey ile ile
 Sommer man Sdsommer Nordsommer dem Kes kevn syksyn vliss
Kes jyrkemmss keskuukausiksi kes- tammi- Yarm sy kar Yarm
Gney Yarm
Data Latin script EnglishFrench
 both so in English although their is is the opposite of rough or is
the opposite of sweet only for wines otherwise is
 mou  mou but
 doux
 Doux rugueux Doux
 while
 hard used
 translate as meaning very dierent coarse can also mean almostsucr
Data Latin script EnglishTransliterated Greek
 at least ways as to is has phila and storg as has historically dicult to
which generally as
 e language distinguishes dierent the Ancient distinct with languages it
been separate the meanings these used outside their respective the senses
in these used
 Greek how word Greek agpe ros However other when were are
 four love used four words for love of words of contexts Nonetheless
words follows
Data Latin script ItalianGerman
 aresc privato Studie denire peritureStierverbands Wissenschastudier
difesa ovvero Szenario Naturwissenschalern
 dellaureola da del di der zum modo dem den drohe Come vom
 custodisce quel es oder per le idee stessa des dass delle E se Ist das seit
 pi Cenacolo vinciano rivoluzionaria Giuda condanna con peccato comin-
ci con cancro faceva intuizioni vita va Dabei Ergebnis in i riccioli poi
pi bacini in Annunciazione con ali la cosa barbaglio anni bei
 ne struggente che amore e non viene ma consapevolezza ad che ha re-
cente Kaum eine Woche vergeht keine neue Umfrage Warnung ema
Fachkremangel Deutschland Certo ma anche consapevole che qualche
mehren letzter Zeit Stimmen Entwarnung geben kam jngst eine Deutsche
ein allgemeiner Fachkremangel eher mehr anche Baista che Leonardo
approfonditamente a Venezia nelle vada alla aento alle dellangelo deli-
catezza punte che non che volare Jahren angemahnte drohenden Fachkre-
mangel Ingenieuren ein
 Milano lesempio psicologia il subito autodistruggersi solo lunghissimo
So il movimento moto sui si bellissima occhio allins sono sogno lo
ossessionava quello und also Mythos
 un r MINT-Berufen
 cura restauro arginato gibt perch caurata sich auch zu nicht richia-
mano acque ricerche chiave anti-Turchi nur
Data Mixed script GreekRussian
          
       
        
      15   
         
     -
          -
      
         
       
        
        -
        
     
Data Mixed script EnglishGreek
 is biblical is will is without self-benet is feelings feelings it be feeling
being high is by his is by will mostly sexual intimate well rened his
denition is initially felt with it beauty within beauty itself use with-
out helps soul beauty spiritual youthful beauty feel suggesting sensually
spiritual nding its like nding all seek
  
 Agpe love brotherly love love of God for of for in known love 1 13
throughout New brotherly love aection good love love given or not per-
son continues love even in for ones for spouse refer love of content or
holding one in unconditional love of God for of love to good of ros
love of e Modern Greek word love own Although eros for person
contemplation becomes of person or even becomes of not of of love of
word mean In Symposium work on subject eros knowledge of of Form
of erotic  even love non-corporeal of is Lovers philosophers through of
 agp means esp charity the man and man God Agape used the pas-
sage as the chapter Corinthians and described there and the Testament
as and benevolence Whether the returned the to any Agape also used
ancient texts to denote children and the a and was also used to to a
feast It can also described as the regard Agape used Christians to express
the children type was further explained omas Aquinas as the another
rs means the passion erotas means It can also apply to dating re-
lationships as as marriage Plato a an appreciation the that appreciation
Plato does talk physical araction as a necessary part hence the the pla-
tonic to physical araction the the most famous ancient the Plato has
Socrates argue that the recall and contributes to an understanding truth
the ideal that leads us humans to desire thus that that based aspires to
the plane existence that truth just any truth leads to transcendence and
are inspired to truth the means eros
Data Mixed script EnglishSpanishArabic
                  
               
 
 ribbon symbol mourning ribbon mourning El un y un en
 black is a of remembrance or Wearing or displaying a black has been used
for remembrance tragedies or as a political statement crespn negro o lazo
negro es smbolo utilizado por personas estados sociedades organizaciones
representando sentimiento poltico-social seal de duelo
 A POWMIA
Data Mixed script EnglishChinese
 e Chinese simplied traditional Chinese invoked motivational speaking
because the composed characters that represent linguists have criticized
this usage because the component simplied Chinese traditional Chinese
has other besides Chinese certain some be based the Chinese that the e
numbers believed have because their similar words that have positive
 
 Western can and Some meanings In are number name and are meanings
names meanings
 0 6 8 9
 crisis is auspicious inauspicious sounds sound
 for pinyin frequently in word of two danger opportunity pronounced
tradition by or on word to to
   wij j   
Data Mixed script UkrainianRussian
         -

  913   
  
   
 
       
         -
  
      
Data Pali abbha
 nt nt Sk dark Idg cp Gr Lat Sk water Gr water dark at SnA S
at It Sn cp SnA Sn S
  A A  J 251 1 1064 249 250 12 64 348 382
 viz 134 101 581 f 289
 53 295 273 487 3 617 317 348 239 687
 cloud also cloud cloudy smallcaps ii smallcaps  list is smallcaps
i smallcaps sama smallcaps vi smallcaps abbha smallcaps iv
smallcaps nl As Dhs DhsA used  clouds cloud also as
 m adj
 abhra mahiy VvA acchdesi Pv PvA dull valhaka Vv valhaka
sikhara
 atafrosat froth of superscript 9 superscript superscript s su-
perscript superscript 1 superscript
 later scum rain ambha rain a Miln megho Miln nlamegha sense expl
 Dh
 m bhro atombrosat ambu mass to obscure moon babbha mahik
b bdh- marajob bRhub pabbata rajo babbhb by
perhaps babbhmaab br bkab or summit stormcloud
bghanab bpaalab mass bmuab from abbhmua b
savilpab
 Vedic imber Oir dense Vin in things that sunshine is referred moun-
tain like thundercloud the point thick free thundering
Data Pali abhijjhitar
 smallcapsismallcaps v l smallcapsvsmallcaps
 abhijjhita abhijjhtar itar itar tar
 n ag fr med M 287 T  A 265
 in function one who covets
Data Pali ajja
 Ajja Ajj adv base a3 divathus Dh326 ajj v PvA59
PvA623 phrase ajjatagge ajjato agge ajja-taggesee agga3adv
 the 3223 Page
 - kla
 Vedic   being see see read as  Mhvs   Mhvs
 of of on food
 and an old not
 adya adya dya dy dyaus day to-daynow bahuta with day
divasa day
 demonstr pron Loc this KernToev s Freq or from this onwardhence-
forth this morning present
 Sn75153158970998 JI279 III425 PvI117 idni 1564 in
VinI18 DI85 DAI235 JVI180 10
Data Pali ghan
 Pug19CppariPage
 Ghanf abstrfrghatighan
 253qv
Data Pali pacati
 VinIV264 NSII225PvA1014 DI52
 DAI159where 382
 at 
 cookppwn cookboilroast
 PacatiVedpacatiIdgpeqAvpac- Obulgpeka to fryroast
Lithkep ripe to gtorment purgatorytrsand pacitv aer roasting
pprpacanto tormentingGenpacatoCauspcayato read pacato for
paccatoby pare pppakka Causpacpeti pceti Passpaccati to roasted
or tormented
 bakeGrpssw intrsNiraye expld daena pentassa qv
- qv be qvPage
Normalized data
 pacati peka pssw ppwn pacitv ppr pacanto Gen pacato Caus pcay-
ato pacato paccato pare pentassa pp pakka Caus pacpeti Pass paccati
 peq bake
 pac- 264 52  382
 1014 159  - 
 fry Niraye I I by
 Av Obulg Gr trs D DA qv qv qv
 Ved to roast kep cook ripe to cook roast torment purgatory and aer
roasting tormenting expld at where read for daena pceti to be roasted
or tormented Page
 Pacati Idg Lith boil VinIV g in intrs in NSII225PvA
Data Twier 1 GreekEnglish
 BUSINESS EXCELLENCE
      Internet of  
 ings IT
Data Twier 2 FrenchEnglish
 Keynote e collective of science-publish or perish it all that counts
 Demain 18h par
 dhiha6 David
 dhiparis dynamics is
Data Twier 3 FrenchEnglish
 FWWC2015
 breuvages go
 Food Edmonton to for the
 in waiting bilingualism
 and are ready just fans
Data Twier 4 EnglishPolish
 comes from with two crates of strawberries jackets omg
 my dad poland and adidas
 back ubrwka
Data Twier 5 Transliterated AmharicEnglish
 coee
 bread is our
 Buna dabo naw
