Mean-eld inference of Hawkes point processes
Emmanuel Bacry1 Stephane Gaas1 Iacopo Mastromatteo12 and
Jean-Francois Muzy13
1Centre de Mathematiques Appliquees Ecole Polytechnique and CNRS
UMR 7641 91128 Palaiseau France
2Capital Fund Management 21-23 Rue de lUniversite 75007 Paris France
3Laboratoire Sciences Pour lEnvironnement CNRS Universite de Corse
UMR 6134 20250 Corte France
Abstract
We propose a fast and ecient estimation method that is able to accurately
recover the parameters of a d-dimensional Hawkes point-process from a set of obser-
vations We exploit a mean-eld approximation that is valid when the uctuations of
the stochastic intensity are small We show that this is notably the case in situations
when interactions are suciently weak when the dimension of the system is high
or when the uctuations are self-averaging due to the large number of past events
they involve In such a regime the estimation of a Hawkes process can be mapped
on a least-squares problem for which we provide an analytic solution Though this
estimator is biased we show that its precision can be comparable to the one of
the Maximum Likelihood Estimator while its computation speed is shown to be
improved considerably We give a theoretical control on the accuracy of our new
approach and illustrate its eciency using synthetic datasets in order to assess the
statistical estimation error of the parameters
Introduction
The use of point processes in particular Hawkes processes 1 2 is ubiquitous in many
elds of applications Such applications include among others geophysics 3 high
frequency nance 4 5 6 neuroscience 7 predictive policing 8 and social networks
dynamics 9 10 11 12 13 14 15 A possible explanation for the success of this
model is certainly its simplicity yet ability to account for several real-world phenomena
such as self-excitement coexistence of exogeneous and endogeneous factors or power-law
distributions 16
Various estimation methods of the Hawkes process parameters have been proposed
in both parametric and non-parametric situations The most commonly used is to max-
imize the likelihood function that can be written explicitely 17 while alternative linear
methods such as the contrast function minimization 7 spectral methods 18 or esti-
mation through the resolution of a Wiener-Hopf system 19 20 have been proposed
In many of the above mentioned applications especially in the eld of network ac-
tivities viral propagation or community detection one has to handle systems of very
large dimensions for which these estimation methods can be heavy to implement Mo-
tivated by the goal to devise a fast and simple estimation procedure we introduce an
alternative approach that is inspired by recent results 21 justifying a mean-eld approx-
imation of a Hawkes process that supposes small uctuations of the stochastic intensities
with respect to their mean values This allows us to solve the estimation problem by
replacing the original objective log-likelihood by a quadratic problem which is much
easier to optimize see Sec 3 below Note that this quadratic problem diers from the
usual least-squares objective for counting processes 7 see Sec 4 below for a precise
comparison
We show that in a wide number of situations this new framework allows a much
faster estimation without inducing losses in precision Indeed we show that its bias can
be negligible and its accuracy as good as the maximum likelihood provided the level of
endogeneity of the process is suciently weak or the interactions are suciently self-
averaging meaning that they involve a large number of events over past times or over
components of the system ie in the large dimensional setting Besides theoretical
arguments we give numerical illustrations of the fact that this our approach leads to an
improvement that can be of several orders of magnitude of the classical ones based on
state-of-the-art convex solvers for the log-likelihood
The organization of the paper is the following in Sec 2 we formally introduce the
Hawkes process and dene the main notations of the paper The mean-eld inference
method is dened In Sec 3 We show how this method is naturally obtained using a
Baysian approach in a regime where the uctuations of the stochastic intensity are very
small We conduct a theoretical analysis of the domain of validity of the method by
comparing its accuracy to the Maximum Likelihood Estimation This notably allows us
to provide a quantitative measure of what weakly endogenous or self-averaging means
for the interactions
In Sec 4 we describe the implementation of the algorithm and
compare it with other state-of-the-art algorithms Sec 5 shows the numerical results
that we obtain with this method on synthetic data Sec 6 is devoted to concluding
remarks and to the discussion of the possible extensions of our method particularly as
far as penalization issues are concerned The more technical parts of the discussion are
relegated to the appendices
2 The Hawkes process
21 Denition
Let us consider a network of d nodes in which one observes a set of events encoded as
a sequence tm umn
m1 where tm labels the time of the event number m and um 
1     d denotes its corresponding node Then we can dene a set of counting functions
m1 ium1tmt where ij indicates the Kronecker delta
Nt  N 1
These counting functions can be associated to a vector of stochastic intensities t 
t      N d
t cid62 dened as
t cid80n
t cid62 as N i
t      d
where the ltration Ft encodes the information available up to time t Then the process
Nt is called a Hawkes process if the stochastic intensities can be written as
t  i 
dcid88j1cid90 t
cid48
tcid48ijt  t
where t  tcid48  ijt  tcid481ijd is a component-wise positive causal ie whose
support is in R locally L1-integrable matrix kernel representing after proper normal-
ization the probability for an event of type j occurring at time tcid48 to trigger an event
of type i at time t while   id
i1 is a vector of positive exogenous intensities see
Ref 22 for a more rigorous denition It is well known that a sucient condition for
the intensity processes i
t to have stationnary
increments is the so-called Stability Condition
t to be stationnary ie for the processes N i
1  1
t  lim
Pcid16N i
tdt  N i
t  1cid12cid12cid12Ftcid17
where 1 stands for the spectral norm of the d d matrixcid104cid82 
of the L1 norms of each kernel ijt
0 ijtdtcid1051ijd
In the following discussion we will restrict our attention to stable Hawkes processes
in the sense of SC for which the matrix t can be written as
ijt 
q gqt
pcid88q1
where we have introduced a set of p  0 known basis kernels gqt satisfying
cid90 
dt gqt  1
22 Notations
For the sake of conciseness it will be useful to preliminary introduce some notation
 We will use the integer indexes a  1     pd in order to identify pairs ja qa 
1     d  1    p Consequently summing over a allows to run both over the
d components and the p basis functions of the kernels While the set of indices
i j k    will be employed to label single nodes the notation a b c    will be used
to label a pair nodebasis kernel In that respect we set
 ia  ija
 gat  tcid48  gqat  tcid48 and
t  dN ja
 dN a
 For convenience we will also dene the deterministic process N 0
associated with the kernel g0t tcid48  t tcid48
t  t which will be
 dt equal to a Dirac delta function
shifted by an innitesimal amount dt  0
 According to previous notations Eq 2 can be compactly rewritten as
iacid90 t
dpcid88a0
cid48
tcid48gat  t
where the parameters ia refer to the Hawkes parameters namely1
i0  i and ia  ia a  0
b0 W abZbc0acdp
 We will adopt the notation x for scalars  y  ya0adp for vectors and Z 
Zab0abdp for square matrices Correspondingly we dene the usual matrix-
vector composition by Zy  cid80dp
b0 Zabyb0adp and the matrix-matrix product
as WZ  cid80dp
 If y is a vector y will refer to the L2 norm of y whereas if Z is a matrix Z will
refer to its spectral norm Moreover if t  ijt1ijd is a matrix of func-
tions 1 will refer to the spectral norm of the matrix cid82 
dtijt1ijd
1Note that in order for the parameters ia to be dimensionally homogeneous we need to assume
a dimensionless  while g0t  tcid48 is taken to be of dimension t1 Moreover we implicitly assume
t  t  t to be dimensionless through a suitable choice of a unit ie we take   1
 Hereafter we will have to handle collections of d scalars vectors or matrices in-
dexed by i  1     d that we will write as xi for scalars yi  yia0adp for
vectors and Zi  Ziab0abdp for matrices
According to these conventions The goal of this paper is to present a mean-eld frame-
work for estimation of the d vectors of parameters i  ia0adp given a set of obser-
vations Nt In the following the set of all the i parameters will be often referred to as
  i1id
23 The Likelihood function
The probability for a Hawkes process parametrized by  to generate a trajectory Nt in
a time interval t  0 T  has been rst computed in Ref 17 and is given by
cid80d
cid82 T
0 dt i
PNt  e
ncid89m1
which implicitly depends on  through the stochastic intensities i
negative log-likelihood
t We dene the
LNt    log PNt
Note that the maximum-likelihood estimator that we will compare to our mean-eld
estimator to throughout this paper
is the most commonly used estimator employed in the literature 17
M LE  argminLNt 
Inference and mean-eld approximation
31 A Bayesian approach
From a Bayesian standpoint the probability for an observed sample Nt to be generated
by a Hawkes process parametrized by  is given by the posterior distribution
PNt 
PNtP0
where P0 is a prior which we assume to be at see Sec 6 below for other choices
Hence we will be interested in averaging the inferred couplings over the posterior so to
compute their averages and their covariances
Eia  cid90 d PNt ia
Cia jb  cid90 d PNtiaib  EiaEjb 
where we are writingcid82 d cid82Rddp1
cid811id0adp dia This can be technically done
by introducing the partition function Zs dened as the Laplace transform of the pos-
Zs cid90 d PNte
T cid80d
i1 sicid62i
where s  si1id is a collection of d Laplace-parameter vectors si  sia0adp Its
computation allows one to obtain Eqs 10 and 11 by dierentiating the free-energy
density f s   log ZsT  so that
Eia  siaf ss0
Cia jb  
siasjbf ss0
We will be interested in nding an approximated expression for the free-energy density
f s allowing to compute eciently Eqs 10 and 11
Remark
Let us point out that since we are interested in derivatives of the free
energy we can drop additive s-independent terms in its computation This will be done
implicitely all along the computation
Let us rst use Eq 6 so to write explicitly the partition function as
T cid80d
i1sihcid62icid80n
m1 log um
Zs cid90 d e
where we have dropped multiplicative s-independent terms and introduced auxiliary
vector h  ha0adp equal to
T cid90 T
dtcid90 t
cid48
tcid48gat  t
Note that in the limit of large sample size T   and in the vicinity of s  0 the
integral is dominated by the maximum of the exponential corresponding to the maximum
likelihood estimator M LE 8 which is commonly employed in the literature relating to
parametric inference of Hawkes processes 17
Let us also point out that Zs can be rewritten as
i1T sihcid62icid82 T
cid80d
Zs cid90 d e
t log i
t log i
T sihcid62icid82 T
dcid89i1cid90 die
and therefore the partition function is a product of independent partition functions as-
sociated with each node i  1     d This means that the inference problem factorizes
in single node problems associated with each vector i
32 Mean-eld approximation
Our strategy in order to approximate Eq 15 is to suppose the uctuations of the i
to be small with respect to their empirical averages
  i1id cid26 N i
T cid271id
This allows us to approximate the partition function appearing in Eq 15 by a quadratic
expansion of the logarithmic functions around the empirical averages  Let us rst sup-
pose that the Hawkes process is stable in the sense of SC see 3 We have seen that
t1id are stationary The mean-eld expansion basically
it means that the processes i
assumes that each process i
t has small uctuations around its empirical averaged value
i More precisely let us suppose that the following Mean-Field Hypothesis holds
MFH 
ri  i
t  i
i cid28 1
log i
t cid39 log i 
t  i
i 
t  i2
2i2
In order to get Zs see 17 we need to compute cid82 T
Substituing 20 in this expression leads to three terms
t log i
t for all i  1     d
 The rst term
 The second term
is independent of both  and s thus it can be dropped2
after dropping  and s-independent terms lead to
cid90 T
t log i
cid90 T
t  i
iakia
dpcid88a0
T cid90tcid54tcid48
where we have introduced a collection of d vectors ki  kia0adp dened as
cid48
tcid48gat  t
 The third term after straightforward computations dropping again  and s-
independent terms leads to
dpcid88ab0
iaibJ iab  T
iakia
dpcid88a0
where we have introduced a collection of d matrices Ji  J iab0abdp dened as
J iab 
T 2cid90tcid90tcid48cid54tcid90tcid48cid48cid54t
Tcid80d
Zs cid39cid90 d e
It then results from 17 that up to a constant factor
i1sih2kicid62i 1
2 icid62Jii
tcid48dN b
tcid48cid48gat  t
cid48
cid48cid48
gbt  t
The integral appearing in Eq 23 can be evaluated analytically as a simple Gaussian
integral3 so that one can write the free-energy density as
f s  
dcid88i1cid1042ki  hi  si
cid62Ci2ki  hi  sicid105 
2Had we introduced  as a variational parameter to estimate self-consistently this term would have
contributed to xing it to its optimal value This is non-necessary in this case since we are implicitly
forcing the empirical value of  to be the mean-eld parameter
3As the region of integration in d is the positive orthant Rddp1
 Eq 24 should in principle
include extra terms preventing the emergence of negative couplings We prefer to extend the region of
integration to the whole space Rddp1 so to include potentially negative couplings A saddle-point
expansion of Eq 23 reveals indeed that these dierences are subleading in T 
where Ci denotes the inverse of matrix Ji Under this approximation one obtains
Eia cid39
Cia jb cid39
Ciab2kib  hb  ia
dpcid88b0
ijCiab
These are the central equations of our paper Eq 25 expresses the denition
of our mean-eld estimator while Eq 26 expresses the convergence rate to the
expected value of the estimator so that   E  T 12
33 Validity conditions for the mean-eld approximation
As discussed previously the mean-eld approximation is likely to be pertinent in the
regime described by MFH 19 ie when the t calculated under the inferred pa-
rameters have small uctuations with respect to the empirical averages  However the
computation we performed in the previous section relying on a second order truncation
of log i
t can be hardly used to obtain a control on the accuracy of this approxima-
tion or to dene its domain of validity These problems are addressed in Appendix
D where we show that the error of the mean-eld estimates i
M F with respect to the
maximum-likehood estimates i
M LE 8
i  i
M F  i
is directly bounded by the ratio of the empirical variance of i
particular at leading order in the uctuations and in the limit of large T  one has
t to its empirical mean In
i 
Vi
i2 Ci 
where  refers to the L2 norm in case of vectors and spectral norm in case of matrices
  adp
a0 and where the variance corresponds to the empirical one computed under
the saddle-point parameters M LE We see that when the uctuation ratio Vi
ti2
is very small the results of the mean-eld estimate can be very close to the maximum-
likelihood estimate
Analysis of the uctuation ratio The natural question that follows these consid-
erations concerns the characterization of the situations when this uctuation ratio is
very small
In appendix A we establish these conditions in the particular case of a
perfectly homogeneous Hawkes process We show that in the limit of small 1 Weak
endogeneity case i below or in the limit of large dimension d or slow interactions
Self-averaging interactions case ii below the uctuation ratio is small More precisely
see Eq 58 we nd that it is directly controlled by the spectral norm 1 the sys-
tem dimension d and the interaction characteristic time-scale g dened in Appendix
i2 
igcid18
d1  121cid19 
2
Let us recall that in the exponential kernel case one has   1 see Appendix A
Intuitively this characterization indicates that the mean-eld approximation is valid
whenever one of these conditions is met
i Weak endogeneity 1 cid28 1  If the spectral norm 1 that controls the level
of endogeneity of the Hawkes process is much smaller than one then the intensity
vector t is dominated by the exogenous component and one can expect a very
small uctuation ratio
ii Self-averaging interactions dg cid29 1 Even if the system is not weakly endoge-
1 is not necessarily small if the component-wise intensities are
nous ie
determined by a very large number of events with comparable contributions a law
of large numbers leads to small uctuation ratios This can notably occur in two
dierent situations First in large dimensional quasi-homogenous systems where
a large number of events associated with each of the components contribute to the
intensity This regime corresponds to large d in Eq 29 and has been studied
notably in Ref 21 The second situation is the case of slow interactions when a
large number of past events equally impact the intensity If the characteristic time
scale of the kernel gt is large with respect to the typical inter-event distance
then one expects the past contribution to the instantaneous intensity i
t to have
small uctuations This regime corresponds to a large g in Eq 29
Figure 1 Fluctuation ratio ri  V12i
ti of the intensity of a simulated Hawkes
process as a function of the number of components d for a xed value of  and the
interaction parameter 1  05 against the theoretical prediction of Eq 63 By
increasing the size of the system with a xed value of the intensity the uctuations
are found to decrease as d12 Indeed the theoretical predictions dashed blue line
perfectly match the results of simulation dots The parameters of the simulation are
dened as in Sec 5 and are equal to     1 The interaction matrix has the
two-block structure as illustrated in Sec 5
in Sec 5 Figs 1 and 2 show that the formula for the uctuation ratio ri cid113 Vi
The perfect homogeneous case is a very good toy model it has the main ingredients
that make the mean-eld hypothesis valid The result we so-obtained in the Appendix
A reproduce much more complex situations Indeed in the following we considered a
Hawkes process with a 2-block interaction matrix and exponential kernels fully specied
i2 see
Eq 29 or equivalently 58 for general kernel case and Eq 63 for exponential kernels
lead to a perfect prediction of the simulated curve
The right plot of Fig 1 shows that the uctuation ratio r1 of the rst block i  1
behaves in very good agreement with Eq 63 It scales like d12 when the dimension
varies Fig 2 shows the same ratio as a function of both the spectral norm 1 and the
dimension d Let us point out that when varying the norm 1 andor the dimension
d the average expectation  is held xed One can notably see that both i for a xed
value of the dimension there exists a suciently small interaction 1 such that the
uctuation ratio can be arbitrarily small and ii for a xed value of the norm even
large there exists a suciently large system dimension d such that the uctuation
0500511522530020406081V12ititT01110100V12itidd1d16d128SimulationresultsTheoryd12ratio can be arbitrarily small The displayed contour plots are the ones predicted by
our analytical considerations 63 proved in Appendix A see Eqs 58 and 63 The
theoretical prediction appears to be in good agreement with simulated data
Finally let us remark that while above considerations are useful for assessing the
validity of the mean-eld approximation under some specic assumption for the model
it is possible to bound a priori the approximation error  by exploiting the results
of App E that allow one to estimate the mean-eld error by supplying the empirical
cumulants of the data without requiring any assumption about the underlying model
Fluctuation ratio ri  V12i
ti for a system with variable number of
Figure 2
components d  1     128 and interaction strengths 1  0 08 given a xed
value of the intensities  We have chosen p  1 and g1t  1t0et with   1
and chosen a two-block structure for the matrix 1 with entries equal to either ij
or ij
1  1c with c  cid100d2cid101cid98d2cid99 as described in Sec 5 The other simulation
parameters are set to   11 1  1 The gure shows that larger values of 1
increase the size of the uctuations Conversely the uctuations can be reduced by an
arbitrary amount by distributing the interaction among a large number of components d
The theoretical predictions of Eq 63 correspond to the contour lines which accurately
reproduce the results of simulation
Estimation error and validity condition Since it basically amounts in solving a
linear system it is clear that the mean-eld estimator will perform much faster than
any MLE based estimator see next section for deatiled analysis on the complexity
The idea is to get while being much faster an estimation as precise as the MLE This
is true provided i is smaller than the statistical error i  i
associated with the maximum likelihood estimator One can obtain a self-consistent
estimation of the regime in which the error is dominated by its statistical component by
using Eq 26
M LE  i
i cid39cid18 trCi
T cid1912
that in principle is valid under the mean-eld approximation Figure 5 below shows
that this assumption is correct under a large choice of parameter values Then a simple
FluctuationratioV12itiFluctuationratioV12iti0020406081110100d00010010110020406081110100d531050301condition for the mean-eld method to perform as well as the MLE is
i cid18 trCi
T cid1912
Eq 28 provides a sucient condition for this to hold after identifying empirical aver-
ages and statistical averages namely
i2 Ci cid46cid18 trCi
T cid1912
Eq 29 has been established in the perfectly homogeneous case for which   di
Moreover we use very conservatively the inequality Ci  trCi which is likely to be
largely underestimating the statistical error so that in practice the performance of the
MF algorithm is much better than what appears from Eq 30 below see Fig 4 Indeed
plugging 29 and assuming the scaling Ci  i1 in the last equation leads to the
sucient condition
igcid18
Therefore
d121  121cid19 cid46cid18 1
2
gcid32 d1  142
Tcid1912
cid33 
4
T cid46 T cid63  i 2
Although far from being optimal this rough bound provides a qualitative idea of how
the dierent values of d 1 and g can contribute in determining the performance of
the mean-eld approximation We will see in Sec 5 that in practive T can be very large
for moderate system dimension and values of the spectral norm In other words the
situations where the performance of the mean-eld method is comparable to those of the
maximum likehood are very easy to meet in particular for large system sizes In Sec 5
we will illustrate this point by showing that in a broad range of regimes of 1 d T  we
will be able to exploit the condition T  T cid63 thus outperforming the maximum-likelihood
algorithm through the superior numerical performance of the mean-eld approximation
Implementation and complexity
In this section we want to illustrate the numerical implementation of the mean-eld
algorithm by comparing its complexity with the one of other state-of-the art algorithm
We nd that the mean-eld algorithm has a cost corresponding to dp iterations of a
gradient ascent in the likelihood both for the quasi-Newton maximisation of the likeli-
hood Sec 42 and the expectation-maximisation algorithm Sec 43 This is typically
much faster than the times required by these methods to achive convergence Fig 6
The complexity of the contrast function algorithm Sec 44 is in principle of the same
order of the mean-eld approximation but is in practice much slower see Fig 6
41 Mean-eld MF algorithm
The simplest procedure to use in order to obtain the estimator Eq 25 and its associated
covariance matrix is summarized in Algorithm 1
Note that the complexity of the preprocessing phase in which one computes the
auxiliary functions h ki and Ji is dominated by the computation of the Ji and scales
as Od2pT  maxT dp The inversion of the matrices can be performed in a time
bounded by Od4p3 In order to speed up the computation one can use the following
tricks
Algorithm 1 Calculation of the mean-eld estimator Eia and its covariance Ciajb
Input Nt gt
Output Eia Cia jb
1 Compute the auxiliary functions h ki and Ji as shown in Appendix B
2 Invert the d matrices Ji
3 Evaluate Eqs 25 and 26
4 return Eia  Cia jb
 The pre-processing time can be reduced by xing a threshold  so to cuto the
integrals over the basis kernels gt when t is suciently large This reduces
the pre-processing time to Od2pT  maxt dp  where t is an -dependent
bound on the characteristic time required for the kernels gat to decay below the
threshold  see Appendix B
 If one is not interested in the covariances Cia ib the inversion of the matrix
can be avoided by replacing it with the solution of the d linear systems Ji i
2ki  h This substantially reduces the time required to nd M F in step 2 and
3 of Algorithm 1 as the matrix inversion can be traded with a faster linear solver
eg Cholesky decomposition or iterative algorithms such as BFGS 23
 The case of weak endogeneity described above allows one to perform an approx-
imate inversion of the matrices Ji as shown in Appendix C reducing the time
required for the solution of the d linear systems from Od4p3 to Od3p2 corre-
sponding to d matrix-vector compositions Note indeed that this is expected to
yield inaccurate results when the uctuations of the Nt are of a larger order than
d1 even though the elements of the  matrix are individually small see again
Appendix C
The time of computation is typically dominated by the preprocessing phase up to very
large sizes As an illustrative example a parallel implementation in python  c on a
four-core machine with a 340 Hz CPU the calculation of the auxiliary functions takes
 6  102 s for d  128 p  1 and n  2  103 events while the matrix inversions
can be performed in a time 101 s Finally we remark that the algorithm can be
straightforwardly parallelized across components meaning that a factor d can be gained
in both the preprocessing and the matrix inversion phase
42 Maximum-Likelihood Estimation MLE
This class of mehods following Ref 17 builds upon the direct maximization of the
likelihood function log LNt This problem is concave so ecient solvers like BFGS
are able to quickly nd a solution Indeed the main drawback for this approach is the
computational cost of each of the iterations of the algorithm as the complexity of each
evaluation of the gradient function scales with the number of points in the sample n
More precisely this strategy requires to minimize the function
 log LNt  H 
cid62
ncid88m1
dcid88i1
Gma cid90 T
dt gatm  t
umaGma 
dpcid88a0
Even though one can reduce the cost of each iteration by precomputing the coecients
G on the right-hand-side of Eq 31 complexity is Od2p2T t one is still left with a
sum over all the n terms on the right hand side This can have a huge impact on the
computational limits of this strategy see Fig 6 which quickly become prohibitive in d
In fact the cost per evaluation of both the likelihood function and LNt  its gradient
scales as Od2pT  Then if number of gradient evaluations is much larger than dp
then mean-eld methods are faster This is typically the case as seen in Fig 6
43 Expectation-Maximization EM
In the case of the model we focus on EM is actually as complex as MLE as it requires
calculating a gradient of the likelihood function at each iteration 24 According to
the type of MLE algorithm adopted the relative speed of EM with respect to MLE
can change Taking BFGS as an example the EM algorithm suers actually from a
slower convergence as it is unable to exploit second-order information as done instead
by Newton and quasi-Newton methods do
44 Contrast Function CF
The closest method to the one that we have presented is a generalization of the constrast
function approach proposed in 7
In this framework the  should minimize a loss
function dened as
dtcid18 dN i
tcid192
dt  i
icid33 
icid62Jcid48
C 
2cid88i cid90 T
C  Tcid32 dcid88i1
ikicid62
T cid90 T
dt dN a
tcid48 dN b
cid48ab
resulting in
where the collection ki is dened as in Eq 21 while the matrix Jcid48 is given by
cid48
tcid48cid48 gat  t
cid48cid48
gbt  t
i  iCcid48
This approach also maps the inference problem on a linear system whose solution is
given by
where Ccid48  Jcid481 This solution is numerically close to Eq 25 The computation of
the Jcid48 has a complexity of Od2p22T t but due to the impossibility of exploiting the
trick illustrated in App B for the computation of Ji it needs to be implemented in a
dierent manner unless one doesnt introduce some articial binning in the data In
practice see Fig 6 we nd MF to be signicantly faster than CF Nevertheless CF
seems to be a method particularly suitable in order to investigate the mean-eld regime
of the Hawkes process although its precise relation with MF still needs to be rigorously
established
5 Numerical examples
We report in this section the results obtained by calibrating a Hawkes process from
synthetic data generated by a known model and by comparing the performance of our
algorithm with the one of known methods
Single exponential basis kernel First we have considered as a benchmark the case
in which p  1 so that gat  g1t  gt For j  0 we have assumed the basis kernel
to have the exponential structure
gt  e
t1t0 
The topology of the matrix  has been chosen from the following ensemble
 A block structure in which each nodes belong to one out of several clusters In
particular we studied the case in which ij is zero if i and j do not belong to the
same cluster or ij  c if they do belong to the same cluster with size given
Regardless of the structure in both cases one has 1  1   so that we used the
parameter   R in order to interpolate between the non-interacting case   0 and
the critical one   1 We have chosen for the purpose of these numerical experiments
the values   1     1
As an illustrative example we rst plot in Fig 3 the results of our algorithm in the
case of a three block structure for the  matrix The results are for this choice of value
essentially the same as the ones obtained by maximum likelihood estimation with a
considerable reduction in the computational time
Figure 3 Illustrative plot of the results of the reconstruction of a Hawkes model with
a three-block structure for the matrix  We used the parameters 1  05   1
and   1 and ran the simulation for T  104 The true underlying matrix left
is reconstructed both with the MF algorithm center and with a maximum likelihood
estimation right yielding very similar results
In order to systematically assess the performance of the algorithm under various cir-
cumstances we have varied  uniformly in the interval 0 1 and addressed the problem
of studying the scaling of the results in d we used d  2 4 8     128 and in T we
considered T  103     105 Unless specifed otherwise throughout the text we have
assumed block structures with two clusters so that c  cid100d2cid101 cid98d2cid99 We measured
the quality of the reconstruction by using
 The negative log-likelihood of the sample LNt    log PNtinf  under the
inferred couplings
 The relative error on the non-zero couplings
rel  cid88ijij
truecid540cid32 ij
true  1cid332
 The absolute error on all the couplings
abs cid88ij cid16ij
inf  ij
truecid172
The results that we have found for the relative error rel in the two-block case are
summarized in Fig 4 where we have shown them for   03 and   07 What
one nds is that as expected the error of the maximum likelihood estimator decreases
as T 12 by increasing the sample size T  Indeed the same plot also shows that the
MF estimator is able to match the results of the maximum likelihood estimator up to
a maximum value Tcid63 beyond which the statistical error no longer dominates rel for
the mean-eld estimator and the performance of MF deteriorates At that point the
approximation error becomes comparable with the statistical one inducing a plateau in
the curve of the error as a function of T  What is interesting is that  even for moderate
values of d  the value of Tcid63 is quite large indicating that the MF approximation is
extremely eective in a broad range of regimes By increasing the value of 1  
even though the relative error decreases due to the increase of the signal  the quality
of the MF approximation decreases and the value of Tcid63 becomes smaller Indeed for
a xed value of T and 1 it is always possible to nd a suciently large d so that
T cid28 Tcid63 and the MF approximation is eective This is conrmed in Fig 5 where we
Figure 4 Relative error on the non-zero couplings rel as a function of the sample size
T for the maximum likelihood estimator dashed lines and the MF approximation solid
lines for a xed value of the empirical intensity  in the two-block case as described in
Sec 5 for various values of  and d The other paramters are set to     1 The gure
shows that even for moderate values of d the error related to the MF approximation is
much smaller than the statistical error in the estimation of the couplings induced by the
nite value of T aecting the maximum likelihood estimator By comparing the left and
the right panel one sees that large values of 1   worsen the quality of the MF
algorithm
have represented the absolute error abs obtained by MLE against the one estimated
by using Eq 26
On the other hand the computational time required in order to run the MF algorithm
is considerably smaller as summarized in Fig 6
In such gure we have represented
the computational time required in order to run the dierent algorithm up to a target
negative log-likelihood LNt  on a machine with the same specications as above
00101110100010000100000RelativeerrorE12cid2inftr12cid3T0300101110100010000100000T07d4d8d16d32T12d4d8d16d32T12Figure 5 Absolute error couplings abs as a function of the sample size T for the
maximum likelihood estimator dashed lines and the estimation of the error provided
by Eq 26 solid lines for a xed value of the empirical intensity  and the same
choice of parameters as in the previous gure The gure shows that the mean-eld
estimation of the absolute error is close to the one nd by maximum-likelihood and
that the mean-eld estimator of the covariance underestimates the error that grows
when approaching the critical point
and shown that by taking into account both the pre-processing phase and the solution
of the linear system via matrix inversion the MF algorithm achieves within a single
iteration a value of the target function which is very close to the the asymptotic one got
from a BFGS minimization
Multiple exponential basis kernels We have also considered the more general case
in which the t is given by a sum of kernels of dierent nature Even though the im-
plementation of an algorithm with multiple basis kernels is technically more challenging
there are no qualitative dierences with respect to what we have stated above That is
why we have chosen to present an illustrative example showing how the MF algorithm
performs in the multiple basis case In particular we have considered an example in
which p  2 in which the matrix 1 has a uniform two-block structure as in the previ-
ous case while for the second matrix 2 only the complementary blocks are uniformly
lled see Fig 7 For both 1 and 2 the non-zero entries have value c being c the
size of the block Finally we chose ij
2  1 so that for 1  2 one recovers
a completely homogeneous interaction structure Fig 7 shows an illustrative plot of the
results of the reconstruction in the multiple basis kernel scenario
1  1 and ij
The role of  We have also studied the eect of the misspecication of  by studying
the eect of an input kernel gint dierent from the gtruet used to generate the data
We did it in the case of p  1 with a block  matrix as described in the paragraph
above relating to the single exponential basis kernel scenario Also in this case we have
chosen   1  true  1 with true denoting the true and unique decay constant for
the exponential kernels whereas in will denote the decay constant chosen to perform
the inference Our results are summarized in Fig 8 where we illustrate them in a
specic case We nd that the likelihood of the model under the inferred couplings is
not strongly aected by the misspecication of  as the inferred intensity turns out to
be very weakly dependent on in What actually happens is that by changing in is is
000100101100010000100000AbsoluteerrorE12cid2inftrue2cid3T03000100101100010000100000T07d8d16d32T12d8d16d32T12Figure 6 Time required by the learning algorithms described in Sec 4 in order to achieve
xed values of accuracy as measured by the negative log-likelihood left panel and the
relative error right panel We have considered the two block case described in Sec 5
and adopted the same parameters as in Fig 4
possible to tune a bias changing  in favor of 
This behavior is easy to understand in the framework of our MF approximation In
fact App C shows that the value of the inferred couplings  is proportional to the
uctuations of the auxiliary function k According to Eq 98 these uctuations are
obtained by integrating the empirical correlation function against against the kernel
gint in order to obtain k Hence when in is large gint is peaked close to the
origin where the correlations are larger resulting in a greater value for k Conversely
for large in such integral assumes a small value because the correlations are small at
the timescales in which gint has large mass Hence small value of in leads to a bias
in favor of  while large values favor  Independently of in the combination of  and
 leading to the inferred value of  is almost unchanged
Even though these conclusions depend on the choice of an exponential kernel for
gin in more general cases it is possible to predict the direction of the bias by checking
the timescales in which the mass of the correlation function lies agains the timescales
relative to gin A maximum overlap favors  while a small one will increase 
6 Conclusion and prospects
We have presented an approximated method for the calibration of a Hawkes process from
empirical data This method allows one to obtain in close form the inferred parameters
for such a process and its implementation is considerably faster than the algorithms
customarily employed in this eld MLEBFGS EM CF In particular we map the
problem of the estimation of a Hawkes process on a least-square regression for which
extremely ecients techniques are available 25 23 Although this method is asymp-
totically biased we have shown numerically and analytically that the bias is negligible
in situations when the interactions of the system are suciently weak or self-averaging
which is thought to be the case in many applications to high-dimensional inference
We believe that our method is not limited to the framework that we have presented
but can be extended to more general settings like marked or non-linear Hawkes processes
Another prospect of particular interest is the case in which a non trivial prior is added
to the Bayesian formula Eq 9 which allows one to embed regularization in our
1525153153515415451551555156110100MinusLog-LikelihoodlogPNtinfComputationaltimes030405060708091110100RelativeerrorE12cid2inftr12cid3ComputationaltimesBFGSEMCFMFBFGSEMCFMFFigure 7 Reconstruction of a Hawkes process in which two dierent exponential kernels
are mixed together In this example we have chosen d  8 p  2 and we have chosen the
matrices 1 and 2 as in the left panels of the gure They both have a block structure
with uniform entries equal to either zero or   05c where c  4 is the connectivity
of the block The other parameters are set to   1 1  1 2  2 and T  105
Also in this case our MF algorithm central panels is able to reconstruct the interaction
network approximately as well as the maximum likelihood estimators right panels
framework Popular priors that are relevant in the eld of optimization include for
example the Gaussian prior
and the Laplace prior
0   expcid32cid88ia
0   expcid32cid88ia
iaia2cid33
iaiacid33 
The inference problem analogous to the evaluation of the partition function 12 asso-
ciated with these priors is obviously more complex than the one with a at prior that
we have presented Nevertheless our mean-eld approach allows even in those cases
a mapping on simpler problems for which ecient solution methods exists 23
particular
 The case of a Gaussian prior can be mapped to the one of the minimization of the
quadratic form
icid62Jii  2ki  h
cid62
iaia2
 The case of a Laplace prior can be mapped to the LASSO problem 26
cid62
icid62Jii  2ki  h
iaia
dcid88i1cid18 1
dcid88i1cid18 1
icid19 cid88ia
icid19 cid88ia
Figure 8 Eect of the mispecication of  on the quality of inference We simulated a
two-block structure for the matrix  as described in Sec 5 with d  8   true  1 and
T  105 where true denotes the true value of the decay constant  We subsequently
inferred the parameters  and  under the maximum likelihood estimator solid line
and the MF one dashed line under various values for the decay constant in of the
exponential kernel gint We nd that even though the likelihood is barely changed by
the shift in in left panel its change reects in a bias in the inferred couplings right
panel
Other types of regularizers might be considered by applying this same scheme
We believe the approach we propose in this paper to be of particular interest for
the problem of high-dimensional inference as by construction it is particularly eective
in the regime of large d especially relevant for big-data analysis where traditional
numerical methods can be computationally expensive Last but not least the mean-
eld method can provide an easy and ecient way to set a good starting point for
standard iterative algorithms that maximize the likelihood objective
Acknowledgements
This research beneted from the support of the Chair Markets in Transition under
the aegis of Louis Bachelier Finance and Sustainable Growth laboratory a joint initia-
tive of Ecole Polytechnique Universite dEvry Val dEssonne and Federation Bancaire
Francaise
A Fluctuation ratio of the intensity on the homogeneous
In order to get an intuition about the behavior of a Hawkes process in a very stylized
setting we present an estimation of the square root of the ratio of the variance of t
over its squared mean Thus we will localize in the phase space of the model the points
in which the regime of large dimension d quasi-homogeneous andor weak interactions
see i and ii in Sec 33 can be detected In order to do this we study the quantity
ri cid115 Vi
0020406060811214161820002004006RelativeerrorstrtrMinusLog-LikelihoodLin00511506081121416182AverageintrtrLAverageAveragein the simplest possible framework that is a perfectly homogeneous Hawkes process of
the form
   1     1
t  t 11ijd
Let us point out that in this appendix we calculate the observables of a Hawkes process
by averaging over its stationary measure instead of using the empirical averages Hence
when using these result in Sec 33 one should keep in mind that we are replacing all
the empirical averages by expectations
1 Mean of t The mean value is easy to compute once one particularizes the results
of 1 2 in the homogeneous case obtaining
  Et cid32I cid20cid90 ijtdtcid211ijdcid331
1 
 I  111ijd
11id
1  d1
Let us point out that we want to consider the focus on a regime in which the stability
condition SC hold so that we hold the averaged intensity  constant when varying the
dimension d In order to do so we choose to keep the exogeneous intensity  constant
and let t vary so that d1 is constant Thus we set
In that case one gets
t  gt1d with g1  1
 cid20
1  1cid211id
 1id
2 Variance of t The variance can be obtained by using the stationary version of
the martingale representation for the Hawkes process see 18
t   cid90 t
where Mt is the martingale Mt  Nt cid82 t
0 tdt one has EdMtFt  0 and t can
be obtained from the Fourier transform of t that we denote as  cid82 dtteit
cid48
t  t
 dMtcid48 
as shown in 1 2 and generalized in 18 Then one can write
1  I
  I  
Ecid20cid90 t
s cid90 t
t  cid881jkd
jcid90 
dcid88j1
dt ij2t  d2 
cid48
scid48ijt  sikt  s
2  2
cid21
where  stands for the L2 norm However since
1  I 
  I  g111ijd
g1
1  g1
11ijd
one has
 
g1
1  g1
2
cid90 d
So the scaling in either limit 1  0 case i in Sec 33 or d   case ii
in Sec 33 is thus of the form Vi
t  2
1d Lets try to be more accurate and
characterize the scaling when approching the critical case 1  1 It will naturally
depend on the behavior of g around 0 We set
1  1g2 
g2
g  1  K  o
Let us point out that in the case g is exponential one has   1 and in the case
gx  x when x   then using a Tauberian theorem one can prove that
    1 Lets set   1  1 Then focusing on   a a xed the integral in
56 becomes
cid90a
g2
1  1  g2 cid39 cid90a
cid39 cid90a
cid39 12cid90a1
 12
1  1  1  K2
2  cid60K22  2cid60K  cid61K22
1  cid60K22  2cid60K  cid61K22
This last result along with 56 gives that
2
g1cid18
21cid18cid90 d
d1  121cid19 
1  xg2cid191
g2
gx  1  x
is a function on 0 1 which is both bounded and bounded away from zero As it is
illustrated below in the exponential case the constant g1 can be interpreted as a
characteristic time scale of the kernel
3 Fluctuations of t and MFH Thus the uctuations of t behave in either
limit i weak interactions 1 cid28 1 case i in Sec 33 or ii large dimension d cid29 1
case ii in Sec 33 like
and the mean-eld hypothesis simply writes
MFH 
d1  121cid19 
2
gcid18
ri cid115 Vi
1
cid112dg1  1112 cid28 1
4 Exponential case In the simpler case in which
t1t0
gt  e
which implies
g 
1  i
and   1
g1 
We see here that g does correspond to the characteristic time scale of the kernel The
mean-eld hypothesis simply writes
MFH 
ri cid115 Vi
This result is illustrated in Fig 1
B Auxiliary functions
1
cid1122d1  1 cid28 1 
This appendix is devoted to a more detailed analysis of the auxiliary functions so to
provide a recipe for an ecient numerical implementation We start by reminding that
J iab 
cid48
dtcid90 t
T cid90 T
T cid90tcid54tcid48
T 2cid90tcid54tcid48tcid48cid48
gbt  t
cid48
tcid48gat  t
tcid48gat  t
tcid48dN b
tcid48cid48
cid48cid48
cid48
 gat  t
required in order to compute the quantities Eq 25 and 26 dened in the main text
First note that while in order to compute Eq 65 one needs to discard the point t  tcid48
in the case of Eq 66 it is necessary to consider the points t cid54 tcid48  tcid48cid48 so that it reads
more explicitly
J iab 
tcid48dN b
tcid48cid48
cid48
T 2cid90tcid54tcid48cid54tcid48cid48
cid48cid48
gbt  t
T 2 iaibcid90tcid54tcid48
 gat  t
cid48
gbt  t
cid48
tcid48gat  t
Implementation The non-zero components of the auxiliary functions can be com-
puted eciently by exploiting the relations
J ijk 
ncid88m1
ncid88m1
ncid88m1
where one has dened the quantities wia
tm and yia
tm  cid90 T
m1cid88mcid481
dt gijt  tm
gijtm  tmcid48 iumjumcid48 
while for a  0 or b  0 one can use the relations J i0a  J ia0  kia and ki0  i Note
 The values of wij
tm and yij
tm dont have to be stored in memory and can rather be
computed on-line during the evaluation of the auxiliary functions
 The basis kernels gtm  tn can be cut-o when they falls below a target value
of  in order to gain computational speed As specied in Sec 3 This implies
that the computation of the auxiliary functions is of the order of Od2pT 
maxt dp where t is an -depedent threshold The max operation arises due to
the computation of yij
tm while the computation of this intermediary term accounts
for the t factor the construction of J iab is responsable for the dp term
C Small uctuations
We want to calculate here the value of the inferred couplings under the MF approxima-
tion in the case in which the uctuations of the process Nt are small Accordingly we
dene the quantities
kia  kia  a
J iab  J iab  iaib1  a0
i ab 
a b
 J iab  J iab
which are expected to be small if the condition MFH holds explicit bounds on the
rest terms are available in App E In above equation we have also chosen the notation
0 dt gatgbt that we will employ extensively in the following Then it is
possible to perturbatively invert the matrix J by using the formula
ab  cid82 
Ci  Ci
cid88n0
JiCi
where we have dened the collection Ci
i1 with elements
j1j2cid80p
qqcid480 Zj
qqcid48
yj1jd
0  icid34 1 cid80d
cid62
1jd
cid8jkZjcid91jkd cid35 
and introduced a collection of d vectors yi1id of dimension p together with a collec-
tion of d matrices of size p  p denoted as Zi1id
yi  i
Zi  iii
pcid88qcid480
1qqcid48p 
qqcid48
qqcid481qp
In above equation we have recovered the original notation of Sec 2 which identies
kernel basis functions with subscripts while the symbol 1 above indicates the matrix
inversion with respect to the p  p block One can easily verify by using above equation
and Eq 76 dening J0 and J that
  i
  iki 
with i  ia00adp Thanks to these last two equations one can prove that
M F  Ci2ki  h
 i  Ciki  h
at any order in k Together with the explicit expression for Ci
of this appendix that allows us to conclude that
0 this is the main result
1 The values of the interaction parameters  are proportional to the uctuations of
ki and h around their average values
2 When the uctuations are very small the MF approximation explains all the events
by forcing the exogenous intensities  to be equal to the empirical ones 
3 One could be tempted to expand Eq 85 as
0ki  h  Ci
M F  i  Ci
0JiCi
0ki  h    
This is not always appropriate as the truncation can lead to large errors even
when the term Ji is extremely small
In order to understand this last point let us note that if one could take the limit of
small Ji independently of the number of summands dp in the tensor compositions the
approximation would be correct this is appropriate in the regime of weak interactions
described in Sec 5 On the other hand if the uctuations are suppressed by the system
size d it is important to control the scaling in d of Ji as one typically expects Ji 
Od1 so that Ji Ci
0 can be estimated to be of order 1 this should be the case of
well-mixed interactions mentioned in Sec 5 and the truncation leads to large errors
D MLE versus MF solution
In this appendix we want to construct a bound on the error induced by the mean-eld
approximation so to be able to compare it with the typical value of the couplings and the
statistical error due to the nite value of T  In particular we are interested in controlling
the distance between the exact saddle-point solution i
given by Eq 25 that are given by
M LE and the mean-eld one i
M LE  argmini LNt 
M F  Ci2ki  h
The idea is to bound their dierence
where LNt    log PNt is the negative log-likelihood function equal to
  M LE  M F 
LNt   h
log um
cid62
ncid88m1
Minimizing it with respect to ia produces the relation
0  hia 
um
ia
um
ncid88m1
ncid88m1
 hia 
 hia  2kia  cid88bAi
t satisfying i
ia cid18 1
J iabib
um 
tm  um
um2 
ncid88m1
um
ia
ti  1
ti  1  i
M LE 
cid19
tm  um2
um3
tm  um2
um3
where in the secion line we have expanded um
the Taylor expansion i
tm around um and we dened the rest of
One can make some progress by rst substituting the denition of i into above
equation and then using the denition of i
M F in order to get
i  Civi 
um
ia
ncid88m1
T cid90 dN i
tm  um2
um3
cid48
tcid48 gat  t
ti  12
This allows us to construct an estimate for v in order to obtain a bound on 
Expansion of  One can rst use a bound on the spectral norm of the Ci matrices
in order to constrain  More precisely
via  via 
i  Civi
T cid90tcid54tcid48
T cid90tcid54tcid48
dpcid88b0
 kia  2
cid48
tcid48 gat  t
cid48
tcid48 gat  t
dpcid88bc0
J iabib
ti  12
ti  12
M LEic
M LEQiabc 
where we have used the relation
dpcid88a0
M LEcid90 dN a
cid48
tcid48 gat  t
calculated at the saddle-point and introduced a further auxiliary function
Qiabc 
tcid48dN b
tcid48cid48dN c
tcid48cid48cid48gat  t
cid48
cid48cid48
gbt  t
gct  t
cid48cid48cid48
This allows to rewrite more compactly
T 3cid90tcid54tcid48tcid48cid48tcid48cid48cid48
via  cid880bcdp
M LE  ibQiabcic
M LE  ic 
where i  ia00adp has been introducted in App C The next step will be to
perform a cumulant expansion of Qiabc so to take into account the dierent contributions
to Eq 94
Before starting the inspection of the various cumulants of Qiabc one-by-one let us expand
t iaib1  a0
rst the higher order dierentials cid82ttcid48 dN a
by writing
Qiabc 
tcid48dN b
tcid48cid48dN c
T 3cid90tcid54tcid48cid54tcid48cid48cid54tcid48cid48cid48
 ibic1  b0
 iaib1  a0
 iaic1  a0
 iaibiaic1  a0
T 3cid90tcid54tcid48cid54tcid48cid48
T 3cid90tcid54tcid48cid54tcid48cid48cid48
T 3cid90tcid54tcid48cid54tcid48cid48
T 3cid90tcid54tcid48
tcid48  cid82tcid54tcid48 dN a
tcid48cid48cid48gat  t
tcid48dN b
cid48
tcid48 cid82t dN a
cid48cid48
gct  t
gbt  t
cid48cid48cid48
cid48
cid48cid48
tcid48cid48gat  t
gbt  t
cid48
gbt  t
tcid48cid48cid48gat  t
cid48cid48
cid48
tcid48cid48gat  t
gbt  t
cid48cid48
gct  t
cid48cid48cid48
cid48
gct  t
cid48
gct  t
tcid48dN c
tcid48dN b
tcid48gat  t
cid48
cid48
gbt  t
gct  t
cid48
First order contribution At rst order in the cumulants of Nt obtained by substi-
tuting dNt  dt we have
i2cid16b c  ibic b1  b0bc
 iaib1  a0cab  iaic1  a0bac  iaibiaic1  a0abccid17 
0 dt gatgbtgct By using the properties of  proved in App C we
where abc cid82 
can check that
bbcibic
M LEic
i2cid18 i2
dpcid88bc0
dpcid88b0
 21  a0 i
dpcid88bc0
1  a0abcib
abiaibib
M LEic
M LEiaibiaiccid19
where we have dened
 i  icid62
  i
which is expected to go to zero with T independently of the mean-eld approximation
The leading order term is the one associated with the second line of Eq 95 as
 The rst and the third line are associated with terms that go to zero with T due
to the presence of  i
 The fourth line term is bound by a quantity the order of p2 max
M LE2 and hence is
expected to be of the order of p2cmax2
1  as one can show by using the bounds
proven in App E  where the functions cijt are the empirical connected cross-
correlation functions which are expected to be of order d1
 The second line term is instead of the order of dp2cmax2
Hence if cmax1  d1 as one expects in the large dimensional setting see eg App A
which proves this scaling in the homogeneous case that is of the order of d1
Higher order contributions At the second order in the cumulants ie terms con-
taining the rst power of the cross-correlation function ct all terms but one contain
contributions smaller than  d1 In fact all terms but one do either
 contain one extra power of cmax1
 or they are proportional to  i 
The only term which can contribute at order d1 is the one associated with the rst line
of Eq 95 Such term induces a contribution
i2
dpcid88bc0
M LEic
M LEcid90 dtdt
cid48
cbct  t
cid48
cid48
gbtgct
 higher order
Terms associated with cumulants of order larger than the second do not contribute at
leading order Hence the dominant contribution is of the order of d1 Putting this
information together we obtain
via  cid12cid12cid12cid12cid12cid12
i2cid20 dpcid88bc0
M LEic
M LEcid90 dtdt
cid48cid16cbct  t
 higher order 
cid48
cid48
  bibict  t
cid48
cid17 gbtgct
cid21cid12cid12cid12cid12cid12cid12
that can be more compactly written as
via cid12cid12cid12cid12
i2
Vi
tcid12cid12cid12cid12  higher order
where the variance corresponds to the empirical one computed under the saddle-point
parameters M LE The term in parenthesis corresponds exactly to the uctuations of
Nt hence the error induced by the mean-eld approximation is bound by the
variance of the intensity t as anticipated in the main text We nally get
i  Ci 
Vi
i2  higher order
that is the formula used in Sec 33
E Cumulant expansion
In this appendix we will be interesting in providing a priori bounds on the value of the
inferred parameters i
M F and on their associated errors i We will be able to relate
with quantities with the empirical cumulants of the process Nt thus allowing to obtain
bounds on the error that do not require assumptions about the underlying model
We start by recalling that after neglecting border eects ie in the limit of large
T  one has
dt ciatgat
hia  a
kia  a 
icid90 
J iab  iaib1  a0cid18 a
icid90 
i cid90 
cid48
a b
i 
i2cid90 
i2cid90 
dt gatgbt 
cid48
cid48
gatgbt
cabt  t
i2cid90 
dt cibtgbt 
dt ciatgat
cid48
cid48
Kiabt t
gatgbt
cid48
dt ciatgatgbtcid19
i2cid90 
where the vectors cit and the matrices Kit tcid48 correspond respectively to the second
and the third empirical cumulant of the process Nt which for t tcid48  0 and a b  0 are
dened as
cijt 
cid48
Kijkt t
iumjumcid48 t  tm  tmcid48  i j
T cid88mcid48m
T cid88mcid48cid54mcid48cid48m
cid48
 i j k  icjkt  t
cid48
iumjumcid48 kumcid48cid48 t  tm  tmcid48t
  kcijt
  jcikt
cid48
 tm  tmcid48cid48
while one obviously has ci0t  Ki0at tcid48  Kia0t tcid48  0 As in the last appendix
0 dt gatgbt and we will also employ the quantities
we write ab cid82 
kia  kia  a
J iab  J iab  iaib1  a0
i ab 
a b
 J iab  J iab
which are proportional to the empirical cumulants The quantities ki and Ji are in
fact bound by
min
gmaxcmax1
kia 
J iab  iaib1  a0cid18 g2
cid19 
 2cid18 gmaxcmax1
min
cid19
maxcmax1
maxKmax1
where min  min1id i gmax  maxat gat and
cmax1  max
Kmax1  max
ij cid90 
ijk cid90 
dtcijt
cid48
cid48
Kijkt t
By particularizing these inequalities to Eq D one can obtain a bound on the mean-eld
error that is of the form
via 
i2cmax1gmaxcid32 dpcid88b0
M LEcid332
M LEcid332
cid32 dpcid88b0
 cmax1gmax max
 max max
max 2
M LEic
M LEibic
M LEic
M LEibic
dpcid88bc0
dpcid88bc0
By constructing the norm of the error and comparing them with the bound on the
inferred couplings Eq 85 yields the expressions
cid19
M F  cid112dpCicid18 gmaxcmax1
i  cid112dpCicmax1gmax max
min
cid32 dpcid88b0
M LEcid332
max 2
dpcid88bc0
M LEic
M LEibic 
References
1 AG Hawkes Point spectra of some mutually exciting point processes J R Statist
Soc B 33438443 1971
2 AG Hawkes Spectra of some self-exciting and mutually exciting point processes
Biometrika 5818390 1971
3 Y Ogata Space-time point-process models for earthquake occurrences Annals of
the Institute of Statistical Mathematics 502379402 1998
4 L Bauwens and N Hautsch Stochastic conditional intensities processes Journal
of Financial Econometrics 4450493 2009
5 CG Bowsher Modelling security market events in continuous time
Intensity
based multivariate point process models Journal of Econometrics 1412876
912 2007
6 Emmanuel Bacry Iacopo Mastromatteo and Jean-Francois Muzy Hawkes processes
in nance Market Microstructure and Liquidity 01011550005 2015
7 P Reynaud-Bouret V Rivoirard and C Tuleau-Malot
Inference of functional
connectivity in neurosciences via hawkes processes In 1st IEEE Global Conference
on Signal and Information Processing 2013
8 G O Mohler M B Short P J Brantingham F P Schoenberg and G E Tita
Self-exciting point process modeling of crime Journal of the American Statistical
Association 2011
9 C Blundell J Beck and K A Heller Modelling reciprocating relationships with
hawkes processes In Advances in Neural Information Processing Systems pages
26002608 2012
10 M Farajtabar N Du M Gomez-Rodriguez I Valera H Zha and L Song Shaping
social activity by incentivizing users In Advances in neural information processing
systems pages 24742482 2014
11 S W Linderman and R P Adams Discovering latent network structure in point
process data arXiv preprint arXiv14020914 2014
12 T Iwata A Shah and Z Ghahramani Discovering latent inuence in online social
In Proceedings of the 19th ACM
activities via shared cascade poisson processes
SIGKDD international conference on Knowledge discovery and data mining pages
266274 ACM 2013
13 K Zhou H Zha and L Song Learning triggering kernels for multi-dimensional
hawkes processes In Proceedings of the 30th International Conference on Machine
Learning ICML-13 pages 13011309 2013
14 L Tran M Farajtabar L Song and H Zha Netcodec Community detection from
individual activities In In SDM 2015
15 M Farajtabar Y Wang M Gomez-Rodriguez S Li H Zha and L Song Co-
evolve A joint point process model for information diusion and network co-
evolution arXiv preprint arXiv150702293 2015
16 R Crane and D Sornette Robust dynamic classes revealed by measuring the re-
sponse function of a social system Proceedings of the National Academy of Sciences
10541 2008
17 Y Ogata and H Akaike On linear intensity models for mixed doubly stochastic
poisson and self- exciting point processes Journal of the Royal Statistical Society
Series B Methodological 441102107 January 1982 ArticleType research-
article  Full publication date 1982  Copyright A ccid13 1982 Royal Statistical Society
18 Emmanuel Bacry Khalil Dayri and Jean-Francois Muzy Non-parametric kernel
estimation for symmetric hawkes processes application to high frequency nancial
data The European Physical Journal B 855112 2012
19 Emmanuel Bacry and Jean-Francois Muzy Hawkes model for price and trades
high-frequency dynamics Quantitative Finance 1411471166 2014
20 Emmanuel Bacry and Jean-Francois Muzy
ization of Hawkes processes and non-parametric estimation
arXiv14010903 2014
Second order statistics character-
arXiv preprint
21 Sylvain Delattre Nicolas Fournier and Marc Homann High dimensional hawkes
processes arXiv preprint arXiv14035764 2014
22 DJ Daley and D Vere-Jones An introduction to the theory of point processes
volume 2 Springer 1988
23 Dong C Liu and Jorge Nocedal On the limited memory bfgs method for large scale
optimization Mathematical programming 451-3503528 1989
24 E Lewis and G Mohler A nonparametric em algorithm for multiscale hawkes
processes preprint 2011
25 John E Dennis Jr and Jorge J More Quasi-newton methods motivation and theory
SIAM review 1914689 1977
26 Robert Tibshirani Regression shrinkage and selection via the lasso Journal of the
Royal Statistical Society Series B Methodological pages 267288 1996
