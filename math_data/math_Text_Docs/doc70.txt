Multivariate Hawkes Processes for Large-scale Inference
Kevin Scaman1
Remi Lemonnier12
Argyris Kalogeratos1
1 CMLA  ENS Cachan CNRS Universite Paris-Saclay France
lemonnier scaman kalogeratoscmlaens-cachanfr
2 Numberly 1000Mercis group Paris France
Abstract
In this paper we present a framework for tting multivariate Hawkes processes for
large-scale problems both in the number of events in the observed history n and the
number of event types d ie dimensions The proposed Low-Rank Hawkes Process
LRHP framework introduces a low-rank approximation of the kernel matrix that
allows to perform the nonparametric learning of the d2 triggering kernels using at
most Ondr2 operations where r is the rank of the approximation r cid28 d n This
comes as a major improvement to the existing state-of-the-art inference algorithms
that are in Ond2 Furthermore the low-rank approximation allows LRHP to learn
representative patterns of interaction between event types which may be valuable
for the analysis of such complex processes in real world datasets The eciency and
scalability of our approach is illustrated with numerical experiments on simulated as
well as real datasets
1 Introduction
In many real-world phenomena such as product adoption or information sharing events ex-
hibit a mutually-exciting behavior in the sense that the occurrence of an event will increase
the occurrence rate of others In the eld of internet marketing the purchasing behavior
of a client of an online shopping website can be to a large extent predicted by his past
navigation history on other websites In nance arrivals of buying and selling orders for
dierent stocks convey information about macroscopic market tendencies In the study of
information propagation users of a social network share information from one to another
leading to information cascades spreading throughout the social graph Over the past few
years the study of point processes gained attention as the acquisition of such datasets by
companies and research laboratories became increasing simple However the traditional
models for time series analysis such as discrete-time auto-regressive models do not apply
in this context due to fact that events happen in a continuous way
Multivariate Hawkes processes MHP 1 2 have emerged in several elds as the gold
standard to deal with such data eg earthquake prediction 3 biology 4 nancial 5 6
and social interactions studies 7 For MHP an event of type u eg the visit of a products
website occurring at time t will increase the conditional rate of occurrence of events of
type v at time s t eg purchases of this product in the future by a rate guvs t
While these processes have been extensively studied from the probabilistic point of view
stability 8 cluster representation 9 their application to real-scale datasets remains quite
challenging For instance social interactions data is at the same time big large number of
posts high-dimensional large number of users and structured social network
Several nonparametric estimation procedures have been proposed for MHP 10 11 12
However due to the dependence of the stochastic rate of occurrence at a given time on
all past occurrences these estimation procedures are quadratic in the number of events
which renders them impractical for large datasets In the direction of tackling this issue
13 proposed a nonparametric linear-time estimation procedure relying on the memoryless
property of Hawkes processes with exponential triggering kernels However the complexity
of their algorithm remains quadratic in the number of dimensions since each of the d2
triggering kernels guv needs to be estimated
In this paper we introduce Low-Rank Hawkes Processes LRHP a model for structured
point processes relying on a low-rank decomposition of the triggering kernel that aim to learn
representative patterns of interaction between event types We also provide an ecient and
scalable inference algorithm for LRHP with linear complexity in the total number of events
and number of event types ie dimensions This inference is performed by combining
minorize-maximization and self-concordant optimization techniques
In addition if the
underlying network of interactions is provided the presented algorithm fully exploits the
networks sparsity which makes it practical for large and structured datasets The major
advantage of the the proposed LRHP algorithm is that it is able to scale-up to graphs much
larger than previous state-of-the-art methods while maintaining performances very close to
state-of-the-art competitors in terms of prediction and inference accuracy on synthetic as
well as real datasets
The rest of the paper is as follows In Section 2 we recall the denition of MHP and
introduce the associated inference problem In Section 3 we project the original dimensions
in a low-rank space and decompose the triggering functions over a basis of exponential
kernels
In Section 4 we develop our new inference algorithm LRHP and show that its
theoretical complexity is lower than state-of-the-art In Section 5 we empirically prove that
LRHP outperforms signicantly the state-of-the-art in terms of computational eciency
while maintaining a very high level of precision for the task of recovering triggering kernels
as well as predicting future events
2 Setup and Notations
A multivariate Hawkes process MHP N t Nut  u  1  d t 0 is a d-dimensional
counting process where Nut represents the number of events along dimension u that oc-
curred during time 0 t We will call event of type u an event that occurs along dimension u
Each one-dimensional counting process Nut can be inuenced by the occurrence of events
of other types Without loss of generality we will consider that these mutual excitations
take place along the edges of an unweighted directed network G  VE of d nodes and
adjacency matrix A0 1dd Finally we denote as H  um tmn
m1 the event history
of the process indicating for each single event m its type um and time of occurrence tm
Then the non-negative stochastic rate of occurrence of each Nut is dened by
cid88
ut  ut 
Aumu gumut  tm
In the above ut 0 is the natural occurrence rate of events of type u ie along that
dimension at time t and the triggering kernel function evaluation gvus t  0 determines
Symbol
Description
G  V E
number of event types ie dimensions of the multivariate Hawkes process
rank of the low-dimensional approximation
number of events of all realizations of the LRHP process
number of triggering kernels
a network of d nodes node set V and edge set E
networks adjacency matrix
maximum node degree of G
u v  1  d
i j  1  r
N t  Nutu
gvut
indices on dimensions of the original space
indices on dimensions of the low-dimensional embedding
d  r event type-to-group projection matrix
d-dimensional counting process t  0 u  1  d
non-negative occurrence rate for event type u at time t
natural occurrence rate for event type u at time t
kernel function evaluating the aection of u due to events of type v at time distance t
h  1  H
m  1  nh
parameters of the triggering kernels
hyperparameters of the triggering kernels
realizations of the LRHP process d-dimensional
events of the realization h which may belong to any event type
history of th
collection of the event histories of all H realizations
maximum number of event types involved in a realization
m1 events of the realization h indicating time of event event type
tensors with four and ve dimensions respectively introduced to simplify our inference algorithm
Table 1 Index of main notations
the increase in the occurrence rate of events of type u at time s caused by an event of type
v at a past time t s
The natural occurrence rates u and triggering kernels gvu are usually inferred by means
of log-likelihood maximization The main practical issue for inferring the parameters of the
model in Eq 1 is that it requires a particularly large dataset of observations and standard
inference algorithms require at least one observation per pair of event types ie d2 obser-
vations In many practical situations the underlying network of interactions is unknown
and in such a case we will consider that each event type can be aected by any other hence
Auv  1 for every pair of event types ucid54 v An index of the main notations used in this
paper is provided in Tab 1
3 Low-Rank Hawkes Processes
31 The proposed model
Model considerations Standard MHP inference requires the learning of d2 triggering
kernels that encode the cross- and self-excitement of the event types This requirement
becomes prohibitive when d is very large eg when the dimensions represent the users of a
social network or websites on the Internet However in a number of practical situations
the d2 complex interactions between event types can be summarized by considering that
there is a small number of r groups to which each event type belongs to a certain extent
Therefore one needs to simultaneously learn a d r event type-to-groups mapping we
specically use soft assignments as well as the r2 interactions between pairs of those groups
Model formulation Low-Rank Hawkes Processes LRHP simplify the standard inference
process by projecting the original d event types ie dimensions of a multivariate Hawkes
process into a smaller and more compact r-dimensional space The natural occurrence rates
u and triggering kernels gvu of Eq 1 are then dened via the low-rank approximation
ut cid80r
gvut cid80r
i1 Pui it
ij1 Pui Pvj gjit
where u v are event types P  Rdr
 is the projection matrix from the original d-dimensional
space to the low-dimensional space and i j are its component directions Besides this
projection can be seen as a low-rank approximation of the kernel matrix g since in matrix
notations g  P gP cid62 and g  Rrr
Then the LRHP occurrence rates are formulated as an extension of Eq 1 that uses an
embedding of event types in a low-dimensional space
 is a matrix of size r cid28 d
rcid88
ut 
Pui it
cid88
rcid88
Pui Pumj Aumu gjit  tm
i1 Pui i
ij1 PuiPvj gji
i with multiplicative weight Pui that is cid80r
evaluated bycid80r
Specically if the projection of event type u along the dimension i is given by Pui then the
event type u essentially inherits the natural occurrence rate of events of that component
In addition if the projection of
event type v along each dimension j is given by Pvj then vs eect on event type u will be
Keeping in mind that r cid28 d the proposed low-rank approximation is a simple and
straightforward way to i impose regularity to the inferred occurrence rates by introducing
constraints to the parameters and ii reduce the number of parameters Specically the d
natural rates and d2 triggering kernels are reduced to r and r2 respectively with the only
extra need of inferring the d r elements of the matrix P 
Remark on the uniqueness of the projection Unless any further assumption is made
on the projection matrix P or the low-dimensional kernel g the low-rank decomposition of
the triggering kernel g  P gP cid62 is not unique More specically any change of basis in the
r-dimensional space will not alter the decomposition Notwithstanding uniqueness is not
required in order to perform the prediction task and therefore we do not address this issue
in the present paper
32 Log-likelihood
General formulation For h  1  H let Hh  th
mmnh be the observed iid real-
izations sampled from the Hawkes process and H  HhhH the recorded history of events
of all realizations For each realization h we denote as T h T h
 the observation period and
m and th
m are respectively the event type and time of occurrence of the m-th event Then
the log-likelihood of the observations can be written as
cid34 nhcid88
Hcid88
cid32 rcid88
mi ith
l j Auh
gjith
m  th
LPH  g 
cid88
cid90 T h
cid88
cid88
 cid88
uvij
isds
cid90 T h
Pui Pvj Avu

gjis  th
computation already necessitates Ocid80H
Our objective is to infer the natural rates i and triggering kernels gji by means of log-
likelihood maximization From Eq 4 we see that for arbitrary gji a single log-likelihood
h triggering kernel evaluations This is in-
tractable when individual realizations can have a number of events of the order 107 or 108
eg a viral video when modeling information cascades This issue can be tackled by rely-
ing on a convenient K-approximation introduced in 13 Each natural occurrence rate and
kernel function are approximated by a sum of K exponential triggering functions
i t cid80K
cid98K
ji t cid80K
cid98gK
k0 ik ekt
k1 jik ekt
where    0 are xed hyperparameter values
cid80H
optimal cid98K
is analytic then supt0T  cid98gK
Due to the memoryless property of exponential functions this approximation allows for
log-likelihood computations with complexity linear in the number of events ie On 
h1 nh Results of polynomial approximation theory also ensures fast convergence of the
ji towards the true i and gji with respect to K For instance if gji
ji t gjit  OeK which means that for smooth enough
i and cid98gK
functions choosing K  10 already provides a good approximation
We therefore search the values of parameters   that will maximize the approximated
log-likelihood as well as the most probable projection matrix P  conditionally to the real-
izations of the process and under the constraint that the approximated natural rates and
triggering kernels remain non-negative At high-level this is formally expressed as
st i j t K  cid98K
arg max
P
cid98LPH  
i t  0 and cid98gK
ji t  0
Above for clarity of notation we actually reformulate the log-likelihood by introducing cid98L
that makes implicit the dependency of L in the xed hyperparameters K  and  Note
also that limiting K and r to small values can be seen as a form of regularization although
more rened approaches could be considered in case of training with datasets of very limited
Algorithm 1 Inference high-level description
Input history of events H hyperparameters K   initialized projection matrix P and
triggering kernel parameters 
  arg max cid98LPH 
Compute D and B
for i  1 to num iters do
i  0 andcid98gK
P  arg maxP cid98LPH 
ji  0 i j  1  r
st cid98K
 see Alg 2
end for
return P 
Simplication with tensor notation In order to perform inference eciently we now
reformulate the log-likelihood using very large and sparse tensors We also introduce the
articial r  1-th dimension to the embedding space in order to remove linear terms of
the equation and store the  parameters as additional dimensions of 
In detail let
r1ik  ik jr1k  0 and Pd1i  1ir1 note that 1 denotes the indicator
function also u1  d Pur1  0 Now the log-likelihood of the model can be
rewritten in the following way
Pui Pvj jik Dhmuvk
Pui Pvj jik Bhuvk
uvijk
 cid88
cid88
cid98LPH  
 cid88

cid80nh

cid80nh
l1 Ihmluv ekth
muekth
mT h
m1 JvumfkT h
fkT h
  T h
huvijk
Bhuvk 
Dhmuvk 
  th
if v  d
if v  d  1
otherwise
if v  d
if v  d  1
otherwise
1  ekxT
for x in  
fkxt 
Jvum  1vuh
Ihmluv  1uuh
l  th
m  vuh
What is suggested by the expressions is the possibility to optimize the approximated log-
likelihood according to the dierent parameters and projection matrices by rst creating
two large and sparse tensors B and D with four and ve dimensions respectively
4 The inference algorithm
The inference is performed by alternating optimization between the projection matrix P and
the Hawkes parameters  When all others parameters are xed the optimization wrt 
Algorithm 2 Construction of D and B tensors
Initialize j  0
for all h do
v  1vd1v0k0  th
0  T h  Bcid48
huk  0u0k0
T h
Initialize C k
hd1k  1expkT h
Bcid48
for all m  1nh do
dt  th
for all kv st C k
m  th
v  C k
v exp1v0k  1dt  1v0dt
v  0 do
end for
for all k do
Dhmuvk  1uumcid80
humk  Bcid48
Bcid48
um  C k
humk 
v0 AumvC k
1expkT h
end for
j  j  1
end for
Bhuvk  AuvBcid48
end for
return B D
is performed using self-concordant function optimization with self-concordant barriers The
technical diculty of this part is due to the need to ensure that non-negativity constraints
are respected For the optimization wrt P  we introduce new optimization techniques
based on a minorize-maximization algorithm Alg 1 outlines the general scheme of our
optimization algorithm Recall that our basic notation is indexed in Tab 1
Computing B and D tensors
In order for the inference algorithm to be tractable
special attention has to be paid to the computation of B and D tensors Alg 2 describes the
computation of the sparse tensors B  Bhuvk and D  Dhmuvk The most expensive
operation in this algorithm is the multiplicative update of all C k
v with the exponential decay
expk  1v0dt Fortunately this update only has to be performed for every node
v that already appeared in the cascade which are at most   d by denition The
complexity of this operation is therefore OnK The number of non-zero elements of D
and B is OnK min  where  is the maximum number of neighbors of a node in
the underlying network G If G is sparse which is usually the case for social networks for
instance then cid28 d and therefore OnKcid28 OnKd Thus storing and computing B
and D is tractable for large dense graphs and for particularly large sparse graphs Note
that because computing the log-likelihood requires the computation of occurrence rates
at each event time which depends on the occurrences of all preceding events the linear
complexity in the number of events is only possible because of the memoryless property of
the decomposition over a basis of exponentials Otherwise the respective complexity would
have been at least cid80H
Hawkes parameters optimization Updating the Hawkes parameters  requires solving
the problem
h cid29 n
hK withcid80H
cid16
cid88
i  0 andcid98gK
ji  0 i j  1  r
cid17  bcid62
chmcid62
st cid98K
  arg max
ijk cid80
bijk cid80
uv Pui Pvj Dhmuvk
uvh Pui Pvj Bhuvk
For the sake of inference tractability we relax the non-negativity constraint and only impose
it for the observed time dierences
jik Dhmuvk  0
Kcid88
Then we approximate the constrained maximization problem by an unconstrained one
using the concept of self-concordant barriers 14 More specically we choose   0 and
  arg max
bhm 
chmcid62
 b
cid17
cid16
cid88
cid88
cid16
cid32 Kcid88
cid17  bcid62
cid33
jik Dhmuvk
ijuv
A feature of the optimization problem in Eq 12 is that it veries the self-concordance prop-
erty Self-concordant functions have the advantage of behaving nicely with barrier optimiza-
tion methods and are among the rare classes of functions for which explicit convergence rates
of Newton methods are known 15 This is the reason why we chose to perform the uncon-
strained optimization using Newtons method which requires OnKr2  K 3r6 operations
Note that since we have n events and aim to learn K Hawkes parameters per pair of groups
we have necessarily Kr2 cid28 n If we do not have K 2r4 cid28 n we can reduce the complexity by
using quasi-Newton methods that necessitates only OnKr2  K 2r4  OnKr2 operations
The computation of c b and b requires multiplying sparse matrices of OnK non-zero
elements with a full matrix of r columns which yields a OnKr complexity Overall the
complexity of the Hawkes parameters optimization is of the order OnKr  r
Projection matrix optimization Let p a reshaping of the projection matrix P to a
vector linearized then p is updated by solving the following maximization procedure
cid88
lncid0pcid62hmpcid1  pcid62p
p  arg max
uivj cid80
2 uivj cid80
kjikDhmuvk  ijkDhmvuk
hkjikBhuvk  ijkBhvuk
The maximization task is performed by a novel minorize-maximization procedure which is
summarized by the following proposition proved in the Appendix
Proposition 1 The log-likelihood is non-decreasing under the update
cid88
12
hmptui
hmptptui
ptcid62
Furthermore if pui is a stable xed point of Eq 15 then pui is a local maximum of the
log-likelihood
Figure 1 Low-dimensional embedding of the event types learned by LRHP in the synthetic
dataset The two groups blue and red of event types are successfully identied
As previously the computation of   and all the matrix-vector products requires
OnKr2 operations and each update necessitates Ond operations Again we consider
settings where we have at least a few events per dimension so the total complexity of the
group anities optimization is OnKr2
In total the complexity of the whole optimization procedure is of the order OnK 
nKr2 and its behavior is linear wrt the number of events and the number of dimensions
5 Experiments
For testing the performance of the proposed LRHP model and the eciency of our inference
algorithm the experimental study consists of two parts First we simulate MHPs on small
random networks and verify that the parameters of the simulation are recovered by our
algorithm Second we provide results on a prediction task for the MemeTracker dataset in
order to show that
i LRHP is highly competitive compared to state-of-the-art inference
algorithms on medium-sized datasets and ii LRHP is the rst framework able to perform
large-scale inference for MHPs
51 Synthetic data
In this section we illustrate the validity and precision of our method in learning the diusion
parameters of simulated Hawkes processes More specically we simulate MHPs such that
event types are separated into two groups of similar activation pattern
In the context
of social networks these groups may encode inuencer-inuencee types of relations We
show that our inference algorithm can recover the groups and corresponding triggering
kernels consistently and with high accuracy Note that LRHP is more generic than this
002040608100204060811214first dimensionsecond dimensionFigure 2 True and inferred triggering kernels gij and natural occurrence rates i for the
synthetic dataset
setting however we believe that such simple scenario may provide a clearer overview of the
capabilities of our approach
Data generation procedure The employed procedure for generation of synthetic datasets
is as follows We assume that the MHPs take place on a random Erdos-Renyi 16 network
of d  100 event types whose adjacency matrix A is generated with parameter p  01 ie 10
neighbors in average Then we consider two distinct groups of event types and assign each
event type to one of the groups at random The natural occurrence rate i of each group
is xed to a constant value chosen uniformly over 0 001 The triggering kernels between
two groups i and j are generated as
cid16 2t
cid17
gijt  ij
2 i  j mod 2
3t  12
where ij and ij are sampled uniformly over respectively 1 10 and 0 150 respectively
These parameter intervals are chosen so that the behavior of the generated process is non-
explosive 17 The rationale behind the kernels in Eq 16 is that they present a power-law
decreasing intensity that allows long term inuence with a periodic behavior This kind of
dynamics could for instance represent the daytime cycles of internet users
Results Following the above procedure we generate 8 datasets by sampling 8 dierent sets
051015200000000100020003000400050006Triggering kernelg00  051015200000000002000040000600008000100001200014000160510152000000002000400060008Natural occurrence rate0TrueLRHP0510152000000002000400060008001000120014051015200000000100020003000400050006000700080510152000000001000200030004000500060007Natural occurrence rate1Triggering kernelg10Triggering kernelg11Triggering kernelg01Table 2 Experiments on the MemeTracker datasets AUC  and Accuracy  for
predicting the next event to happen using LRHP MEMIP and NAIVE approach In each
case the CPU time secs needed for training is also reported The experiments for the
missing measurements denoted with  did not nish in reasonable time
Dataset
Training Time
LRHP MEMIP LRHP MEMIP NAIVE LRHP MEMIP NAIVE
Accuracy
195 103
377 105
714 103
174 105
of parameters ij ijirj  r iir Finally we simulate 105 iid realizations of the
resulting Hawkes process that we use as training set The ability of LRHP to recover the
true group triggering kernels gij is shown in Fig 2 and evaluated by means of the normalized
L2 error 
cid98gij  gij2
cid98gij2  gij2
cid88
In average this is only 42 with minimum and maximum value amongst the 8 datasets of
38 and 47 respectively
In order to nd the group assignments we infer the parameters of an LRHP of rank
r  2 and recover the group structure by a clustering algorithm on the projected event
types Then choosing as basis of the two-dimensional space the centers of the two clus-
ters enables the recovery of the group triggering kernels Fig 1 shows the two-dimensional
embedding learned by our inference algorithm for one of the 8 sample datasets Two partic-
ularly separate clusters appear which indicates that the group assignments were perfectly
recovered The other 7 datasets gave similar results Moreover Fig 2 compares visually the
tness of the inferred to the true natural occurrence rates and triggering kernel functions
These results provide strong indication regarding the validity of our algorithm for infer-
ring the underlying dynamics of MHPs
52 Results on the MemeTracker dataset
Our nal set of experiments are conducted on the MemeTracker 18 dataset MemeTracker
is a corpus of 96 105 blog posts published between August 2008 and April 2009 We use
posts from the period August 2008 to December 2008 as training set and evaluate our
models on the four remaining months An event for website u is dened as the creation
of a hyperlink on website u towards any other website We also consider that an edge
exists between two websites if at least one hyperlink exists between them in the training
set In order to compare the inference algorithms on datasets of dierent size prediction
was performed on four subsets of the MemeTracker dataset MT1 MT2 MT3 and MT4
These subsets are created by removing the events taking place on websites that appear less
than a xed number of times in the training set This threshold value thd in Tab 2 is
respectively 50000 10000 5000 and 1000
Prediction task  The task consists in predicting the next website to create a post More
specically for each event of the test dataset we are interested in predicting the website on
Figure 3 Training time secs for LRHP and MEMIP algorithm against the quantity nd
The linear behavior for LRHP and super-linear for MEMIP are clearly visible
which it will take place knowing its time of occurrence For MEMIP and LRHP prediction
will be achieved by scoring the websites according to utm since this value is proportional
to the theoretical conditional probability for event m to be of type u We evaluate the
prediction with two metrics the area under the ROC curve AUC and a classication
accuracy with a xed number of candidate types Due to the high bias towards major
news websites eg CNN the number of candidate types has to be relatively large to see
dierences in the performance of algorithms and we set this value to 30 of the total
number of event types d in our experiments
Baselines In the following experiments we use as main competitor the state-of-the-art
MEMIP algorithm 13 which is to the best of our knowledge the only inference algorithm
with linear complexity in the number of events n in the training history Also previous work
13 shows that this algorithm outperforms the more standard inference algorithm MMEL
12 on the MemeTracker dataset In addition we also use the NAIVE baseline which ranks
the nodes according to their frequency of appearance in the training set Note that this is
equivalent to tting a Poisson process and hence does not consider mutual-excitation
Results Tab 2 summarizes the experimental results comparing the proposed LRHP against
MEMIP and NAIVE algorithms on four subsets of the MemeTracker dataset In each row
the table describes the dataset characteristics and for each method it provides the training
time AUC and accuracy with the best parameter settings for LRHP K  6 and r  2 ex-
cept for MT3 for which r  3 On small to medium-sized datasets MT1 MT2 and MT3
LRHP is as ecient as its main competitor MEMIP while orders of magnitude faster On
the large dataset MT4 LRHP still runs in reasonable time while substantially outperform-
ing the NAIVE baseline Note that MEMIP could not be computed in reasonable time for
this dataset less than a few days
Fig 3 shows the computational time needed for the inference algorithm on all the Meme-
105106107108101102103104105106number of events x number of dimensions ndCPU time seconds  LRHPMEMIPFigure 4 Sensitivity analysis of the accuracy of LRHP wrt the rank r of the approximation
used for inference and a comparison to the best scores for MMEL and Naive baselines on
the MT3 dataset
Tracker datasets with respect to nd This time is indeed linear in nd for LRHP while super-
linear for the state-of-the-art competitor of the related literature In Fig 4 it is indicated
that the accuracy measurements are relatively stable wrt the rank of the approximation
r with a maximum for r  3 Finally Fig 5 shows the two-dimensional embedding learned
by LRHP for the MT3 dataset In the embedding space the websites seem to align along
the axes of the embedding space with varying amplitudes This may indicate that two
basic groups of similar activities were recovered by the algorithm although with a large
variability in the activity of the websites
6 Conclusion
This work focused on modeling multivariate time series where both a very large number
of event types can occur and a very large number of historical observations are available
for training We introduce a model based on multivariate Hawkes processes that we call
Low-Rank Hawkes Processes LRHP and develop an inference algorithm for parameter
estimation Theoretical complexity analysis as well as experimental results show that our
approach is highly scalable while performing as eciently as state-of-the-art inference al-
gorithms in terms of prediction accuracy
Appendix Proof of Proposition 1
For this proof we will make use of the concept of auxiliary functions
Denition 1 Let g X 2 R is an auxiliary function for f X R i x yX 2
gx y f x and xX  gx x  f x
123465707580859095rank of the approximation raccuracy   LRHPNAIVEMEMIPFigure 5 Low-dimensional embeddings of the event types learned by LRHP for the MT3
dataset
The reason why these functions are an important tool for deriving iterative optimization
algorithms is given by the following lemma
Lemma 1 If g is an auxiliary function for f  then
cid16
gx y
cid17  f y
Proof Let z  argminx gx y Then
f z  gz z  gz y  gy y  f y
where the rst inequality comes from the denition of g and the second from the denition
Therefore if an auxiliary function g is available constructing the sequence yt1 
argminx gx yt that veries f yt1 f yt for all t constitutes a candidate method for
nding the minimum of f  In our case we are able to make use of the following result
Lemma 2 Let f p  cid80K
  1  Kare positive
k1 lncid0pcid62kpcid1  pcid62p where p  RK
 lncid0qcid62kqcid1cid19
cid18 2qcid62kq lnpq
gp q   Kcid88
qcid62kq
symmetric matrices and  is a symmetric matrix then
 qcid62p2q
is an auxiliary function for f 
In the lemma above the vectors q lnpq and p2q are to be understood as coordinate-
wise operations ie qi lnpiqii and p2
i qii
0123450051152253354first dimensionsecond dimensionProof It is clear that gp p  f p so the proof reduces to showing that gp q f p Let
k  K By concavity of the logarithm function we have for every weight matrix ijij
such thatcid80
ij ij  1
lncid0pcid62kpcid1 cid88
cid32
cid33
ij ln
Note that the right-hand side term of the equation is well-dened because of the positivity
ijqjqcid62kq and using the symmetry
constraint imposed on each k
of k we get
ij By choosing ij  qik
lncid0pcid62kpcid1  2qcid62kq lnpq
 lncid0qcid62kqcid1 
qcid62kq
For the right-hand side of the above equation we use the fact that for every i j it holds
pipj  p2
and the symmetry of  in order to conclude that pcid62p qcid62p2q
Using Lemma 2 we are now in position to prove Proposition 1 by showing that the
proposed update pt1 is indeed the global minimum of gp pt g being the sum of univariate
convex functions of the pi it is sucient to show that for every i the partial derivative of
gp pt with respect to pi vanishes in pt1
 We therefore need
cid88
ikpti
ptcid62
pti
which only positive solution is given by
cid32cid88
cid3312
kpti
kptpti
ptcid62
Finally if p is a stable xed point of Eq 20 then by denition there exists   0 such
that pcid48 for which p pcid482   the iterative algorithm starting at p0  pcid48 converges to p
However since f is continuous a simple iteration of the inequality of Lemma 1 implies that
f pcid48 f p1  limt f pt  f p and p is a local minimum of f 
References
1 David Oakes The Markovian self-exciting process Journal of Applied Probability
pages 6977 1975
2 Thomas Josef Liniger Multivariate Hawkes processes PhD thesis Diss Eidgenossische
Technische Hochschule ETH Zurich Nr 18403 2009
3 David Vere-Jones Earthquake prediction  statisticians view Journal of Physics of
the Earth 262129146 1978
4 Patricia Reynaud-Bouret Vincent Rivoirard Franck Grammont and Christine Tuleau-
Malot Goodness-of-t tests and nonparametric adaptive estimation for spike train
analysis Journal of Mathematical Neurosciences page 43 2014
5 Luc Bauwens and Nikolaus Hautsch Modelling nancial high frequency data using point
processes Springer 2009
6 Aurelien Alfonsi and Pierre Blanc Dynamic optimal execution in a mixed-market-
impact hawkes price model Finance and Stochastics 201183218 2015
7 Riley Crane and Didier Sornette Robust dynamic classes revealed by measuring the
response function of a social system Proceedings of the National Academy of Sciences
105411564915653 2008
8 Pierre Bremaud and Laurent Massoulie Stability of nonlinear Hawkes processes The
Annals of Probability pages 15631588 1996
9 Charles Bordenave and Giovanni Luca Torrisi Large deviations of Poisson cluster
processes Stochastic Models 234593625 2007
10 Niels Richard Hansen Patricia Reynaud-Bouret and Vincent Rivoirard Lasso and
probabilistic inequalities for multivariate point processes Bernoulli 21183143 2015
11 Emmanuel Bacry Sylvain Delattre Marc Homann and Jean-Francois Muzy Mod-
elling microstructure noise with mutually exciting point processes Quantitative Fi-
nance 1316577 2013
12 Ke Zhou Hongyuan Zha and Le Song
Learning triggering kernels for multi-
dimensional Hawkes processes In Proceedings of International Conference on Machine
Learning volume 28 of ICML pages 13011309 2013
13 Remi Lemonnier and Nicolas Vayatis Nonparametric markovian learning of triggering
kernels for mutually exciting and mutually inhibiting multivariate Hawkes processes
In Machine Learning and Knowledge Discovery in Databases pages 161176 Springer
14 Yurii Nesterov Arkadii Semenovich Nemirovskii and Yinyu Ye Interior-point polyno-
mial algorithms in convex programming volume 13 SIAM 1994
15 Stephen Poythress Boyd and Lieven Vandenberghe Convex optimization Cambridge
university press 2004
16 P Erdos and A Renyi On the evolution of random graphs Selected Papers of Alfred
Renyi 2482525 1976
17 Daryl J Daley and David Vere-Jones An introduction to the theory of point processes
Springer 2007
18 Jure Leskovec and Andrej Krevl SNAP Datasets Stanford large network dataset
collection httpsnapstanfordedudata 2014
