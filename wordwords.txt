Presenters:

Theodore LaGrow
Jacob Bieker

Abstract Title: 

Do You Know Where Your Research Is Being Used? 
An exploration of the National Science Foundation’s databases using Natural Language Processing.

Abstract Body:

We have developed software to decipher the uses of previous technology and topics relating to research used in other scholars’ research.  In such a complex and dynamic field as computer science, it is of interest to understand what resources are available, how much the resources are used, and for what the resources are used.  We demonstrated the feasibility of automatically identifying resource names on a large-scale from scientific literature in arXiv’s database and showed that the generated data can be used for exploration of software and topics.  While scholarly literature surveys can provide some insights, large-scale computer-based approaches to identify mentions of technology and methods from primary literature is needed to automate systematic cataloguing and facilitate the monitoring of usage in a more effective method.  We developed a pdf parser to extract text from articles in the database that we then trained using Natural Language Processing to evaluate if the article relates to the technology and methods in question.  .  The articles were then passed using n-grams of length 15 using a dictionary of words as a center point that usually directly correspond to technology and methods.  The n-grams were then passed using parts-of-speech based on tokenization of nouns, titles, verbs, adjectives, conjunctions, and interjections.  We took all of the noun-phrases and added them to a dictionary and counted all of the iterations to see what would be most common.  With this frequency dictionary, we were then able to evaluate a trend of technology and methods used in each specific niché of science.  As we continue to expand this software, we will analyze the researchers’ sentiment about the technology and methods.

Introduction:

With expanding databases of scientific articles, there is exceeding greater access to publishing of specific scientific niche topics.  These articles can be incorporated into large sets of collects.  The use of linguistic machine learning can be used to understand greater meaning with this type of large data.  We used Natural Language Processing to explore and infer about what types of technologies and methods are being used in each discipline of science.

NLP Overview:

Natural language processing, NLP for short, is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and natural languages.  The inference usually consists of human language.  Modern NLP is based on machine learning, especially statistical machine learning.  The programing paradigm of machine learning is different from that of most prior attempts at language processing.  Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to the steady increase in computational power as times went on (Moore’s Law). Machine learning calls for using general learning algorithms, often, grounded in statistical inference. This is to automatically learn such rules through the analysis of large corpora of typical real-world examples.  A corpus is a set of documents (or sometimes, individual sentences or strings) that have been hand-annotated with the correct values to be learned.

Methodology:

Results:


Conclusion:


Sources: (MLA)

Hucka, Michael, and Matthew J. Graham. "Software Search Is Not a Science, Even among Scientists." ArXiv (2016). ArXiv. Web. 13 May 2016.

"Noam Chomsky's Theories on Language - Video & Lesson Transcript | Study.com." Study.com. Web. 18 May 2016.

